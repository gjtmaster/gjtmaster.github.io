<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="心有多大，舞台就有多大！"><title>kafka消费者Consumer参数设置及调优 | Joker's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">kafka消费者Consumer参数设置及调优</h1><a id="logo" href="/.">Joker's Blog</a><p class="description">高金涛</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">kafka消费者Consumer参数设置及调优</h1><div class="post-meta">Sep 4, 2018<span> | </span><span class="category"><a href="/categories/消息中间件/">消息中间件</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#消息的接收-gt-基于Consumer-Group"><span class="toc-number">1.</span> <span class="toc-text">消息的接收-&gt;基于Consumer Group</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#位移保存-gt-基于Consumer-Group"><span class="toc-number">2.</span> <span class="toc-text">位移保存-&gt;基于Consumer Group</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#位移提交-gt-抛弃ZooKeeper"><span class="toc-number">3.</span> <span class="toc-text">位移提交-&gt;抛弃ZooKeeper</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#官方案例"><span class="toc-number">4.</span> <span class="toc-text">官方案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#自动提交位移"><span class="toc-number">4.1.</span> <span class="toc-text">自动提交位移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#手动提交位移"><span class="toc-number">4.2.</span> <span class="toc-text">手动提交位移</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-Consumer参数设置"><span class="toc-number">5.</span> <span class="toc-text">kafka Consumer参数设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线上采坑"><span class="toc-number">6.</span> <span class="toc-text">线上采坑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><h2 id="消息的接收-gt-基于Consumer-Group"><a href="#消息的接收-gt-基于Consumer-Group" class="headerlink" title="消息的接收-&gt;基于Consumer Group"></a>消息的接收-&gt;基于Consumer Group</h2><p>Consumer Group 主要用于实现高伸缩性，高容错性的Consumer机制。因此，消息的接收是基于Consumer Group 的。组内多个Consumer实例可以同时读取Kafka消息，同一时刻一条消息只能被一个消费者消费，而且一旦某一个consumer “挂了”， Consumer Group 会立即将已经崩溃的Consumer负责的分区转交给其他Consumer来负责。从而保证 Consumer Group 能够正常工作。</p>
<h2 id="位移保存-gt-基于Consumer-Group"><a href="#位移保存-gt-基于Consumer-Group" class="headerlink" title="位移保存-&gt;基于Consumer Group"></a>位移保存-&gt;基于Consumer Group</h2><p>说来奇怪，位移保存是基于Consumer Group，同时引入检查点模式，定期实现offset的持久化。</p>
<h2 id="位移提交-gt-抛弃ZooKeeper"><a href="#位移提交-gt-抛弃ZooKeeper" class="headerlink" title="位移提交-&gt;抛弃ZooKeeper"></a>位移提交-&gt;抛弃ZooKeeper</h2><p>Consumer会定期向kafka集群汇报自己消费数据的进度，这一过程叫做位移的提交。这一过程已经抛弃Zookeeper，因为Zookeeper只是一个协调服务组件，不能作为存储组件，高并发的读取势必造成Zk的压力。</p>
<ul>
<li>新版本位移提交是在kafka内部维护了一个内部Topic(_consumer_offsets)。</li>
<li>在kafka内部日志目录下面，总共有50个文件夹，每一个文件夹包含日志文件和索引文件。日志文件主要是K-V结构，（group.id,topic,分区号）。</li>
<li>假设线上有很多的consumer和ConsumerGroup，通过对group.id做Hash求模运算，这50个文件夹就可以分散同时位移提交的压力。</li>
</ul>
<h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><h3 id="自动提交位移"><a href="#自动提交位移" class="headerlink" title="自动提交位移"></a>自动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="手动提交位移"><a href="#手动提交位移" class="headerlink" title="手动提交位移"></a>手动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">200</span>;</span><br><span class="line">List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        buffer.add(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">        insertIntoDb(buffer);</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">        buffer.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="kafka-Consumer参数设置"><a href="#kafka-Consumer参数设置" class="headerlink" title="kafka Consumer参数设置"></a>kafka Consumer参数设置</h2><ul>
<li>consumer.poll(1000) 重要参数</li>
<li>新版本的Consumer的Poll方法使用了类似于Select I/O机制，因此所有相关事件（包括reblance，消息获取等）都发生在一个事件循环之中。</li>
<li>1000是一个超时时间，一旦拿到足够多的数据（参数设置），consumer.poll(1000)会立即返回 ConsumerRecords&lt;String, String&gt; records。</li>
<li>如果没有拿到足够多的数据，会阻塞1000ms，但不会超过1000ms就会返回。</li>
</ul>
<hr>
<ul>
<li>session. timeout. ms &lt;=  coordinator检测失败的时间</li>
<li>默认值是10s</li>
<li>该参数是 Consumer Group 主动检测 (组内成员consummer)崩溃的时间间隔。若设置10min，那么Consumer Group的管理者（group coordinator）可能需要10分钟才能感受到。太漫长了是吧。</li>
</ul>
<hr>
<ul>
<li>max. poll. interval. ms &lt;= 处理逻辑最大时间</li>
<li>这个参数是0.10.1.0版本后新增的，可能很多地方看不到喔。这个参数需要根据实际业务处理时间进行设置，一旦Consumer处理不过来，就会被踢出Consumer Group。</li>
<li>注意：如果业务平均处理逻辑为1分钟，那么max. poll. interval. ms需要设置稍微大于1分钟即可，但是session. timeout. ms可以设置小一点（如10s），用于快速检测Consumer崩溃。</li>
</ul>
<hr>
<ul>
<li>auto.offset.reset</li>
<li>该属性指定了消费者在读取一个没有偏移量或者偏移量无效（消费者长时间失效当前的偏移量已经过时并且被删除了）的分区的情况下，应该作何处理，默认值是latest，也就是从最新记录读取数据（消费者启动之后生成的记录），另一个值是earliest，意思是在偏移量无效的情况下，消费者从起始位置开始读取数据。</li>
</ul>
<hr>
<ul>
<li>enable.auto.commit</li>
<li>对于精确到一次的语义，最好手动提交位移</li>
</ul>
<hr>
<ul>
<li>fetch.max.bytes</li>
<li>单次获取数据的最大消息数。</li>
</ul>
<hr>
<ul>
<li>max.poll.records  &lt;=  吞吐量</li>
<li>单次poll调用返回的最大消息数，如果处理逻辑很轻量，可以适当提高该值。</li>
<li>一次从kafka中poll出来的数据条数,max.poll.records条数据需要在在session.timeout.ms这个时间内处理完</li>
<li>默认值为500</li>
</ul>
<hr>
<ul>
<li>heartbeat. interval. ms &lt;= 居然拖家带口</li>
<li>heartbeat心跳主要用于沟通交流，及时返回请求响应。这个时间间隔真是越快越好。因为一旦出现reblance,那么就会将新的分配方案或者通知重新加入group的命令放进心跳响应中。</li>
</ul>
<hr>
<ul>
<li>connection. max. idle. ms &lt;= socket连接</li>
<li>kafka会定期的关闭空闲Socket连接。默认是9分钟。如果不在乎这些资源开销，推荐把这些参数值为-1，即不关闭这些空闲连接。</li>
</ul>
<hr>
<ul>
<li>request. timeout. ms</li>
<li>这个配置控制一次请求响应的最长等待时间。如果在超时时间内未得到响应，kafka要么重发这条消息，要么超过重试次数的情况下直接置为失败。</li>
<li>消息发送的最长等待时间.需大于session.timeout.ms这个时间</li>
</ul>
<hr>
<ul>
<li>fetch.min.bytes</li>
<li>server发送到消费端的最小数据，若是不满足这个数值则会等待直到满足指定大小。默认为1表示立即接收。</li>
</ul>
<hr>
<ul>
<li><p>fetch.wait.max.ms</p>
</li>
<li><p>若是不满足fetch.min.bytes时，等待消费端请求的最长等待时间</p>
</li>
</ul>
<hr>
<ul>
<li>0.11 新功能</li>
<li>空消费组延时rebalance，主要在server.properties文件配置</li>
<li>group.initial.rebalance.delay.ms&lt;= ，防止成员加入请求后本应立即开启的rebalance</li>
<li>对于用户来说，这个改进最直接的效果就是新增了一个broker配置：group.initial.rebalance.delay.ms，</li>
<li>默认是3秒钟。</li>
<li>主要作用是让coordinator推迟空消费组接收到成员加入请求后本应立即开启的rebalance。在实际使用时，假设你预估你的所有consumer组成员加入需要在10s内完成，那么你就可以设置该参数=10000。</li>
</ul>
<h2 id="线上采坑"><a href="#线上采坑" class="headerlink" title="线上采坑"></a>线上采坑</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.consumer</span><span class="selector-class">.CommitFailedException</span>:</span><br><span class="line"> Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. </span><br><span class="line">This means that the <span class="selector-tag">time</span> between subsequent calls to poll() was longer than the configured session<span class="selector-class">.timeout</span><span class="selector-class">.ms</span>, which typically implies that the poll loop is spending too much <span class="selector-tag">time</span> message processing. </span><br><span class="line">You can <span class="selector-tag">address</span> this either by increasing the session timeout or by reducing the maximum size of batches returned <span class="keyword">in</span> poll() with max<span class="selector-class">.poll</span><span class="selector-class">.records</span>. [com<span class="selector-class">.bonc</span><span class="selector-class">.framework</span><span class="selector-class">.server</span><span class="selector-class">.kafka</span><span class="selector-class">.consumer</span><span class="selector-class">.ConsumerLoop</span>]</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure>

<p>基于最新版本10，注意此版本session. timeout. ms 与max.poll.interval.ms进行功能分离了。</p>
<ul>
<li>可以发现频繁reblance，并伴随者重复性消费，这是一个很严重的问题，就是处理逻辑过重,max.poll. interval.ms过小导致。发生的原因就是 poll（）的循环调用时间过长，出现了处理超时。此时只用调大max.poll. interval.ms ，调小max.poll.records即可，同时要把request. timeout. ms设置大于max.poll. interval.ms</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化会继续，暂时把核心放在request. timeout. ms, max. poll. interval. ms，max.poll.records 上，避免因为处理逻辑过重，导致Consumer被频繁的踢出Consumer group。</p>
</div><iframe src="/donate/?AliPayQR=/img/AliPayQR.png&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>gaojintao999@163.com</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/09/04/kafka消费者Consumer参数设置及调优/">https://gjtmaster.github.io/2018/09/04/kafka消费者Consumer参数设置及调优/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>The author owns the copyright, please indicate the source reproduced.</li></ul></div><br><div class="tags"><a href="/tags/Kafka/">Kafka</a><a href="/tags/消息中间件/">消息中间件</a></div><div class="post-nav"><a class="pre" href="/2018/09/13/利用ogg实现oracle到kafka的增量数据实时同步/">利用ogg实现oracle到kafka的增量数据实时同步</a><a class="next" href="/2018/09/03/kafka生产者Producer参数设置及调优/">kafka生产者Producer参数设置及调优</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JVM/">JVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/实时计算框架/">实时计算框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据存储格式/">数据存储格式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据的导入导出/">数据的导入导出</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/日志框架/">日志框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/消息中间件/">消息中间件</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Oracle/" style="font-size: 15px;">Oracle</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/实时计算/" style="font-size: 15px;">实时计算</a> <a href="/tags/Flink-on-Yarn/" style="font-size: 15px;">Flink on Yarn</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/消息中间件/" style="font-size: 15px;">消息中间件</a> <a href="/tags/FlinkSQL/" style="font-size: 15px;">FlinkSQL</a> <a href="/tags/Logback/" style="font-size: 15px;">Logback</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/内存回收/" style="font-size: 15px;">内存回收</a> <a href="/tags/Json/" style="font-size: 15px;">Json</a> <a href="/tags/ogg/" style="font-size: 15px;">ogg</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/11/15/Flink1.7.1与Kafka0.11.0.1/">Flink1.7.1与Kafka0.11.0.1</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/Flink on Yarn HA/">Flink On Yarn HA</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/15/Flink On Yan集群部署/">Flink On Yarn集群部署</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/13/Flink使用Logback作为日志框架的相关配置/">Flink使用Logback作为日志框架的相关配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/18/FlinkSQL深度解析/">Flink SQL 深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/13/利用ogg实现oracle到kafka的增量数据实时同步/">利用ogg实现oracle到kafka的增量数据实时同步</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/04/kafka消费者Consumer参数设置及调优/">kafka消费者Consumer参数设置及调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/03/kafka生产者Producer参数设置及调优/">kafka生产者Producer参数设置及调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/02/kafka生产者和消费者吞吐量测试/">kafka生产者和消费者吞吐量测试</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/01/kafka生产环境规划/">kafka生产环境规划</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://matt33.com/" title="Matt's Blog" target="_blank">Matt's Blog</a><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="Maupassant's usage" target="_blank">Maupassant's usage</a><ul></ul><a href="https://www.jianshu.com/p/f4332764e8bd" title="hexo's usage" target="_blank">hexo's usage</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Joker's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>