<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="心有多大，舞台就有多大！"><title>Kafka 偏移量管理实现精确一次语义在Spark&amp;Flink中的技术实践 | Joker's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 偏移量管理实现精确一次语义在Spark&amp;Flink中的技术实践</h1><a id="logo" href="/.">Joker's Blog</a><p class="description">高金涛</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 偏移量管理实现精确一次语义在Spark&amp;Flink中的技术实践</h1><div class="post-meta">Oct 13, 2018<span> | </span><span class="category"><a href="/categories/消息中间件/">消息中间件</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-偏移量"><span class="toc-number">1.</span> <span class="toc-text">Kafka 偏移量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-9-之前版本"><span class="toc-number">1.1.</span> <span class="toc-text">Kafka 0.9 之前版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-0-9-之后版本"><span class="toc-number">1.2.</span> <span class="toc-text">Kafka 0.9 之后版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#消息处理语义"><span class="toc-number">1.3.</span> <span class="toc-text">消息处理语义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#kafka-offset-提交方式"><span class="toc-number">1.4.</span> <span class="toc-text">kafka offset 提交方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-位移处理方式"><span class="toc-number">2.</span> <span class="toc-text">Spark 位移处理方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#auto-offset-reset设置思路"><span class="toc-number">2.1.</span> <span class="toc-text">auto.offset.reset设置思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#订阅-Kafka-主题"><span class="toc-number">2.2.</span> <span class="toc-text">订阅 Kafka 主题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用老式zookeeper手动管理位移代码分析"><span class="toc-number">2.3.</span> <span class="toc-text">使用老式zookeeper手动管理位移代码分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flink-位移处理方式"><span class="toc-number">3.</span> <span class="toc-text">Flink 位移处理方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Flink-消费者精确到一次语义"><span class="toc-number">3.1.</span> <span class="toc-text">Flink 消费者精确到一次语义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Flink-生产者精确到一次语义"><span class="toc-number">3.2.</span> <span class="toc-text">Flink 生产者精确到一次语义</span></a></li></ol></li></ol></div></div><div class="post-content"><h2 id="Kafka-偏移量"><a href="#Kafka-偏移量" class="headerlink" title="Kafka 偏移量"></a>Kafka 偏移量</h2><h3 id="Kafka-0-9-之前版本"><a href="#Kafka-0-9-之前版本" class="headerlink" title="Kafka 0.9 之前版本"></a>Kafka 0.9 之前版本</h3><p>这里的偏移量是指 kafka consumer offset，在 Kafka 0.9 版本之前消费者偏移量默认被保存在 zookeeper 中（/consumers/&lt;group.id&gt;/offsets//），因此在初始化消费者的时候需要指定 zookeeper.hosts。</p>
<h3 id="Kafka-0-9-之后版本"><a href="#Kafka-0-9-之后版本" class="headerlink" title="Kafka 0.9 之后版本"></a>Kafka 0.9 之后版本</h3><p>随着 Kafka consumer 在实际场景的不断应用，社区发现旧版本 consumer 把位移提交到 ZooKeeper 的做法并不合适。ZooKeeper 本质上只是一个协调服务组件，它并不适合作为位移信息的存储组件，毕竟频繁高并发的读/写操作并不是 ZooKeeper 擅长的事情。因此在 0.9 版本开始 consumer 将位移提交到 Kafka 的一个内部 topic（__consumer_offsets）中，该主题默认有 50 个分区，每个分区 3 个副本。</p>
<h3 id="消息处理语义"><a href="#消息处理语义" class="headerlink" title="消息处理语义"></a>消息处理语义</h3><ul>
<li>at-most-once：最多一次，消息可能丢失，但不会被重复处理；</li>
<li>at-least-once：至少一次，消息不会丢失，但可能被处理多次；</li>
<li>exactly-once：精确一次，消息一定会被处理且只会被处理一次。</li>
<li>若 consumer 在消息消费之前就提交位移，那么便可以实现 at-most-once，因为若 consumer 在提交位移与消息消费之间崩溃，则 consumer 重启后会从新的 offset 位置开始消费，前面的那条消息就丢失了；相反地，</li>
<li>若提交位移在消息消费之后，则可实现 at-least-once 语义。由于 Kafka 没有办法保证消息处理成功与位移提交在同一个事务中完成，若消息消费成功了，也提交位移了，但是处理失败了，因此 Kafka 默认提供的就是 at-least-once 的处理语义。</li>
</ul>
<h3 id="kafka-offset-提交方式"><a href="#kafka-offset-提交方式" class="headerlink" title="kafka offset 提交方式"></a>kafka offset 提交方式</h3><ul>
<li><p>默认情况下，consumer 是自动提交位移的，自动提交间隔是 5 秒，可以通过设置 auto.commit.interval.ms 参数可以控制自动提交的间隔。</p>
<p>自动位移提交的优势是降低了用户的开发成本使得用户不必亲自处理位移提交；劣势是用户不能细粒度地处理位移的提交，特别是在有较强的精确一次处理语义时（在这种情况下，用户可以使用手动位移提交）。</p>
</li>
<li><p>手动位移提交就是用户自行确定消息何时被真正处理完并可以提交位移，用户可以确保只有消息被真正处理完成后再提交位移。如果使用自动位移提交则无法保证这种时序性，因此在这种情况下必须使用手动提交位移。</p>
<p>设置使用手动提交位移非常简单，仅仅需要在构建 KafkaConsumer 时设置 enable.auto.commit=false，然后调用 commitSync 或 commitAsync 方法即可。</p>
</li>
</ul>
<h2 id="Spark-位移处理方式"><a href="#Spark-位移处理方式" class="headerlink" title="Spark 位移处理方式"></a>Spark 位移处理方式</h2><h3 id="auto-offset-reset设置思路"><a href="#auto-offset-reset设置思路" class="headerlink" title="auto.offset.reset设置思路"></a>auto.offset.reset设置思路</h3><ul>
<li><p>对于 auto.offset.reset 个人推荐设置为 earliest，初次运行的时候，由于 __consumer_offsets 没有相关偏移量信息，因此消息会从最开始的地方读取；当第二次运行时，由于 __consumer_offsets 已经存在消费的 offset 信息，因此会根据 __consumer_offsets 中记录的偏移信息继续读取数据。</p>
</li>
<li><p>此外，对于使用 zookeeper 管理偏移量而言，只需要删除对应的节点，数据即可从头读取，也是非常方便。不过如果你希望从最新的地方读取数据，不需要读取旧消息，则可以设置为 latest。</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">earili<span class="symbol">st:</span>提交过分区，从<span class="built_in">Offset</span>处读取，如果没有提交过<span class="built_in">offset</span>,从头读取</span><br><span class="line">late<span class="symbol">st:</span>提交过分区，从<span class="built_in">Offset</span>处读取，没有从最新的数据开始读取</span><br><span class="line">None：如果没有提交<span class="built_in">offset</span>,就会报错，提交过<span class="built_in">offset</span>,就从<span class="built_in">offset</span>处读取</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="订阅-Kafka-主题"><a href="#订阅-Kafka-主题" class="headerlink" title="订阅 Kafka 主题"></a>订阅 Kafka 主题</h3><ul>
<li><p>基于正则订阅主题，有以下好处：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">无需罗列主题名，一两个主题还好，如果有几十个，罗列过于麻烦了；</span><br><span class="line">可实现动态订阅的效果（新增的符合正则的主题也会被读取）。</span><br><span class="line"></span><br><span class="line">stream = KafkaUtils<span class="selector-class">.createDirectStream</span>[String, String](ssc,</span><br><span class="line">        LocationStrategies<span class="selector-class">.PreferConsistent</span>,</span><br><span class="line">        ConsumerStrategies<span class="selector-class">.SubscribePattern</span>[String, String](Pattern.compile(topicStr), kafkaConf, customOffset))</span><br></pre></td></tr></table></figure>
</li>
<li><p>LocationStrategies 分配分区策略，LocationStrategies：根据给定的主题和集群地址创建consumer</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">创建<span class="selector-tag">DStream</span>，返回接收到的输入数据</span><br><span class="line"><span class="selector-tag">LocationStrategies</span><span class="selector-class">.PreferConsistent</span>：持续的在所有<span class="selector-tag">Executor</span>之间匀分配分区 (均匀分配，选中的每一个<span class="selector-tag">Executor</span>都会分配 <span class="selector-tag">partition</span>)</span><br><span class="line"><span class="selector-tag">LocationStrategies</span><span class="selector-class">.PreferBrokers</span>: 如果<span class="selector-tag">executor</span>和<span class="selector-tag">kafka</span> <span class="selector-tag">brokers</span> 在同一台机器上，选择该<span class="selector-tag">executor</span>。</span><br><span class="line"><span class="selector-tag">LocationStrategies</span><span class="selector-class">.PreferFixed</span>: 如果机器不是均匀的情况下，可以指定特殊的<span class="selector-tag">hosts</span>。当然如果不指定，采用 <span class="selector-tag">LocationStrategies</span><span class="selector-class">.PreferConsistent</span>模式</span><br></pre></td></tr></table></figure>
</li>
<li><p>SparkStreaming 序列化问题</p>
<p>在 driver 中使用到的变量或者对象无需序列化，传递到 exector 中的变量或者对象需要序列化。因此推荐的做法是，在 exector 中最好只处理数据的转换，在 driver 中对处理的结果进行存储等操作。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// driver 代码运行区域</span></span><br><span class="line">  val offsetRanges = rdd<span class="selector-class">.asInstanceOf</span>[HasOffsetRanges].offsetRanges</span><br><span class="line">  kafkaOffset.updateOffset(offsetRanges)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// exector 代码运行区域</span></span><br><span class="line">  val resultRDD = rdd.map(xxxxxxxx)</span><br><span class="line">  <span class="comment">//endregion</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//对结果进行存储</span></span><br><span class="line">  resultRDD.saveToES(xxxxxx)</span><br><span class="line">  kafkaOffset.commitOffset(offsetRanges)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="使用老式zookeeper手动管理位移代码分析"><a href="#使用老式zookeeper手动管理位移代码分析" class="headerlink" title="使用老式zookeeper手动管理位移代码分析"></a>使用老式zookeeper手动管理位移代码分析</h3><ul>
<li><p>Zookeeper 偏移量管理ZkKafkaOffset实现，借助 zookeeper 管理工具可以对任何一个节点的信息进行修改、删除，如果希望从最开始读取消息，则只需要删除 zk 某个节点的数据即可。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.<span class="type">I0Itec</span>.zkclient.<span class="type">ZkClient</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">OffsetRange</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZkKafkaOffset</span>(<span class="params">getClient: (</span>) <span class="title">=&gt;</span> <span class="title">ZkClient</span>, <span class="title">getZkRoot</span> </span>: () =&gt; <span class="type">String</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义为 lazy 实现了懒汉式的单例模式，解决了序列化问题，方便使用 broadcast</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> zkClient: <span class="type">ZkClient</span> = getClient()</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> zkRoot: <span class="type">String</span> = getZkRoot()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// offsetId = md5(groupId+join(topics))</span></span><br><span class="line">  <span class="comment">// 初始化偏移量的 zk 存储路径 zkRoot</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initOffset</span></span>(offsetId: <span class="type">String</span>) : <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span>(!zkClient.exists(zkRoot))&#123;</span><br><span class="line">      zkClient.createPersistent(zkRoot, <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 从 zkRoot 读取偏移量信息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffset</span></span>(): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> keys = zkClient.getChildren(zkRoot)</span><br><span class="line">    <span class="keyword">var</span> initOffsetMap: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = <span class="type">Map</span>()</span><br><span class="line">    <span class="keyword">if</span>(!keys.isEmpty)&#123;</span><br><span class="line">      <span class="keyword">for</span> (k:<span class="type">String</span> &lt;- keys.asScala) &#123;</span><br><span class="line">        <span class="keyword">val</span> ks = k.split(<span class="string">"!"</span>)</span><br><span class="line">        <span class="keyword">val</span> value:<span class="type">Long</span> = zkClient.readData(zkRoot + <span class="string">"/"</span> + k)</span><br><span class="line">        initOffsetMap += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(ks(<span class="number">0</span>), <span class="type">Integer</span>.parseInt(ks(<span class="number">1</span>))) -&gt; value)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    initOffsetMap</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 根据单条消息，更新偏移量信息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateOffset</span></span>(consumeRecord: <span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> path = zkRoot + <span class="string">"/"</span> + consumeRecord.topic + <span class="string">"!"</span> + consumeRecord.partition</span><br><span class="line">    zkClient.writeData(path, consumeRecord.offset())</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 消费消息前，批量更新偏移量信息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">updateOffset</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (offset: <span class="type">OffsetRange</span> &lt;- offsetRanges) &#123;</span><br><span class="line">      <span class="keyword">val</span> path = zkRoot + <span class="string">"/"</span> + offset.topic + <span class="string">"!"</span> + offset.partition</span><br><span class="line">      <span class="keyword">if</span>(!zkClient.exists(path))&#123;</span><br><span class="line">        zkClient.createPersistent(path, offset.fromOffset)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">        zkClient.writeData(path, offset.fromOffset)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 消费消息后，批量提交偏移量信息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commitOffset</span></span>(offsetRanges: <span class="type">Array</span>[<span class="type">OffsetRange</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (offset: <span class="type">OffsetRange</span> &lt;- offsetRanges) &#123;</span><br><span class="line">      <span class="keyword">val</span> path = zkRoot + <span class="string">"/"</span> + offset.topic + <span class="string">"!"</span> + offset.partition</span><br><span class="line">      <span class="keyword">if</span>(!zkClient.exists(path))&#123;</span><br><span class="line">        zkClient.createPersistent(path, offset.untilOffset)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">        zkClient.writeData(path, offset.untilOffset)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">finalize</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    zkClient.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ZkKafkaOffset</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(cong: <span class="type">SparkConf</span>, offsetId: <span class="type">String</span>): <span class="type">ZkKafkaOffset</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> getClient = () =&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> zkHost = cong.get(<span class="string">"kafka.zk.hosts"</span>, <span class="string">"127.0.0.1:2181"</span>)</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ZkClient</span>(zkHost, <span class="number">30000</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> getZkRoot = () =&gt;&#123;</span><br><span class="line">      <span class="keyword">val</span> zkRoot = <span class="string">"/kafka/ss/offset/"</span> + offsetId</span><br><span class="line">      zkRoot</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ZkKafkaOffset</span>(getClient, getZkRoot)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Spark Streaming 消费 Kafka 消息</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">第一步：val customOffset: <span class="built_in">Map</span>[TopicPartition, Long] = kafkaOffset.getOffset(ssc)</span><br><span class="line">第二步：stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](ssc,</span><br><span class="line">        LocationStrategies.PreferConsistent,</span><br><span class="line">        ConsumerStrategies.Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaConf, customOffset))</span><br><span class="line">第三步：处理后，kafkaOffset.commitOffset(offsetRanges)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.JavaConverters._</span><br><span class="line"></span><br><span class="line">object RtDataLoader &#123;</span><br><span class="line">  def main(args: Array[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    <span class="comment">// 从配置文件读取 kafka 配置信息</span></span><br><span class="line">    val props = <span class="keyword">new</span> Props(<span class="string">"xxx.properties"</span>)</span><br><span class="line">    val groupId = props.getStr(<span class="string">"groupId"</span>, <span class="string">""</span>)</span><br><span class="line">    <span class="keyword">if</span>(StrUtil.isBlank(groupId))&#123;</span><br><span class="line">      StaticLog.error(<span class="string">"groupId is empty"</span>)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    val kfkServers = props.getStr(<span class="string">"kfk_servers"</span>)</span><br><span class="line">    <span class="keyword">if</span>(StrUtil.isBlank(kfkServers))&#123;</span><br><span class="line">      StaticLog.error(<span class="string">"bootstrap.servers is empty"</span>)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    val topicStr = props.getStr(<span class="string">"topics"</span>)</span><br><span class="line">    <span class="keyword">if</span>(StrUtil.isBlank(kfkServers))&#123;</span><br><span class="line">      StaticLog.error(<span class="string">"topics is empty"</span>)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// KAFKA 配置设定</span></span><br><span class="line">    val topics = topicStr.split(<span class="string">","</span>)</span><br><span class="line">    val kafkaConf = <span class="built_in">Map</span>[<span class="built_in">String</span>, <span class="built_in">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; kfkServers,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; groupId,</span><br><span class="line">      <span class="string">"receive.buffer.bytes"</span> -&gt; (<span class="number">102400</span>: java.lang.Integer),</span><br><span class="line">      <span class="string">"max.partition.fetch.bytes"</span> -&gt; (<span class="number">5252880</span>: java.lang.Integer),</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="keyword">false</span>: java.lang.Boolean)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"ss-kafka"</span>).setIfMissing(<span class="string">"spark.master"</span>, <span class="string">"local[2]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// streaming 相关配置</span></span><br><span class="line">    conf.<span class="keyword">set</span>(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>,<span class="string">"true"</span>)</span><br><span class="line">    conf.<span class="keyword">set</span>(<span class="string">"spark.streaming.backpressure.enabled"</span>,<span class="string">"true"</span>)</span><br><span class="line">    conf.<span class="keyword">set</span>(<span class="string">"spark.streaming.backpressure.initialRate"</span>,<span class="string">"1000"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 zookeeper 连接信息</span></span><br><span class="line">    conf.<span class="keyword">set</span>(<span class="string">"kafka.zk.hosts"</span>, props.getStr(<span class="string">"zk_hosts"</span>, <span class="string">"sky-01:2181"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 StreamingContext</span></span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(sc, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据 groupId 和 topics 获取 offset</span></span><br><span class="line">    val offsetId = SecureUtil.md5(groupId + topics.mkString(<span class="string">","</span>))</span><br><span class="line">    val kafkaOffset = ZkKafkaOffset(ssc.sparkContext.getConf, offsetId)</span><br><span class="line">    kafkaOffset.initOffset(ssc, offsetId)</span><br><span class="line">    val customOffset: <span class="built_in">Map</span>[TopicPartition, Long] = kafkaOffset.getOffset(ssc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建数据流</span></span><br><span class="line">    <span class="keyword">var</span> stream:InputDStream[ConsumerRecord[<span class="built_in">String</span>, <span class="built_in">String</span>]] = <span class="keyword">null</span></span><br><span class="line">    <span class="keyword">if</span>(topicStr.contains(<span class="string">"*"</span>)) &#123;</span><br><span class="line">      StaticLog.warn(<span class="string">"使用正则匹配读取 kafka 主题："</span> + topicStr)</span><br><span class="line">      stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](ssc,</span><br><span class="line">        LocationStrategies.PreferConsistent,</span><br><span class="line">        ConsumerStrategies.SubscribePattern[<span class="built_in">String</span>, <span class="built_in">String</span>](<span class="built_in">Pattern</span>.compile(topicStr), kafkaConf, customOffset))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      StaticLog.warn(<span class="string">"待读取的 kafka 主题："</span> + topicStr)</span><br><span class="line">      stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](ssc,</span><br><span class="line">        LocationStrategies.PreferConsistent,</span><br><span class="line">        ConsumerStrategies.Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaConf, customOffset))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 消费数据</span></span><br><span class="line">    stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="comment">// 消息消费前，更新 offset 信息</span></span><br><span class="line">      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">      kafkaOffset.updateOffset(offsetRanges)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//region 处理详情数据</span></span><br><span class="line">      StaticLog.info(<span class="string">"开始处理 RDD 数据！"</span>)</span><br><span class="line">      <span class="comment">//endregion</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 消息消费结束，提交 offset 信息</span></span><br><span class="line">      kafkaOffset.commitOffset(offsetRanges)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Flink-位移处理方式"><a href="#Flink-位移处理方式" class="headerlink" title="Flink 位移处理方式"></a>Flink 位移处理方式</h2><h3 id="Flink-消费者精确到一次语义"><a href="#Flink-消费者精确到一次语义" class="headerlink" title="Flink 消费者精确到一次语义"></a>Flink 消费者精确到一次语义</h3><ul>
<li><p>setStartFromGroupOffsets()【默认消费策略】默认读取上次保存的offset信息如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</p>
</li>
<li><p>setStartFromEarliest()从最早的数据开始进行消费，忽略存储的offset信息</p>
</li>
<li><p>setStartFromLatest()从最新的数据进行消费，忽略存储的offset信息</p>
</li>
<li><p>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)从指定位置进行消费。</p>
</li>
<li><p>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</p>
</li>
<li><p>为了能够使用支持容错的kafka Consumer，需要开启checkpointenv.enableCheckpointing(5000); // 每5s checkpoint一次</p>
</li>
<li><p>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:</p>
<p>(1) Flink Checkpoint关闭时： 可以通过Kafka下面两个Properties参数配置</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">enable</span><span class="selector-class">.auto</span><span class="selector-class">.commit</span></span><br><span class="line"><span class="selector-tag">auto</span><span class="selector-class">.commit</span><span class="selector-class">.interval</span><span class="selector-class">.ms</span></span><br></pre></td></tr></table></figure>

<p>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。可以通过这个参数设置</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">setCommitOffsetsOnCheckpoints</span><span class="params">(boolean)</span></span></span><br></pre></td></tr></table></figure>

<p>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略。</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取Flink的运行环境</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">//checkpoint配置</span></span><br><span class="line">env.enableCheckpointing(<span class="number">5000</span>);</span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置statebackend</span></span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(<span class="string">"hdfs://hadoop100:9000/flink/checkpoints"</span>,<span class="literal">true</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">String</span> topic = <span class="string">"kafkaConsumer"</span>;</span><br><span class="line">Properties prop = <span class="keyword">new</span> <span class="type">Properties</span>();</span><br><span class="line">prop.setProperty(<span class="string">"bootstrap.servers"</span>,<span class="string">"SparkMaster:9092"</span>);</span><br><span class="line">prop.setProperty(<span class="string">"group.id"</span>,<span class="string">"kafkaConsumerGroup"</span>);</span><br><span class="line"></span><br><span class="line">FlinkKafkaConsumer011&lt;<span class="keyword">String</span>&gt; myConsumer = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>&lt;&gt;(topic, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), prop);</span><br><span class="line"></span><br><span class="line">myConsumer.setStartFromGroupOffsets();<span class="comment">//默认消费策略</span></span><br><span class="line">myConsumer.setCommitOffsetsOnCheckpoints(<span class="literal">true</span>);</span><br><span class="line">DataStreamSource&lt;<span class="keyword">String</span>&gt; text = env.addSource(myConsumer);</span><br><span class="line"></span><br><span class="line">text.print().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"StreamingFromCollection"</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>Flink KafkaConsumer允许配置向 Kafka brokers（或者向Zookeeper）提交offset的行为。需要注意的是，Flink Kafka Consumer并不依赖于这些提交回Kafka或Zookeeper的offset来保证容错。这些被提交的offset只是意味着Flink将消费的状态暴露在外以便于监控。</p>
</li>
<li><p>FlinkKafkaConsumer提供了一套健壮的机制保证了在高吞吐量的情况下exactly-once的消费Kafka的数据，它的API的使用与配置也比较简单，同时也便于监控。</p>
</li>
<li><p>barrier可以理解为checkpoint之间的分隔符，在它之前的data属于前一个checkpoint，而在它之后的data属于另一个checkpoint。同时，barrier会由source(如FlinkKafkaConsumer)发起，并混在数据中，同数据一样传输给下一级的operator，直到sink为止。如果barrier已经被sink收到，那么说明checkpoint已经完成了(这个checkpoint的状态为completed并被存到了state backend中)，它之前的数据已经被处理完毕并sink。</p>
</li>
<li><p>Flink异步记录checkpoint的行为是由我们的来配置的，只有当我们设置了enableCheckpointing()时，Flink才会在checkpoint完成时(整个job的所有的operator都收到了这个checkpoint的barrier才意味这checkpoint完成，具体参考我们对Flink checkpoint的介绍)将offset记录起来并提交，这时候才能够保证exactly-once。</p>
</li>
</ul>
<h3 id="Flink-生产者精确到一次语义"><a href="#Flink-生产者精确到一次语义" class="headerlink" title="Flink 生产者精确到一次语义"></a>Flink 生产者精确到一次语义</h3><ul>
<li><p>Kafka Producer的容错-Kafka 0.9 and 0.10</p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 <span class="keyword">at</span>-least-once的语义，还需要配置下面两个参数:</span><br><span class="line">  setLogFailuresOnly(<span class="literal">false</span>)</span><br><span class="line">  setFlushOnCheckpoint(<span class="literal">true</span>)</span><br><span class="line">  注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是<span class="number">0</span>】</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure>
</li>
<li><p>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义</p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">具体的语义设置方式 </span><br><span class="line">Semantic.NONE</span><br><span class="line">Semantic.AT_LEAST_ONCE【默认】</span><br><span class="line">Semantic.EXACTLY_ONCE</span><br><span class="line"></span><br><span class="line">checkpoint配置</span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.enableCheckpointing(<span class="number">5000</span>);</span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);</span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(<span class="number">500</span>);</span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(<span class="number">60000</span>);</span><br><span class="line">env.getCheckpointConfig().setMaxConcurrentCheckpoints(<span class="number">1</span>);</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);</span><br><span class="line"></span><br><span class="line"><span class="comment">//第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间</span></span><br><span class="line"><span class="comment">//设置事务超时时间</span></span><br><span class="line"><span class="comment">//prop.setProperty("transaction.timeout.ms",60000*15+"");</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。</span></span><br><span class="line"><span class="comment">//FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//使用仅一次语义的kafkaProducer</span></span><br><span class="line">FlinkKafkaProducer011&lt;<span class="keyword">String</span>&gt; myProducer = <span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>&lt;&gt;(topic, <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>&lt;<span class="keyword">String</span>&gt;(<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);</span><br><span class="line"></span><br><span class="line">text.addSink(myProducer);</span><br></pre></td></tr></table></figure>

</li>
</ul>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>gaojintao999@163.com</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2018/10/13/Kafka 偏移量管理实现精确一次语义在Spark&amp;Flink中的技术实践/">https://gjtmaster.github.io/2018/10/13/Kafka 偏移量管理实现精确一次语义在Spark&amp;Flink中的技术实践/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>The author owns the copyright, please indicate the source reproduced.</li></ul></div><br><div class="tags"><a href="/tags/Kafka/">Kafka</a><a href="/tags/消息中间件/">消息中间件</a></div><div class="post-nav"><a class="pre" href="/2018/10/15/消息中间件高级技术要点企业级架构深入分析/">消息中间件高级技术要点企业级架构深入分析</a><a class="next" href="/2018/10/08/kafka集群消息格式之V0版本到V2版本的平滑过渡详解/">kafka集群消息格式之V0版本到V2版本的平滑过渡详解</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/JVM/">JVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/实时计算框架/">实时计算框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据存储格式/">数据存储格式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据的导入导出/">数据的导入导出</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/日志框架/">日志框架</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/消息中间件/">消息中间件</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Oracle/" style="font-size: 15px;">Oracle</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/Yarn/" style="font-size: 15px;">Yarn</a> <a href="/tags/实时计算/" style="font-size: 15px;">实时计算</a> <a href="/tags/Flink-on-Yarn/" style="font-size: 15px;">Flink on Yarn</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/消息中间件/" style="font-size: 15px;">消息中间件</a> <a href="/tags/FlinkSQL/" style="font-size: 15px;">FlinkSQL</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/内存回收/" style="font-size: 15px;">内存回收</a> <a href="/tags/Logback/" style="font-size: 15px;">Logback</a> <a href="/tags/Json/" style="font-size: 15px;">Json</a> <a href="/tags/ogg/" style="font-size: 15px;">ogg</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/19/Flink 进阶：Time 深度解析/">Flink 进阶：Time 深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/19/Flink 进阶：Flink Connector详解/">Flink 进阶：Flink Connector详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/16/Flink 进阶：增量 Checkpoint 详解/">Flink 进阶：增量 Checkpoint 详解</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/15/Flink 进阶：Runtime 核心机制剖析/">Flink 进阶：Runtime 核心机制剖析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/18/FlinkSQL深度解析/">Flink SQL 深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/Flink on Yarn HA/">Flink On Yarn HA</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/15/Flink使用Logback作为日志框架的相关配置/">Flink使用Logback作为日志框架的相关配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/15/Flink On Yan集群部署/">Flink On Yarn集群部署</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/15/Flink1.7.1与Kafka0.11.0.1/">Flink1.7.1与Kafka0.11.0.1</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/15/消息中间件高级技术要点企业级架构深入分析/">消息中间件高级技术要点企业级架构深入分析</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://matt33.com/" title="Matt's Blog" target="_blank">Matt's Blog</a><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="Maupassant's usage" target="_blank">Maupassant's usage</a><ul></ul><a href="https://www.jianshu.com/p/f4332764e8bd" title="hexo's usage" target="_blank">hexo's usage</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Joker's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>