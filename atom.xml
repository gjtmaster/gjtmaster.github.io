<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Joker&#39;s Blog</title>
  
  <subtitle>高金涛</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://gjtmaster.github.io/"/>
  <updated>2019-09-25T08:53:04.587Z</updated>
  <id>https://gjtmaster.github.io/</id>
  
  <author>
    <name>高金涛</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 进阶：Time 深度解析</title>
    <link href="https://gjtmaster.github.io/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2019/08/19/Flink 进阶：Time 深度解析/</id>
    <published>2019-08-19T11:18:56.000Z</published>
    <updated>2019-09-25T08:53:04.587Z</updated>
    
    <content type="html"><![CDATA[<p>原  作  者 | 崔星灿</p><p>原整理者 | 沙晟阳（成阳）</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/001-1024x449.png" alt="img"></p><h3 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h3><p>在不同的应用场景中时间语义是各不相同的，Flink 作为一个先进的分布式流处理引擎，它本身支持不同的时间语义。其核心是 Processing Time 和 Event Time（Row Time），这两类时间主要的不同点如下表所示：</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/002-1024x415.png" alt="img"></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time 则是根据每一条处理记录所携带的时间戳来判定。</p><p>这两种时间在 Flink 内部的处理以及还是用户的实际使用方面，难易程度都是不同的。相对而言的 Processing Time 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用 Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，使用 Event Time 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同。如果希望结果完全相同，就只能用 Event Time；如果接受结果不同，则可以用 Processing Time。Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用 Processing Time。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/003-1024x499.png" alt="img"></p><h4 id="时间的特性"><a href="#时间的特性" class="headerlink" title="时间的特性"></a>时间的特性</h4><p><strong>时间的一个重要特性是：时间只能递增，不会来回穿越。</strong> 在使用时间的时候我们要充分利用这个特性。假设我们有这么一些记录，然后我们来分别看一下 Processing Time 还有 Event Time 对于时间的处理。</p><ul><li>对于 Processing Time，因为我们是使用的是本地节点的时间（假设这个节点的时钟同步没有问题），我们每一次取到的 Processing Time 肯定都是递增的，递增就代表着有序，所以说我们相当于拿到的是一个有序的数据流。</li><li>而在用 Event Time 的时候因为时间是绑定在每一条的记录上的，由于网络延迟、程序内部逻辑、或者其他一些分布式系统的原因，数据的时间可能会存在一定程度的乱序，比如上图的例子。在 Event Time 场景下，我们把每一个记录所包含的时间称作 Record Timestamp。如果 Record Timestamp 所得到的时间序列存在乱序，我们就需要去处理这种情况。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/004.png" alt="img"></p><p>如果单条数据之间是乱序，我们就考虑对于整个序列进行更大程度的离散化。简单地讲，就是把数据按照一定的条数组成一些小批次，但这里的小批次并不是攒够多少条就要去处理，而是为了对他们进行时间上的划分。经过这种更高层次的离散化之后，我们会发现最右边方框里的时间就是一定会小于中间方框里的时间，中间框里的时间也一定会小于最左边方框里的时间。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/005.png" alt="img"></p><p>这个时候我们在整个时间序列里插入一些类似于标志位的一些特殊的处理数据，这些特殊的处理数据叫做 watermark。一个 watermark 本质上就代表了这个 watermark 所包含的 timestamp 数值，表示以后到来的数据已经再也没有小于或等于这个时间的了。</p><h3 id="Timestamp-和-Watermark-行为概览"><a href="#Timestamp-和-Watermark-行为概览" class="headerlink" title="Timestamp 和 Watermark 行为概览"></a>Timestamp 和 Watermark 行为概览</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/006-1024x442.png" alt="img"></p><p>接下来我们重点看一下 Event Time 里的 Record Timestamp（简写成 timestamp）和 watermark 的一些基本信息。绝大多数的分布式流计算引擎对于数据都是进行了 DAG 图的抽象，它有自己的数据源，有处理算子，还有一些数据汇。数据在不同的逻辑算子之间进行流动。watermark 和 timestamp 有自己的生命周期，接下来我会从 watermark 和 timestamp 的产生、他们在不同的节点之间的传播、以及在每一个节点上的处理，这三个方面来展开介绍。</p><h4 id="Timestamp-分配和-Watermark-生成"><a href="#Timestamp-分配和-Watermark-生成" class="headerlink" title="Timestamp 分配和 Watermark 生成"></a>Timestamp 分配和 Watermark 生成</h4><p>Flink 支持两种 watermark 生成方式。第一种是在 SourceFunction 中产生，相当于把整个的 timestamp 分配和 watermark 生成的逻辑放在流处理应用的源头。我们可以在 SourceFunction 里面通过这两个方法产生 watermark：</p><ul><li>通过 collectWithTimestamp 方法发送一条数据，其中第一个参数就是我们要发送的数据，第二个参数就是这个数据所对应的时间戳；也可以调用 emitWatermark 方法去产生一条 watermark，表示接下来不会再有时间戳小于等于这个数值记录。</li><li>另外，有时候我们不想在 SourceFunction 里生成 timestamp 或者 watermark，或者说使用的 SourceFunction 本身不支持，我们还可以在使用 DataStream API 的时候指定，调用的 DataStream.assignTimestampsAndWatermarks 这个方法，能够接收不同的 timestamp 和 watermark 的生成器。</li></ul><p>总体上而言生成器可以分为两类：第一类是定期生成器；第二类是根据一些在流处理数据流中遇到的一些特殊记录生成的。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/007.png" alt="img"></p><p>两者的区别主要有三个方面，首先定期生成是现实时间驱动的，这里的“定期生成”主要是指 watermark（因为 timestamp 是每一条数据都需要有的），即定期会调用生成逻辑去产生一个 watermark。而根据特殊记录生成是数据驱动的，即是否生成 watermark 不是由现实时间来决定，而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了，这个时候相当于每一次分配 Timestamp 之后都会调用用户实现的 watermark 生成方法，用户需要在生成方法中去实现 watermark 的生成逻辑。</p><p>大家要注意的是就是我们在分配 timestamp 和生成 watermark 的过程，虽然在 SourceFunction 和 DataStream 中都可以指定，但是还是建议生成的工作越靠近 DataSource 越好。这样会方便让程序逻辑里面更多的 operator 去判断某些数据是否乱序。Flink 内部提供了很好的机制去保证这些 timestamp 和 watermark 被正确地传递到下游的节点。</p><h4 id="Watermark-传播"><a href="#Watermark-传播" class="headerlink" title="Watermark 传播"></a>Watermark 传播</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/008-1024x474.png" alt="img"></p><p>具体的传播策略基本上遵循这三点。</p><ul><li>首先，watermark 会以广播的形式在算子之间进行传播。比如说上游的算子，它连接了三个下游的任务，它会把自己当前的收到的 watermark 以广播的形式传到下游。</li><li>第二，如果在程序里面收到了一个 Long.MAX_VALUE 这个数值的 watermark，就表示对应的那一条流的一个部分不会再有数据发过来了，它相当于就是一个终止的一个标志。</li><li>第三，对于单流而言，这个策略比较好理解，而对于有多个输入的算子，watermark 的计算就有讲究了，一个原则是：单输入取其大，多输入取小。</li></ul><p>举个例子，假设这边蓝色的块代表一个算子的一个任务，然后它有三个输入，分别是 W1、W2、W3，这三个输入可以理解成任何的输入，这三个输入可能是属于同一个流，也可能是属于不同的流。然后在计算 watermark 的时候，对于单个输入而言是取他们的最大值，因为我们都知道 watermark 应该遵循一个单调递增的一个原则。对于多输入，它要统计整个算子任务的 watermark 时，就会取这三个计算出来的 watermark 的最小值。即一个多个输入的任务，它的 watermark 受制于最慢的那条输入流。这一点类似于木桶效应，整个木桶中装的水会就是受制于最矮的那块板。</p><p>watermark 在传播的时候有一个特点是，它的传播是幂等的。多次收到相同的 watermark，甚至收到之前的 watermark 都不会对最后的数值产生影响，因为对于单个输入永远是取最大的，而对于整个任务永远是取一个最小的。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在它没有区分你这个输入是一条流多个 partition 还是来自于不同的逻辑上的流的 JOIN。对于同一个流的不同 partition，我们对他做这种强制的时钟同步是没有问题的，因为一开始就是把一条流拆散成不同的部分，但每一个部分之间共享相同的时钟。但是如果算子的任务是在做类似于 JOIN 操作，那么要求你两个输入的时钟强制同步其实没有什么道理的，因为完全有可能是把一条离现在时间很近的数据流和一个离当前时间很远的数据流进行 JOIN，这个时候对于快的那条流，因为它要等慢的那条流，所以说它可能就要在状态中去缓存非常多的数据，这对于整个集群来说是一个很大的性能开销。</p><h4 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h4><p>在正式介绍 watermark 的处理之前，先简单介绍 ProcessFunction，因为 watermark 在任务里的处理逻辑分为内部逻辑和外部逻辑。外部逻辑其实就是通过 ProcessFunction 来体现的，如果你需要使用 Flink 提供的时间相关的 API 的话就只能写在 ProcessFunction 里。</p><p>ProcessFunction 和时间相关的功能主要有三点：</p><ul><li>第一点就是根据你当前系统使用的时间语义不同，你可以去获取当前你正在处理这条记录的 Record Timestamp，或者当前的 Processing Time。</li><li>第二点就是它可以获取当前算子的时间，可以把它理解成当前的 watermark。</li><li>第三点就是为了在 ProcessFunction 中去实现一些相对复杂的功能，允许注册一些 timer（定时器）。比如说在 watermark 达到某一个时间点的时候就触发定时器，所有的这些回调逻辑也都是由用户来提供，涉及到如下三个方法，registerEventTimeTimer、registerProcessingTimeTimer 和 onTimer。在 onTimer 方法中就需要去实现自己的回调逻辑，当条件满足时回调逻辑就会被触发。</li></ul><p>一个简单的应用是，我们在做一些时间相关的处理的时候，可能需要缓存一部分数据，但这些数据不能一直去缓存下去，所以需要有一些过期的机制，我们可以通过 timer 去设定这么一个时间，指定某一些数据可能在将来的某一个时间点过期，从而把它从状态里删除掉。所有的这些和时间相关的逻辑在 Flink 内部都是由自己的 Time Service（时间服务）完成的。</p><h4 id="Watermark-处理"><a href="#Watermark-处理" class="headerlink" title="Watermark 处理"></a>Watermark 处理</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/009-1024x376.png" alt="img"></p><p>一个算子的实例在收到 watermark 的时候，首先要更新当前的算子时间，这样的话在 ProcessFunction 里方法查询这个算子时间的时候，就能获取到最新的时间。第二步它会遍历计时器队列，这个计时器队列就是我们刚刚说到的 timer，你可以同时注册很多 timer，Flink 会把这些 Timer 按照触发时间放到一个优先队列中。第三步 Flink 得到一个时间之后就会遍历计时器的队列，然后逐一触发用户的回调逻辑。 通过这种方式，Flink 的某一个任务就会将当前的 watermark 发送到下游的其他任务实例上，从而完成整个 watermark 的传播，从而形成一个闭环。</p><h3 id="Table-API-中的时间"><a href="#Table-API-中的时间" class="headerlink" title="Table API 中的时间"></a>Table API 中的时间</h3><p>下面我们来看一看 Table/SQL API 中的时间。为了让时间参与到 Table/SQL 这一层的运算中，我们需要提前把时间属性放到表的 schema 中，这样的话我们才能够在 SQL 语句或者 Table 的一些逻辑表达式里面去使用这些时间去完成需求。</p><h4 id="Table-中指定时间列"><a href="#Table-中指定时间列" class="headerlink" title="Table 中指定时间列"></a>Table 中指定时间列</h4><p>其实之前社区就怎么在 Table/SQL 中去使用时间这个问题做过一定的讨论，是把获取当前 Processing Time 的方法是作为一个特殊的 UDF，还是把这一个列物化到整个的 schema 里面，最终采用了后者。我们这里就分开来讲一讲 Processing Time 和 Event Time 在使用的时候怎么在 Table 中指定。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/010-1024x487.png" alt="img"></p><p>对于 Processing Time，我们知道要得到一个 Table 对象（或者注册一个 Table）有两种手段：</p><ol><li>可以从一个 DataStream 转化成一个 Table；</li><li>直接通过 TableSource 去生成这么一个 Table；</li></ol><p>对于第一种方法而言，我们只需要在你已有的这些列中（例子中 f1 和 f2 就是两个已有的列），在最后用“列名.proctime”这种写法就可以把最后的这一列注册为一个 Processing Time，以后在写查询的时候就可以去直接使用这一列。如果 Table 是通过 TableSource 生成的，就可以通过实现这一个 DefinedRowtimeAttributes 接口，然后就会自动根据你提供的逻辑去生成对应的 Processing Time。</p><p>相对而言，在使用 Event Time 时则有一个限制，因为 Event Time 不像 Processing Time 那样是随拿随用。如果你要从 DataStream 去转化得到一个 Table，必须要提前保证原始的 DataStream 里面已经存在了 Record Timestamp 和 watermark。如果你想通过 TableSource 生成的，也一定要保证你要接入的一个数据里面存在一个类型为 long 或者 timestamp 的这么一个时间字段。</p><p>具体来说，如果你要从 DataStream 去注册一个表，和 proctime 类似，你只需要加上“列名.rowtime”就可以。需要注意的是，如果你要用 Processing Time，必须保证你要新加的字段是整个 schema 中的最后一个字段，而 Event Time 的时候你其实可以去替换某一个已有的列，然后 Flink 会自动的把这一列转化成需要的 rowtime 这个类型。 如果是通过 TableSource 生成的，只需要实现 DefinedRowtimeAttributes 接口就可以了。需要说明的一点是，在 DataStream API 这一侧其实不支持同时存在多个 Event Time（rowtime），但是在 Table 这一层理论上可以同时存在多个 rowtime。因为 DefinedRowtimeAttributes 接口的返回值是一个对于 rowtime 描述的 List，即其实可以同时存在多个 rowtime 列，在将来可能会进行一些其他的改进，或者基于去做一些相应的优化。</p><h4 id="时间列和-Table-操作"><a href="#时间列和-Table-操作" class="headerlink" title="时间列和 Table 操作"></a>时间列和 Table 操作</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/011-1024x475.png" alt="img"></p><p>指定完了时间列之后，当我们要真正去查询时就会涉及到一些具体的操作。这里我列举的这些操作都是和时间列紧密相关，或者说必须在这个时间列上才能进行的。比如说“Over 窗口聚合”和“Group by 窗口聚合”这两种窗口聚合，在写 SQL 提供参数的时候只能允许你在这个时间列上进行这种聚合。第三个就是时间窗口聚合，你在写条件的时候只支持对应的时间列。最后就是排序，我们知道在一个无尽的数据流上对数据做排序几乎是不可能的事情，但因为这个数据本身到来的顺序已经是按照时间属性来进行排序，所以说我们如果要对一个 DataStream 转化成 Table 进行排序的话，你只能是按照时间列进行排序，当然同时你也可以指定一些其他的列，但是时间列这个是必须的，并且必须放在第一位。</p><p>为什么说这些操作只能在时间列上进行？因为我们有的时候可以把到来的数据流就看成是一张按照时间排列好的一张表，而我们任何对于表的操作，其实都是必须在对它进行一次顺序扫描的前提下完成的。因为大家都知道数据流的特性之一就是一过性，某一条数据处理过去之后，将来其实不太好去访问它。当然因为 Flink 中内部提供了一些状态机制，我们可以在一定程度上去弱化这个特性，但是最终还是不能超越的限制状态不能太大。所有这些操作为什么只能在时间列上进行，因为这个时间列能够保证我们内部产生的状态不会无限的增长下去，这是一个最终的前提。</p>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Time机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 进阶：Flink Connector详解</title>
    <link href="https://gjtmaster.github.io/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9AFlink%20Connector%E8%AF%A6%E8%A7%A3/"/>
    <id>https://gjtmaster.github.io/2019/08/19/Flink 进阶：Flink Connector详解/</id>
    <published>2019-08-19T10:23:51.000Z</published>
    <updated>2019-09-26T05:08:41.079Z</updated>
    
    <content type="html"><![CDATA[<p><strong>原作者 | 董亭亭</strong></p><p><strong>快手 实时计算引擎团队负责人</strong></p><p>董亭亭，快手大数据架构实时计算引擎团队负责人。目前负责Flink引擎在快手内的研发、应用以及周边子系统建设。2013年毕业于大连理工大学，曾就职于奇虎360、58集团。主要研究领域包括：分布式计算、调度系统、分布式存储等系统。</p><p>本文主要分享 Flink connector 相关内容，分为以下三个部分的内容：第一部分会首先介绍一下 Flink Connector 有哪些。第二部分会重点介绍在生产环境中经常使用的 kafka connector 的基本的原理以及使用方法。第三部分答疑，对社区反馈的问题进行答疑。</p><h3 id="Flink-Streaming-Connector"><a href="#Flink-Streaming-Connector" class="headerlink" title="Flink Streaming Connector"></a>Flink Streaming Connector</h3><p>Flink 是新一代流批统一的计算引擎，它需要从不同的第三方存储引擎中把数据读过来，进行处理，然后再写出到另外的存储引擎中。Connector 的作用就相当于一个连接器，连接 Flink 计算引擎跟外界存储系统。Flink 里有以下几种方式，当然也不限于这几种方式可以跟外界进行数据交换：</p><ul><li>第一种 Flink 里面预定义了一些 source 和 sink。</li><li>第二种 Flink 内部也提供了一些 Boundled connectors。</li><li>第三种可以使用第三方 Apache Bahir 项目中提供的连接器。</li><li>第四种是通过异步 IO 方式。</li></ul><p>下面分别简单介绍一下这四种数据读写的方式。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/1-1024x576.jpg" alt="img"></p><h4 id="1-预定义的-source-和-sink"><a href="#1-预定义的-source-和-sink" class="headerlink" title="1. 预定义的 source 和 sink"></a>1. 预定义的 source 和 sink</h4><p>Flink 里预定义了一部分 source 和 sink。在这里分了几类。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/2-1024x576.jpg" alt="img"></p><h5 id="基于文件的-source-和-sink"><a href="#基于文件的-source-和-sink" class="headerlink" title="基于文件的 source 和 sink"></a><strong>基于文件的 source 和 sink</strong></h5><p>****如果要从文本文件中读取数据，可以直接使用：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">env</span><span class="selector-class">.readTextFile</span>(<span class="selector-tag">path</span>)</span><br></pre></td></tr></table></figure><p>就可以以文本的形式读取该文件中的内容。当然也可以使用：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">env</span><span class="selector-class">.readFile</span>(<span class="selector-tag">fileInputFormat</span>, <span class="selector-tag">path</span>)</span><br></pre></td></tr></table></figure><p>根据指定的 fileInputFormat 格式读取文件中的内容。</p><p>如果数据在 Flink 内进行了一系列的计算，想把结果写出到文件里，也可以直接使用内部预定义的一些 sink，比如将结果已文本或 csv 格式写出到文件中，可以使用 DataStream 的 writeAsText(path) 和 writeAsCsv(path)。</p><h5 id="基于-Socket-的-Source-和-Sink"><a href="#基于-Socket-的-Source-和-Sink" class="headerlink" title="基于 Socket 的 Source 和 Sink"></a><strong>基于 Socket 的 Source 和 Sink</strong></h5><p>提供 Socket 的 host name 及 port，可以直接用 StreamExecutionEnvironment 预定的接口 socketTextStream 创建基于 Socket 的 source，从该 socket 中以文本的形式读取数据。当然如果想把结果写出到另外一个 Socket，也可以直接调用 DataStream writeToSocket。</p><h5 id="基于内存-Collections、Iterators-的-Source"><a href="#基于内存-Collections、Iterators-的-Source" class="headerlink" title="基于内存 Collections、Iterators 的 Source"></a><strong>基于内存 Collections、Iterators 的 Source</strong></h5><p>可以直接基于内存中的集合或者迭代器，调用 StreamExecutionEnvironment fromCollection、fromElements 构建相应的 source。结果数据也可以直接 print、printToError 的方式写出到标准输出或标准错误。</p><p>详细也可以参考 Flink 源码中提供的一些相对应的 Examples 来查看异常预定义 source 和 sink 的使用方法，例如 WordCount、SocketWindowWordCount。</p><h4 id="2-Bundled-Connectors"><a href="#2-Bundled-Connectors" class="headerlink" title="2. Bundled Connectors"></a><strong>2. Bundled Connectors</strong></h4><p>Flink 里已经提供了一些绑定的 Connector，例如 kafka source 和 sink，Es sink等。读写 kafka、es、rabbitMQ 时可以直接使用相应 connector 的 api 即可。第二部分会详细介绍生产环境中最常用的 kafka connector。</p><p>虽然该部分是 Flink 项目源代码里的一部分，但是真正意义上不算作 Flink 引擎相关逻辑，并且该部分没有打包在二进制的发布包里面。所以在提交 Job 时候需要注意， job 代码 jar 包中一定要将相应的 connetor 相关类打包进去，否则在提交作业时就会失败，提示找不到相应的类，或初始化某些类异常。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/3-1024x576.jpg" alt="img"></p><h4 id="3-Apache-Bahir-中的连接器"><a href="#3-Apache-Bahir-中的连接器" class="headerlink" title="3. Apache Bahir 中的连接器"></a><strong>3. Apache Bahir 中的连接器</strong></h4><p>Apache Bahir 最初是从 Apache Spark 中独立出来项目提供，以提供不限于 Spark 相关的扩展/插件、连接器和其他可插入组件的实现。通过提供多样化的流连接器（streaming connectors）和 SQL 数据源扩展分析平台的覆盖面。如有需要写到 flume、redis 的需求的话，可以使用该项目提供的 connector。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/4-1024x576.jpg" alt="img"></p><h4 id="4-Async-I-O"><a href="#4-Async-I-O" class="headerlink" title="4. Async I/O"></a><strong>4. Async I/O</strong></h4><p>流计算中经常需要与外部存储系统交互，比如需要关联 MySQL 中的某个表。一般来说，如果用同步 I/O 的方式，会造成系统中出现大的等待时间，影响吞吐和延迟。为了解决这个问题，异步 I/O 可以并发处理多个请求，提高吞吐，减少延迟。</p><p>Async 的原理可参考官方文档：</p><p><strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html</a></strong></p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/5-1024x576.jpg" alt="img"></p><h3 id="Flink-Kafka-Connector"><a href="#Flink-Kafka-Connector" class="headerlink" title="Flink Kafka Connector"></a>Flink Kafka Connector</h3><p>本章重点介绍生产环境中最常用到的 Flink kafka connector。使用 Flink 的同学，一定会很熟悉 kafka，它是一个分布式的、分区的、多副本的、 支持高吞吐的、发布订阅消息系统。生产环境环境中也经常会跟 kafka 进行一些数据的交换，比如利用 kafka consumer 读取数据，然后进行一系列的处理之后，再将结果写出到 kafka 中。这里会主要分两个部分进行介绍，一是 Flink kafka Consumer，一个是 Flink kafka Producer。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/6-1024x576.jpg" alt="img"></p><p>首先看一个例子来串联下 Flink kafka connector。代码逻辑里主要是从 kafka 里读数据，然后做简单的处理，再写回到 kafka 中。</p><p>分别用红框框出如何构造一个 Source sink Function。Flink 提供了现成的构造FlinkKafkaConsumer、Producer 的接口，可以直接使用。这里需要注意，因为 kafka 有多个版本，多个版本之间的接口协议会不同。Flink 针对不同版本的 kafka 有相应的版本的 Consumer 和 Producer。例如：针对 08、09、10、11 版本，Flink 对应的 consumer 分别是 FlinkKafkaConsumer 08、09、010、011，producer 也是。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/7-1024x576.jpg" alt="img"></p><h4 id="1-Flink-kafka-Consumer"><a href="#1-Flink-kafka-Consumer" class="headerlink" title="1. Flink kafka Consumer"></a><strong>1. Flink kafka Consumer</strong></h4><h5 id="反序列化数据"><a href="#反序列化数据" class="headerlink" title="反序列化数据"></a><strong>反序列化数据</strong></h5><p>因为 kafka 中数据都是以二进制 byte 形式存储的。读到 Flink 系统中之后，需要将二进制数据转化为具体的 java、scala 对象。具体需要实现一个 schema 类，定义如何序列化和反序列数据。反序列化时需要实现 DeserializationSchema 接口，并重写 deserialize(byte[] message) 函数，如果是反序列化 kafka 中 kv 的数据时，需要实现 KeyedDeserializationSchema 接口，并重写 deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) 函数。</p><p>另外 Flink 中也提供了一些常用的序列化反序列化的 schema 类。例如，SimpleStringSchema，按字符串方式进行序列化、反序列化。TypeInformationSerializationSchema，它可根据 Flink 的 TypeInformation 信息来推断出需要选择的 schema。JsonDeserializationSchema 使用 jackson 反序列化 json 格式消息，并返回 ObjectNode，可以使用 .get(“property”) 方法来访问相应字段。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/8-1024x576.jpg" alt="img"></p><h5 id="消费起始位置设置"><a href="#消费起始位置设置" class="headerlink" title="消费起始位置设置"></a><strong>消费起始位置设置</strong></h5><p>如何设置作业从 kafka 消费数据最开始的起始位置，这一部分 Flink 也提供了非常好的封装。在构造好的 FlinkKafkaConsumer 类后面调用如下相应函数，设置合适的起始位置。</p><ul><li><strong>setStartFromGroupOffsets</strong>，也是默认的策略，从 group offset 位置读取数据，group offset 指的是 kafka broker 端记录的某个 group 的最后一次的消费位置。但是 kafka broker 端没有该 group 信息，会根据 kafka 的参数”auto.offset.reset”的设置来决定从哪个位置开始消费。</li><li><strong>setStartFromEarliest</strong>，从 kafka 最早的位置开始读取。</li><li><strong>setStartFromLatest</strong>，从 kafka 最新的位置开始读取。</li><li><strong>setStartFromTimestamp(long)</strong>，从时间戳大于或等于指定时间戳的位置开始读取。Kafka 时戳，是指 kafka 为每条消息增加另一个时戳。该时戳可以表示消息在 proudcer 端生成时的时间、或进入到 kafka broker 时的时间。</li><li><strong>setStartFromSpecificOffsets</strong>，从指定分区的 offset 位置开始读取，如指定的 offsets 中不存某个分区，该分区从 group offset 位置开始读取。此时需要用户给定一个具体的分区、offset 的集合。</li></ul><p>一些具体的使用方法可以参考下图。需要注意的是，因为 Flink 框架有容错机制，如果作业故障，如果作业开启 checkpoint，会从上一次 checkpoint 状态开始恢复。或者在停止作业的时候主动做 savepoint，启动作业时从 savepoint 开始恢复。这两种情况下恢复作业时，作业消费起始位置是从之前保存的状态中恢复，与上面提到跟 kafka 这些单独的配置无关。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/9-1024x576.jpg" alt="img"></p><h5 id="topic-和-partition-动态发现"><a href="#topic-和-partition-动态发现" class="headerlink" title="topic 和 partition 动态发现"></a><strong>topic 和 partition 动态发现</strong></h5><p>实际的生产环境中可能有这样一些需求，比如场景一，有一个 Flink 作业需要将五份数据聚合到一起，五份数据对应五个 kafka topic，随着业务增长，新增一类数据，同时新增了一个 kafka topic，如何在不重启作业的情况下作业自动感知新的 topic。场景二，作业从一个固定的 kafka topic 读数据，开始该 topic 有 10 个 partition，但随着业务的增长数据量变大，需要对 kafka partition 个数进行扩容，由 10 个扩容到 20。该情况下如何在不重启作业情况下动态感知新扩容的 partition？</p><p>针对上面的两种场景，首先需要在构建 FlinkKafkaConsumer 时的 properties 中设置 flink.partition-discovery.interval-millis 参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时 FlinkKafkaConsumer 内部会启动一个单独的线程定期去 kafka 获取最新的 meta 信息。针对场景一，还需在构建 FlinkKafkaConsumer 时，topic 的描述可以传一个正则表达式描述的 pattern。每次获取最新 kafka meta 时获取正则匹配的最新 topic 列表。针对场景二，设置前面的动态发现参数，在定期获取 kafka 最新 meta 信息时会匹配新的 partition。为了保证数据的正确性，新发现的 partition 从最早的位置开始读取。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/10-1024x576.jpg" alt="img"></p><h5 id="commit-offset-方式"><a href="#commit-offset-方式" class="headerlink" title="commit offset 方式"></a><strong>commit offset 方式</strong></h5><p>Flink kafka consumer commit offset 方式需要区分是否开启了 checkpoint。</p><p>如果 checkpoint 关闭，commit offset 要依赖于 kafka 客户端的 auto commit。需设置 enable.auto.commit，auto.commit.interval.ms 参数到 consumer properties，就会按固定的时间间隔定期 auto commit offset 到 kafka。</p><p>如果开启 checkpoint，这个时候作业消费的 offset 是 Flink 在 state 中自己管理和容错。此时提交 offset 到 kafka，一般都是作为外部进度的监控，想实时知道作业消费的位置和 lag 情况。此时需要 setCommitOffsetsOnCheckpoints 为 true 来设置当 checkpoint 成功时提交 offset 到 kafka。此时 commit offset 的间隔就取决于 checkpoint 的间隔，所以此时从 kafka 一侧看到的 lag 可能并非完全实时，如果 checkpoint 间隔比较长 lag 曲线可能会是一个锯齿状。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/11-1024x576.jpg" alt="img"></p><h5 id="Timestamp-Extraction-Watermark-生成"><a href="#Timestamp-Extraction-Watermark-生成" class="headerlink" title="Timestamp Extraction/Watermark 生成"></a><strong>Timestamp Extraction/Watermark 生成</strong></h5><p>我们知道当 Flink 作业内使用 EventTime 属性时，需要指定从消息中提取时戳和生成水位的函数。FlinkKakfaConsumer 构造的 source 后直接调用 assignTimestampsAndWatermarks 函数设置水位生成器的好处是此时是每个 partition 一个 watermark assigner，如下图。source 生成的时戳为多个 partition 时戳对齐后的最小时戳。此时在一个 source 读取多个 partition，并且 partition 之间数据时戳有一定差距的情况下，因为在 source 端 watermark 在 partition 级别有对齐，不会导致数据读取较慢 partition 数据丢失。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/12-1024x576.jpg" alt="img"></p><h4 id="2-Flink-kafka-Producer"><a href="#2-Flink-kafka-Producer" class="headerlink" title="2. Flink kafka Producer"></a><strong>2. Flink kafka Producer</strong></h4><h5 id="Producer-分区"><a href="#Producer-分区" class="headerlink" title="Producer 分区"></a><strong>Producer 分区</strong></h5><p>使用 FlinkKafkaProducer 往 kafka 中写数据时，如果不单独设置 partition 策略，会默认使用 FlinkFixedPartitioner，该 partitioner 分区的方式是 task 所在的并发 id 对 topic 总 partition 数取余：parallelInstanceId % partitions.length。</p><ul><li>此时如果 sink 为 4，paritition 为 1，则 4 个 task 往同一个 partition 中写数据。但当 sink task &lt; partition 个数时会有部分 partition 没有数据写入，例如 sink task 为2，partition 总数为 4，则后面两个 partition 将没有数据写入。</li><li>如果构建 FlinkKafkaProducer 时，partition 设置为 null，此时会使用 kafka producer 默认分区方式，非 key 写入的情况下，使用 round-robin 的方式进行分区，每个 task 都会轮循的写下游的所有 partition。该方式下游的 partition 数据会比较均衡，但是缺点是 partition 个数过多的情况下需要维持过多的网络连接，即每个 task 都会维持跟所有 partition 所在 broker 的连接。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/13-1024x576.jpg" alt="img"></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a><strong>容错</strong></h4><p>Flink kafka 09、010 版本下，通过 setLogFailuresOnly 为 false，setFlushOnCheckpoint 为 true，能达到 at-least-once 语义。setLogFailuresOnly，默认为 false，是控制写 kafka 失败时，是否只打印失败的 log 不抛异常让作业停止。setFlushOnCheckpoint，默认为 true，是控制是否在 checkpoint 时 fluse 数据到 kafka，保证数据已经写到 kafka。否则数据有可能还缓存在 kafka 客户端的 buffer 中，并没有真正写出到 kafka，此时作业挂掉数据即丢失，不能做到至少一次的语义。</p><p>Flink kafka 011 版本下，通过两阶段提交的 sink 结合 kafka 事务的功能，可以保证端到端精准一次。详细原理可以参考：<a href="https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka?__hstc=212828427.9e09d869892d3ee97973eb4d3a811b9f.1569201644494.1569389273878.1569459347925.10&__hssc=212828427.3.1569459347925&__hsfp=3591275194" target="_blank" rel="noopener">https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka</a></p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/14-1024x576.jpg" alt="img"></p><h3 id="一些疑问与解答"><a href="#一些疑问与解答" class="headerlink" title="一些疑问与解答"></a><strong>一些疑问与解答</strong></h3><h4 id="Q：在-Flink-consumer-的并行度的设置：是对应-topic-的-partitions-个数吗？要是有多个主题数据源，并行度是设置成总体的-partitions-数吗？"><a href="#Q：在-Flink-consumer-的并行度的设置：是对应-topic-的-partitions-个数吗？要是有多个主题数据源，并行度是设置成总体的-partitions-数吗？" class="headerlink" title="Q：在 Flink consumer 的并行度的设置：是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？"></a>Q：在 Flink consumer 的并行度的设置：是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？</h4><p><strong>A：</strong>这个并不是绝对的，跟 topic 的数据量也有关，如果数据量不大，也可以设置小于 partitions 个数的并发数。但不要设置并发数大于 partitions 总数，因为这种情况下某些并发因为分配不到 partition 导致没有数据处理。</p><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="Q：如果-partitioner-传-null-的时候是-round-robin-发到每一个-partition？如果有-key-的时候行为是-kafka-那种按照-key-分布到具体分区的行为吗？"><a href="#Q：如果-partitioner-传-null-的时候是-round-robin-发到每一个-partition？如果有-key-的时候行为是-kafka-那种按照-key-分布到具体分区的行为吗？" class="headerlink" title="Q：如果 partitioner 传 null 的时候是 round-robin 发到每一个 partition？如果有  key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？"></a>Q：如果 partitioner 传 null 的时候是 round-robin 发到每一个 partition？如果有  key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？</h4><p><strong>A：</strong>如果在构造 FlinkKafkaProducer 时，如果没有设置单独的 partitioner，则默认使用 FlinkFixedPartitioner，此时无论是带 key 的数据，还是不带 key。如果主动设置 partitioner 为 null 时，不带 key 的数据会 round-robin 的方式写出，带 key 的数据会根据 key，相同 key 数据分区的相同的 partition，如果 key 为 null，再轮询写。不带 key 的数据会轮询写各 partition。</p><h4 id="Q：如果-checkpoint-时间过长，offset-未提交到-kafka，此时节点宕机了，重启之后的重复消费如何保证呢？"><a href="#Q：如果-checkpoint-时间过长，offset-未提交到-kafka，此时节点宕机了，重启之后的重复消费如何保证呢？" class="headerlink" title="Q：如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？"></a>Q：如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？</h4><p><strong>A：</strong>首先开启 checkpoint 时 offset 是 Flink 通过状态 state 管理和恢复的，并不是从 kafka 的 offset 位置恢复。在 checkpoint 机制下，作业从最近一次 checkpoint 恢复，本身是会回放部分历史数据，导致部分数据重复消费，Flink 引擎仅保证计算状态的精准一次，要想做到端到端精准一次需要依赖一些幂等的存储系统或者事务操作。</p>]]></content>
    
    <summary type="html">
    
      本文主要分享 Flink connector 相关内容，重点介绍在生产环境中经常使用的 kafka connector 的基本的原理以及使用方法。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 进阶：增量 Checkpoint 详解</title>
    <link href="https://gjtmaster.github.io/2019/08/16/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9A%E5%A2%9E%E9%87%8F%20Checkpoint%20%E8%AF%A6%E8%A7%A3/"/>
    <id>https://gjtmaster.github.io/2019/08/16/Flink 进阶：增量 Checkpoint 详解/</id>
    <published>2019-08-16T10:52:50.000Z</published>
    <updated>2019-09-25T08:53:04.610Z</updated>
    
    <content type="html"><![CDATA[<p>原作者 | Stefan Ricther &amp; Chris Ward<br>原翻译 | 邱从贤（山智）</p><p>via | <a href="https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html" target="_blank" rel="noopener">https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html</a></p><p>Apache Flink 是一个有状态的流计算框架，状态是作业算子中已经处理过的内存状态，供后续处理时使用。状态在流计算很多复杂场景中非常重要，比如：</p><ul><li>保存所有历史记录，用来寻找某种记录模式</li><li>保存最近一分钟的所有记录，用于对每分钟的记录进行聚合统计</li><li>保存当前的模型参数，用于进行模型训练</li></ul><p>有状态的流计算框架必须有很好的容错性，才能在生产环境中发挥用处。这里的容错性是指，不管是发生硬件故障，还是程序异常，最终的结果不丢也不重。</p><p><strong>Flink 的容错性从一开始就是一个非常强大的特性，在遇到故障时，能够保证不丢不重，且对正常逻辑处理的性能影响很小。</strong></p><p>这里面的核心就是 checkpoint 机制，Flink 使用 checkpoint 机制来进行状态保证，在 Flink 中 checkpoint 是一个定时触发的全局异步快照，并持久化到持久存储系统上（通常是分布式文件系统）。发生故障后，Flink 选择从最近的一个快照进行恢复。有用户的作业状态达到 GB 甚至 TB 级别，对这么大的作业状态做一次 checkpoint 会非常耗时，耗资源，因此我们在 Flink 1.3 中引入了增量 checkpoint 机制。</p><p>在增量 checkpoint 之前，Flink 的每个 checkpoint 都包含作业的所有状态。我们在观察到状态在 checkpoint 之间的变化并没有那么大之后，支持了增量 checkpoint。增量 checkpoint 仅包含上次 checkpoint 和本次 checkpoint 之间状态的差异（也就是“增量”）。</p><p>对于状态非常大的作业，增量 checkpoint 对性能的提升非常明显。<strong>有生产用户反馈对于 TB 级别的作业，使用增量 checkpoint 后能将 checkpoint 的整体时间从 3 分钟降到 30 秒。</strong>这些时间节省主要归功于不需要在每次 checkpoint 都将所有状态写到持久化存储系统。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>当前，仅能够在 RocksDB StateBackend 上使用增量 checkpoint 机制，Flink 依赖 RocksDB 内部的备份机制来生成 checkpoint 文件。Flink 会自动清理掉之前的 checkpoint 文件, 因此增量 checkpoint 的历史记录不会无限增长。</p><p>为了在作业中开启增量 checkpoint，建议详细阅读 Apache Flink 的 checkpoint 文档，简单的说，你可以像之前一样开启 checkpoint，然后将构造函数的第二个参数设置为 true 来启用增量 checkpoint。</p><h3 id="Java-示例"><a href="#Java-示例" class="headerlink" title="Java 示例"></a>Java 示例</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(filebackend, <span class="literal">true</span>));</span><br></pre></td></tr></table></figure><h3 id="Scala-示例"><a href="#Scala-示例" class="headerlink" title="Scala 示例"></a>Scala 示例</h3><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(filebackend, <span class="keyword">true</span>))</span><br></pre></td></tr></table></figure><p>Flink 默认保留一个成功的 checkpoint，如果你需要保留多个的话，可以通过下面的配置进行设置：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">state</span>.checkpoints.num-retained</span><br></pre></td></tr></table></figure><h3 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h3><p>Flink 的增量 checkpoint 以 RocksDB 的 checkpoint 为基础。RocksDB 是一个 LSM 结构的 KV 数据库，把所有的修改保存在内存的可变缓存中（称为 memtable），所有对 memtable 中 key 的修改，会覆盖之前的 value，当前 memtable 满了之后，RocksDB 会将所有数据以有序的写到磁盘。当 RocksDB 将 memtable 写到磁盘后，整个文件就不再可变，称为有序字符串表（sstable）。</p><p>RocksDB 的后台压缩线程会将 sstable 进行合并，就重复的键进行合并，合并后的 sstable 包含所有的键值对，RocksDB 会删除合并前的 sstable。</p><p>在这个基础上，Flink 会记录上次 checkpoint 之后所有新生成和删除的 sstable，另外因为 sstable 是不可变的，Flink 用 sstable 来记录状态的变化。为此，<strong>Flink 调用 RocksDB 的 flush，强制将 memtable 的数据全部写到 sstable，并硬链到一个临时目录中。这个步骤是在同步阶段完成，其他剩下的部分都在异步阶段完成，不会阻塞正常的数据处理。</strong></p><p>Flink 将所有新生成的 sstable 备份到持久化存储（比如 HDFS，S3），并在新的 checkpoint 中引用。Flink 并不备份前一个 checkpoint 中已经存在的 sstable，而是引用他们。Flink 还能够保证所有的 checkpoint 都不会引用已经删除的文件，因为 RocksDB 中文件删除是由压缩完成的，压缩后会将原来的内容合并写成一个新的 sstable。因此，Flink 增量 checkpoint 能够切断 checkpoint 历史。</p><p>为了追踪 checkpoint 间的差距，备份合并后的 sstable 是一个相对冗余的操作。但是 Flink 会增量的处理，增加的开销通常很小，并且可以保持一个更短的 checkpoint 历史，恢复时从更少的 checkpoint 进行读取文件，因此我们认为这是值得的。</p><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/Checkpoint-.jpg" alt="img"></p><p>上图以一个有状态的算子为例，checkpoint 最多保留 2 个，上图从左到右分别记录每次 checkpoint 时本地的 RocksDB 状态文件，引用的持久化存储上的文件，以及当前 checkpoint 完成后文件的引用计数情况。</p><ul><li><strong>Checkpoint 1 的时候</strong>，本地 RocksDB 包含两个 sstable 文件，该 checkpoint 会把这两个文件备份到持久化存储，当 checkpoint 完成后，对这两个文件的引用计数进行加 1，引用计数使用键值对的方式保存，其中键由算子的当前并发以及文件名所组成。我们同时会维护一个引用计数中键到对应文件的隐射关系。</li><li><strong>Checkpoint 2 的时候</strong>，RocksDB 生成两个新的 sstable 文件，并且两个旧的文件还存在。Flink 会把两个新的文件进行备份，然后引用两个旧的文件，当 checkpoint 完成时，Flink 对这 4 个文件都进行引用计数 +1 操作。</li><li><strong>Checkpoint 3 的时候</strong>，RocksDB 将 sstable-(1)，sstable-(2) 以及 sstable-(3) 合并成 sstable-(1,2,3)，并且删除了三个旧文件，新生成的文件包含了三个删除文件的所有键值对。sstable-(4) 还继续存在，生成一个新的 sstable-(5) 文件。Flink 会将 sstable-(1,2,3) 和 sstable-(5) 备份到持久化存储，然后增加 sstable-4 的引用计数。由于保存的 checkpoint 数达到上限（2 个），因此会删除 checkpoint 1，然后对 checkpoint 1 中引用的所有文件（sstable-(1) 和 sstable-(2)）的引用计数进行 -1 操作。</li><li><strong>Checkpoint 4 的时候</strong>，RocksDB 将 sstable-(4)，sstable-(5) 以及新生成的 sstable-(6) 合并成一个新的 sstable-(4,5,6)。Flink 将 sstable-(4,5,6) 备份到持久化存储，并对 sstabe-(1,2,3) 和 sstable-(4,5,6) 进行引用计数 +1 操作，然后删除 checkpoint 2，并对 checkpoint 引用的文件进行引用计数 -1 操作。这个时候 sstable-(1)，sstable-(2) 以及 sstable-(3) 的引用计数变为 0，Flink 会从持久化存储删除这三个文件。</li></ul><h3 id="竞争问题以及并发-checkpoint"><a href="#竞争问题以及并发-checkpoint" class="headerlink" title="竞争问题以及并发 checkpoint"></a>竞争问题以及并发 checkpoint</h3><p>Flink 支持并发 checkpoint，有时晚触发的 checkpoint 会先完成，因此增量 checkpoint 需要选择一个正确的基准。Flink 仅会引用成功的 checkpoint 文件，从而防止引用一些被删除的文件。</p><h3 id="从-checkpoint-恢复以及性能"><a href="#从-checkpoint-恢复以及性能" class="headerlink" title="从 checkpoint 恢复以及性能"></a>从 checkpoint 恢复以及性能</h3><p>开启增量 checkpoint 之后，不需要再进行其他额外的配置。如果 Job 异常，Flink 的 JobMaster 会通知所有 task 从上一个成功的 checkpoint 进行恢复，不管是全量 checkpoint 还是增量 checkpoint。每个 TaskManager 会从持久化存储下载他们需要的状态文件。</p><p>尽管增量 checkpoint 能减少大状态下的 checkpoint 时间，但是天下没有免费的午餐，我们需要在其他方面进行舍弃。增量 checkpoint 可以减少 checkpoint 的总时间，但是也可能导致恢复的时候需要更长的时间<strong>。</strong>如果集群的故障频繁，Flink 的 TaskManager 需要从多个 checkpoint 中下载需要的状态文件（这些文件中包含一些已经被删除的状态），作业恢复的整体时间可能比不使用增量 checkpoint 更长。</p><p>另外在增量 checkpoint 情况下，我们不能删除旧 checkpoint 生成的文件，因为新的 checkpoint 会继续引用它们，这可能导致需要更多的存储空间，并且恢复的时候可能消耗更多的带宽。</p><p>关于控制便捷性与性能之间平衡的策略可以参考此文档：</p><p><strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html</a></strong></p>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Checkpoint机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 进阶：Runtime 核心机制剖析</title>
    <link href="https://gjtmaster.github.io/2019/08/15/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ARuntime%20%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2019/08/15/Flink 进阶：Runtime 核心机制剖析/</id>
    <published>2019-08-15T10:22:32.000Z</published>
    <updated>2019-09-25T08:53:04.571Z</updated>
    
    <content type="html"><![CDATA[<p>原作者：高赟（云骞）</p><h3 id="1-综述"><a href="#1-综述" class="headerlink" title="1. 综述"></a>1. 综述</h3><p>本文主要介绍 Flink Runtime 的作业执行的核心机制。本文将首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍在这个过程，Flink 是怎么进行资源管理、作业调度以及错误恢复的。最后，本文还将简要介绍 Flink Runtime 层当前正在进行的一些工作。</p><h3 id="2-Flink-Runtime-整体架构"><a href="#2-Flink-Runtime-整体架构" class="headerlink" title="2. Flink Runtime 整体架构"></a>2. Flink Runtime 整体架构</h3><p>Flink 的整体架构如图 1 所示。Flink 是可以运行在多种不同的环境中的，例如，它可以通过单进程多线程的方式直接运行，从而提供调试的能力。它也可以运行在 Yarn 或者 K8S 这种资源管理系统上面，也可以在各种云环境中执行。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-1.png" alt="img"></p><p>​                            图1. Flink 的整体架构，其中 Runtime 层对不同的执行环境提供了一套统一的分布式执行引擎。</p><p>针对不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。Flink 在 Runtime 层之上提供了 DataStream 和 DataSet 两套 API，分别用来编写流作业与批作业，以及一组更高级的 API 来简化特定作业的编写。本文主要介绍 Flink Runtime 层的整体架构。</p><p>Flink Runtime 层的主要架构如图 2 所示，它展示了一个 Flink 集群的基本结构。Flink Runtime 层的整个架构主要是在 FLIP-6 中实现的，整体来说，它采用了标准 master-slave 的结构，其中左侧白色圈中的部分即是 master，它负责管理整个集群中的资源和作业；而右侧的两个 TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-2.png" alt="img">                         </p><p>​                                     图2. Flink 集群的基本结构。Flink Runtime 层采用了标准的 master-slave 架构。</p><p>其中，Master 部分又包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程中。</p><p>基于上述结构，当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。</p><p>当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。这时根据 Session 和 Per-Job 模式的区别， TaskExecutor 可能已经启动或者尚未启动。如果是前者，此时 ResourceManager 中已有记录了 TaskExecutor 注册的资源，可以直接选取空闲资源进行分配。否则，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配。目前 Flink 中 TaskExecutor 的资源是通过 Slot 来描述的，一个 Slot 一般可以执行一个具体的 Task，但在一些情况下也可以执行多个相关联的 Task，这部分内容将在下文进行详述。ResourceManager 选择到空闲的 Slot 之后，就会通知相应的 TM “将该 Slot 分配分 JobManager XX ”，然后 TaskExecutor 进行相应的记录后，会向 JobManager 进行注册。JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。</p><p>TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><p>以上就是 Flink Runtime 层执行作业的基本流程。可以看出，Flink 支持两种不同的模式，即 Per-job 模式与 Session 模式。如图 3 所示，Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。与之对应，在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-3.png" alt="img">  </p><p>​                                                                   图3. Flink Runtime 支持两种作业执行的模式。</p><h3 id="3-资源管理与作业调度"><a href="#3-资源管理与作业调度" class="headerlink" title="3. 资源管理与作业调度"></a>3. 资源管理与作业调度</h3><p>本节对 Flink 中资源管理与作业调度的功能进行更深入的说明。实际上，作业调度可以看做是对资源和任务进行匹配的过程。如上节所述，在 Flink 中，资源是通过 Slot 来表示的，每个 Slot 可以用来执行不同的 Task。而在另一端，任务即 Job 中实际的 Task，它包含了待执行的用户逻辑。调度的主要目的就是为了给 Task 找到匹配的 Slot。逻辑上来说，每个 Slot 都应该有一个向量来描述它所能提供的各种资源的量，每个 Task 也需要相应的说明它所需要的各种资源的量。但是实际上在 1.9 之前，Flink 是不支持细粒度的资源描述的，而是统一的认为每个 Slot 提供的资源和 Task 需要的资源都是相同的。从 1.9 开始，Flink 开始增加对细粒度的资源匹配的支持的实现，但这部分功能目前仍在完善中。</p><p>作业调度的基础是首先提供对资源的管理，因此我们首先来看下 Flink 中资源管理的实现。如上文所述，Flink 中的资源是由 TaskExecutor 上的 Slot 来表示的。如图 4 所示，在 ResourceManager 中，有一个子组件叫做 SlotManager，它维护了当前集群中所有 TaskExecutor 上的 Slot 的信息与状态，如该 Slot 在哪个 TaskExecutor 中，该 Slot 当前是否空闲等。当 JobManger 来为特定 Task 申请资源的时候，根据当前是 Per-job 还是 Session 模式，ResourceManager 可能会去申请资源来启动新的 TaskExecutor。当 TaskExecutor 启动之后，它会通过服务发现找到当前活跃的 ResourceManager 并进行注册。在注册信息中，会包含该 TaskExecutor中所有 Slot 的信息。 ResourceManager 收到注册信息后，其中的 SlotManager 就会记录下相应的 Slot 信息。当 JobManager 为某个 Task 来申请资源时， SlotManager 就会从当前空闲的 Slot 中按一定规则选择一个空闲的 Slot 进行分配。当分配完成后，如第 2 节所述，RM 会首先向 TaskManager 发送 RPC 要求将选定的 Slot 分配给特定的 JobManager。TaskManager 如果还没有执行过该 JobManager 的 Task 的话，它需要首先向相应的 JobManager 建立连接，然后发送提供 Slot 的 RPC 请求。在 JobManager 中，所有 Task 的请求会缓存到 SlotPool 中。当有 Slot 被提供之后，SlotPool 会从缓存的请求中选择相应的请求并结束相应的请求过程。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-4.png" alt="img"></p><p>​                                                                   图4. Flink 中资源管理功能各模块交互关系。</p><p>当 Task 结束之后，无论是正常结束还是异常结束，都会通知 JobManager 相应的结束状态，然后在 TaskManager 端将 Slot 标记为已占用但未执行任务的状态。JobManager 会首先将相应的 Slot 缓存到 SlotPool 中，但不会立即释放。这种方式避免了如果将 Slot 直接还给 ResourceManager，在任务异常结束之后需要重启时，需要立刻重新申请 Slot 的问题。通过延时释放，Failover 的 Task 可以尽快调度回原来的 TaskManager，从而加快 Failover 的速度。当 SlotPool 中缓存的 Slot 超过指定的时间仍未使用时，SlotPool 就会发起释放该 Slot 的过程。与申请 Slot 的过程对应，SlotPool 会首先通知 TaskManager 来释放该 Slot，然后 TaskExecutor 通知 ResourceManager 该 Slot 已经被释放，从而最终完成释放的逻辑。</p><p>除了正常的通信逻辑外，在 ResourceManager 和 TaskExecutor 之间还存在定时的心跳消息来同步 Slot 的状态。在分布式系统中，消息的丢失、错乱不可避免，这些问题会在分布式系统的组件中引入不一致状态，如果没有定时消息，那么组件无法从这些不一致状态中恢复。此外，当组件之间长时间未收到对方的心跳时，就会认为对应的组件已经失效，并进入到 Failover 的流程。</p><p>在 Slot 管理基础上，Flink 可以将 Task 调度到相应的 Slot 当中。如上文所述，Flink 尚未完全引入细粒度的资源匹配，默认情况下，每个 Slot 可以分配给一个 Task。但是，这种方式在某些情况下会导致资源利用率不高。如图 5 所示，假如 A、B、C 依次执行计算逻辑，那么给 A、B、C 分配分配单独的 Slot 就会导致资源利用率不高。为了解决这一问题，Flink 提供了 Share Slot 的机制。如图 5 所示，基于 Share Slot，每个 Slot 中可以部署来自不同 JobVertex 的多个任务，但是不能部署来自同一个 JobVertex 的 Task。如图5所示，每个 Slot 中最多可以部署同一个 A、B 或 C 的 Task，但是可以同时部署 A、B 和 C 的各一个 Task。当单个 Task 占用资源较少时，Share Slot 可以提高资源利用率。 此外，Share Slot 也提供了一种简单的保持负载均衡的方式。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-5.png" alt="img"></p><p>​                                                                       图5.Flink Share Slot 示例。<br>使用 Share Slot 可以在每个 Slot 中部署来自不同 JobVertex 的多个 Task。</p><p>基于上述 Slot 管理和分配的逻辑，JobManager 负责维护作业中 Task执行的状态。如上文所述，Client 端会向 JobManager 提交一个 JobGraph，它代表了作业的逻辑结构。JobManager 会根据 JobGraph 按并发展开，从而得到 JobManager 中关键的 ExecutionGraph。ExecutionGraph 的结构如图 5 所示，与 JobGraph 相比，ExecutionGraph 中对于每个 Task 与中间结果等均创建了对应的对象，从而可以维护这些实体的信息与状态。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-6.png" alt="img"></p><p>​                                                               图6.Flink 中的 JobGraph 与 ExecutionGraph。<br>ExecutionGraph 是 JobGraph 按并发展开所形成的，它是 JobMaster 中的核心数据结构。</p><p>在一个 Flink Job 中是包含多个 Task 的，因此另一个关键的问题是在 Flink 中按什么顺序来调度 Task。如图 7 所示，目前 Flink 提供了两种基本的调度逻辑，即 Eager 调度与 Lazy From Source。Eager 调度如其名子所示，它会在作业启动时申请资源将所有的 Task 调度起来。这种调度算法主要用来调度可能没有终止的流作业。与之对应，Lazy From Source 则是从 Source 开始，按拓扑顺序来进行调度。简单来说，Lazy From Source 会先调度没有上游任务的 Source 任务，当这些任务执行完成时，它会将输出数据缓存到内存或者写入到磁盘中。然后，对于后续的任务，当它的前驱任务全部执行完成后，Flink 就会将这些任务调度起来。这些任务会从读取上游缓存的输出数据进行自己的计算。这一过程继续进行直到所有的任务完成计算。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-7.png" alt="img"></p><p>​                                                                        图7. Flink 中两种基本的调度策略。<br>其中 Eager 调度适用于流作业，而Lazy From Source 适用于批作业。</p><h3 id="4-错误恢复"><a href="#4-错误恢复" class="headerlink" title="4. 错误恢复"></a>4. 错误恢复</h3><p>在 Flink 作业的执行过程中，除正常执行的流程外，还有可能由于环境等原因导致各种类型的错误。整体上来说，错误可能分为两大类：Task 执行出现错误或 Flink 集群的 Master 出现错误。由于错误不可避免，为了提高可用性，Flink 需要提供自动错误恢复机制来进行重试。</p><p>对于第一类 Task 执行错误，Flink 提供了多种不同的错误恢复策略。如图 8 所示，第一种策略是 Restart-all，即直接重启所有的 Task。对于 Flink 的流任务，由于 Flink 提供了 Checkpoint 机制，因此当任务重启后可以直接从上次的 Checkpoint 开始继续执行。因此这种方式更适合于流作业。第二类错误恢复策略是 Restart-individual，它只适用于 Task 之间没有数据传输的情况。这种情况下，我们可以直接重启出错的任务。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-8.png" alt="img"></p><p>​                                                                           图8.Restart-all 错误恢复策略示例。<br>​                                                                            该策略会直接重启所有的 Task。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-9.png" alt="img"></p><p>​                                                                              图9.Restart-individual 错误恢复策略示例。<br>​                                     该策略只适用于 Task之间不需要数据传输的作业，对于这种作业可以只重启出现错误的 Task。</p><p>由于 Flink 的批作业没有 Checkpoint 机制，因此对于需要数据传输的作业，直接重启所有 Task 会导致作业从头计算，从而导致一定的性能问题。为了增强对 Batch 作业，Flink 在1.9中引入了一种新的Region-Based的Failover策略。在一个 Flink 的 Batch 作业中 Task 之间存在两种数据传输方式，一种是 Pipeline 类型的方式，这种方式上下游 Task 之间直接通过网络传输数据，因此需要上下游同时运行；另外一种是 Blocking 类型的试，如上节所述，这种方式下，上游的 Task 会首先将数据进行缓存，因此上下游的 Task 可以单独执行。基于这两种类型的传输，Flink 将 ExecutionGraph 中使用 Pipeline 方式传输数据的 Task 的子图叫做 Region，从而将整个 ExecutionGraph 划分为多个子图。可以看出，Region 内的 Task 必须同时重启，而不同 Region 的 Task 由于在 Region 边界存在 Blocking 的边，因此，可以单独重启下游 Region 中的 Task。</p><p>基于这一思路,如果某个 Region 中的某个 Task 执行出现错误，可以分两种情况进行考虑。如图 8 所示，如果是由于 Task 本身的问题发生错误，那么可以只重启该 Task 所属的 Region 中的 Task，这些 Task 重启之后，可以直接拉取上游 Region 缓存的输出结果继续进行计算。</p><p>另一方面，如图如果错误是由于读取上游结果出现问题，如网络连接中断、缓存上游输出数据的 TaskExecutor 异常退出等，那么还需要重启上游 Region 来重新产生相应的数据。在这种情况下，如果上游 Region 输出的数据分发方式不是确定性的（如 KeyBy、Broadcast 是确定性的分发方式，而 Rebalance、Random 则不是，因为每次执行会产生不同的分发结果），为保证结果正确性，还需要同时重启上游 Region 所有的下游 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-10.png" alt="img"></p><p>​                                                                   图10.Region-based 错误恢复策略示例一。<br>​                                             如果是由于下游任务本身导致的错误，可以只重启下游对应的 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-11.png" alt="img"></p><p>​                                                                       图11.Region-based 错误恢复策略示例二。<br>​                                                     如果是由于上游失败导致的错误，那么需要同时重启上游的 Region 和下游的 Region。实际上，如果下游的输出使用了非确定的数据分割方式，为了保持数据一致性，还需要同时重启所有上游 Region 的下游 Region。</p><p>除了 Task 本身执行的异常外，另一类异常是 Flink 集群的 Master 进行发生异常。目前 Flink 支持启动多个 Master 作为备份，这些 Master 可以通过 ZK 来进行选主，从而保证某一时刻只有一个 Master 在运行。当前活路的 Master 发生异常时,某个备份的 Master 可以接管协调的工作。为了保证 Master 可以准确维护作业的状态，Flink 目前采用了一种最简单的实现方式，即直接重启整个作业。实际上，由于作业本身可能仍在正常运行，因此这种方式存在一定的改进空间。</p><h3 id="5-未来展望"><a href="#5-未来展望" class="headerlink" title="5. 未来展望"></a>5. 未来展望</h3><p>Flink目前仍然在Runtime部分进行不断的迭代和更新。目前来看，Flink未来可能会在以下几个方式继续进行优化和扩展：</p><ul><li><strong>更完善的资源管理</strong>：从 1.9 开始 Flink 开始了对细粒度资源匹配的支持。基于细粒度的资源匹配，用户可以为 TaskExecutor 和 Task 设置实际提供和使用的 CPU、内存等资源的数量，Flink 可以按照资源的使用情况进行调度。这一机制允许用户更大范围的控制作业的调度，从而为进一步提高资源利用率提供了基础。</li><li><strong>统一的 Stream 与 Batch</strong>：Flink 目前为流和批分别提供了 DataStream 和 DataSet 两套接口，在一些场景下会导致重复实现逻辑的问题。未来 Flink 会将流和批的接口都统一到 DataStream 之上。</li><li><strong>更灵活的调度策略</strong>：Flink 从 1.9 开始引入调度插件的支持，从而允许用户来扩展实现自己的调度逻辑。未来 Flink 也会提供更高性能的调度策略的实现。</li><li><strong>Master Failover 的优化</strong>：如上节所述，目前 Flink 在 Master Failover 时需要重启整个作业，而实际上重启作业并不是必须的逻辑。Flink 未来会对 Master failover 进行进一步的优化来避免不必要的作业重启。</li></ul>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Runtime 核心机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 深度解析</title>
    <link href="https://gjtmaster.github.io/2018/11/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2018/11/18/FlinkSQL深度解析/</id>
    <published>2018-11-18T10:19:05.000Z</published>
    <updated>2019-10-28T06:26:34.231Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据计算领域对SQL的应用"><a href="#大数据计算领域对SQL的应用" class="headerlink" title="大数据计算领域对SQL的应用"></a>大数据计算领域对SQL的应用</h1><h2 id="离线计算（批计算）"><a href="#离线计算（批计算）" class="headerlink" title="离线计算（批计算）"></a>离线计算（批计算）</h2><p>提及大数据计算领域不得不说MapReduce计算模型，MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法，并发于2004年发表了论文Simplified Data Processing on Large Clusters。论文发表之后Apache 开源社区参考Google MapReduce，基于Java设计开发了一个称为Hadoop的开源MapReduce并行计算框架。很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并了解MapReduce的运行原理，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛，进而也成功的将SQL应用到了大数据计算领域当中来。</p><h2 id="实时计算（流计算）"><a href="#实时计算（流计算）" class="headerlink" title="实时计算（流计算）"></a>实时计算（流计算）</h2><p>SQL不仅仅被成功的应用到了离线计算，SQL的易用性也吸引了流计算产品，目前最热的Spark，Flink也纷纷支持了SQL，尤其是Flink支持的更加彻底，集成了Calcite，完全遵循ANSI-SQL标准。Apache Flink在low-level API上面用DataSet支持批计算，用DataStream支持流计算，但在High-Level API上面利用SQL将流与批进行了统一，使得用户编写一次SQL既可以在流计算中使用，又可以在批计算中使用，为既有流计算业务，又有批计算业务的用户节省了大量开发成本。</p><h1 id="SQL高性能与简洁性"><a href="#SQL高性能与简洁性" class="headerlink" title="SQL高性能与简洁性"></a>SQL高性能与简洁性</h1><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>SQL经过传统数据库领域几十年的不断打磨，查询优化器已经能够极大的优化SQL的查询性能，Apache Flink 应用Calcite进行查询优化，复用了大量数据库查询优化规则，在性能上不断追求极致，能够让用户关心但不用担心性能问题。如下图(Alibaba 对 Apache Flink 进行架构优化后的组件栈)</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN43HQMdZty0IxMowiaBs1oaPZwyEeVpLvkLakk4V51uz6iaMbz9toslicw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>相对于DataStream而言，SQL会经过Optimization模块透明的为用户进行查询优化，用户专心编写自己的业务逻辑，不用担心性能，却能得到最优的查询性能!</p><h2 id="简洁"><a href="#简洁" class="headerlink" title="简洁"></a>简洁</h2><p>就简洁性而言，SQL与DataSet和DataStream相比具有很大的优越性，我们先用一个WordCount示例来直观的查看用户的代码量：</p><p>DataStream/DataSetAPI</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">... <span class="comment">//省略初始化代码</span></span><br><span class="line"><span class="comment">// 核心逻辑</span></span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> WordCount.Tokenizer()).keyBy(<span class="keyword">new</span> <span class="built_in">int</span>[]&#123;<span class="number">0</span>&#125;).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// flatmap 代码定义</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> class Tokenizer implements FlatMapFunction&lt;<span class="keyword">String</span>, Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> Tokenizer() &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> flatMap(<span class="keyword">String</span> value, Collector&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; out) &#123;</span><br><span class="line"><span class="keyword">String</span>[] tokens = value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>);</span><br><span class="line"><span class="keyword">String</span>[] var4 = tokens;</span><br><span class="line"><span class="built_in">int</span> var5 = tokens.length;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">int</span> var6 = <span class="number">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class="line"><span class="keyword">String</span> token = var4[var6];</span><br><span class="line"><span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">out.collect(<span class="keyword">new</span> Tuple2(token, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SQL</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...//省略初始化代码</span><br><span class="line"><span class="keyword">SELECT</span> word, <span class="built_in">COUNT</span>(word) <span class="keyword">FROM</span> tab <span class="keyword">GROUP</span> <span class="keyword">BY</span> word;</span><br></pre></td></tr></table></figure><p>我们直观的体会到相同的统计功能使用SQL的简洁性。</p><h1 id="Flink-SQL-Job的组成"><a href="#Flink-SQL-Job的组成" class="headerlink" title="Flink SQL Job的组成"></a>Flink SQL Job的组成</h1><p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这三个部分，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNmwGyZgbFaPfs2bjXzSGdh9jSTKnxrYlSbLzwMUn95uVLOuHcueGLnw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p><ul><li>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</li><li>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</li><li>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</li></ul><h1 id="Flink-SQL-核心算子"><a href="#Flink-SQL-核心算子" class="headerlink" title="Flink SQL 核心算子"></a>Flink SQL 核心算子</h1><p>目前Flink SQL支持Union，Join，Projection,Difference, Intersection以及Window等大多数传统数据库支持的操作，接下来为大家分别进行简单直观的介绍。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>为了很好的体验和理解Apache Flink SQL算子我们需要先准备一下测试环境，我们选择IDEA，以ITCase测试方式来进行体验。IDEA 安装这里不占篇幅介绍了，相信大家能轻松搞定！我们进行功能体验有两种方式，具体如下：</p><h2 id="源码方式"><a href="#源码方式" class="headerlink" title="源码方式"></a>源码方式</h2><p>对于开源爱好者可能更喜欢源代码方式理解和体验Apache Flink SQL功能，那么我们需要下载源代码并导入到IDEA中：</p><ul><li>下载源码：</li></ul><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载源代码</span></span><br><span class="line">git clone https:<span class="comment">//github.com/apache/flink.git study</span></span><br><span class="line"><span class="comment">// 进入源码目录</span></span><br><span class="line">cd study</span><br><span class="line"><span class="comment">// 拉取稳定版release-1.6</span></span><br><span class="line">git fetch origin <span class="built_in">release</span><span class="number">-1.6</span>:<span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//切换到稳定版</span></span><br><span class="line">git checkout <span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//将依赖安装到本地mvn仓库，耐心等待需要一段时间</span></span><br><span class="line">mvn clean install -DskipTests</span><br></pre></td></tr></table></figure><ul><li>导入到IDEA<br>将Flink源码导入到IDEA过程这里不再占用篇幅，导入后确保在IDEA中可以运行 <code>org.apache.flink.table.runtime.stream.sql.SqlITCase</code> 并测试全部通过，即证明体验环境已经完成，即证明体验环境已经完成。如下图所示：</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNo09iaFxmhAfdNGPSjCc6qnjDUWyZaCO8UBSkyUJy1EEcicoSv4qa8wzg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上图运行测试后显示测试通过，我们就可以继续下面的Apache Flink SQL功能体验了。</p><h2 id="依赖Flink包方式"><a href="#依赖Flink包方式" class="headerlink" title="依赖Flink包方式"></a>依赖Flink包方式</h2><p>我们还有一种更简单直接的方式，就是新建一个mvn项目，并在pom中添加如下依赖：</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">table.version</span>&gt;</span>1.6-SNAPSHOT<span class="tag">&lt;/<span class="name">table.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>完成环境准备后，我们开始准备测试数据和写一个简单的测试类。</p><h2 id="示例数据及测试类"><a href="#示例数据及测试类" class="headerlink" title="示例数据及测试类"></a>示例数据及测试类</h2><h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><ul><li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li></ul><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><ul><li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li></ul><table><thead><tr><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr></tbody></table><ul><li>Item_tab<br> 商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li></ul><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>30</td></tr><tr><td>ITEM004</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td></tr></tbody></table><ul><li>PageAccess_tab<br>页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><ul><li>PageAccessCount_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userCount</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>100</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>86</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>210</td><td>2017.11.11 10:06:00</td></tr><tr><td>BeiJing</td><td>33</td><td>2017.11.11 10:10:00</td></tr><tr><td>ShangHai</td><td>129</td><td>2017.11.11 12:10:00</td></tr></tbody></table><ul><li>PageAccessSession_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:01:00</td></tr><tr><td>ShangHai</td><td>U0012</td><td>2017-11-11 10:02:00</td></tr><tr><td>ShangHai</td><td>U0013</td><td>2017-11-11 10:03:00</td></tr><tr><td>ShangHai</td><td>U0015</td><td>2017-11-11 10:05:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U0110</td><td>2017-11-11 10:10:00</td></tr><tr><td>ShangHai</td><td>U2010</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0410</td><td>2017-11-11 12:16:00</td></tr></tbody></table><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><p>我们创建一个<code>SqlOverviewITCase.scala</code> 用于接下来介绍Flink SQL算子的功能体验。代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.<span class="type">StateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.<span class="type">MemoryStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.<span class="type">RichSinkFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">TableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.junit.rules.<span class="type">TemporaryFolder</span></span><br><span class="line"><span class="keyword">import</span> org.junit.&#123;<span class="type">Rule</span>, <span class="type">Test</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqlOverviewITCase</span> </span>&#123;</span><br><span class="line"><span class="keyword">val</span> _tempFolder = <span class="keyword">new</span> <span class="type">TemporaryFolder</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Rule</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFolder</span></span>: <span class="type">TemporaryFolder</span> = _tempFolder</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStateBackend</span></span>: <span class="type">StateBackend</span> = &#123;</span><br><span class="line"><span class="keyword">new</span> <span class="type">MemoryStateBackend</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 客户表数据</span></span><br><span class="line"><span class="keyword">val</span> customer_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">customer_data.+=((<span class="string">"c_001"</span>, <span class="string">"Kevin"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_002"</span>, <span class="string">"Sunny"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_003"</span>, <span class="string">"JinCheng"</span>, <span class="string">"from HeBei"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 订单表数据</span></span><br><span class="line"><span class="keyword">val</span> order_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">order_data.+=((<span class="string">"o_001"</span>, <span class="string">"c_002"</span>, <span class="string">"2018-11-05 10:01:01"</span>, <span class="string">"iphone"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_002"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:01:55"</span>, <span class="string">"ipad"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_003"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:03:44"</span>, <span class="string">"flink book"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 商品销售表数据</span></span><br><span class="line"><span class="keyword">val</span> item_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="number">20</span>, <span class="string">"ITEM001"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="number">50</span>, <span class="string">"ITEM002"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">30</span>, <span class="string">"ITEM003"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">60</span>, <span class="string">"ITEM004"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365780000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="number">40</span>, <span class="string">"ITEM005"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="number">20</span>, <span class="string">"ITEM006"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365960000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366020000</span>L, (<span class="number">1510366020000</span>L, <span class="number">70</span>, <span class="string">"ITEM007"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366020000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366080000</span>L, (<span class="number">1510366080000</span>L, <span class="number">20</span>, <span class="string">"ITEM008"</span>, <span class="string">"Clothes"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">151036608000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据</span></span><br><span class="line"><span class="keyword">val</span> pageAccess_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问量表数据2</span></span><br><span class="line"><span class="keyword">val</span> pageAccessCount_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="number">100</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="number">86</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="string">"BeiJing"</span>, <span class="number">210</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="number">33</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="number">129</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据3</span></span><br><span class="line"><span class="keyword">val</span> pageAccessSession_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0012"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0013"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0015"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373760000</span>L, (<span class="number">1510373760000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0410"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373760000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">procTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将order_tab, customer_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> customer = env.fromCollection(customer_data).toTable(tEnv).as(<span class="symbol">'c_id</span>, <span class="symbol">'c_name</span>, <span class="symbol">'c_desc</span>)</span><br><span class="line">    <span class="keyword">val</span> order = env.fromCollection(order_data).toTable(tEnv).as(<span class="symbol">'o_id</span>, <span class="symbol">'c_id</span>, <span class="symbol">'o_time</span>, <span class="symbol">'o_desc</span>)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"order_tab"</span>, order)</span><br><span class="line">    tEnv.registerTable(<span class="string">"customer_tab"</span>, customer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rowTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setStateBackend(getStateBackend)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将item_tab, pageAccess_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> item =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">String</span>)](item_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'onSellTime</span>, <span class="symbol">'price</span>, <span class="symbol">'itemID</span>, <span class="symbol">'itemType</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccess =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccess_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessCount =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">Int</span>)](pageAccessCount_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'accessCount</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessSession =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccessSession_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"item_tab"</span>, item)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccess_tab"</span>, pageAccess)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessCount_tab"</span>, pageAccessCount)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessSession_tab"</span>, pageAccessSession)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testSelect</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">"替换想要测试的SQL"</span></span><br><span class="line">    <span class="comment">// 非window 相关用 procTimePrint(sql)</span></span><br><span class="line">    <span class="comment">// Window 相关用 rowTimePrint(sql)</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义Sink</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RetractingSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> retractedResults: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(v: (<span class="type">Boolean</span>, <span class="type">Row</span>)) &#123;</span><br><span class="line">    retractedResults.synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> value = v._2.toString</span><br><span class="line">    <span class="keyword">if</span> (v._1) &#123;</span><br><span class="line">    retractedResults += value</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = retractedResults.indexOf(value)</span><br><span class="line">    <span class="keyword">if</span> (idx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    retractedResults.remove(idx)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Tried to retract a value that wasn't added first. "</span> +</span><br><span class="line">    <span class="string">"This is probably an incorrectly implemented test. "</span> +</span><br><span class="line">    <span class="string">"Try to set the parallelism of the sink to 1."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    retractedResults.sorted.foreach(println(_))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Water mark 生成器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventTimeSourceFunction</span>[<span class="type">T</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">dataWithTimestampList: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span></span>), <span class="title">Long</span>]]) <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(w) =&gt; ctx.emitWatermark(<span class="keyword">new</span> <span class="type">Watermark</span>(w))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h2><p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某</p><p>些列, 如下图所示:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNibZ25Mic3yIbEcG8icTWkkJiaMcTr5oq0wTkT7rdZ5EkUpXEp26ZKVTrKw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_name, <span class="keyword">CONCAT</span>(c_name, <span class="string">' come '</span>, c_desc) <span class="keyword">as</span> <span class="keyword">desc</span> <span class="keyword">FROM</span> customer_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><table><thead><tr><th>c_name</th><th>desc</th></tr></thead><tbody><tr><td>Kevin</td><td>Kevin come from JinLin</td></tr><tr><td>Sunny</td><td>Sunny come from JinLin</td></tr><tr><td>Jincheng</td><td>Jincheng come from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是携带 <code>DISTINCT</code> 关键字，示例如下：</p><p><strong>SQL 示例</strong></p><p>在订单表查询所有的客户id，消除重复客户id, 如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c_id <span class="keyword">FROM</span> order_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th></tr></thead><tbody><tr><td>c_001</td></tr><tr><td>c_002</td></tr></tbody></table><h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> 用于从数据集/流中过滤数据，与<span class="keyword">SELECT</span>一起使用，语法遵循<span class="keyword">ANSI</span>-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</span><br></pre></td></tr></table></figure><p><strong>SQL 示例</strong></p><p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id = <span class="string">'c_001'</span> <span class="keyword">OR</span> c_id = <span class="string">'c_003'</span>;</span><br></pre></td></tr></table></figure><p><strong>Result</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>AND</code>， <code>OR</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 <code>WHERE</code> 可以结合<code>IN</code>,<code>NOT IN</code>联合使用，具体如下：</p><p><strong>SQL 示例 (IN 常量)</strong></p><p>使用 <code>IN</code> 在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="string">'c_001'</span>, <span class="string">'c_003'</span>);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>SQL 示例 (IN 子查询)</strong></p><p>使用 <code>IN</code>和 子查询 在<code>customer_tab</code>查询已经下过订单的客户信息，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="keyword">SELECT</span> c_id <span class="keyword">FROM</span> order_tab);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr></tbody></table><p><strong>IN/NOT IN 与关系代数</strong></p><p>如上介绍IN是关系代数中的Intersection， NOT IN是关系代数的Difference， 如下图示意：</p><ul><li>IN(Intersection</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblp6icardxtCKeaf7RHrFbN6TVbyqyGOGuSWYY7uY3DJb5ODYsOqvv1mWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li><li>NOT IN(Difference）</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpcofHeFia7icQorYjiaGmHO9yiclrFaMCk3l6sBuQa2sm5QlrtepLOrdIMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li></ul><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpoeicHXKPbhnpAKEe8cMRzf4WHDQiagwAHRIlH6icqn107hHkiaeJh2CWDQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>SQL 示例</strong></p><p>将order_tab信息按customer_tab分组统计订单数量，简单示例如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT c_id, count(o_id) as o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY c_id;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>o_count</th></tr></thead><tbody><tr><td>c_001</td><td>2</td></tr><tr><td>c_002</td><td>1</td></tr></tbody></table><p><strong>特别说明</strong></p><p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p><p><strong>SQL 示例</strong></p><p>按时间进行分组，查询每分钟的订单数量，如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT SUBSTRING(o_time, 1, 16) AS o_time_min, count(o_id) AS o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY SUBSTRING(o_time, 1, 16)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>o_time_min</th><th>o_count</th></tr></thead><tbody><tr><td>2018-11-05 10:01</td><td>2</td></tr><tr><td>2018-11-05 10:03</td><td>1</td></tr></tbody></table><p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p><h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNpaHuv6VYq4P9Zyke2cuHCIwibTbicJpXicRJWemZsJN6Y1Nq3vKVNzpNg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p><h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNALmfI36VBMGEontFaDkRleLsSbErPHtRYvT0dBQ4ic6kwQD3AEJIhfQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p><h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p><ul><li>JOIN - INNER JOIN</li><li>LEFT JOIN - LEFT OUTER JOIN</li><li>RIGHT JOIN - RIGHT OUTER JOIN</li><li>FULL JOIN - FULL OUTER JOIN</li></ul><p>JOIN与关系代数的Join语义相同，具体如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN8qxel16siciaMAH8x3aQCcZ6q0ic8QrtZtco3D9frZFjHfZYj4q33hszg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例 (JOIN)</strong></p><p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr></tbody></table><p><strong>SQL 示例 (LEFT JOIN)</strong></p><p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNLcrI9iar3vKlgxMwRceIAMCZm2uNbhUWINhK1yRAllPdkwVJ1PcHhqQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>对应的SQL语句如下(LEFT JOIN)：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, <span class="built_in">T2</span>.ColC, ColE FROM TI LEFT <span class="keyword">JOIN </span><span class="built_in">T2</span> ON <span class="built_in">T1</span>.ColC = <span class="built_in">T2</span>.ColC <span class="comment">;</span></span><br></pre></td></tr></table></figure><ul><li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li></ul><p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr></tbody></table><p><strong>特别说明</strong></p><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p><h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p><h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p><ul><li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li><li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li></ul><h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p><p><strong>语义</strong></p><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN6yotUibfmTVgbnFd7dvC4tgfFEddh0xJ6PzC9wzLDgiaemZoCCjVNxaw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p><p><strong>语法</strong></p><p>Bounded ROWS OVER Window 语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">ROWS</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | rowCount) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>rowCount - 是定义根据当前行开始向前追溯几行元素。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> onSellTime </span><br><span class="line">        <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>50</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>60</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p><p><strong>语义</strong></p><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNTtvBlDvT0wfxJvTOL8e9CbVJg6YVxAfLMKskjXibicrCeOGgIZxAJxdw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p><p><strong>语法</strong></p><p>Bounded RANGE OVER Window的语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">RANGE</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | timeInterval) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li></ul><p><strong>SQL 示例</strong></p><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下（Bounded RANGE OVER Windo</strong>w）</p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>60</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>40</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><p><strong>特别说明</strong></p><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p><ul><li>GROUP BY - <code>SELECT d, MAX(c) FROM table GROUP BY d</code></li><li>OVER Window = <code>SELECT a, b, c, d, MAX(c) OVER(PARTITION BY d, ORDER BY ProcTime())</code><br>如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li></ul><h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p><ul><li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li><li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li><li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li></ul><p>说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p><h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><p><strong>语义</strong></p><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN9PPeHiaOUQib8BG2xs3YPxpN8EYibnRNkFxgicW1kPrNeicE8vpcUB7tspA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Tumble 滚动窗口对应的语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)], </span><br><span class="line">    [TUMBLE_END(timeCol, size)], </span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] - 决定了流是Keyed还是/Non-Keyed;</li><li>TUMBLE_START - 窗口开始时间;</li><li>TUMBLE_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>size - 表示窗口的大小，如 秒，分钟，小时，天。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region,</span><br><span class="line">    TUMBLE_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    TUMBLE_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv</span><br><span class="line"><span class="keyword">FROM</span> pageAccess_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, TUMBLE(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p><p><strong>语义</strong></p><p>Hop 滑动窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNCwyBicMTKEicSxibebwTfwvImiaA2TlN0FuM0wuG6zAibYyk5JrfBTmrwEA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Hop 滑动窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,  </span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggN(colN) </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>HOP_START - 窗口开始时间;</li><li>HOP_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>slide - 是滑动步伐的大小；</li><li>size - 是窗口的大小，如 秒，分钟，小时，天；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  HOP_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">  HOP_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">  <span class="keyword">SUM</span>(accessCount) <span class="keyword">AS</span> accessCount  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessCount_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>winStart</th><th>winEnd</th><th>accessCount</th></tr></thead><tbody><tr><td>2017-11-11 01:55:00.0</td><td>2017-11-11 02:05:00.0</td><td>186</td></tr><tr><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:10:00.0</td><td>396</td></tr><tr><td>2017-11-11 02:05:00.0</td><td>2017-11-11 02:15:00.0</td><td>243</td></tr><tr><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:20:00.0</td><td>33</td></tr><tr><td>2017-11-11 04:05:00.0</td><td>2017-11-11 04:15:00.0</td><td>129</td></tr><tr><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:20:00.0</td><td>129</td></tr></tbody></table><h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p><p>语义</p><p>Session 会话窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNfhx9EQvp4OyBYHue50QEzW3qZfxeRV5DCb8CkcneoGjadj7NqNHq9w/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Seeeion 会话窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    SESSION_START(timeCol, gap) AS winStart,  </span><br><span class="line">    SESSION_END(timeCol, gap) AS winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>SESSION_START - 窗口开始时间；</li><li>SESSION_END - 窗口结束时间；</li><li>timeCol - 是流表中表示时间字段；</li><li>gap - 是窗口数据非活跃周期的时长；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessSession_tab</code>测试数据，我们按地域统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region, </span><br><span class="line">    SESSION_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd, </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessSession_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, <span class="keyword">SESSION</span>(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:13:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:01:00.0</td><td>2017-11-11 02:08:00.0</td><td>4</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:14:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:16:00.0</td><td>2017-11-11 04:19:00.0</td><td>1</td></tr></tbody></table><h2 id="UDX"><a href="#UDX" class="headerlink" title="UDX"></a>UDX</h2><p>Apache Flink 除了提供了大部分ANSI-SQL的核心算子，也为用户提供了自己编写业务代码的机会，那就是User-Defined Function,目前支持如下三种 User-Defined Function：</p><ul><li>UDF - User-Defined Scalar Function</li><li>UDTF - User-Defined Table Function</li><li>UDAF - User-Defined Aggregate Funciton</li></ul><p>UDX都是用户自定义的函数，那么Apache Flink框架为啥将自定义的函数分成三类呢？是根据什么划分的呢？Apache Flink对自定义函数进行分类的依据是根据函数语义的不同，函数的输入和输出不同来分类的，具体如下：</p><table><thead><tr><th>UDX</th><th>INPUT</th><th>OUTPUT</th><th>INPUT:OUTPUT</th></tr></thead><tbody><tr><td>UDF</td><td>单行中的N(N&gt;=0)列</td><td>单行中的1列</td><td>1:1</td></tr><tr><td>UDTF</td><td>单行中的N(N&gt;=0)列</td><td>M(M&gt;=0)行</td><td>1:N(N&gt;=0)</td></tr><tr><td>UDAF</td><td>M(M&gt;=0)行中的每行的N(N&gt;=0)列</td><td>单行中的1列</td><td>M：1(M&gt;=0)</td></tr></tbody></table><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ul><li>定义<br>用户想自己编写一个字符串联接的UDF，我们只需要实现<code>ScalarFunction#eval()</code>方法即可，简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyConnect</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="meta">@varargs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(args: <span class="type">String</span>*): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; args.length) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args(i) == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      sb.append(args(i))</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    sb.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> <span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = MyConnect</span></span><br><span class="line"> tEnv.registerFunction(<span class="string">"myConnect"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"> <span class="keyword">val</span> sql = <span class="string">"SELECT myConnect(a, b) as str FROM tab"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><ul><li>定义<br>用户想自己编写一个字符串切分的UDTF，我们只需要实现<code>TableFunction#eval()</code>方法即可，简单实现如下：</li></ul><p>ScalarFunction#eval()`</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>))&#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(collect)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>, prefix: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>)) &#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(s =&gt; collect(prefix + s))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MySplit</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"mySplit"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT c, s FROM MyTable, LATERAL TABLE(mySplit(c)) AS T(s)"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><ul><li>定义<br>UDAF 要实现的接口比较多，我们以一个简单的CountAGG为例，做简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The initial accumulator for count aggregate function */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span> <span class="keyword">extends</span> <span class="title">JTuple1</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  f0 = <span class="number">0</span>L <span class="comment">//count</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * User-defined count aggregate function</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCount</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">JLong</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 += <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 -= <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 -= <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">JLong</span> = &#123;</span><br><span class="line">    acc.f0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">CountAccumulator</span>, its: <span class="type">JIterable</span>[<span class="type">CountAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = its.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      acc.f0 += iter.next().f0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CountAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getAccumulatorType</span></span>: <span class="type">TypeInformation</span>[<span class="type">CountAccumulator</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TupleTypeInfo</span>(classOf[<span class="type">CountAccumulator</span>], <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResultType</span></span>: <span class="type">TypeInformation</span>[<span class="type">JLong</span>] =</span><br><span class="line">    <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MyCount</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"myCount"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT myCount(c) FROM MyTable GROUP BY  a"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="Source-amp-Sink"><a href="#Source-amp-Sink" class="headerlink" title="Source&amp;Sink"></a>Source&amp;Sink</h1><p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p><h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MySourceFunction</span>[<span class="type">T</span>](<span class="title">dataWithTimestampList</span>: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span>), <span class="type">Long</span>]]) </span></span><br><span class="line"><span class="class">  extends <span class="type">SourceFunction</span>[<span class="type">T</span>] &#123;</span></span><br><span class="line"><span class="class">  override def run(<span class="title">ctx</span>: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span></span><br><span class="line"><span class="class">    dataWithTimestampList.foreach &#123;</span></span><br><span class="line"><span class="class">      case <span class="type">Left</span>(<span class="title">t</span>) =&gt; ctx.collectWithTimestamp(<span class="title">t</span>.<span class="title">_2</span>, <span class="title">t</span>.<span class="title">_1</span>)</span></span><br><span class="line"><span class="class">      case <span class="type">Right</span>(<span class="title">w</span>) =&gt; ctx.emitWatermark(<span class="title">new</span> <span class="type">Watermark(w)</span>)</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">  override def cancel(): <span class="type">Unit</span> = ???</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTableSource</span> <span class="keyword">extends</span> <span class="title">StreamTableSource</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fieldNames = <span class="type">Array</span>(<span class="string">"accessTime"</span>, <span class="string">"region"</span>, <span class="string">"userId"</span>)</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">TableSchema</span>(fieldNames, <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">SQL_TIMESTAMP</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">  <span class="keyword">val</span> rowType = <span class="keyword">new</span> <span class="type">RowTypeInfo</span>(</span><br><span class="line">    <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">LONG</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>).asInstanceOf[<span class="type">Array</span>[<span class="type">TypeInformation</span>[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 页面访问表数据 rows with timestamps and watermarks</span></span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366200000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366200000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366200000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366260000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366260000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366260000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510373400000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510373400000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510373400000</span>L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRowtimeAttributeDescriptors</span></span>: util.<span class="type">List</span>[<span class="type">RowtimeAttributeDescriptor</span>] = &#123;</span><br><span class="line">    <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">RowtimeAttributeDescriptor</span>(</span><br><span class="line">      <span class="string">"accessTime"</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExistingField</span>(<span class="string">"accessTime"</span>),</span><br><span class="line">      <span class="type">PreserveWatermarks</span>.<span class="type">INSTANCE</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDataStream</span></span>(execEnv: <span class="type">StreamExecutionEnvironment</span>): <span class="type">DataStream</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    execEnv.addSource(<span class="keyword">new</span> <span class="type">MySourceFunction</span>[<span class="type">Row</span>](data)).setParallelism(<span class="number">1</span>).returns(rowType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReturnType</span></span>: <span class="type">TypeInformation</span>[<span class="type">Row</span>] = rowType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getTableSchema</span></span>: <span class="type">TableSchema</span> = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = <span class="built_in">File</span>.createTempFile(<span class="string">"csv_sink_"</span>, <span class="string">"tem"</span>)</span><br><span class="line">    <span class="comment">// 打印sink的文件路径，方便我们查看运行结果</span></span><br><span class="line">    <span class="built_in">println</span>(<span class="string">"Sink path : "</span> + tempFile)</span><br><span class="line">    <span class="built_in">if</span> (tempFile.<span class="built_in">exists</span>()) &#123;</span><br><span class="line">      tempFile.<span class="keyword">delete</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[<span class="keyword">String</span>](<span class="string">"region"</span>, <span class="string">"winStart"</span>, <span class="string">"winEnd"</span>, <span class="string">"pv"</span>),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//方便我们查出输出数据</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sourceTableName = <span class="string">"mySource"</span></span><br><span class="line">    <span class="comment">// 创建自定义source数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSource = <span class="keyword">new</span> <span class="type">MyTableSource</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sinkTableName = <span class="string">"csvSink"</span></span><br><span class="line">    <span class="comment">// 创建CSV sink 数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册source</span></span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    <span class="comment">// 注册sink</span></span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">"SELECT  "</span> +</span><br><span class="line">      <span class="string">"  region, "</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,"</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv "</span> +</span><br><span class="line">      <span class="string">" FROM mySource "</span> +</span><br><span class="line">      <span class="string">" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region"</span></span><br><span class="line"></span><br><span class="line">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink <span class="string">path :</span> <span class="regexp">/var/</span>folders<span class="regexp">/88/</span><span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn<span class="regexp">/T/</span>csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure><p>Cat 方式查看计算结果，如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/<span class="number">88</span>/<span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:12:00.0</span>,<span class="number">2</span></span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">04:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">04:12:00.0</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure><p>表格化如上结果：</p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job收尾。</p>]]></content>
    
    <summary type="html">
    
      本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="FlinkSQL" scheme="https://gjtmaster.github.io/tags/FlinkSQL/"/>
    
  </entry>
  
  <entry>
    <title>Flink On Yarn HA</title>
    <link href="https://gjtmaster.github.io/2018/11/16/Flink%20on%20Yarn%20HA/"/>
    <id>https://gjtmaster.github.io/2018/11/16/Flink on Yarn HA/</id>
    <published>2018-11-16T05:15:07.000Z</published>
    <updated>2019-10-28T06:27:38.233Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 关闭防火墙</span><br><span class="line"><span class="meta">#</span> 配好主机映射</span><br><span class="line"><span class="meta">#</span> 配置免密登录</span><br><span class="line"><span class="meta">#</span> 准备好安装包 hadoop-2.8.5.tar.gz、flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz</span><br><span class="line"><span class="meta">#</span> 创建flink用户，后续操作均在flink用户下操作</span><br><span class="line"><span class="meta">#</span> 将Hadoop安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 将flink安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 节点配置如下：</span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">192.168.23.51</td><td align="center">flink01</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、NameNode、DFSZKFailoverController、DataNode</td></tr><tr><td align="center">192.168.23.52</td><td align="center">flink02</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、 NameNode、DFSZKFailoverController、DataNode、ResourceManager</td></tr><tr><td align="center">192.168.23.53</td><td align="center">flink03</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、ResourceManager、DataNode</td></tr></tbody></table><h2 id="Hadoop-HA配置"><a href="#Hadoop-HA配置" class="headerlink" title="Hadoop HA配置"></a>Hadoop HA配置</h2><h3 id="进入hadoop配置目录"><a href="#进入hadoop配置目录" class="headerlink" title="进入hadoop配置目录"></a>进入hadoop配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 进入hadoop配置目录</span><br><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改Java环境配置"><a href="#修改Java环境配置" class="headerlink" title="修改Java环境配置"></a>修改Java环境配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 修改hadoop-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置yarn-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置mapred-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/name1,/data/apps/hadoop-2.8.5/tmp/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/data1,/data/apps/hadoop-2.8.5/tmp/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://flink01:8485;flink02:8485;flink03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行</span></span><br><span class="line"><span class="comment">sshfence:当Active出问题后，standby切换成Active，此时，原Active又没有停止服务，这种情况下会被强制杀死进程。</span></span><br><span class="line"><span class="comment">shell(/bin/true)：NN Active和它的ZKFC一起挂了，没有人通知ZK，ZK长期没有接到通知，standby要切换，此时，standby调一个shell（脚本内容），这个脚本返回true则切换成功。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/flink/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rmcluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群的地址--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--启用自动恢复--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置capacity-scheduler-xml"><a href="#配置capacity-scheduler-xml" class="headerlink" title="配置capacity-scheduler.xml"></a>配置capacity-scheduler.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>集群中可用于运行application master的资源比例上限.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="启动Zookeeper集群"><a href="#启动Zookeeper集群" class="headerlink" title="启动Zookeeper集群"></a>启动Zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="初始化Hadoop环境"><a href="#初始化Hadoop环境" class="headerlink" title="初始化Hadoop环境"></a>初始化Hadoop环境</h3><p>启动journalnode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>格式化zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行完成后，会在zookeeper 上创建一个目录，查看是否创建成功：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入zookeeper家目录，执行bin/zkCli.sh客户端连接ZK。在ZK客户端的shell命令行查看：ls /</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 出现hadoop-ha即表示成功。</span></span><br></pre></td></tr></table></figure><p>启动主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>备用NN 同步主NN信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>关闭已启动的所有journalnode和主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="启动hadoop集群"><a href="#启动hadoop集群" class="headerlink" title="启动hadoop集群"></a>启动hadoop集群</h3><p>启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令（建议先启动所有journalnode以防出现namenode连接journalnode超时）</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个namenode的状态</span></span><br><span class="line">bin/hdfs haadmin -getServiceState nn1     #查看nn1状态</span><br><span class="line">bin/hdfs haadmin -getServiceState nn2     #查看nn2状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动切换namenode状态（此处禁用，有需要再执行）</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1##切换成active</span><br><span class="line">bin/hdfs haadmin -transitionToStandby nn1##切换成standby</span><br></pre></td></tr></table></figure><p>启动Yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在flink03执行以下命令</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个Resourcemanager的状态</span></span><br><span class="line">bin/yarn rmadmin -getServiceState rm1      ##查看rm1的状态</span><br><span class="line">bin/yarn rmadmin -getServiceState rm2      ##查看rm2的状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当flink02的ResourceManager是Active状态的时候，访问flink03的ResourceManager会自动跳转到flink02的web页面</span></span><br></pre></td></tr></table></figure><h2 id="Flink-HA配置"><a href="#Flink-HA配置" class="headerlink" title="Flink HA配置"></a>Flink HA配置</h2><h3 id="进入flink配置目录"><a href="#进入flink配置目录" class="headerlink" title="进入flink配置目录"></a>进入flink配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html" target="_blank" rel="noopener">点此查看flink配置说明</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The config parameter defining the network address to connect to <span class="keyword">for</span> communication with the job manager. This value is only interpreted <span class="keyword">in</span> setups <span class="built_in">where</span> a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used <span class="keyword">in</span> many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</span></span><br><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the JobManager.</span></span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the TaskManagers, <span class="built_in">which</span> are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager<span class="string">'s YARN container, minus a certain tolerance value.</span></span></span><br><span class="line">taskmanager.heap.size: 2048m</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of parallel operator or user <span class="keyword">function</span> instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a <span class="keyword">function</span> or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or <span class="keyword">function</span> instances. This value is typically proportional to the number of physical CPU cores that the TaskManager<span class="string">'s machine has (e.g., equal to the number of cores, or half the number of cores).</span></span></span><br><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line"><span class="meta">#</span><span class="bash"> Default parallelism <span class="keyword">for</span> <span class="built_in">jobs</span>.</span></span><br><span class="line">parallelism.default: 2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defines high-availability mode used <span class="keyword">for</span> the cluster execution. To <span class="built_in">enable</span> high-availability, <span class="built_in">set</span> this mode to <span class="string">"ZOOKEEPER"</span> or specify FQN of factory class.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> high-availability mode (required): The high-availability mode has to be <span class="built_in">set</span> <span class="keyword">in</span> conf/flink-conf.yaml to zookeeper <span class="keyword">in</span> order to <span class="built_in">enable</span> high availability mode. Alternatively this option can be <span class="built_in">set</span> to FQN of factory class Flink should use to create HighAvailabilityServices instance.</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash"> File system path (URI) <span class="built_in">where</span> Flink persists metadata <span class="keyword">in</span> high-availability setups.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Storage directory (required): JobManager metadata is persisted <span class="keyword">in</span> the file system storageDir and only a pointer to this state is stored <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The storageDir stores all metadata needed to recover a JobManager failure.</span></span><br><span class="line">high-availability.storageDir: hdfs://ns1/flink/recovery</span><br><span class="line"><span class="meta">#</span><span class="bash"> The ZooKeeper quorum to use, when running Flink <span class="keyword">in</span> a high-availability mode with ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper quorum (required): A ZooKeeper quorum is a replicated group of ZooKeeper servers, <span class="built_in">which</span> provide the distributed coordination service.</span></span><br><span class="line">high-availability.zookeeper.quorum: flink01:2181,flink02:2181,flink03:2181</span><br><span class="line"><span class="meta">#</span><span class="bash"> The root path under <span class="built_in">which</span> Flink stores its entries <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper root (recommended): The root ZooKeeper node, under <span class="built_in">which</span> all cluster nodes are placed.</span></span><br><span class="line">high-availability.zookeeper.path.root: /flink</span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts: The number of ApplicationMaster (+ its TaskManager containers) attempts. If this value is <span class="built_in">set</span> to 1 (default), the entire YARN session will fail when the Application master fails. Higher values specify the number of restarts of the ApplicationMaster by YARN.</span></span><br><span class="line">yarn.application-attempts: 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The state backend to be used to store and checkpoint state.</span></span><br><span class="line">state.backend: rocksdb</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory used <span class="keyword">for</span> storing the data files and meta data of checkpoints <span class="keyword">in</span> a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).</span></span><br><span class="line">state.checkpoints.dir: hdfs://ns1/flink/flink-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory <span class="keyword">for</span> savepoints. Used by the state backends that write savepoints to file systems (MemoryStateBackend, FsStateBackend, RocksDBStateBackend).</span></span><br><span class="line">state.savepoints.dir: hdfs://ns1/flink/save-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> Option whether the state backend should create incremental checkpoints, <span class="keyword">if</span> possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Some state backends may not support incremental checkpoints and ignore this option.</span></span><br><span class="line">state.backend.incremental: true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Directories <span class="keyword">for</span> temporary files, separated by<span class="string">","</span>, <span class="string">"|"</span>, or the system<span class="string">'s java.io.File.pathSeparator.</span></span></span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br></pre></td></tr></table></figure><p>切记：Flink On Yarn HA一定不要手动配置high-availability.cluster-id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper cluster-id (recommended): The cluster-id ZooKeeper node, under <span class="built_in">which</span> all required coordination data <span class="keyword">for</span> a cluster is placed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Important: You should not <span class="built_in">set</span> this value manually when running a YARN cluster, a per-job YARN session, or on another cluster manager. In those cases a cluster-id is automatically being generated based on the application id. Manually setting a cluster-id overrides this behaviour <span class="keyword">in</span> YARN. Specifying a cluster-id with the -z CLI option, <span class="keyword">in</span> turn, overrides manual configuration. If you are running multiple Flink HA clusters on bare metal, you have to manually configure separate cluster-ids <span class="keyword">for</span> each cluster.</span></span><br><span class="line">high-availability.cluster-id: /default</span><br></pre></td></tr></table></figure><h3 id="替换日志框架为logback"><a href="#替换日志框架为logback" class="headerlink" title="替换日志框架为logback"></a>替换日志框架为logback</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><p><strong>logback-yarn.xml配置示例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn-HA测试说明"><a href="#Flink-on-Yarn-HA测试说明" class="headerlink" title="Flink on Yarn HA测试说明"></a>Flink on Yarn HA测试说明</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开始一个yarn-session（命名为FlinkTestCluster）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager内存2048M</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个TaskManager内存2048M且分配4个slot（The session cluster will automatically allocate additional containers <span class="built_in">which</span> run the Task Managers when <span class="built_in">jobs</span> are submitted to the cluster.）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式启动</span></span><br><span class="line">yarn-session.sh -jm 2048 -tm 2048 -s 4 -nm FlinkTestCluster -d</span><br></pre></td></tr></table></figure><table><thead><tr><th>配置</th><th>测试方案</th><th>现象</th><th>备注</th></tr></thead><tbody><tr><td>Job本身配置了Flink的重启策略</td><td>提供bug程序，导致Job失败</td><td>重启失败的Job</td><td>保证Job HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnSessionClusterEntrypoint进程（<em>JobManager</em>和AM的共同进程）</td><td>重启JobManager和AM，该进程会迁移到其它节点（非必须）且进程号改变，全部Job重启</td><td>保证JobManager HA</td></tr><tr><td>Job本身配置了Flink的重启策略、Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnTaskExecutorRunner进程（TaskManager进程）</td><td>重启TaskManager，该进程会迁移到其它节点（非必须）且进程号改变，被Kill掉的TaskManager包含的Job重启</td><td>保证TaskManager HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts</td><td>未主动Cancel掉Flink集群中的Job，但不小心kill掉对应的yarn-session(对应Yarn队列中的一个Application)、之后在命令行重新提交yarn-session</td><td>启动新的yarn-session、之前未Cancel掉的Job自动迁移到当前yarn-session、JobManager和TaskManager自动创建</td><td>保证 YarnSessionHA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts、配置了Yarn的HA</td><td>Kill掉Resourcemanager</td><td>ResourceManager迁移到另一台节点，yarn-session重启，所有Job重启</td><td>保证Yarn HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts（也可在yarn-session提交时通过-D动态配置）、配置了HDFS的HA</td><td>Kill掉NameNode</td><td>NameNode迁移到另一台节点</td><td>保证HDFS HA</td></tr></tbody></table><h2 id="Yarn的基本思想"><a href="#Yarn的基本思想" class="headerlink" title="Yarn的基本思想"></a>Yarn的基本思想</h2><p>YARN的基本思想是将资源管理和作业调度/监视的功能分解为单独的守护进程。我们的想法是拥有一个全局ResourceManager（<em>RM</em>）和每个应用程序ApplicationMaster（<em>AM</em>）。应用程序可以是单个作业，也可以是作业的DAG。</p><p>ResourceManager和NodeManager构成了数据计算框架。ResourceManager是在系统中的所有应用程序之间仲裁资源的最终权限。NodeManager是每台机器上负责Containers的代理框架，监视其资源使用情况（CPU，内存，磁盘，网络）并将其报告给ResourceManager / Scheduler。</p><p>每个应用程序ApplicationMaster实际上是一个含具体库的框架，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</p><p><img src="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="MapReduce NextGen架构"></p><p>ResourceManager有两个主要组件：Scheduler和ApplicationsManager。</p><p>Scheduler负责根据熟悉的容量，队列等约束将资源分配给各种正在运行的应用程序。Scheduler是纯调度程序，因为它不执行应用程序状态的监视或跟踪。此外，当出现应用程序故障或硬件故障，它无法保证重新启动失败的任务。Scheduler根据应用程序的资源需求执行其调度功能; 它是基于资源<em>Container</em>的抽象概念，它包含内存，CPU，磁盘，网络等元素。</p><p>Scheduler具有可插入策略，该策略负责在各种队列，应用程序等之间对集群资源进行分区。当前的调度程序（如<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler）</a>将是插件的一些示例。</p><p>ApplicationsManager负责接受作业提交，协商第一个容器以执行特定于应用程序的ApplicationMaster，并提供在失败时重新启动ApplicationMaster容器的服务。每个应用程序ApplicationMaster负责从Scheduler协调适当的资源容器，跟踪其状态并监视进度。</p><h2 id="Flink-on-Yarn的基本思想"><a href="#Flink-on-Yarn的基本思想" class="headerlink" title="Flink on Yarn的基本思想"></a>Flink on Yarn的基本思想</h2><p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.7/fig/FlinkOnYarn.svg" alt="img"></p><p>YARN客户端需要访问Hadoop配置以连接到YARN资源管理器和HDFS。它使用以下策略确定Hadoop配置：</p><ul><li>按顺序测试是否配置<code>YARN_CONF_DIR</code>，<code>HADOOP_CONF_DIR</code>或<code>HADOOP_CONF_PATH</code>。如果设置了其中一个变量，则用于读取配置。</li><li>如果上述策略失败（在正确的YARN设置中不应该这样），则客户端使用配置的<code>HADOOP_HOME</code>环境变量。如果<code>HADOOP_HOME</code>环境变量已配置，则客户端尝试访问<code>$HADOOP_HOME/etc/hadoop</code>（Hadoop 2）或<code>$HADOOP_HOME/conf</code>（Hadoop 1）。</li></ul><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（ApplicationMaster的memory和vcores）是否可用。之后，它将包含Flink的jar包和配置信息上传到HDFS（步骤1）。</p><p>客户端的下一步是请求（步骤2）YARN容器以启动<em>ApplicationMaster</em>（步骤3）。客户端将配置信息和jar文件注册为容器的资源，在特定机器上运行的NodeManager将负责准备容器（例如下载文件的工作）。完成后，将启动<em>ApplicationMaster</em>（AM）。</p><p>该<em>JobManager</em>和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（Flink主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager），该文件也上传到HDFS。此外，<em>AM</em>容器还提供Flink的Web界面。YARN代码分配的所有端口都是<em>临时端口</em>。这允许用户并行执行多个Flink YARN会话。</p><p>之后，AM开始为Flink的TaskManagers分配容器（步骤4），这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a> </p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html#yarn-cluster-high-availability" target="_blank" rel="noopener">JobManager High Availability (HA)</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Flink on Yarn</a></p>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink on Yarn 高可用的集群部署方案。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="Yarn" scheme="https://gjtmaster.github.io/tags/Yarn/"/>
    
      <category term="Flink on Yarn" scheme="https://gjtmaster.github.io/tags/Flink-on-Yarn/"/>
    
  </entry>
  
  <entry>
    <title>Flink使用Logback作为日志框架的相关配置</title>
    <link href="https://gjtmaster.github.io/2018/11/15/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
    <id>https://gjtmaster.github.io/2018/11/15/Flink使用Logback作为日志框架的相关配置/</id>
    <published>2018-11-15T08:13:17.000Z</published>
    <updated>2019-10-28T06:27:24.378Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink切换日志框架为Logback"><a href="#Flink切换日志框架为Logback" class="headerlink" title="Flink切换日志框架为Logback"></a>Flink切换日志框架为Logback</h1><h2 id="client端pom文件配置"><a href="#client端pom文件配置" class="headerlink" title="client端pom文件配置"></a>client端pom文件配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Add the two required logback dependencies --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add the log4j -&gt; sfl4j (-&gt; logback) bridge into the classpath</span></span><br><span class="line"><span class="comment">     Hadoop is logging to log4j! --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>添加logback-core、logback-classic及log4j-over-slf4j依赖，</li><li>之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；</li><li><strong>最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</strong></li></ul><h2 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h2><ul><li><p>添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)</p><p>相关jar包在logback官网上都有，<a href="https://download.csdn.net/download/qq_36643786/11190626" target="_blank" rel="noopener">嫌麻烦的可以点此链接直接下载！</a></p></li><li><p>移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)</p></li><li><p>如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</p></li></ul><h3 id="flink-daemon-sh"><a href="#flink-daemon-sh" class="headerlink" title="flink-daemon.sh"></a>flink-daemon.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-daemon.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#  Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment">#  or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment">#  distributed with this work for additional information</span></span><br><span class="line"><span class="comment">#  regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment">#  to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment">#  "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment">#  with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">#  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">#  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start/stop a Flink daemon.</span></span><br><span class="line">USAGE=<span class="string">"Usage: flink-daemon.sh (start|stop|stop-all) (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">STARTSTOP=<span class="variable">$1</span></span><br><span class="line">DAEMON=<span class="variable">$2</span></span><br><span class="line">ARGS=(<span class="string">"<span class="variable">$&#123;@:3&#125;</span>"</span>) <span class="comment"># get remaining arguments as array</span></span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"<span class="variable">$0</span>"</span>`</span><br><span class="line">bin=`<span class="built_in">cd</span> <span class="string">"<span class="variable">$bin</span>"</span>; <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">. <span class="string">"<span class="variable">$bin</span>"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$DAEMON</span> <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unknown daemon '<span class="variable">$&#123;DAEMON&#125;</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$FLINK_IDENT_STRING</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_IDENT_STRING=<span class="string">"<span class="variable">$USER</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">pid=<span class="variable">$FLINK_PID_DIR</span>/flink-<span class="variable">$FLINK_IDENT_STRING</span>-<span class="variable">$DAEMON</span>.pid</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Log files for daemons are indexed from the process ID's position in the PID</span></span><br><span class="line"><span class="comment"># file. The following lock prevents a race condition during daemon startup</span></span><br><span class="line"><span class="comment"># when multiple daemons read, index, and write to the PID file concurrently.</span></span><br><span class="line"><span class="comment"># The lock is created on the PID directory since a lock file cannot be safely</span></span><br><span class="line"><span class="comment"># removed. The daemon is started with the lock closed and the lock remains</span></span><br><span class="line"><span class="comment"># active in this script until the script exits.</span></span><br><span class="line"><span class="built_in">command</span> -v flock &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">exec</span> 200&lt;<span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line">    flock 200</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ascending ID depending on number of lines in pid file.</span></span><br><span class="line"><span class="comment"># This allows us to start multiple daemon of each type.</span></span><br><span class="line">id=$([ -f <span class="string">"<span class="variable">$pid</span>"</span> ] &amp;&amp; <span class="built_in">echo</span> $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>"</span>) || <span class="built_in">echo</span> <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">FLINK_LOG_PREFIX=<span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>/flink-<span class="variable">$&#123;FLINK_IDENT_STRING&#125;</span>-<span class="variable">$&#123;DAEMON&#125;</span>-<span class="variable">$&#123;id&#125;</span>-<span class="variable">$&#123;HOSTNAME&#125;</span>"</span></span><br><span class="line"><span class="built_in">log</span>=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.log"</span></span><br><span class="line">out=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.out"</span></span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog.file=<span class="variable">$&#123;log&#125;</span>"</span> <span class="string">"-Dlog4j.configuration=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/log4j.properties"</span> <span class="string">"-Dlogback.configurationFile=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/logback.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$(<span class="variable">$&#123;JAVA_RUN&#125;</span> -version 2&gt;&amp;1 | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only set JVM 8 arguments if we have correctly extracted the version</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$&#123;JAVA_VERSION&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_VERSION</span>"</span> -lt 18 ]; <span class="keyword">then</span></span><br><span class="line">        JVM_ARGS=<span class="string">"<span class="variable">$JVM_ARGS</span> -XX:MaxPermSize=256m"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STARTSTOP</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">    (start)</span><br><span class="line">        <span class="comment"># Rotate log files</span></span><br><span class="line">        rotateLogFilesWithPrefix <span class="string">"<span class="variable">$FLINK_LOG_DIR</span>"</span> <span class="string">"<span class="variable">$FLINK_LOG_PREFIX</span>"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print a warning if daemons are already running on host</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          active=()</span><br><span class="line">          <span class="keyword">while</span> IFS=<span class="string">''</span> <span class="built_in">read</span> -r p || [[ -n <span class="string">"<span class="variable">$p</span>"</span> ]]; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">kill</span> -0 <span class="variable">$p</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">              active+=(<span class="variable">$p</span>)</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">          <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          count=<span class="string">"<span class="variable">$&#123;#active[@]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> [ <span class="variable">$&#123;count&#125;</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"[INFO] <span class="variable">$count</span> instance(s) of <span class="variable">$DAEMON</span> are already running on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">          <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Evaluate user options for local variable expansion</span></span><br><span class="line">        FLINK_ENV_JAVA_OPTS=$(<span class="built_in">eval</span> <span class="built_in">echo</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Starting <span class="variable">$DAEMON</span> daemon on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="variable">$JAVA_RUN</span> <span class="variable">$JVM_ARGS</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span> <span class="string">"<span class="variable">$&#123;log_setting[@]&#125;</span>"</span> -classpath <span class="string">"`manglePathList "</span><span class="variable">$FLINK_TM_CLASSPATH</span>:<span class="variable">$INTERNAL_HADOOP_CLASSPATHS</span><span class="string">"`"</span> <span class="variable">$&#123;CLASS_TO_RUN&#125;</span> <span class="string">"<span class="variable">$&#123;ARGS[@]&#125;</span>"</span> &gt; <span class="string">"<span class="variable">$out</span>"</span> 200&lt;&amp;- 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line">        mypid=$!</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to pid file if successful start</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="variable">$&#123;mypid&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]] &amp;&amp; <span class="built_in">kill</span> -0 <span class="variable">$mypid</span> &gt; /dev/null 2&gt;&amp;1 ; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="variable">$mypid</span> &gt;&gt; <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Error starting <span class="variable">$DAEMON</span> daemon."</span></span><br><span class="line">            <span class="built_in">exit</span> 1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Remove last in pid file</span></span><br><span class="line">            to_stop=$(tail -n 1 <span class="string">"<span class="variable">$pid</span>"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> [ -z <span class="variable">$to_stop</span> ]; <span class="keyword">then</span></span><br><span class="line">                rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                sed \<span class="variable">$d</span> <span class="string">"<span class="variable">$pid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="comment"># all but last line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                [ $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>.tmp"</span>) -eq 0 ] &amp;&amp; rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$pid</span>.tmp"</span> || mv <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) is running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop-all)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            mv <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">read</span> to_stop; <span class="keyword">do</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Skipping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>), because it is not running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">            rm <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unexpected argument '<span class="variable">$STARTSTOP</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml</li></ul><h3 id="flink-console-sh"><a href="#flink-console-sh" class="headerlink" title="flink-console.sh"></a>flink-console.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-console.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line"># Start a Flink service <span class="keyword">as</span> a console application. Must be stopped <span class="keyword">with</span> Ctrl-C</span><br><span class="line"># or <span class="keyword">with</span> SIGTERM by kill or the controlling process.</span><br><span class="line">USAGE=<span class="string">"Usage: flink-console.sh (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">SERVICE=$<span class="number">1</span></span><br><span class="line">ARGS=(<span class="string">"$&#123;@:2&#125;"</span>) # get remaining arguments <span class="keyword">as</span> array</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> $SERVICE <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        echo <span class="string">"Unknown service '$&#123;SERVICE&#125;'. $USAGE."</span></span><br><span class="line">        exit <span class="number">1</span></span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog4j.configuration=file:$&#123;FLINK_CONF_DIR&#125;/log4j-console.properties"</span> <span class="string">"-Dlogback.configurationFile=file:$&#123;FLINK_CONF_DIR&#125;/logback-console.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$($&#123;JAVA_RUN&#125; -version <span class="number">2</span>&gt;&amp;<span class="number">1</span> | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"># Only set JVM <span class="number">8</span> arguments <span class="keyword">if</span> we have correctly extracted the version</span><br><span class="line"><span class="keyword">if</span> [[ $&#123;JAVA_VERSION&#125; =~ $&#123;IS_NUMBER&#125; ]]; then</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"$JAVA_VERSION"</span> -lt <span class="number">18</span> ]; then</span><br><span class="line">        JVM_ARGS=<span class="string">"$JVM_ARGS -XX:MaxPermSize=256m"</span></span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"Starting $SERVICE as a console application on host $HOSTNAME."</span></span><br><span class="line">exec $JAVA_RUN $JVM_ARGS $&#123;FLINK_ENV_JAVA_OPTS&#125; <span class="string">"$&#123;log_setting[@]&#125;"</span> -classpath <span class="string">"`manglePathList "</span>$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS<span class="string">"`"</span> $&#123;CLASS_TO_RUN&#125; <span class="string">"$&#123;ARGS[@]&#125;"</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml</li></ul><h3 id="yarn-session-sh"><a href="#yarn-session-sh" class="headerlink" title="yarn-session.sh"></a>yarn-session.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line"># get Flink config</span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$FLINK_IDENT_STRING"</span> = <span class="string">""</span> ]; then</span><br><span class="line">        FLINK_IDENT_STRING=<span class="string">"$USER"</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">JVM_ARGS=<span class="string">"$JVM_ARGS -Xmx512m"</span></span><br><span class="line"></span><br><span class="line">CC_CLASSPATH=`manglePathList $(constructFlinkClassPath):$INTERNAL_HADOOP_CLASSPATHS`</span><br><span class="line"></span><br><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-yarn-session-$HOSTNAME.log</span><br><span class="line">log_setting=<span class="string">"-Dlog.file="</span>$log<span class="string">" -Dlog4j.configuration=file:"</span>$FLINK_CONF_DIR<span class="string">"/log4j-yarn-session.properties -Dlogback.configurationFile=file:"</span>$FLINK_CONF_DIR<span class="string">"/logback-yarn.xml"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> FLINK_CONF_DIR</span><br><span class="line"></span><br><span class="line">$JAVA_RUN $JVM_ARGS -classpath <span class="string">"$CC_CLASSPATH"</span> $log_setting org.apache.flink.yarn.cli.FlinkYarnSessionCli -j <span class="string">"$FLINK_LIB_DIR"</span>/flink-dist*.jar <span class="string">"$@"</span></span><br></pre></td></tr></table></figure><ul><li>使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h2 id="doc"><a href="#doc" class="headerlink" title="doc"></a>doc</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/best_practices.html#using-logback-instead-of-log4j" target="_blank" rel="noopener">Using Logback instead of Log4j</a></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>client端使用logback的话，要在pom文件添加logback-core、logback-classic及log4j-over-slf4j依赖，之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</li><li>服务端使用logback的话，要在添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)；移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)；如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</li><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h1 id="Logback配置文件详解"><a href="#Logback配置文件详解" class="headerlink" title="Logback配置文件详解"></a>Logback配置文件详解</h1><p>Logback，Java 日志框架。</p><p>Logback 如何加载配置的</p><ol><li>logback 首先会查找 logback.groovy 文件</li><li>当没有找到，继续试着查找 logback-test.xml 文件</li><li>当没有找到时，继续试着查找 logback.xml 文件</li><li>如果仍然没有找到，则使用默认配置（打印到控制台）</li></ol><h2 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h2><p>configuration 是配置文件的根节点，他包含的属性：</p><ul><li>scan<br>　　当此属性设置为 true 时，配置文件如果发生改变，将会被重新加载，默认值为 true</li><li>scanPeriod<br>　　设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。但 scan 为 true 时，此属性生效，默认的时间间隔为 1 分钟</li><li>debug<br>　　当此属性设置为 true 时，将打印出 logback 内部日志信息，实时查看 logback 运行状态，默认值为 false。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="configuration-的子节点"><a href="#configuration-的子节点" class="headerlink" title="configuration 的子节点"></a>configuration 的子节点</h2><h4 id="设置上下文名称：contextName"><a href="#设置上下文名称：contextName" class="headerlink" title="设置上下文名称：contextName"></a>设置上下文名称：contextName</h4><p>每个 logger 度关联到 logger 上下文，默认上下文名称为 “default”。可以通过设置 contextName 修改上下文名称，用于区分不同应该程序的记录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>myAppName<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置变量：property"><a href="#设置变量：property" class="headerlink" title="设置变量：property"></a>设置变量：property</h4><p>用于定义键值对的变量， property 有两个属性 name 和 value，name 是键，value 是值，通过 property 定义的键值对会保存到logger 上下文的 map 集合内。定义变量后，可以使用 “${}” 来使用变量</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"APP_Name"</span> <span class="attr">value</span>=<span class="string">"myAppName"</span> /&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;APP_Name&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="获取时间戳字符串：timestamp"><a href="#获取时间戳字符串：timestamp" class="headerlink" title="获取时间戳字符串：timestamp"></a>获取时间戳字符串：timestamp</h4><p>timestamp 有两个属性，key：标识此 timestamp 的名字；datePattern：时间输出格式，遵循SimpleDateFormat 的格式</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">timestamp</span> <span class="attr">key</span>=<span class="string">"bySecond"</span> <span class="attr">datePattern</span>=<span class="string">"yyyyMMdd'T'HHmmss"</span>/&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;bySecond&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="logger"><a href="#logger" class="headerlink" title="logger"></a>logger</h2><p>logger 有两种级别，一种是 root，一种是普通的 logger，logger 是用来设置某一个包或者具体的某一个类的日志打印机级别，以及制定的 appender。<br>logger 有三个属性</p><ul><li>name：用来指定此 logger 约束的某一个包或者具体的某一个类</li><li>level：用来设置打印机别，</li><li>addtivity：是否向上级 logger 传递打印信息。默认是 true</li></ul><p>每个 logger 都有对应的父级关系，它通过包名来决定父级关系，root 是最高级的父元素。<br>下面定义了四个 logger，他们的父子关系从小到大为：<br>com.lwc.qg.test.logbackDemo → com.lwc.qg.tes → com.lwc.qg → root</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 根 logger --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    普通的 logger</span></span><br><span class="line"><span class="comment">    name：类名或包名，标志该 logger 与哪个包或哪个类绑定</span></span><br><span class="line"><span class="comment">    level：该 logger 的日志级别</span></span><br><span class="line"><span class="comment">    additivity：是否将日志信息传递给上一级</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test.logbackDemo"</span> <span class="attr">level</span>=<span class="string">"debug"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br></pre></td></tr></table></figure><p>　　从该种级别来看，如果此时在最低层的 logger 输出日志信息，以该配置作为基础，它将会向父级的所有 logger 依次传递，所以按理来说一个打印信息将会打印四次</p><p>　　从控制台上看，的确每条日志信息都被打印出了四次，但是细心从配置文件上来看，root 的日志级别配置的为 info，但是却输出<br>debug 级别的日志信息，所以从测试结果可以看出，向上传递的日志信息的日志级别将由最底层的子元素决定（最初传递信息的<br>logger），因为子元素设置的日志级别为 debug，所以也输出了 debug 级别的信息。<br>　　因此，从理论上来说，如果子元素日志级别设置高一点，那么也将会只输出高级别的日志信息。实际上也是如此，如果我们把 com.lwc.qg.test.logbackDemo 对应的 logger 日志级别设为 warn，那么将只会输出 warn 及其以上的信息</p><h2 id="root"><a href="#root" class="headerlink" title="root"></a>root</h2><p>root 也是 logger 元素，但它是根 logger。只有一个 level 属性</p><h2 id="appender"><a href="#appender" class="headerlink" title="appender"></a>appender</h2><p>appender 是负责写日志的组件，常用的组件有：</p><ul><li>ConsoleAppender</li><li>FileAppender</li><li>RollingFileAppender</li></ul><h2 id="ConsoleAppender"><a href="#ConsoleAppender" class="headerlink" title="ConsoleAppender"></a>ConsoleAppender</h2><p>控制台日志组件，该组件将日志信息输出到控制台,该组件有以下节点</p><ul><li>encoder：对日志进行格式化</li><li>target：System.out 或者 System.err，默认是 System.out</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="FileAppender"><a href="#FileAppender" class="headerlink" title="FileAppender"></a>FileAppender</h2><p>文件日志组件，该组件将日志信息输出到日志文件中，该组件有以下节点</p><ul><li>file：被写入的文件名，可以是相对路径，也可以是绝对路径。如果上级目录不存在会自动创建，没有默认值</li><li>append：如果是 true，日志被追加到文件结尾；如果是 false，清空现存文件，默认是 true。</li><li>encoder：格式化</li><li>prudent：如果是 true，日志会被安全的写入文件，即使其他的 FileAppender 也在向此文件做写入操作，效率低，默认是 false。</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>testFile.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>true<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">prudent</span>&gt;</span>true<span class="tag">&lt;/<span class="name">prudent</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="RollingFileAppender"><a href="#RollingFileAppender" class="headerlink" title="RollingFileAppender"></a>RollingFileAppender</h2><p>滚动记录文件日志组件，先将日志记录记录到指定文件，当符合某个条件时，将日志记录到其他文件，该组件有以下节点</p><ul><li>file：文件名</li><li>encoder：格式化</li><li>rollingPolicy：当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名</li><li>triggeringPolicy：告知 RollingFileAppender 合适激活滚动</li><li>prudent：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。</li></ul><p>#### </p><h3 id="rollingPolicy"><a href="#rollingPolicy" class="headerlink" title="rollingPolicy"></a>rollingPolicy</h3><p>滚动策略</p><ol><li>TimeBasedRollingPolicy：最常用的滚动策略，它根据时间来制定滚动策略，即负责滚动也负责触发滚动，包含节点：<ul><li>fileNamePattern：文件名模式</li><li>maxHistoury：控制文件的最大数量，超过数量则删除旧文件</li></ul></li><li>FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略，包含节点<ul><li>minInedx：窗口索引最小值</li><li>maxIndex：串口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12</li><li>fileNamePattern：文件名模式，必须包含%i，命名模式为 log%i.log，会产生 log1.log，log2.log 这样的文件</li></ul></li><li>triggeringPolicy：根据文件大小的滚动策略，包含节点<ul><li>maxFileSize：日志文件最大大小</li></ul></li></ol><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>logFile.%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="filter-过滤器"><a href="#filter-过滤器" class="headerlink" title="filter 过滤器"></a>filter 过滤器</h2><p>过滤器是用于日志组件中的，每经过一个过滤器都会返回一个确切的枚举值，分别是</p><ul><li>DENY：返回 DENY，日志将立即被抛弃不再经过其他过滤器</li><li>NEUTRAL：有序列表的下个过滤器接着处理日志</li><li>ACCEPT：日志会被立即处理，不再经过剩余过滤器</li></ul><h3 id="常用过滤器"><a href="#常用过滤器" class="headerlink" title="常用过滤器"></a>常用过滤器</h3><p>常用的过滤器有以下：</p><ul><li>LevelFilter<br>级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 omMatch 和 omMismatch 接受或拒绝日志。他有以下节点<br>　　level：过滤级别<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作<br>例：该组件设置一个 INFO 级别的过滤器，那么所有非 INFO 级别的日志都会被过滤掉　　</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>ThresholdFilter<br>临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器会返回 NEUTRAL；当日志级别低于临界值时，日志会被拒绝<br>例：过滤掉所有低于 INFO 级别的日志</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span> </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>EvaluatorFilter<br>求值过滤器，评估、鉴别日志是否符合指定条件，包含节点：<br>　　evaluator：鉴别器，通过子标签 expression 配置求值条件<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink切换日志框架为Logback的详细步骤，并对Logback框架的配置文件进行了详细的介绍。
    
    </summary>
    
      <category term="日志框架" scheme="https://gjtmaster.github.io/categories/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Logback" scheme="https://gjtmaster.github.io/tags/Logback/"/>
    
  </entry>
  
  <entry>
    <title>Flink On Yarn集群部署</title>
    <link href="https://gjtmaster.github.io/2018/11/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://gjtmaster.github.io/2018/11/15/Flink On Yan集群部署/</id>
    <published>2018-11-15T05:15:07.000Z</published>
    <updated>2019-10-28T06:27:04.956Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配好主机映射</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建flink用户</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置免密登录</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 准备好相关资源：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.8.5.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> flink-1.7.1-bin-hadoop28-scala_2.11</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 节点配置如下：(建议每台NM节点预留2G内存给系统)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">hostname</th><th align="center">资源配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">flink01</td><td align="center">16G/16cores</td><td align="center">NameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink02</td><td align="center">16G/16cores</td><td align="center">ResourceManager/DataNode/NodeManager</td></tr><tr><td align="center">flink03</td><td align="center">16G/16cores</td><td align="center">SecondaryNameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink04</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink05</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink06</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr></tbody></table><h2 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h2><h3 id="将Hadoop安装包解压至flink01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至flink01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至flink01节点的/data/apps路径下"></a>将Hadoop安装包解压至flink01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录"><a href="#进入配置目录" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改hadoop-env-sh中的JAVA-HOME"><a href="#修改hadoop-env-sh中的JAVA-HOME" class="headerlink" title="修改hadoop-env.sh中的JAVA_HOME"></a>修改hadoop-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置yarn-env-sh中的JAVA-HOME"><a href="#配置yarn-env-sh中的JAVA-HOME" class="headerlink" title="配置yarn-env.sh中的JAVA_HOME"></a>配置yarn-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置mapred-env-sh中的JAVA-HOME"><a href="#配置mapred-env-sh中的JAVA-HOME" class="headerlink" title="配置mapred-env.sh中的JAVA_HOME"></a>配置mapred-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br><span class="line">fli<span class="symbol">nk04</span></span><br><span class="line">fli<span class="symbol">nk05</span></span><br><span class="line">fli<span class="symbol">nk06</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS的路径的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该属性定义了 HDFS WEB访问服务器的主机名和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 定义secondarynamenode 外部地址 访问的主机和端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager 的主机位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>7168<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>14336<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>含义</th><th>值</th><th>备注</th></tr></thead><tbody><tr><td>yarn.nodemanager.aux-services</td><td>设置yarn中的服务类</td><td>mapreduce_shuffle</td><td></td></tr><tr><td>yarn.resourcemanager.hostname</td><td>配置resourcemanager 的主机位置</td><td>flink02</td><td></td></tr><tr><td>yarn.resourcemanager.am.max-attempts</td><td>AM重启最大尝试次数</td><td>4</td><td></td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>开启物理内存限制</td><td>true</td><td>检测物理内存的使用是否超出分配值，若任务超出分配值，则将其杀掉，默认true。</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>关闭虚拟内存限制</td><td>false</td><td>检测虚拟内存的使用是否超出；若任务超出分配值，则将其杀掉，默认true。在确定内存不会泄漏的情况下可以设置此项为 False；</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>每个Container请求的最小内存</td><td>1024</td><td>单个容器/调度器可申请的最少物理内存量，默认是1024（MB）；一般每个contain都分配这个值；即：capacity memory:3072, vCores:1，如果提示物理内存溢出，提高这个值即可；</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>每个Container请求的最大内存</td><td>7168</td><td>单个容器/调度器可申请的最大物理内存量</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container请求的最小virtual CPU cores</td><td>1</td><td></td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container请求的最大virtual CPU cores</td><td>16</td><td></td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>限制 NodeManager 能够使用的最大物理内存</td><td>14336</td><td>该节点上YARN可使用的物理内存总量，【向操作系统申请的总量】默认是8192（MB）</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>限制 NodeManager 能够使用的最大virtual CPU cores</td><td>16</td><td>该节点上YARN可使用的总核心数；一般设为cat /proc/cpuinfo| grep “processor”| wc -l 的值。默认是8个</td></tr><tr><td>yarn.log-aggregation-enable</td><td>启用日志聚集功能</td><td>true</td><td></td></tr><tr><td>yarn.nodemanager.log.retain-seconds</td><td>设置HDFS上日志的保存时间,默认设置为7天</td><td>10800</td><td></td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>虚拟内存率</td><td>5</td><td>任务每使用1MB物理内存，最多可使用虚拟内存量比率，默认2.1；关闭虚拟内存限制的情况下，配置此项就无意义了</td></tr></tbody></table><h3 id="修改capacity-scheduler-xml"><a href="#修改capacity-scheduler-xml" class="headerlink" title="修改capacity-scheduler.xml"></a>修改capacity-scheduler.xml</h3><p><strong>（flink yarn session启用的jobmanager占用的资源总量受此参数限制）</strong></p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;    &lt;value&gt;0.3&lt;/value&gt;    &lt;description&gt;集群中可用于运行application master的资源比例上限.&lt;/description&gt;&lt;/property&gt;</code></pre><h3 id="快速安装Hadoop"><a href="#快速安装Hadoop" class="headerlink" title="快速安装Hadoop"></a>快速安装Hadoop</h3><p><strong>（使用此脚本安装完后需要单独修改capacity-scheduler.xml）</strong></p><p><strong>将安装脚本和安装包放在相同路径下并执行以下命令可快速完成上述配置步骤！</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认相关资源已放在当前用户的~路径下</span></span><br><span class="line">sh ~/install-hadoop.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">加入以下内容（这里提前加上了flink的环境变量）：</span><br><span class="line">export FLINK_HOME = /data/apps/flink-1.7.1</span><br><span class="line">export HADOOP_HOME=/data/apps/hadoop-2.8.5</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLINK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h3><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>在NameNode所在节点启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>在ResourceManager所在节点启动YARN</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="Flink集群"><a href="#Flink集群" class="headerlink" title="Flink集群"></a>Flink集群</h2><h3 id="将Hadoop安装包解压至kafka01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至kafka01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至kafka01节点的/data/apps路径下"></a>将Hadoop安装包解压至kafka01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录-1"><a href="#进入配置目录-1" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line">parallelism.default: 2</span><br><span class="line">taskmanager.numberOfTaskSlots: 8</span><br><span class="line">state.backend: rocksdb</span><br><span class="line">state.checkpoints.dir: hdfs://flink01:9000/flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs://flink01:9000/flink-savepoints</span><br><span class="line">state.backend.incremental: true</span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br><span class="line">yarn.application-attempts: 4</span><br></pre></td></tr></table></figure><h3 id="删除Flink原先使用的日志框架log4j相关资源"><a href="#删除Flink原先使用的日志框架log4j相关资源" class="headerlink" title="删除Flink原先使用的日志框架log4j相关资源"></a>删除Flink原先使用的日志框架log4j相关资源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br></pre></td></tr></table></figure><h3 id="更换Flink的日志框架为logback"><a href="#更换Flink的日志框架为logback" class="headerlink" title="更换Flink的日志框架为logback"></a>更换Flink的日志框架为logback</h3><p>（1）添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</p><p>（2）自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn的两种运行模式"><a href="#Flink-on-Yarn的两种运行模式" class="headerlink" title="Flink on Yarn的两种运行模式"></a>Flink on Yarn的两种运行模式</h2><h3 id="Start-a-long-running-Flink-cluster-on-YARN"><a href="#Start-a-long-running-Flink-cluster-on-YARN" class="headerlink" title="Start a long-running Flink cluster on YARN"></a>Start a long-running Flink cluster on YARN</h3><p>​    这种方式需要先启动集群，然后在提交Flink-Job（同一个Session中可以提交多个Flink-Job，可以在Flink的WebUI上submit，也可以使用Flink run命令提交）。启动集群时会向yarn申请一块空间，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成，释放了资源，那下一个作业才会正常提交.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认配置启动flink on yarn（默认启动资源如下）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> &#123;masterMemoryMB=1024, taskManagerMemoryMB=1024,numberTaskManagers=1, slotsPerTaskManager=1&#125;</span></span><br><span class="line">yarn-session.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 系统默认使用con/flink-conf.yaml里的配置，Flink on yarn将会覆盖掉几个参数：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jobmanager.rpc.address因为jobmanager的在集群的运行位置并不是事先确定的，其实就是AM的地址；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.tmp.dirs使用yarn给定的临时目录;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism.default也会被覆盖掉，如果在命令行里指定了slot数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 自定义配置可选参数如下 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Required     </span></span><br><span class="line"> -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)   </span><br><span class="line"><span class="meta">#</span><span class="bash"> Optional     </span></span><br><span class="line"> -D &lt;arg&gt;                        Dynamic properties     </span><br><span class="line"> -d,--detached                   Start detached     </span><br><span class="line"> -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)     </span><br><span class="line"> -nm,--name                      Set a custom name for the application on YARN     </span><br><span class="line"> -q,--query                      Display available YARN resources (memory, cores)     </span><br><span class="line"> -qu,--queue &lt;arg&gt;               Specify YARN queue.     </span><br><span class="line"> -s,--slots &lt;arg&gt;                Number of slots per TaskManager     </span><br><span class="line"> -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)     </span><br><span class="line"> -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例：启动15个TaskManager，1个JobManager，JobManager内存1024M，每个TaskManager内存1024M且含有8个slot，自定义该应用的名称为FlinkOnYarnSession，-d以分离式模式执行（不指定-d则以客户端模式执行）</span></span><br><span class="line">yarn-session.sh -n 15 -jm 1024 -tm 1024 -s 8 -nm FlinkOnYarnSession -d</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 客户端模式指的是在终端启动一个客户端，这种方式是不能断开终端的，断开即相当于<span class="built_in">kill</span>掉Flink集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式指的是启动Flink on Yarn后，Flink YARN客户端将仅向Yarn提交Flink，然后自行关闭。，要<span class="built_in">kill</span>掉Flink集群需要使用如下命令：</span></span><br><span class="line">yarn application -kill &lt;appId&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;appId&gt;指的是发布在Yarn上的作业ID，在Yarn集群上可以查到对应的ID</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于Flink On Yarn来说，一个JobManager占用一个Container，一个TaskManager占用一个Container</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager的数量+TaskManager的数量 = 申请的Container的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下以6台16核，16G内存的机器举例说明（每台节点预留2G内存给系统）</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcores=16 每台NodeManager节点为YARN集群分配的cpu为16核</span><br><span class="line">yarn.nodemanager.resource.memory-mb=14336 每台NodeManager节点为YARN集群分配的物理内存为14G</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores=1 每台NodeManager节点上每个Contaniner最小使用1核cpu</span><br><span class="line">yarn.scheduler.minimum-allocation-mb=1024 每台NodeManager节点上每个Contaniner最小使用1G的物理内存</span><br><span class="line"><span class="meta">#</span><span class="bash"> 若所有节点全部用于Flink作业,推荐提供的Flink集群：</span></span><br><span class="line">（总的资源为14*6=84G内存，16*6=96核）</span><br><span class="line">yarn-session.sh -n 8 -jm 4096 -tm 3584 -s 16 -nm FlinkOnYarnSession -d</span><br><span class="line">一共占用32G内存，9cores，申请了1个4G/1cores的JobManager和8个3.5G/1cores/16slots的TaskManager</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############## Recovery behavior of Flink on YARN</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Flink’s YARN client has the following configuration parameters to control how to behave <span class="keyword">in</span> <span class="keyword">case</span> of container failures. These parameters can be <span class="built_in">set</span> either from the conf/flink-conf.yaml or when starting the YARN session, using -D parameters</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.reallocate-failed : 控制 Flink是否应该重新分配失败的TaskManager容器，默认<span class="literal">true</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.maximum-failed-containers : ApplicationMaster接收container失败的最大次数，默认是TaskManager的次数（-n的值）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts : ApplicationMaster尝试次数。如果这个值为1（默认），那么当Application Master失败时，整个YARN session就会失败。更高的值是指ApplicationMaster重新启动的次数</span></span><br></pre></td></tr></table></figure><h3 id="Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）"><a href="#Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）" class="headerlink" title="Run a Flink job on YARN（Flink per-job cluster模式）"></a>Run a Flink job on YARN（Flink per-job cluster模式）</h3><p>这种方式不需要先启动集群，每提交一个Flink-Job都会在Yarn上启动一个Flink集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> TaskManager slots number配置</span></span><br><span class="line">这个参数是配置一个TaskManager有多少个并发的slot数。有两种配置方式：</span><br><span class="line">- taskmanager.numberOfTaskSlots. 在conf/flink-conf.yaml中更改，默认值为1，表示默认一个TaskManager只有1个task slot.</span><br><span class="line">- 提交作业时通过参数配置。--yarnslots 1，表示TaskManager的slot数为1.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TaskManager的个数</span></span><br><span class="line">注意： Per job模式提交作业时并不像session模式能够指定拉起多少个TaskManager，TaskManager的数量是在提交作业时根据并发度动态计算。</span><br><span class="line">首先，根据设定的operator的最大并发度计算，例如，如果作业中operator的最大并发度为10，则 Parallelism/numberOfTaskSlots为向YARN申请的TaskManager数。</span><br></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#######################示例</span><br><span class="line"># flink run -m yarn-cluster 必须指定</span><br><span class="line"># -d 分离模式启动（不指定则以客户端模式启动）</span><br><span class="line"># 启动<span class="number">1</span>个JobManager，内存占用<span class="number">1024</span>M</span><br><span class="line"># 每台TaskManager指定<span class="number">4</span>个slot、内存占用<span class="number">1024</span>M</span><br><span class="line"># 假设abc.jar所有operator中最大并发度为<span class="number">8</span>，则会启动<span class="number">8</span>/<span class="number">4</span>=<span class="number">2</span>台TaskManager</span><br><span class="line">flink run -m yarn-cluster -d --yarnslots <span class="number">4</span> -yjm <span class="number">1024</span> -ytm <span class="number">1024</span> /data/abc.jar</span><br></pre></td></tr></table></figure><h2 id="Log-Files"><a href="#Log-Files" class="headerlink" title="Log Files"></a>Log Files</h2><p>In cases where the Flink YARN session fails during the deployment itself, users have to rely on the logging capabilities of Hadoop YARN. The most useful feature for that is the <a href="http://hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="noopener">YARN log aggregation</a>. To enable it, users have to set the <code>yarn.log-aggregation-enable</code>property to <code>true</code> in the <code>yarn-site.xml</code> file. Once that is enabled, users can use the following command to retrieve all log files of a (failed) YARN session.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt;</span><br></pre></td></tr></table></figure><p>Note that it takes a few seconds after the session has finished until the logs show up.</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink On Yarn的集群部署流程以及两种运行模式。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="Yarn" scheme="https://gjtmaster.github.io/tags/Yarn/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink1.7.1与Kafka0.11.0.1</title>
    <link href="https://gjtmaster.github.io/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/"/>
    <id>https://gjtmaster.github.io/2018/11/15/Flink1.7.1与Kafka0.11.0.1/</id>
    <published>2018-11-15T02:17:01.000Z</published>
    <updated>2019-07-27T05:59:18.146Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink的Checkpoint"><a href="#Flink的Checkpoint" class="headerlink" title="Flink的Checkpoint"></a>Flink的Checkpoint</h1><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><ul><li>使用StreamExecutionEnvironment.enableCheckpointing方法来设置开启checkpoint；具体可以使用enableCheckpointing(long interval)，或者enableCheckpointing(long interval, CheckpointingMode mode)；interval用于指定checkpoint的触发间隔(单位milliseconds)，而CheckpointingMode默认是CheckpointingMode.EXACTLY_ONCE，也可以指定为CheckpointingMode.AT_LEAST_ONCE</li><li>也可以通过StreamExecutionEnvironment.getCheckpointConfig().setCheckpointingMode来设置CheckpointingMode，一般对于超低延迟的应用(大概几毫秒)可以使用CheckpointingMode.AT_LEAST_ONCE，其他大部分应用使用CheckpointingMode.EXACTLY_ONCE就可以</li><li>checkpointTimeout用于指定checkpoint执行的超时时间(单位milliseconds)，超时没完成就会被abort掉</li><li>minPauseBetweenCheckpoints用于指定checkpoint coordinator上一个checkpoint完成之后最小等多久可以出发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</li><li>maxConcurrentCheckpoints用于指定运行中的checkpoint最多可以有多少个，用于包装topology不会花太多的时间在checkpoints上面；如果有设置了minPauseBetweenCheckpoints，则maxConcurrentCheckpoints这个参数就不起作用了(大于1的值不起作用)</li><li>enableExternalizedCheckpoints用于开启checkpoints的外部持久化，但是在job失败的时候不会自动清理，需要自己手工清理state；ExternalizedCheckpointCleanup用于指定当job canceled的时候externalized checkpoint该如何清理，DELETE_ON_CANCELLATION的话，在job canceled的时候会自动删除externalized state，但是如果是FAILED的状态则会保留；RETAIN_ON_CANCELLATION则在job canceled的时候会保留externalized checkpoint state</li><li>failOnCheckpointingErrors用于指定在checkpoint发生异常的时候，是否应该fail该task，默认为true，如果设置为false，则task会拒绝checkpoint然后继续运行</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.env.getCheckpointConfig().setFailOnCheckpointingErrors(true);</code></pre><h1 id="FlinkKafkaConsumer011"><a href="#FlinkKafkaConsumer011" class="headerlink" title="FlinkKafkaConsumer011"></a>FlinkKafkaConsumer011</h1><h2 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h2><ul><li>setStartFromGroupOffsets()【默认消费策略】<br>默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</li><li>setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息</li><li>setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息</li><li>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)</li></ul><ul><li>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</li><li>为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次</li><li>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:<br>(1) Checkpoint关闭时： 可以通过下面两个参数配置<br>enable.auto.commit<br>auto.commit.interval.ms<br>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置<br>setCommitOffsetsOnCheckpoints(boolean)<br>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略</li></ul><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; public class StreamingKafkaSource {    public static void main(String[] args) throws Exception {        //获取Flink的运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //checkpoint配置        env.enableCheckpointing(5000);        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);        env.getCheckpointConfig().setCheckpointTimeout(60000);        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);        //设置statebackend        //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://hadoop100:9000/flink/checkpoints&quot;,true));        String topic = &quot;kafkaConsumer&quot;;        Properties prop = new Properties();        prop.setProperty(&quot;bootstrap.servers&quot;,&quot;SparkMaster:9092&quot;);        prop.setProperty(&quot;group.id&quot;,&quot;kafkaConsumerGroup&quot;);        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;&gt;(topic, new SimpleStringSchema(), prop);        myConsumer.setStartFromGroupOffsets();//默认消费策略        DataStreamSource&lt;String&gt; text = env.addSource(myConsumer);        text.print().setParallelism(1);        env.execute(&quot;StreamingFromCollection&quot;);    }}</code></pre><h1 id="FlinkKafkaProducer011"><a href="#FlinkKafkaProducer011" class="headerlink" title="FlinkKafkaProducer011"></a>FlinkKafkaProducer011</h1><h2 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h2><ul><li>Kafka Producer的容错-Kafka 0.9 and 0.10</li><li>如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数:<br>setLogFailuresOnly(false)<br>setFlushOnCheckpoint(true)</li><li>注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是0】</li><li>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义<br>Semantic.NONE<br>Semantic.AT_LEAST_ONCE【默认】<br>Semantic.EXACTLY_ONCE</li></ul><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><pre><code>public class StreamingKafkaSink {    public static void main(String[] args) throws Exception {    //获取Flink的运行环境    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    //checkpoint配置    env.enableCheckpointing(5000);    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);    env.getCheckpointConfig().setCheckpointTimeout(60000);    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);    env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);    //设置statebackend    //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://SparkMaster:9000/flink/checkpoints&quot;,true));    DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;SparkMaster&quot;, 9001, &quot;\n&quot;);    String brokerList = &quot;SparkMaster:9092&quot;;    String topic = &quot;kafkaProducer&quot;;    Properties prop = new Properties();    prop.setProperty(&quot;bootstrap.servers&quot;,brokerList);    //第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间    //设置事务超时时间    //prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);    //第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。    //FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());    //使用EXACTLY_ONCE语义的kafkaProducer    FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(topic, new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);    text.addSink(myProducer);    env.execute(&quot;StreamingFromCollection&quot;);  }}</code></pre>]]></content>
    
    <summary type="html">
    
      本文主要讲述Flink1.7.1与Kafka0.11.0.1交互相关API的使用与案例。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Kafka EOS 之事务性实现</title>
    <link href="https://gjtmaster.github.io/2018/09/26/Kafka%20EOS%20%E4%B9%8B%E4%BA%8B%E5%8A%A1%E6%80%A7%E5%AE%9E%E7%8E%B0/"/>
    <id>https://gjtmaster.github.io/2018/09/26/Kafka EOS 之事务性实现/</id>
    <published>2018-09-26T10:38:55.000Z</published>
    <updated>2019-10-28T06:33:37.728Z</updated>
    
    <content type="html"><![CDATA[<p>原作者：王蒙</p><p><a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">原文链接</a></p><p>这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempotent Producer 不提供跨多个 Partition 和跨会话场景下的保证，因此，我们是需要一种更强的事务保证，能够原子处理多个 Partition 的写入操作，数据要么全部写入成功，要么全部失败，不期望出现中间状态。这就是 Kafka Transactions 希望解决的问题，简单来说就是能够实现 <code>atomic writes across partitions</code>，本文以 Apache Kafka 2.0.0 代码实现为例，深入分析一下 Kafka 是如何实现这一机制的。</p><p>Apache Kafka 在 Exactly-Once Semantics（EOS）上三种粒度的保证如下（来自 <a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="noopener">Exactly-once Semantics in Apache Kafka</a>）：</p><ol><li>Idempotent Producer：Exactly-once，in-order，delivery per partition；</li><li>Transactions：Atomic writes across partitions；</li><li>Exactly-Once stream processing across read-process-write tasks；</li></ol><p>第二种情况就是本文讲述的主要内容，在讲述整个事务处理流程时，也顺便分析第三种情况。</p><h2 id="Kafka-Transactions"><a href="#Kafka-Transactions" class="headerlink" title="Kafka Transactions"></a>Kafka Transactions</h2><p>Kafka 事务性最开始的出发点是为了在 Kafka Streams 中实现 Exactly-Once 语义的数据处理，这个问题提出之后，在真正的方案讨论阶段，社区又挖掘了更多的应用场景，也为了尽可能覆盖更多的应用场景，在真正的实现中，在很多地方做了相应的 tradeoffs，后面会写篇文章对比一下 RocketMQ 事务性的实现，就能明白 Kafka 事务性实现及应用场景的复杂性了。</p><p>Kafka 的事务处理，主要是允许应用可以把消费和生产的 batch 处理（涉及多个 Partition）在一个原子单元内完成，操作要么全部完成、要么全部失败。为了实现这种机制，我们需要应用能提供一个唯一 id，即使故障恢复后也不会改变，这个 id 就是 TransactionnalId（也叫 txn.id，后面会详细讲述），txn.id 可以跟内部的 PID 1:1 分配，它们不同的是 txn.id 是用户提供的，而 PID 是 Producer 内部自动生成的（并且故障恢复后这个 PID 会变化），有了 txn.id 这个机制，就可以实现多 partition、跨会话的 EOS 语义。</p><p>当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：</p><ol><li>跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；</li><li>跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；</li><li>跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。</li></ol><p>上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：</p><ol><li>对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；</li><li>一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；</li><li>Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；</li><li>Consumer 可能没有订阅这个事务涉及的全部 Partition。</li></ol><p>简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：</p><ol><li>Atomic writes across multiple partitions.</li><li>All messages in a transaction are made visible together, or none are.</li><li>Consumers must be configured to skip uncommitted messages.</li></ol><h2 id="事务性示例"><a href="#事务性示例" class="headerlink" title="事务性示例"></a>事务性示例</h2><p>Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 <code>transactional.id</code>，通过 <code>initTransactions()</code> 初始化事务状态信息，再通过 <code>beginTransaction()</code> 标识一个事务的开始，然后通过 <code>commitTransaction()</code> 或 <code>abortTransaction()</code> 对事务进行 commit 或 abort，示例如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"client.id"</span>, <span class="string">"ProducerTranscationnalExample"</span>);</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"test-transactional"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    String msg = <span class="string">"matt test"</span>;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"0"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"1"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"2"</span>, msg.toString()));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException e1) &#123;</span><br><span class="line">    e1.printStackTrace();</span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e2) &#123;</span><br><span class="line">    e2.printStackTrace();</span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure><p>事务性的 API 也同样保持了 Kafka 一直以来的简洁性，使用起来是非常方便的。</p><h2 id="事务性要解决的问题"><a href="#事务性要解决的问题" class="headerlink" title="事务性要解决的问题"></a>事务性要解决的问题</h2><p>回想一下，前面一篇文章中关于幂等性要解决的问题，事务性其实更多的是解决幂等性中没有解决的问题，比如：</p><ol><li>在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），这相当于出现了中间的状态，这并不是我们期望的结果；</li><li>Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；</li></ol><p>再来分析一下，Kafka 提供的事务性是如何解决上面两个问题的：</p><ol><li>如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；</li><li>其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，具体的实现后续会有文章详细解揭秘）。</li></ol><h2 id="事务性实现的关键"><a href="#事务性实现的关键" class="headerlink" title="事务性实现的关键"></a>事务性实现的关键</h2><p>对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。对于一个事务操作而言，其会涉及到多个 Topic-Partition 数据的写入，如果是一个 long transaction 操作，可能会涉及到非常多的数据，如何才能保证这个事务操作的原子性（要么全部完成，要么全部失败）呢？</p><ol><li>关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：<ul><li>我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；</li><li>Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；</li><li>这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 <strong>TransactionCoordinator</strong> 角色；</li></ul></li><li>有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？<ul><li>TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 <strong>事务状态信息如何恢复？</strong> 也就是事务的状态信息需要<strong>很强的容错性、一致性</strong>；</li><li>关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），那就是 <code>min.isr + ack</code> 机制；</li><li>分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 <code>__consumer_offset</code> 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，而对应事务数据（transaction log）就是 <code>__transaction_state</code> 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator 在做故障恢复也是从这个 topic 中恢复数据；</li></ul></li><li>有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<ul><li>Kafka 在 Producer 端引入了一个 <strong>TransactionalId</strong> 来解决这个问题，这个 txn.id 是由应用来配置的；</li><li>TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方（同一个 TransactionCoordinator）进行处理；</li></ul></li><li>再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：<ul><li>这个就是<strong>事务状态转移</strong>，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态（前提是这个状态转移是有效的）。</li></ul></li></ol><blockquote><p>上面的分析都是个人见解，有问题欢迎指正~</p></blockquote><p>下面这节就讲述一下事务性实现的一些关键的实现机制（对这些细节不太感兴趣或者之前没有深入接触过 Kafka，可以直接跳过，直接去看下一节的事务流程处理，先去了解一下一个事务操作的主要流程步骤）。</p><h3 id="TransactionCoordinator"><a href="#TransactionCoordinator" class="headerlink" title="TransactionCoordinator"></a>TransactionCoordinator</h3><p>TransactionCoordinator 与 GroupCoordinator 有一些相似之处，它主要是处理来自 Transactional Producer 的一些与事务相关的请求，涉及的请求如下表所示（关于这些请求处理的详细过程会在下篇文章详细讲述，这里先有个大概的认识即可）：</p><table><thead><tr><th>请求类型</th><th>用途说明</th></tr></thead><tbody><tr><td>ApiKeys.FIND_COORDINATOR</td><td>Transaction Producer 会发送这个 FindCoordinatorRequest 请求，来查询当前事务（txn.id）对应的 TransactionCoordinator，这个与 GroupCoordinator 查询类似，是根据 txn.id 的 hash 值取模找到对应 Partition 的 leader，这个 leader 就是该事务对应的 TransactionCoordinator</td></tr><tr><td>ApiKeys.INIT_PRODUCER_ID</td><td>Producer 初始化时，会发送一个 InitProducerIdRequest 请求，来获取其分配的 PID 信息，对于幂等性的 Producer，会随机选择一台 broker 发送请求，而对于 Transaction Producer 会选择向其对应的 TransactionCoordinator 发送该请求（目的是为了根据 txn.id 对应的事务状态做一些判断）</td></tr><tr><td>ApiKeys.ADD_PARTITIONS_TO_TXN</td><td>将这个事务涉及到的 topic-partition 列表添加到事务的 meta 信息中（通过 AddPartitionsToTxnRequest 请求），事务 meta 信息需要知道当前的事务操作涉及到了哪些 Topic-Partition 的写入</td></tr><tr><td>ApiKeys.ADD_OFFSETS_TO_TXN</td><td>Transaction Producer 的这个 AddOffsetsToTxnRequest 请求是由 <code>sendOffsetsToTransaction()</code> 接口触发的，它主要是用在 consume-process-produce 的场景中，这时候 consumer 也是整个事务的一部分，只有这个事务 commit 时，offset 才会被真正 commit（主要还是用于 Failover）</td></tr><tr><td>ApiKeys.END_TXN</td><td>当提交事务时， Transaction Producer 会向 TransactionCoordinator 发送一个 EndTxnRequest 请求，来 commit 或者 abort 事务</td></tr></tbody></table><p>TransactionCoordinator 对象中还有两个关键的对象，分别是:</p><ol><li>TransactionStateManager：这个对象，从名字应该就能大概明白其作用是关于事务的状态管理，它会维护分配到这个 TransactionCoordinator 的所有事务的 meta 信息；</li><li>TransactionMarkerChannelManager：这个主要是用于向其他的 Broker 发送 Transaction Marker 数据，关于 Transaction Marker，第一次接触的人，可能会有一些困惑，什么是 Transaction Marker，Transaction Marker 是用来解决什么问题的呢？这里先留一个疑问，后面会来解密。</li></ol><p>总结一下，TransactionCoordinator 主要的功能有三个，分别是：</p><ol><li>处理事务相关的请求；</li><li>维护事务的状态信息；</li><li>向其他 Broker 发送 Transaction Marker 数据。</li></ol><h3 id="Transaction-Log（-transaction-state）"><a href="#Transaction-Log（-transaction-state）" class="headerlink" title="Transaction Log（__transaction_state）"></a>Transaction Log（__transaction_state）</h3><p>在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 topic <code>__transaction_state</code>，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 <code>__transaction_state</code> 的 partition 数取模得到，<code>__transaction_state</code> Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 <code>__transaction_state</code> Partition 2 的 leader 来处理。</p><p>对于 <code>__transaction_state</code> 这个 topic 默认是由 Server 端的 <code>transaction.state.log.replication.factor</code> 参数来配置，默认是3，如果当前 leader 故障，需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？</p><p>正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 <code>__transaction_state</code>中读取之前事务的日志信息，来恢复其状态信息，前提是要求事务日志写入做相应的不丢配置。这也是 <code>__transaction_state</code> 一个重要作用之一，用于 TransactionCoordinator 的恢复，<code>__transaction_state</code> 与 <code>__consumer_offsets</code> 一样是 compact 类型的 topic，其 scheme 如下：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Key</span> =&gt; <span class="type">Version</span> <span class="type">TransactionalId</span></span><br><span class="line">    <span class="type">Version</span> =&gt; <span class="number">0</span> (int16)</span><br><span class="line">    <span class="type">TransactionalId</span> =&gt; <span class="type">String</span></span><br><span class="line"></span><br><span class="line"><span class="type">Value</span> =&gt; <span class="type">Version</span> <span class="type">ProducerId</span> <span class="type">ProducerEpoch</span> <span class="type">TxnTimeoutDuration</span> <span class="type">TxnStatus</span> [<span class="type">TxnPartitions</span>] <span class="type">TxnEntryLastUpdateTime</span> <span class="type">TxnStartTime</span></span><br><span class="line">    <span class="type">Version</span> =&gt; <span class="number">0</span> (int16)</span><br><span class="line">    <span class="type">ProducerId</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">ProducerEpoch</span> =&gt; int16</span><br><span class="line">    <span class="type">TxnTimeoutDuration</span> =&gt; <span class="built_in">int32</span></span><br><span class="line">    <span class="type">TxnStatus</span> =&gt; int8</span><br><span class="line">    <span class="type">TxnPartitions</span> =&gt; [<span class="type">Topic</span> [<span class="type">Partition</span>]]</span><br><span class="line">        <span class="type">Topic</span> =&gt; <span class="type">String</span></span><br><span class="line">        <span class="type">Partition</span> =&gt; <span class="built_in">int32</span></span><br><span class="line">    <span class="type">TxnLastUpdateTime</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">TxnStartTime</span> =&gt; <span class="built_in">int64</span></span><br></pre></td></tr></table></figure><h3 id="Transaction-Marker"><a href="#Transaction-Marker" class="headerlink" title="Transaction Marker"></a>Transaction Marker</h3><p>终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？</p><p>Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）</p><ol><li><p>Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；</p></li><li><p>在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？</p><ul><li>这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；</li></ul><p>再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？</p><ol><li>首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，是可以共享这个 TCP 连接的；</li><li>有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 信息做相应的处理。</li></ol></li></ol><p>Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ControlMessageKey</span> =&gt; <span class="type">Version</span> <span class="type">ControlMessageType</span></span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">ControlMessageType</span> =&gt; int16</span><br><span class="line"></span><br><span class="line"><span class="type">TransactionControlMessageValue</span> =&gt; <span class="type">Version</span> <span class="type">CoordinatorEpoch</span></span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">CoordinatorEpoch</span> =&gt; <span class="built_in">int32</span></span><br></pre></td></tr></table></figure><p>这里再讲一个额外的内容，对于事务写入的数据，为了给消息添加一个标识（标识这条消息是不是来自事务写入的），<strong>数据格式（消息协议）发生了变化</strong>，这个改动主要是在 Attribute 字段，对于 MessageSet，Attribute 是16位，新的格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (6-15) | Control (5) | Transactional (4) | Timestamp<span class="built_in"> Type </span>(3) | Compression<span class="built_in"> Type </span>(0-2) |</span><br></pre></td></tr></table></figure><p>对于 Message，也就是单条数据存储时（其中 Marker 数据都是单条存储的），在 Kafka 中，只有 MessageSet 才可以做压缩，所以 Message 就没必要设置压缩字段，其格式如下：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|<span class="string"> Unused (1-7) </span>|<span class="string"> Control Flag(0) </span>|</span><br></pre></td></tr></table></figure><h3 id="Server-端事务状态管理"><a href="#Server-端事务状态管理" class="headerlink" title="Server 端事务状态管理"></a>Server 端事务状态管理</h3><p>TransactionCoordinator 会维护相应的事务的状态信息（也就是 TxnStatus），对于一个事务，总共有以下几种状态：</p><table><thead><tr><th>状态</th><th>状态码</th><th>说明</th></tr></thead><tbody><tr><td>Empty</td><td>0</td><td>Transaction has not existed yet</td></tr><tr><td>Ongoing</td><td>1</td><td>Transaction has started and ongoing</td></tr><tr><td>PrepareCommit</td><td>2</td><td>Group is preparing to commit</td></tr><tr><td>PrepareAbort</td><td>3</td><td>Group is preparing to abort</td></tr><tr><td>CompleteCommit</td><td>4</td><td>Group has completed commit</td></tr><tr><td>CompleteAbort</td><td>5</td><td>Group has completed abort</td></tr><tr><td>Dead</td><td>6</td><td>TransactionalId has expired and is about to be removed from the transaction cache</td></tr><tr><td>PrepareEpochFence</td><td>7</td><td>We are in the middle of bumping the epoch and fencing out older producers</td></tr></tbody></table><p>其相应有效的状态转移图如下：</p><p><a href="http://matt33.com/images/kafka/server-txn.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/server-txn.png" alt="Server 端 Transaction 的状态转移图"></a>Server 端 Transaction 的状态转移图</p><p>正常情况下，对于一个事务而言，其状态状态流程应该是 Empty –&gt; Ongoing –&gt; PrepareCommit –&gt; CompleteCommit –&gt; Empty 或者是 Empty –&gt; Ongoing –&gt; PrepareAbort –&gt; CompleteAbort –&gt; Empty。</p><h3 id="Client-端事务状态管理"><a href="#Client-端事务状态管理" class="headerlink" title="Client 端事务状态管理"></a>Client 端事务状态管理</h3><p>Client 的事务状态信息主要记录本地事务的状态，当然跟其他的系统类似，本地的状态信息与 Server 端的状态信息并不完全一致（状态的设置，就像 GroupCoodinator 会维护一个 Group 的状态，每个 Consumer 也会维护本地的 Consumer 对象的状态一样）。Client 端的事务状态信息主要用于 Client 端的事务状态处理，其主要有以下几种：</p><ol><li>UNINITIALIZED：Transactional Producer 初始化时的状态，此时还没有事务处理；</li><li>INITIALIZING：Transactional Producer 调用 <code>initTransactions()</code> 方法初始化事务相关的内容，比如发送 InitProducerIdRequest 请求；</li><li>READY：对于新建的事务，Transactional Producer 收到来自 TransactionCoordinator 的 InitProducerIdResponse 后，其状态会置为 READY（对于已有的事务而言，是当前事务完成后 Client 的状态会转移为 READY）；</li><li>IN_TRANSACTION：Transactional Producer 调用 <code>beginTransaction()</code> 方法，开始一个事务，标志着一个事务开始初始化；</li><li>COMMITTING_TRANSACTION：Transactional Producer 调用 <code>commitTransaction()</code> 方法时，会先更新本地的状态信息；</li><li>ABORTING_TRANSACTION：Transactional Producer 调用 <code>abortTransaction()</code> 方法时，会先更新本地的状态信息；</li><li>ABORTABLE_ERROR：在一个事务操作中，如果有数据发送失败，本地状态会转移到这个状态，之后再自动 abort 事务；</li><li>FATAL_ERROR：转移到这个状态之后，再进行状态转移时，会抛出异常；</li></ol><p>Client 端状态如下图：</p><p><a href="http://matt33.com/images/kafka/client-txn.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/client-txn.png" alt="Client 端 Transaction 的状态转移图"></a>Client 端 Transaction 的状态转移图</p><h2 id="事务性的整体流程"><a href="#事务性的整体流程" class="headerlink" title="事务性的整体流程"></a>事务性的整体流程</h2><p>有了前面对 Kafka 事务性关键实现的讲述之后，这里详细讲述一个事务操作的处理流程，当然这里只是重点讲述事务性相关的内容，官方版的流程图可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-DataFlow" target="_blank" rel="noopener">Kafka Exactly-Once Data Flow</a>，这里我做了一些改动，其流程图如下：</p><p><a href="http://matt33.com/images/kafka/txn-data-flow.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/txn-data-flow.png" alt="consume-process-produce 事务的处理流程"></a>consume-process-produce 事务的处理流程</p><p>这个流程是以 consume-process-produce 场景为例（主要是 kafka streams 的场景），图中红虚框及 4.3a 部分是关于 consumer 的操作，去掉这部分的话，就是只考虑写入情况的场景。这种只考虑写入场景的事务操作目前在业内应用也是非常广泛的，比如 Flink + Kafka 端到端的 Exactly-Once 实现就是这种场景，下面来详细讲述一下整个流程。</p><h3 id="1-Finding-a-TransactionCoordinator"><a href="#1-Finding-a-TransactionCoordinator" class="headerlink" title="1. Finding a TransactionCoordinator"></a>1. Finding a TransactionCoordinator</h3><p>对于事务性的处理，第一步首先需要做的就是找到这个事务 txn.id 对应的 TransactionCoordinator，Transaction Producer 会向 Broker （随机选择一台 broker，一般选择本地连接最少的这台 broker）发送 FindCoordinatorRequest 请求，获取其 TransactionCoordinator。</p><p>怎么找到对应的 TransactionCoordinator 呢？这个前面已经讲过了，主要是通过下面的方法获取 <code>__transaction_state</code> 的 Partition，该 Partition 对应的 leader 就是这个 txn.id 对应的 TransactionCoordinator。</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def partitionFor(transactionalId: <span class="built_in">String</span>): <span class="built_in">Int</span> = Utils.<span class="built_in">abs</span>(transactionalId.hashCode) % transactionTopicPartitionCount</span><br></pre></td></tr></table></figure><h3 id="2-Getting-a-PID"><a href="#2-Getting-a-PID" class="headerlink" title="2. Getting a PID"></a>2. Getting a PID</h3><p>PID 这里就不再介绍了，不了解的可以看前面那篇文章（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#PID" target="_blank" rel="noopener">Producer ID</a>）。</p><p>Transaction Producer 在 <code>initializeTransactions()</code> 方法中会向 TransactionCoordinator 发送 InitPidRequest 请求获取其分配的 PID，有了 PID，事务写入时可以保证幂等性，PID 如何分配可以参考 <a href="http://matt33.com/2018/10/24/kafka-idempotent/#Producer-PID-%E7%94%B3%E8%AF%B7" target="_blank" rel="noopener">PID 分配</a>，但是 TransactionCoordinator 在给事务 Producer 分配 PID 会做一些判断，主要的内容是：</p><ol><li><p>如果这个 txn.id 之前没有相应的事务状态（new txn.id），那么会初始化其事务 meta 信息 TransactionMetadata（会给其分配一个 PID，初始的 epoch 为-1），如果有事务状态，获取之前的状态；</p></li><li><p>校验其 TransactionMetadata 的状态信息（参考下面代码中</p></li></ol>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">prepareInitProduceIdTransit</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法）：</p><ol><li>如果前面还有状态转移正在进行，直接返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果此时的状态为 PrepareAbort 或 PrepareCommit，返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果之前的状态为 CompleteAbort、CompleteCommit 或 Empty，那么先将状态转移为 Empty，然后更新一下 epoch 值；</li><li>如果之前的状态为 Ongoing，状态会转移成 PrepareEpochFence，然后再 abort 当前的事务，并向 client 返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果状态为 Dead 或 PrepareEpochFence，直接抛出相应的 FATAL 异常；</li></ol><ol start="3"><li>将 txn.id 与相应的 TransactionMetadata 持久化到事务日志中，对于 new txn.id，这个持久化的数据主要时 txn.id 与 pid 关系信息，如图中的 3a 所示。</li></ol><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: producer 启用事务性的情况下，检测此时事务的状态信息</span></span><br><span class="line"><span class="keyword">private</span> def prepareInitProduceIdTransit(transactionalId: <span class="built_in">String</span>,</span><br><span class="line">                                        transactionTimeoutMs: Int,</span><br><span class="line">                                        coordinatorEpoch: Int,</span><br><span class="line">                                        txnMetadata: TransactionMetadata): ApiResult[(Int, TxnTransitMetadata)] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (txnMetadata.pendingTransitionInProgress) &#123;</span><br><span class="line">    <span class="comment">// return a retriable exception to let the client backoff and retry</span></span><br><span class="line">    Left(Errors.CONCURRENT_TRANSACTIONS)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// caller should have synchronized on txnMetadata already</span></span><br><span class="line">    txnMetadata.state match &#123;</span><br><span class="line">      <span class="keyword">case</span> PrepareAbort | <span class="function"><span class="params">PrepareCommit</span> =&gt;</span></span><br><span class="line">        <span class="comment">// reply to client and let it backoff and retry</span></span><br><span class="line">        Left(Errors.CONCURRENT_TRANSACTIONS)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> CompleteAbort | CompleteCommit | <span class="function"><span class="params">Empty</span> =&gt;</span> <span class="comment">//note: 此时需要将状态转移到 Empty（此时状态并没有转移，只是在 PendingState 记录了将要转移的状态）</span></span><br><span class="line">        val transitMetadata = <span class="keyword">if</span> (txnMetadata.isProducerEpochExhausted) &#123;</span><br><span class="line">          val newProducerId = producerIdManager.generateProducerId()</span><br><span class="line">          txnMetadata.prepareProducerIdRotation(newProducerId, transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 增加 producer 的 epoch 值</span></span><br><span class="line">          txnMetadata.prepareIncrementProducerEpoch(transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Right(coordinatorEpoch, transitMetadata)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="function"><span class="params">Ongoing</span> =&gt;</span> <span class="comment">//note: abort 当前的事务，并返回一个 CONCURRENT_TRANSACTIONS 异常，强制 client 去重试</span></span><br><span class="line">        <span class="comment">// indicate to abort the current ongoing txn first. Note that this epoch is never returned to the</span></span><br><span class="line">        <span class="comment">// user. We will abort the ongoing transaction and return CONCURRENT_TRANSACTIONS to the client.</span></span><br><span class="line">        <span class="comment">// This forces the client to retry, which will ensure that the epoch is bumped a second time. In</span></span><br><span class="line">        <span class="comment">// particular, if fencing the current producer exhausts the available epochs for the current producerId,</span></span><br><span class="line">        <span class="comment">// then when the client retries, we will generate a new producerId.</span></span><br><span class="line">        Right(coordinatorEpoch, txnMetadata.prepareFenceProducerEpoch())</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> Dead | <span class="function"><span class="params">PrepareEpochFence</span> =&gt;</span> <span class="comment">//note: 返回错误</span></span><br><span class="line">        val errorMsg = s<span class="string">"Found transactionalId $transactionalId with state $&#123;txnMetadata.state&#125;. "</span> +</span><br><span class="line">          s<span class="string">"This is illegal as we should never have transitioned to this state."</span></span><br><span class="line">        fatal(errorMsg)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(errorMsg)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-Starting-a-Transaction"><a href="#3-Starting-a-Transaction" class="headerlink" title="3. Starting a Transaction"></a>3. Starting a Transaction</h3><p>前面两步都是 Transaction Producer 调用 <code>initTransactions()</code> 部分，到这里，Producer 可以调用 <code>beginTransaction()</code> 开始一个事务操作，其实现方法如下面所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 应该在一个事务操作之前进行调用</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    transactionManager.beginTransaction();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TransactionManager</span></span><br><span class="line"><span class="comment">//note: 在一个事务开始之前进行调用，这里实际上只是转换了状态（只在 producer 本地记录了状态的开始）</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.IN_TRANSACTION);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里只是将本地事务状态转移成 IN_TRANSACTION，并没有与 Server 端进行交互，所以在流程图中没有体现出来（TransactionManager 初始化时，其状态为 UNINITIALIZED，Producer 调用 <code>initializeTransactions()</code> 方法，其状态转移成 INITIALIZING）。</p><h3 id="4-Consume-Porcess-Produce-Loop"><a href="#4-Consume-Porcess-Produce-Loop" class="headerlink" title="4. Consume-Porcess-Produce Loop"></a>4. Consume-Porcess-Produce Loop</h3><p>在这个阶段，Transaction Producer 会做相应的处理，主要包括：从 consumer 拉取数据、对数据做相应的处理、通过 Producer 写入到下游系统中（对于只有写入场景，忽略前面那一步即可），下面有一个示例（start 和 end 中间的部分），是一个典型的 consume-process-produce 场景：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = consumer.poll(Long.MAX_VALUE);</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    //start</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord <span class="keyword">record</span> : <span class="type">records</span>)&#123;</span><br><span class="line">        producer.send(producerRecord(“outputTopic1”, <span class="keyword">record</span>));</span><br><span class="line">        producer.send(producerRecord(“outputTopic2”, <span class="keyword">record</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class="line">    //<span class="keyword">end</span></span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面来结合前面的流程图来讲述一下这部分的实现。</p><h4 id="4-1-AddPartitionsToTxnRequest"><a href="#4-1-AddPartitionsToTxnRequest" class="headerlink" title="4.1. AddPartitionsToTxnRequest"></a>4.1. AddPartitionsToTxnRequest</h4><p>Producer 在调用 <code>send()</code> 方法时，Producer 会将这个对应的 Topic—Partition 添加到 TransactionManager 的记录中，如下所示：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如何开启了幂等性或事务性，需要做一些处理</span></span><br><span class="line"><span class="keyword">if</span> (transactionManager != <span class="built_in">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</span><br></pre></td></tr></table></figure><p>如果这个 Topic-Partition 之前不存在，那么就添加到 newPartitionsInTransaction 集合中，如下所示：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 tp 添加到 newPartitionsInTransaction 中，记录当前进行事务操作的 tp</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> maybeAddPartitionToTransaction(TopicPartition topicPartition) &#123;</span><br><span class="line">    failIfNotReadyForSend();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 partition 已经添加到 partitionsInTransaction、pendingPartitionsInTransaction、newPartitionsInTransaction中</span></span><br><span class="line">    <span class="keyword">if</span> (isPartitionAdded(topicPartition) || isPartitionPendingAdd(topicPartition))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">log</span>.debug(<span class="string">"Begin adding new partition &#123;&#125; to transaction"</span>, topicPartition);</span><br><span class="line">    newPartitionsInTransaction.<span class="built_in">add</span>(topicPartition);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Producer 端的 Sender 线程会将这个信息通过 AddPartitionsToTxnRequest 请求发送给 TransactionCoordinator，也就是图中的 4.1 过程，TransactionCoordinator 会将这个 Topic-Partition 列表更新到 txn.id 对应的 TransactionMetadata 中，并且会持久化到事务日志中，也就是图中的 4.1 a 部分，这里持久化的数据主要是 txn.id 与其涉及到的 Topic-Partition 信息。</p><h4 id="4-2-ProduceRequest"><a href="#4-2-ProduceRequest" class="headerlink" title="4.2. ProduceRequest"></a>4.2. ProduceRequest</h4><p>这一步与正常 Producer 写入基本上一样，就是相应的 Leader 在持久化数据时会在头信息中标识这条数据是不是来自事务 Producer 的写入（主要是数据协议有变动，Server 处理并不需要做额外的处理）。</p><h4 id="4-3-AddOffsetsToTxnRequest"><a href="#4-3-AddOffsetsToTxnRequest" class="headerlink" title="4.3. AddOffsetsToTxnRequest"></a>4.3. AddOffsetsToTxnRequest</h4><p>Producer 在调用 <code>sendOffsetsToTransaction()</code> 方法时，第一步会首先向 TransactionCoordinator 发送相应的 AddOffsetsToTxnRequest 请求，如下所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProcducer</span></span><br><span class="line"><span class="comment">//note: 当你需要 batch 的消费-处理-写入消息，这个方法需要被使用</span></span><br><span class="line"><span class="comment">//note: 发送指定的 offset 给 group coordinator，用来标记这些 offset 是作为当前事务的一部分，只有这次事务成功时</span></span><br><span class="line"><span class="comment">//note: 这些 offset 才会被认为 commit 了</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="line">                                     String consumerGroupId) <span class="keyword">throws</span> ProducerFencedException &#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 发送 AddOffsetsToTxRequest</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="line">                                                                        String consumerGroupId) &#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.IN_TRANSACTION)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Cannot send offsets to transaction either because the producer is not in an "</span> +</span><br><span class="line">                <span class="string">"active transaction"</span>);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding offsets &#123;&#125; for consumer group &#123;&#125; to transaction"</span>, offsets, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnRequest.Builder builder = <span class="keyword">new</span> AddOffsetsToTxnRequest.Builder(transactionalId,</span><br><span class="line">            producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnHandler <span class="keyword">handler</span> = <span class="keyword">new</span> AddOffsetsToTxnHandler(builder, offsets);</span><br><span class="line">    enqueueRequest(<span class="keyword">handler</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">handler</span>.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransactionCoordinator 在收到这个请求时，处理方法与 4.1 中的一样，把这个 group.id 对应的 <code>__consumer_offsets</code> 的 Partition （与写入涉及的 Topic-Partition 一样）保存到事务对应的 meta 中，之后会持久化相应的事务日志，如图中 4.3a 所示。</p><h4 id="4-4-TxnOffsetsCommitRequest"><a href="#4-4-TxnOffsetsCommitRequest" class="headerlink" title="4.4. TxnOffsetsCommitRequest"></a>4.4. TxnOffsetsCommitRequest</h4><p>Producer 在收到 TransactionCoordinator 关于 AddOffsetsToTxnRequest 请求的结果后，后再次发送 TxnOffsetsCommitRequest 请求给对应的 GroupCoordinator，AddOffsetsToTxnHandler 的 <code>handleResponse()</code> 的实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(AbstractResponse response)</span> </span>&#123;</span><br><span class="line">    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;</span><br><span class="line">    Errors <span class="keyword">error</span> = addOffsetsToTxnResponse.<span class="keyword">error</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">error</span> == Errors.NONE) &#123;</span><br><span class="line">        log.debug(<span class="string">"Successfully added partition for consumer group &#123;&#125; to transaction"</span>, builder.consumerGroupId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// note the result is not completed until the TxnOffsetCommit returns</span></span><br><span class="line">        <span class="comment">//note: AddOffsetsToTnxRequest 之后，还会再发送 TxnOffsetCommitRequest</span></span><br><span class="line">        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));</span><br><span class="line">        transactionStarted = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.COORDINATOR_NOT_AVAILABLE || <span class="keyword">error</span> == Errors.NOT_COORDINATOR)</span> </span>&#123;</span><br><span class="line">        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.COORDINATOR_LOAD_IN_PROGRESS || <span class="keyword">error</span> == Errors.CONCURRENT_TRANSACTIONS)</span> </span>&#123;</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.INVALID_PRODUCER_EPOCH)</span> </span>&#123;</span><br><span class="line">        fatalError(<span class="keyword">error</span>.exception());</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED)</span> </span>&#123;</span><br><span class="line">        fatalError(<span class="keyword">error</span>.exception());</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.GROUP_AUTHORIZATION_FAILED)</span> </span>&#123;</span><br><span class="line">        abortableError(<span class="keyword">new</span> GroupAuthorizationException(builder.consumerGroupId()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fatalError(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in AddOffsetsToTxnResponse: "</span> + <span class="keyword">error</span>.message()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>GroupCoordinator 在收到相应的请求后，会将 offset 信息持久化到 consumer offsets log 中（包含对应的 PID 信息），但是<strong>不会更新到缓存</strong>中，除非这个事务 commit 了，这样的话就可以保证这个 offset 信息对 consumer 是不可见的（没有更新到缓存中的数据是不可见的，通过接口是获取的，这是 GroupCoordinator 本身来保证的）。</p><h3 id="5-Committing-or-Aborting-a-Transaction"><a href="#5-Committing-or-Aborting-a-Transaction" class="headerlink" title="5.Committing or Aborting a Transaction"></a>5.Committing or Aborting a Transaction</h3><p>在一个事务操作处理完成之后，Producer 需要调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p><h4 id="5-1-EndTxnRequest"><a href="#5-1-EndTxnRequest" class="headerlink" title="5.1. EndTxnRequest"></a>5.1. EndTxnRequest</h4><p>无论是 Commit 还是 Abort，对于 Producer 而言，都是向 TransactionCoordinator 发送 EndTxnRequest 请求，这个请求的内容里会标识是 commit 操作还是 abort 操作，Producer 的 <code>commitTransaction()</code> 方法实现如下所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: commit 正在进行的事务操作，这个方法在真正发送 commit 之后将会 flush 所有未发送的数据</span></span><br><span class="line"><span class="comment">//note: 如果在发送中遇到任何一个不能修复的错误，这个方法抛出异常，事务也不会被提交，所有 send 必须成功，这个事务才能 commit 成功</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginCommit();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 开始 commit，转移本地本地保存的状态以及发送相应的请求</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">TransactionalRequestResult <span class="title">beginCommit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.COMMITTING_TRANSACTION);</span><br><span class="line">    <span class="function"><span class="keyword">return</span> <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult.COMMIT)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Producer 的 <code>abortTransaction()</code> 方法实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 取消正在进行事务，任何没有 flush 的数据都会被丢弃</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginAbort();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">TransactionalRequestResult <span class="title">beginAbort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.ABORTABLE_ERROR)</span><br><span class="line">        maybeFailWithError();</span><br><span class="line">    transitionTo(State.ABORTING_TRANSACTION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We're aborting the transaction, so there should be no need to add new partitions</span></span><br><span class="line">    newPartitionsInTransaction.clear();</span><br><span class="line">    <span class="function"><span class="keyword">return</span> <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult.ABORT)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它们最终都是调用了 TransactionManager 的 <code>beginCompletingTransaction()</code> 方法，这个方法会向其 待发送请求列表 中添加 EndTxnRequest 请求，其实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 发送 EndTxnRequest 请求，添加到 pending 队列中</span></span><br><span class="line"><span class="keyword">private</span> <span class="function">TransactionalRequestResult <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult transactionResult)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!newPartitionsInTransaction.isEmpty())</span><br><span class="line">        enqueueRequest(addPartitionsToTransactionHandler());</span><br><span class="line">    EndTxnRequest.Builder builder = <span class="keyword">new</span> EndTxnRequest.Builder(transactionalId, producerIdAndEpoch.producerId,</span><br><span class="line">            producerIdAndEpoch.epoch, transactionResult);</span><br><span class="line">    EndTxnHandler <span class="keyword">handler</span> = <span class="keyword">new</span> EndTxnHandler(builder);</span><br><span class="line">    enqueueRequest(<span class="keyword">handler</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">handler</span>.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransactionCoordinator 在收到 EndTxnRequest 请求后，会做以下处理：</p><ol><li>更新事务的 meta 信息，状态转移成 PREPARE_COMMIT 或 PREPARE_ABORT，并将事务状态信息持久化到事务日志中；</li><li>根据事务 meta 信息，向其涉及到的所有 Topic-Partition 的 leader 发送 Transaction Marker 信息（也就是 WriteTxnMarkerRquest 请求，见下面的 5.2 分析）；</li><li>最后将事务状态更新为 COMMIT 或者 ABORT，并将事务的 meta 持久化到事务日志中，也就是 5.3 步骤。</li></ol><h4 id="5-2-WriteTxnMarkerRquest"><a href="#5-2-WriteTxnMarkerRquest" class="headerlink" title="5.2. WriteTxnMarkerRquest"></a>5.2. WriteTxnMarkerRquest</h4><p>WriteTxnMarkerRquest 是 TransactionCoordinator 收到 Producer 的 EndTxnRequest 请求后向其他 Broker 发送的请求，主要是告诉它们事务已经完成。不论是普通的 Topic-Partition 还是 <code>__consumer_offsets</code>，在收到这个请求后，都会把事务结果（Transaction Marker 的格数据式见前面）持久化到对应的日志文件中，这样下游 Consumer 在消费这个数据时，就知道这个事务是 commit 还是 abort。</p><h4 id="5-3-Writing-the-Final-Commit-or-Abort-Message"><a href="#5-3-Writing-the-Final-Commit-or-Abort-Message" class="headerlink" title="5.3. Writing the Final Commit or Abort Message"></a>5.3. Writing the Final Commit or Abort Message</h4><p>当这个事务涉及到所有 Topic-Partition 都已经把这个 marker 信息持久化到日志文件之后，TransactionCoordinator 会将这个事务的状态置为 COMMIT 或 ABORT，并持久化到事务日志文件中，到这里，这个事务操作就算真正完成了，TransactionCoordinator 缓存的很多关于这个事务的数据可以被清除了。</p><h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>在上面讲述完 Kafka 事务性处理之后，我们来思考一下以下这些问题，上面的流程可能会出现下面这些问题或者很多人可能会有下面的疑问：</p><ol><li>txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？</li><li>TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？</li><li>对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？</li><li>对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？</li><li>如果 txn.id 长期不使用，server 端怎么处理？</li><li>PID Snapshot 是做什么的？是用来解决什么问题？</li></ol><p>下面，来详细分析一下上面提到的这些问题。</p><h3 id="如果多个-Producer-使用同一个-txn-id-会出现什么情况？"><a href="#如果多个-Producer-使用同一个-txn-id-会出现什么情况？" class="headerlink" title="如果多个 Producer 使用同一个 txn.id 会出现什么情况？"></a>如果多个 Producer 使用同一个 txn.id 会出现什么情况？</h3><p>对于这个情况，我们这里直接做了一个相应的实验，两个 Producer 示例都使用了同一个 txn.id（为 test-transactional-matt），Producer 1 先启动，然后过一会再启动 Producer 2，这时候会发现一个现象，那就是 Producer 1 进程会抛出异常退出进程，其异常信息为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.KafkaException</span>: Cannot execute transactional method because we are <span class="keyword">in</span> an error state</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.internals</span><span class="selector-class">.TransactionManager</span><span class="selector-class">.maybeFailWithError</span>(TransactionManager<span class="selector-class">.java</span>:<span class="number">784</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.internals</span><span class="selector-class">.TransactionManager</span><span class="selector-class">.beginTransaction</span>(TransactionManager<span class="selector-class">.java</span>:<span class="number">215</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.KafkaProducer</span><span class="selector-class">.beginTransaction</span>(KafkaProducer<span class="selector-class">.java</span>:<span class="number">606</span>)</span><br><span class="line">at com<span class="selector-class">.matt</span><span class="selector-class">.test</span><span class="selector-class">.kafka</span><span class="selector-class">.producer</span><span class="selector-class">.ProducerTransactionExample</span><span class="selector-class">.main</span>(ProducerTransactionExample<span class="selector-class">.java</span>:<span class="number">68</span>)</span><br><span class="line">Caused by: org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.errors</span><span class="selector-class">.ProducerFencedException</span>: Producer attempted an operation with an old epoch. Either there is <span class="selector-tag">a</span> newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br></pre></td></tr></table></figure><p>这里抛出了 ProducerFencedException 异常，如果打开相应的 Debug 日志，在 Producer 1 的日志文件会看到下面的日志信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-03 12:48:52,495] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Transition from state COMMITTING_TRANSACTION to error state FATAL_ERROR (org.apache.kafka.clients.producer.internals.TransactionManager)</span><br><span class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation <span class="keyword">with</span> an <span class="keyword">old</span> epoch. Either there <span class="keyword">is</span> a newer producer <span class="keyword">with</span> the same transactionalId, <span class="keyword">or</span> the producer<span class="string">'s transaction has been expired by the broker.</span></span><br><span class="line"><span class="string">[2018-11-03 12:48:52,498] ERROR [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)</span></span><br><span class="line"><span class="string">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer'</span>s <span class="keyword">transaction</span> has been expired <span class="keyword">by</span> the broker.</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">599</span>] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Closing the Kafka producer <span class="keyword">with</span> timeoutMillis = <span class="number">9223372036854775807</span> ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">599</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] <span class="keyword">Beginning</span> <span class="keyword">shutdown</span> <span class="keyword">of</span> Kafka producer I/O <span class="keyword">thread</span>, sending remaining records. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">601</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> connections-closed: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">601</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> connections-created: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> successful-<span class="keyword">authentication</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">failed</span>-<span class="keyword">authentication</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-sent-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">603</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-sent: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">603</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">select</span>-<span class="built_in">time</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> io-<span class="built_in">time</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.bytes-received (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.latency (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span><span class="keyword">bytes</span>-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span><span class="keyword">bytes</span>-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span>latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span><span class="keyword">bytes</span>-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span><span class="keyword">bytes</span>-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span>latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">607</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] <span class="keyword">Shutdown</span> <span class="keyword">of</span> Kafka producer I/O <span class="keyword">thread</span> has completed. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">607</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] <span class="keyword">ERROR</span> Forcing producer <span class="keyword">close</span>! (com.matt.test.kafka.producer.ProducerTransactionExample)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Closing the Kafka producer <span class="keyword">with</span> timeoutMillis = <span class="number">9223372036854775807</span> ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br></pre></td></tr></table></figure><p>Producer 1 本地事务状态从 COMMITTING_TRANSACTION 变成了 FATAL_ERROR 状态，导致 Producer 进程直接退出了，出现这个异常的原因，就是抛出的 ProducerFencedException 异常，简单来说 Producer 1 被 Fencing 了（这是 Producer Fencing 的情况）。因此，这个问题的答案就很清除了，如果多个 Producer 共用一个 txn.id，那么最后启动的 Producer 会成功运行，会它之前启动的 Producer 都 Fencing 掉（至于为什么会 Fencing 下一小节会做分析）。</p><h3 id="Fencing"><a href="#Fencing" class="headerlink" title="Fencing"></a>Fencing</h3><p>关于 Fencing 这个机制，在分布式系统还是很常见的，我第一个见到这个机制是在 HDFS 中，可以参考我之前总结的一篇文章 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">HDFS NN 脑裂问题</a>，Fencing 机制解决的主要也是这种类型的问题 —— 脑裂问题，简单来说就是，本来系统这个组件在某个时刻应该只有一个处于 active 状态的，但是在实际生产环境中，特别是切换期间，可能会同时出现两个组件处于 active 状态，这就是脑裂问题，在 Kafka 的事务场景下，用到 Fencing 机制有两个地方：</p><ol><li>TransactionCoordinator Fencing；</li><li>Producer Fencing；</li></ol><h4 id="TransactionCoordinator-Fencing"><a href="#TransactionCoordinator-Fencing" class="headerlink" title="TransactionCoordinator Fencing"></a>TransactionCoordinator Fencing</h4><p>TransactionCoordinator 在遇到上 long FGC 时，可能会导致 脑裂 问题，FGC 时会 stop-the-world，这时候可能会与 zk 连接超时导致临时节点消失进而触发 leader 选举，如果 <code>__transaction_state</code> 发生了 leader 选举，TransactionCoordinator 就会切换，如果此时旧的 TransactionCoordinator FGC 完成，在还没来得及同步到最细 meta 之前，会有一个短暂的时刻，对于一个 txn.id 而言就是这个时刻可能出现了两个 TransactionCoordinator。</p><p>相应的解决方案就是 TransactionCoordinator Fencing，这里 Fencing 策略不像离线场景 HDFS 这种直接 Kill 旧的 NN 进程或者强制切换状态这么暴力，而是通过 CoordinatorEpoch 来判断，每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 <code>__transaction_state</code>Partition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。</p><p>明白了 TransactionCoordinator 脑裂问题发生情况及解决方案之后，来分析下，Fencing 机制会在哪里发挥作用？仔细想想，是可以推断出来的，只可能是 TransactionCoordinator 向别人发请求时影响才会比较严重（特别是乱发 admin 命令）。有了 CoordinatorEpoch 之后，其他 Server 在收到请求时做相应的判断，如果发现 CoordinatorEpoch 值比缓存的最新的值小，那么 Fencing 就生效，拒绝这个请求，也就是 TransactionCoordinator 发送 WriteTxnMarkerRequest 时可能会触发这一机制。</p><h4 id="Producer-Fencing"><a href="#Producer-Fencing" class="headerlink" title="Producer Fencing"></a>Producer Fencing</h4><p>Producer Fencing 与前面的类似，如果对于相同 PID 和 txn.id 的 Producer，Server 端会记录最新的 Epoch 值，拒绝来自 zombie Producer （Epoch 值小的 Producer）的请求。前面第一个问题的情况，Producer 2 在启动时，会向 TransactionCoordinator 发送 InitPIDRequest 请求，此时 TransactionCoordinator 已经有了这个 txn.id 对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样 Producer 2 就被认为是最新的 Producer，而 Producer 1 就会被认为是 zombie Producer，因此，TransactionCoordinator 在处理 Producer 1 的事务请求时，会返回相应的异常信息。</p><h3 id="Consumer-端如何消费事务数据"><a href="#Consumer-端如何消费事务数据" class="headerlink" title="Consumer 端如何消费事务数据"></a>Consumer 端如何消费事务数据</h3><p>在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 <code>isolation.level</code> 配置，这个是配置对于事务性数据的消费策略，有以下两种可选配置：</p><ol><li><code>read_committed</code>: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.</li><li><code>read_uncommitted</code>: consume all available messages in offset ordering. This is the <strong>default value</strong>.</li></ol><p>简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），这部分的难点在于 read_committed 机制的实现。</p><h4 id="Last-Stable-Offset（LSO）"><a href="#Last-Stable-Offset（LSO）" class="headerlink" title="Last Stable Offset（LSO）"></a>Last Stable Offset（LSO）</h4><p>在事务机制的实现中，Kafka 又设置了一个新的 offset 概念，那就是 Last Stable Offset，简称 LSO（其他的 Offset 概念可参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B" target="_blank" rel="noopener">Kafka Offset 那些事</a>），先看下 LSO 的定义：</p><blockquote><p>The LSO is defined as the latest offset such that the status of all transactional messages at lower offsets have been determined (i.e. committed or aborted).</p></blockquote><p>对于一个 Partition 而言，offset 小于 LSO 的数据，全都是已经确定的数据，这个主要是对于事务操作而言，在这个 offset 之前的事务操作都是已经完成的事务（已经 commit 或 abort），如果这个 Partition 没有涉及到事务数据，那么 LSO 就是其 HW（水位）。</p><h4 id="Server-处理-read-committed-类型的-Fetch-请求"><a href="#Server-处理-read-committed-类型的-Fetch-请求" class="headerlink" title="Server 处理 read_committed 类型的 Fetch 请求"></a>Server 处理 read_committed 类型的 Fetch 请求</h4><p>如果 Consumer 的消费策略设置的是 read_committed，其在向 Server 发送 Fetch 请求时，Server 端<strong>只会返回 LSO 之前的数据</strong>，在 LSO 之后的数据不会返回。</p><p>这种机制有没有什么问题呢？我现在能想到的就是如果有一个 long transaction，比如其 first offset 是 1000，另外有几个已经完成的小事务操作，比如：txn1（offset：1100<del>1200）、txn2（offset：1400</del>1500），假设此时的 LSO 是 1000，也就是说这个 long transaction 还没有完成，那么已经完成的 txn1、txn2 也会对 consumer 不可见（假设都是 commit 操作），此时<strong>受 long transaction 的影响可能会导致数据有延迟</strong>。</p><p>那么我们再来想一下，如果不设计 LSO，又会有什么问题呢？可能分两种情况：</p><ol><li>允许读未完成的事务：那么 Consumer 可以直接读取到 Partition 的 HW 位置，对于未完成的事务，因为设置的是 read_committed 机制，所以不能对用户可见，需要在 Consumer 端做缓存，这个缓存应该设置多大？（不限制肯定会出现 OOM 的情况，当然也可以现在 client 端持久化到硬盘，这样的设计太过于复杂，还需要考虑 client 端 IO、磁盘故障等风险），明显这种设计方案是不可行的；</li><li>如果不允许读未完成的事务：相当于还是在 Server 端处理，与前面的区别是，这里需要先把示例中的 txn1、txn2 的数据发送给 Consumer，这样的设计会带来什么问题呢？<ol><li>假设这个 long transaction commit 了，其 end offset 是 2000，这时候有两种方案：第一种是把 1000-2000 的数据全部读出来（可能是磁盘读），把这个 long transaction 的数据过滤出来返回给 Consumer；第二种是随机读，只读这个 long transaction 的数据，无论哪种都有多触发一次磁盘读的风险，可能影响影响 Server 端的性能；</li><li>Server 端需要维护每个 consumer group 有哪些事务读了、哪些事务没读的 meta 信息，因为 consumer 是随机可能挂掉，需要接上次消费的，这样实现就复杂很多了；</li><li>还有一个问题是，消费的顺序性无法保证，两次消费其读取到的数据顺序可能是不同的（两次消费启动时间不一样）；</li></ol></li></ol><p>从这些分析来看，个人认为 LSO 机制还是一种相当来说 实现起来比较简单、而且不影响原来 server 端性能、还能保证顺序性的一种设计方案，它不一定是最好的，但也不会差太多。在实际的生产场景中，尽量避免 long transaction 这种操作，而且 long transaction可能也会容易触发事务超时。</p><h4 id="Consumer-如何过滤-abort-的事务数据"><a href="#Consumer-如何过滤-abort-的事务数据" class="headerlink" title="Consumer 如何过滤 abort 的事务数据"></a>Consumer 如何过滤 abort 的事务数据</h4><p>Consumer 在拉取到相应的数据之后，后面该怎么处理呢？它拉取到的这批数据并不能保证都是完整的事务数据，很有可能是拉取到一个事务的部分数据（marker 数据还没有拉取到），这时候应该怎么办？难道 Consumer 先把这部分数据缓存下来，等后面的 marker 数据到来时再确认数据应该不应该丢弃？（还是又 OOM 的风险）有没有更好的实现方案？</p><p>Kafka 的设计总是不会让我们失望，这部分做的优化也是非常高明，Broker 会追踪每个 Partition 涉及到的 abort transactions，Partition 的每个 log segment 都会有一个单独只写的文件（append-only file）来存储 abort transaction 信息，因为 abort transaction 并不是很多，所以这个开销是可以可以接受的，之所以要持久化到磁盘，主要是为了故障后快速恢复，要不然 Broker 需要把这个 Partition 的所有数据都读一遍，才能直到哪些事务是 abort 的，这样的话，开销太大（如果这个 Partition 没有事务操作，就不会生成这个文件）。这个持久化的文件是以 <code>.txnindex</code> 做后缀，前面依然是这个 log segment 的 offset 信息，存储的数据格式如下：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TransactionEntry</span> =&gt;</span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">PID</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">FirstOffset</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">LastOffset</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">LastStableOffset</span> =&gt; <span class="built_in">int64</span></span><br></pre></td></tr></table></figure><p>有了这个设计，Consumer 在拉取数据时，Broker 会把这批数据涉及到的所有 abort transaction 信息都返回给 Consumer，Server 端会根据拉取的 offset 范围与 abort transaction 的 offset 做对比，返回涉及到的 abort transaction 集合，其实现如下：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> collectAbortedTxns(fetchOffset: <span class="keyword">Long</span>, upperBoundOffset: <span class="keyword">Long</span>): TxnIndexSearchResult = &#123;</span><br><span class="line">  val abortedTransactions = ListBuffer.empty[AbortedTxn]</span><br><span class="line">  <span class="keyword">for</span> ((abortedTxn, _) &lt;- iterator()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastOffset &gt;= fetchOffset &amp;&amp; abortedTxn.firstOffset &lt; upperBoundOffset)</span><br><span class="line">      abortedTransactions += abortedTxn <span class="comment">//note: 这个 abort 的事务有在在这个范围内，就返回</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastStableOffset &gt;= upperBoundOffset)</span><br><span class="line">      <span class="keyword">return</span> TxnIndexSearchResult(abortedTransactions.<span class="keyword">toList</span>, isComplete = <span class="keyword">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  TxnIndexSearchResult(abortedTransactions.<span class="keyword">toList</span>, isComplete = <span class="keyword">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Consumer 在拿到这些数据之后，会进行相应的过滤，大概的判断逻辑如下（Server 端返回的 abort transaction 列表就保存在 <code>abortedTransactions</code> 集合中，<code>abortedProducerIds</code> 最开始时是为空的）：</p><ol><li>如果这个数据是 control msg（也即是 marker 数据），是 ABORT 的话，那么与这个事务相关的 PID 信息从 <code>abortedProducerIds</code> 集合删掉，是 COMMIT 的话，就忽略（每个这个 PID 对应的 marker 数据收到之后，就从 <code>abortedProducerIds</code> 中清除这个 PID 信息）；</li><li>如果这个数据是正常的数据，把它的 PID 和 offset 信息与 <code>abortedTransactions</code> 队列（有序队列，头部 transaction 的 first offset 最小）第一个 transaction 做比较，如果 PID 相同，并且 offset 大于等于这个 transaction 的 first offset，就将这个 PID 信息添加到 <code>abortedProducerIds</code> 集合中，同时从 <code>abortedTransactions</code> 队列中删除这个 transaction，最后再丢掉这个 batch（它是 abort transaction 的数据）；</li><li>检查这个 batch 的 PID 是否在 <code>abortedProducerIds</code> 集合中，在的话，就丢弃，不在的话就返回上层应用。</li></ol><p>这部分的实现确实有些绕（有兴趣的可以慢慢咀嚼一下），它严重依赖了 Kafka 提供的下面两种保证：</p><ol><li>Consumer 拉取到的数据，在处理时，其 offset 是严格有序的；</li><li>同一个 txn.id（PID 相同）在某一个时刻最多只能有一个事务正在进行；</li></ol><p>这部分代码实现如下：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">private Record <span class="built_in">nextFetchedRecord</span>() &#123;</span><br><span class="line">    <span class="keyword">while</span> (true) &#123;</span><br><span class="line">        <span class="keyword">if</span> (records == <span class="built_in">null</span> || !records.hasNext()) &#123; <span class="comment">//note: records 为空（数据全部丢掉了），records 没有数据（是 control msg）</span></span><br><span class="line">            <span class="built_in">maybeCloseRecordStream</span>();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!batches.hasNext()) &#123;</span><br><span class="line">                <span class="comment">// Message format v2 preserves the last offset in a batch even if the last record is removed</span></span><br><span class="line">                <span class="comment">// through compaction. By using the next offset computed from the last offset in the batch,</span></span><br><span class="line">                <span class="comment">// we ensure that the offset of the next fetch will point to the next batch, which avoids</span></span><br><span class="line">                <span class="comment">// unnecessary re-fetching of the same batch (in the worst case, the consumer could get stuck</span></span><br><span class="line">                <span class="comment">// fetching the same batch repeatedly).</span></span><br><span class="line">                <span class="keyword">if</span> (currentBatch != <span class="built_in">null</span>)</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                <span class="built_in">drain</span>();</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            currentBatch = batches.next();</span><br><span class="line">            <span class="built_in">maybeEnsureValid</span>(currentBatch);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isolationLevel == IsolationLevel.READ_COMMITTED &amp;&amp; currentBatch.hasProducerId()) &#123;</span><br><span class="line">                <span class="comment">//note: 需要做相应的判断</span></span><br><span class="line">                <span class="comment">// remove from the aborted transaction queue all aborted transactions which have begun</span></span><br><span class="line">                <span class="comment">// before the current batch's last offset and add the associated producerIds to the</span></span><br><span class="line">                <span class="comment">// aborted producer set</span></span><br><span class="line">                <span class="comment">//note: 如果这个 batch 的 offset 已经大于等于 abortedTransactions 中第一事务的 first offset</span></span><br><span class="line">                <span class="comment">//note: 那就证明下个 abort transaction 的数据已经开始到来，将 PID 添加到 abortedProducerIds 中</span></span><br><span class="line">                <span class="built_in">consumeAbortedTransactionsUpTo</span>(currentBatch.<span class="built_in">lastOffset</span>());</span><br><span class="line"></span><br><span class="line">                long producerId = currentBatch.producerId();</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">containsAbortMarker</span>(currentBatch)) &#123;</span><br><span class="line">                    abortedProducerIds.remove(producerId); <span class="comment">//note: 这个 PID（当前事务）涉及到的数据已经处理完</span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">isBatchAborted</span>(currentBatch)) &#123; <span class="comment">//note: 丢弃这个数据</span></span><br><span class="line">                    log.debug(<span class="string">"Skipping aborted record batch from partition &#123;&#125; with producerId &#123;&#125; and "</span> +</span><br><span class="line">                                  <span class="string">"offsets &#123;&#125; to &#123;&#125;"</span>,</span><br><span class="line">                              partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            records = currentBatch.streamingIterator(decompressionBufferSupplier);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Record record = records.next();</span><br><span class="line">            <span class="comment">// skip any records out of range</span></span><br><span class="line">            <span class="keyword">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class="line">                <span class="comment">// we only do validation when the message should not be skipped.</span></span><br><span class="line">                <span class="built_in">maybeEnsureValid</span>(record);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// control records are not returned to the user</span></span><br><span class="line">                <span class="keyword">if</span> (!currentBatch.isControlBatch()) &#123; <span class="comment">//note: 过滤掉 marker 数据</span></span><br><span class="line">                    <span class="keyword">return</span> record;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class="line">                    nextFetchOffset = record.offset() + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Consumer-消费数据时，其顺序如何保证"><a href="#Consumer-消费数据时，其顺序如何保证" class="headerlink" title="Consumer 消费数据时，其顺序如何保证"></a>Consumer 消费数据时，其顺序如何保证</h3><p>有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。</p><h3 id="如果-txn-id-长期不使用，server-端怎么处理？"><a href="#如果-txn-id-长期不使用，server-端怎么处理？" class="headerlink" title="如果 txn.id 长期不使用，server 端怎么处理？"></a>如果 txn.id 长期不使用，server 端怎么处理？</h3><p>Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 <code>transaction.timeout.ms</code>，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间（参数是 <code>transaction.timeout.ms</code>，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。</p><p>上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，Server 端提供了一个 <code>transaction.id.expiration.ms</code> 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，那么 TransactionCoordinator 将会使这个 txn.id 过期。</p><h3 id="PID-Snapshot-是做什么的？用来解决什么问题？"><a href="#PID-Snapshot-是做什么的？用来解决什么问题？" class="headerlink" title="PID Snapshot 是做什么的？用来解决什么问题？"></a>PID Snapshot 是做什么的？用来解决什么问题？</h3><p>对于每个 Topic-Partition，Broker 都会在内存中维护其 PID 与 sequence number（最后成功写入的 msg 的 sequence number）的对应关系（这个在上面幂等性文章应讲述过，主要是为了不丢补充的实现）。</p><p>Broker 重启时，如果想恢复上面的状态信息，那么它读取所有的 log 文件。相比于之下，定期对这个 state 信息做 checkpoint（Snapshot），明显收益是非常大的，此时如果 Broker 重启，只需要读取最近一个 Snapshot 文件，之后的数据再从 log 文件中恢复即可。</p><p>这个 PID Snapshot 样式如 00000000000235947656.snapshot，以 <code>.snapshot</code> 作为后缀，其数据格式如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[matt<span class="meta">@XXX</span><span class="number">-35</span> app.matt_test_transaction_json_3<span class="number">-2</span>]$ <span class="regexp">/usr/</span>local<span class="regexp">/java18/</span>bin<span class="regexp">/java -Djava.ext.dirs=/</span>XXX<span class="regexp">/kafka/</span>libs kafka.tools.DumpLogSegments --files <span class="number">00000000000235947656.</span>snapshot</span><br><span class="line">Dumping <span class="number">00000000000235947656.</span>snapshot</span><br><span class="line"><span class="string">producerId:</span> <span class="number">2000</span> <span class="string">producerEpoch:</span> <span class="number">1</span> <span class="string">coordinatorEpoch:</span> <span class="number">4</span> <span class="string">currentTxnFirstOffset:</span> None <span class="string">firstSequence:</span> <span class="number">95769510</span> <span class="string">lastSequence:</span> <span class="number">95769511</span> <span class="string">lastOffset:</span> <span class="number">235947654</span> <span class="string">offsetDelta:</span> <span class="number">1</span> <span class="string">timestamp:</span> <span class="number">1541325156503</span></span><br><span class="line"><span class="string">producerId:</span> <span class="number">3000</span> <span class="string">producerEpoch:</span> <span class="number">5</span> <span class="string">coordinatorEpoch:</span> <span class="number">6</span> <span class="string">currentTxnFirstOffset:</span> None <span class="string">firstSequence:</span> <span class="number">91669662</span> <span class="string">lastSequence:</span> <span class="number">91669666</span> <span class="string">lastOffset:</span> <span class="number">235947651</span> <span class="string">offsetDelta:</span> <span class="number">4</span> <span class="string">timestamp:</span> <span class="number">1541325156454</span></span><br></pre></td></tr></table></figure><p>在实际的使用中，这个 snapshot 文件一般只会保存最近的两个文件。</p><h3 id="中间流程故障如何恢复"><a href="#中间流程故障如何恢复" class="headerlink" title="中间流程故障如何恢复"></a>中间流程故障如何恢复</h3><p>对于上面所讲述的一个事务操作流程，实际生产环境中，任何一个地方都有可能出现的失败：</p><ol><li>Producer 在发送 <code>beginTransaction()</code> 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li><li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li><li>Producer 发送 <code>commitTransaction()</code> 时出现 timeout 或者错误：Producer 应该重试这个请求；</li><li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li></ol><p>陆陆续续写了几天，终于把这篇文章总结完了。</p><hr><p>参考：</p><ol><li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="noopener">Idempotent Producer</a>；</li><li><a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="noopener">Exactly-once Semantics in Apache Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="noopener">Transactional Messaging in Kafka</a>；</li><li><a href="https://www.confluent.io/blog/transactions-apache-kafka/" target="_blank" rel="noopener">Transactions in Apache Kafka</a>；</li></ol>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Kafka Exactly-Once 之事务性实现的基本原理。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 事务性之幂等性实现</title>
    <link href="https://gjtmaster.github.io/2018/09/25/Kafka%20%E4%BA%8B%E5%8A%A1%E6%80%A7%E4%B9%8B%E5%B9%82%E7%AD%89%E6%80%A7%E5%AE%9E%E7%8E%B0/"/>
    <id>https://gjtmaster.github.io/2018/09/25/Kafka 事务性之幂等性实现/</id>
    <published>2018-09-25T09:28:24.000Z</published>
    <updated>2019-10-28T06:33:37.775Z</updated>
    
    <content type="html"><![CDATA[<p>原作者：王蒙</p><p><a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">原文链接</a></p><p>Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 Kafka 事务性系列的文章我们只重点关注前两种层面上的事务性，与 Kafka Streams 相关的内容暂时不做讨论。社区从开始讨论事务性，前后持续近半年时间，相关的设计文档有六十几页（参考 <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>）。事务性这部分的实现也是非常复杂的，之前 Producer 端的代码实现其实是非常简单的，增加事务性的逻辑之后，这部分代码复杂度提高了很多，本篇及后面几篇关于事务性的文章会以 2.0.0 版的代码实现为例，对这部分做了一下分析，计划分为五篇文章：</p><ol><li>第一篇：Kafka 幂等性实现；</li><li>第二篇：Kafka 事务性实现；</li><li>第三篇：Kafka 事务性相关处理请求在 Server 端如何处理及其实现细节；</li><li>第四篇：关于 Kafka 事务性实现的一些思考，也会简单介绍一下 RocketMQ 事务性的实现，做一下对比；</li><li>第五篇：Flink + Kafka 如何实现 Exactly Once；</li></ol><p>这篇是 Kafka 事务性系列的第一篇文章，主要讲述幂等性实现的整体流程，幂等性的实现相对于事务性的实现简单很多，也是事务性实现的基础。</p><h2 id="一、Producer-幂等性"><a href="#一、Producer-幂等性" class="headerlink" title="一、Producer 幂等性"></a>一、Producer 幂等性</h2><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p><ul><li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li><li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li></ul><p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p><h2 id="二、幂等性示例"><a href="#二、幂等性示例" class="headerlink" title="二、幂等性示例"></a>二、幂等性示例</h2><p>Producer 使用幂等性的示例非常简单，与正常情况下 Producer 使用相比变化不大，只需要把 Producer 的配置 enable.idempotence 设置为 true 即可，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">// 当 enable.idempotence 为 true，这里默认为 all</span></span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line"></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"test"</span>);</span><br></pre></td></tr></table></figure><p>Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。</p><h2 id="三、幂等性要解决的问题"><a href="#三、幂等性要解决的问题" class="headerlink" title="三、幂等性要解决的问题"></a>三、幂等性要解决的问题</h2><p>在看 Producer 是如何实现幂等性之前，首先先考虑一个问题：<strong>幂等性是来解决什么问题的？</strong> 在 0.11.0 之前，Kafka 通过 Producer 端和 Server 端的相关配置可以做到<strong>数据不丢</strong>，也就是 at least once，但是在一些情况下，可能会导致数据重复，比如：网络请求延迟等导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。</p><p>对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。</p><p>在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。</p><h2 id="四、幂等性的实现原理"><a href="#四、幂等性的实现原理" class="headerlink" title="四、幂等性的实现原理"></a>四、幂等性的实现原理</h2><p>在讲述幂等性处理流程之前，先看下 Producer 是如何来保证幂等性的，正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是：</p><p><strong>at least once + 幂等 = exactly once</strong></p><p>通过在 al least once 的基础上加上 幂等性来坐到 exactly once，当然这个层面的 exactly once 是有限制的，比如它会要求单会话内有效或者跨会话使用事务性有效等。这里我们先分析最简单的情况，那就是在单会话内如何做到幂等性，进而保证 exactly once。</p><p>要做到幂等性，要解决下面的问题：</p><ol><li>系统需要有能力鉴别一条数据到底是不是重复的数据？常用的手段是通过 <strong>唯一键/唯一 id</strong> 来判断，这时候系统一般是需要缓存已经处理的唯一键记录，这样才能更有效率地判断一条数据是不是重复；</li><li>唯一键应该选择什么粒度？对于分布式存储系统来说，肯定不能用全局唯一键（全局是针对集群级别），核心的解决思路依然是 <strong>分而治之</strong>，数据密集型系统为了实现分布式都是有分区概念的，而分区之间是有相应的隔离，对于 Kafka 而言，这里的解决方案就是在分区的维度上去做，重复数据的判断让 partition 的 leader 去判断处理，前提是 Produce 请求需要把唯一键值告诉 leader；</li><li>分区粒度实现唯一键会不会有其他问题？这里需要考虑的问题是当一个 Partition 有来自多个 client 写入的情况，这些 client 之间是很难做到使用同一个唯一键（一个是它们之间很难做到唯一键的实时感知，另一个是这样实现是否有必要）。而如果系统在实现时做到了 <strong>client + partition</strong> 粒度，这样实现的好处是每个 client 都是完全独立的（它们之间不需要有任何的联系，这是非常大的优点），只是在 Server 端对不同的 client 做好相应的区分即可，当然同一个 client 在处理多个 Topic-Partition 时是完全可以使用同一个 PID 的。</li></ol><p>有了上面的分析（都是个人见解，如果有误，欢迎指教），就不难理解 Producer 幂等性的实现原理，Kafka Producer 在实现时有以下两个重要机制：</p><ol><li>PID（Producer ID），用来标识每个 producer client；</li><li>sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。</li></ol><p>下面详细讲述这两个实现机制。</p><h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。</p><h4 id="Producer-PID-申请"><a href="#Producer-PID-申请" class="headerlink" title="Producer PID 申请"></a>Producer PID 申请</h4><p>这里看下 PID 在 Server 端是如何分配的？Client 通过向 Server 发送一个 InitProducerIdRequest 请求获取 PID（幂等性时，是选择一台连接数最少的 Broker 发送这个请求），这里看下 Server 端是如何处理这个请求的？KafkaApis 中 <code>handleInitProducerIdRequest()</code> 方法的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">handleInitProducerIdRequest</span><span class="params">(request: RequestChannel.Request)</span>: Unit </span>= &#123;</span><br><span class="line">  val initProducerIdRequest = request.body[InitProducerIdRequest]</span><br><span class="line">  val transactionalId = initProducerIdRequest.transactionalId</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId != <span class="keyword">null</span>) &#123; <span class="comment">//note: 设置 txn.id 时，验证对 txn.id 的权限</span></span><br><span class="line">    <span class="keyword">if</span> (!authorize(request.session, Write, Resource(TransactionalId, transactionalId, LITERAL))) &#123;</span><br><span class="line">      sendErrorResponseMaybeThrottle(request, Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED.exception)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!authorize(request.session, IdempotentWrite, Resource.ClusterResource)) &#123; <span class="comment">//note: 没有设置 txn.id 时，验证对集群是否有幂等性权限</span></span><br><span class="line">    sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">sendResponseCallback</span><span class="params">(result: InitProducerIdResult)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="function">def <span class="title">createResponse</span><span class="params">(requestThrottleMs: Int)</span>: AbstractResponse </span>= &#123;</span><br><span class="line">      val responseBody = <span class="keyword">new</span> InitProducerIdResponse(requestThrottleMs, result.error, result.producerId, result.producerEpoch)</span><br><span class="line">      trace(s<span class="string">"Completed $transactionalId's InitProducerIdRequest with result $result from client $&#123;request.header.clientId&#125;."</span>)</span><br><span class="line">      responseBody</span><br><span class="line">    &#125;</span><br><span class="line">    sendResponseMaybeThrottle(request, createResponse)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 生成相应的了 pid，返回给 producer</span></span><br><span class="line">  txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.transactionTimeoutMs, sendResponseCallback)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里实际上是调用了 TransactionCoordinator （Broker 在启动 server 服务时都会初始化这个实例）的 <code>handleInitProducerId()</code> 方法做了相应的处理，其实现如下（这里只关注幂等性的处理）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">handleInitProducerId</span><span class="params">(transactionalId: String,</span></span></span><br><span class="line"><span class="function"><span class="params">                         transactionTimeoutMs: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">                         responseCallback: InitProducerIdCallback)</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId == <span class="keyword">null</span>) &#123; <span class="comment">//note: 只设置幂等性时，直接分配 pid 并返回</span></span><br><span class="line">    <span class="comment">// if the transactional id is null, then always blindly accept the request</span></span><br><span class="line">    <span class="comment">// and return a new producerId from the producerId manager</span></span><br><span class="line">    val producerId = producerIdManager.generateProducerId()</span><br><span class="line">    responseCallback(InitProducerIdResult(producerId, producerEpoch = <span class="number">0</span>, Errors.NONE))</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Server 在给一个 client 初始化 PID 时，实际上是通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID。</p><h4 id="Server-PID-管理"><a href="#Server-PID-管理" class="headerlink" title="Server PID 管理"></a>Server PID 管理</h4><p>如前面所述，在幂等性的情况下，直接通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID，其中 ProducerIdManager 是在 TransactionCoordinator 对象初始化时初始化的，这个对象主要是用来管理 PID 信息：</p><ul><li>在本地的 PID 端用完了或者处于新建状态时，申请 PID 段（默认情况下，每次申请 1000 个 PID）；</li><li>TransactionCoordinator 对象通过 <code>generateProducerId()</code> 方法获取下一个可以使用的 PID；</li></ul><p><strong>PID 端申请是向 ZooKeeper 申请</strong>，zk 中有一个 <code>/latest_producer_id_block</code> 节点，每个 Broker 向 zk 申请一个 PID 段后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"version"</span>:<span class="number">1</span>,<span class="string">"broker"</span>:<span class="number">35</span>,<span class="string">"block_start"</span>:<span class="string">"4000"</span>,<span class="string">"block_end"</span>:<span class="string">"4999"</span>&#125;</span><br></pre></td></tr></table></figure><p>ProducerIdManager 向 zk 申请 PID 段的方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">getNewProducerIdBlock</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">var</span> zkWriteComplete = <span class="keyword">false</span></span><br><span class="line">  <span class="keyword">while</span> (!zkWriteComplete) &#123; <span class="comment">//note: 直到从 zk 拿取到分配的 PID 段</span></span><br><span class="line">    <span class="comment">// refresh current producerId block from zookeeper again</span></span><br><span class="line">    val (dataOpt, zkVersion) = zkClient.getDataAndVersion(ProducerIdBlockZNode.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate the new producerId block</span></span><br><span class="line">    currentProducerIdBlock = dataOpt match &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">Some</span><span class="params">(data)</span> </span>=&gt;</span><br><span class="line">        <span class="comment">//note: 从 zk 获取当前最新的 pid 信息，如果后面更新失败，这里也会重新从 zk 获取</span></span><br><span class="line">        val currProducerIdBlock = ProducerIdManager.parseProducerIdBlockData(data)</span><br><span class="line">        debug(s<span class="string">"Read current producerId block $currProducerIdBlock, Zk path version $zkVersion"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currProducerIdBlock.blockEndId &gt; Long.MaxValue - ProducerIdManager.PidBlockSize) &#123;<span class="comment">//note: 不足以分配1000个 PID</span></span><br><span class="line">          <span class="comment">// we have exhausted all producerIds (wow!), treat it as a fatal error</span></span><br><span class="line">          <span class="comment">//note: 当 PID 分配超过限制时，直接报错了（每秒分配1个，够用2百亿年了）</span></span><br><span class="line">          fatal(s<span class="string">"Exhausted all producerIds as the next block's end producerId is will has exceeded long type limit (current block end producerId is $&#123;currProducerIdBlock.blockEndId&#125;)"</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Have exhausted all producerIds."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ProducerIdBlock(brokerId, currProducerIdBlock.blockEndId + <span class="number">1L</span>, currProducerIdBlock.blockEndId + ProducerIdManager.PidBlockSize)</span><br><span class="line">      <span class="keyword">case</span> None =&gt; <span class="comment">//note: 该节点还不存在，第一次初始化</span></span><br><span class="line">        debug(s<span class="string">"There is no producerId block yet (Zk path version $zkVersion), creating the first block"</span>)</span><br><span class="line">        ProducerIdBlock(brokerId, <span class="number">0L</span>, ProducerIdManager.PidBlockSize - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val newProducerIdBlockData = ProducerIdManager.generateProducerIdBlockJson(currentProducerIdBlock)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try to write the new producerId block into zookeeper</span></span><br><span class="line">    <span class="comment">//note: 将新的 pid 信息写入到 zk，如果写入失败（写入之前会比对 zkVersion，如果这个有变动，证明这期间有别的 Broker 在操作，那么写入失败），重新申请</span></span><br><span class="line">    val (succeeded, version) = zkClient.conditionalUpdatePath(ProducerIdBlockZNode.path,</span><br><span class="line">      newProducerIdBlockData, zkVersion, Some(checkProducerIdBlockZkData))</span><br><span class="line">    zkWriteComplete = succeeded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (zkWriteComplete)</span><br><span class="line">      info(s<span class="string">"Acquired new producerId block $currentProducerIdBlock by writing to Zk with path version $version"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ProducerIdManager 申请 PID 段的流程如下：</p><ol><li>先从 zk 的 <code>/latest_producer_id_block</code> 节点读取最新已经分配的 PID 段信息；</li><li>如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）；</li><li>如果该节点存在，读取其中数据，根据 block_end 选择 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）；</li><li>在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。</li></ol><p>明白了 ProducerIdManager 如何申请 PID 段之后，再看 <code>generateProducerId()</code> 这个方法就简单很多了，这个方法在每次调用时，都会更新 nextProducerId 值（下一次可以使用 PID 值），如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">generateProducerId</span><span class="params">()</span>: Long </span>= &#123;</span><br><span class="line">  <span class="keyword">this</span> <span class="keyword">synchronized</span> &#123;</span><br><span class="line">    <span class="comment">// grab a new block of producerIds if this block has been exhausted</span></span><br><span class="line">    <span class="keyword">if</span> (nextProducerId &gt; currentProducerIdBlock.blockEndId) &#123;</span><br><span class="line">      <span class="comment">//note: 如果分配的 pid 用完了，重新再向 zk 申请一批</span></span><br><span class="line">      getNewProducerIdBlock()</span><br><span class="line">      nextProducerId = currentProducerIdBlock.blockStartId + <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      nextProducerId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    nextProducerId - <span class="number">1</span> <span class="comment">//note: 返回当前分配的 pid</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里就是 Producer PID 如何申请（事务性情况下 PID 的申请会复杂一些，下篇文章再讲述）以及 Server 端如何管理 PID 的。</p><h3 id="sequence-numbers"><a href="#sequence-numbers" class="headerlink" title="sequence numbers"></a>sequence numbers</h3><p>再有了 PID 之后，在 PID + Topic-Partition 级别上添加一个 sequence numbers 信息，就可以实现 Producer 的幂等性了。ProducerBatch 也提供了一个 <code>setProducerState()</code> 方法，它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ProducerBatch</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(ProducerIdAndEpoch producerIdAndEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryRecordsBuilder</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(<span class="keyword">long</span> producerId, <span class="keyword">short</span> producerEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isClosed()) &#123;</span><br><span class="line">        <span class="comment">// Sequence numbers are assigned when the batch is closed while the accumulator is being drained.</span></span><br><span class="line">        <span class="comment">// If the resulting ProduceRequest to the partition leader failed for a retriable error, the batch will</span></span><br><span class="line">        <span class="comment">// be re queued. In this case, we should not attempt to set the state again, since changing the producerId and sequence</span></span><br><span class="line">        <span class="comment">// once a batch has been sent to the broker risks introducing duplicates.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Trying to set producer state of an already closed batch. This indicates a bug on the client."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.producerId = producerId;</span><br><span class="line">    <span class="keyword">this</span>.producerEpoch = producerEpoch;</span><br><span class="line">    <span class="keyword">this</span>.baseSequence = baseSequence;</span><br><span class="line">    <span class="keyword">this</span>.isTransactional = isTransactional;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="五、幂等性实现整体流程"><a href="#五、幂等性实现整体流程" class="headerlink" title="五、幂等性实现整体流程"></a>五、幂等性实现整体流程</h2><p>在前面讲述完 Kafka 幂等性的两个实现机制（PID+sequence numbers）之后，这里详细讲述一下，幂等性时其整体的处理流程，主要讲述幂等性相关的内容，其他的部分会简单介绍（可以参考前面【Kafka 源码分析系列文章】了解 Producer 端处理流程以及 Server 端关于 ProduceRequest 请求的处理流程），其流程如下图所示：</p><p><a href="http://matt33.com/images/kafka/kafka-idemoptent.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/kafka-idemoptent.png" alt="Producer 幂等性时处理流程"></a>Producer 幂等性时处理流程</p><p>这个图只展示了幂等性情况下，Producer 的大概流程，很多部分在前面的文章中做过分析，本文不再讲述，这里重点关注与幂等性相关的内容（事务性实现更加复杂，后面的文章再讲述），首先 KafkaProducer 在初始化时会初始化一个 TransactionManager 实例，它的作用有以下几个部分：</p><ol><li>记录本地的事务状态（事务性时必须）；</li><li>记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；</li><li>记录 ProducerIdAndEpoch 信息（PID 信息）。</li></ol><h3 id="Client-幂等性时发送流程"><a href="#Client-幂等性时发送流程" class="headerlink" title="Client 幂等性时发送流程"></a>Client 幂等性时发送流程</h3><p>如前面图中所示，幂等性时，Producer 的发送流程如下：</p><ol><li><p>应用通过 KafkaProducer 的 <code>send()</code> 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；</p></li><li><p>Producer 后台发送线程 Sender，在 <code>run()</code> 方法中，会先根据 TransactionManager 的 <code>shouldResetProducerStateAfterResolvingSequences()</code> 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 <code>clear+new</code> 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）；</p></li><li><p>Sender 线程通过 <code>maybeWaitForProducerId()</code> 方法判断是否需要申请 PID，如果需要的话，这里会阻塞直到获取到相应的 PID 信息；</p></li><li><p>Sender 线程通过</p></li></ol>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">sendProducerData</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的</p><p>​    </p>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">drain</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法中，在加了幂等性之后，</p>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">drain</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法多了如下几步判断：</p><ol><li>常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，<strong>最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1</strong>（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）；</li><li>如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置；</li></ol><ol start="5"><li>最后 Sender 线程再调用 <code>sendProduceRequests()</code> 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。</li></ol><p>这里看下几个关键方法的实现，首先是 Sender 线程获取 PID 信息的方法 <code>maybeWaitForProducerId()</code> ，其实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 等待直到 Producer 获取到相应的 PID 和 epoch 信息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWaitForProducerId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!transactionManager.hasProducerId() &amp;&amp; !transactionManager.hasError()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Node node = awaitLeastLoadedNodeReady(requestTimeoutMs); <span class="comment">//note: 选取 node（本地连接数最少的 node）</span></span><br><span class="line">            <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">                ClientResponse response = sendAndAwaitInitProducerIdRequest(node); <span class="comment">//note: 发送 InitPidRequest</span></span><br><span class="line">                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();</span><br><span class="line">                Errors error = initProducerIdResponse.error();</span><br><span class="line">                <span class="keyword">if</span> (error == Errors.NONE) &#123; <span class="comment">//note: 更新 Producer 的 PID 和 epoch 信息</span></span><br><span class="line">                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">new</span> ProducerIdAndEpoch(</span><br><span class="line">                            initProducerIdResponse.producerId(), initProducerIdResponse.epoch());</span><br><span class="line">                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                    log.debug(<span class="string">"Retriable error from InitProducerId response"</span>, error.message());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    transactionManager.transitionToFatalError(error.exception());</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Could not find an available broker to send InitProducerIdRequest to. "</span> +</span><br><span class="line">                        <span class="string">"We will back off and try again."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</span><br><span class="line">            transactionManager.transitionToFatalError(e);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.debug(<span class="string">"Broker &#123;&#125; disconnected while awaiting InitProducerId response"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        log.trace(<span class="string">"Retry InitProducerIdRequest in &#123;&#125;ms."</span>, retryBackoffMs);</span><br><span class="line">        time.sleep(retryBackoffMs);</span><br><span class="line">        metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再看下 RecordAccumulator 的 <code>drain()</code> 方法，重点需要关注的是关于幂等性和事务性相关的处理，具体如下所示，这里面关于事务性相关的判断在上面的流程中已经讲述。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Drain all the data for the given nodes and collate them into a list of batches that will fit within the specified</span></span><br><span class="line"><span class="comment"> * size on a per-node basis. This method attempts to avoid choosing the same topic-node over and over.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nodes The list of node to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSize The maximum number of bytes to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current unix time in milliseconds</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A list of &#123;<span class="doctag">@link</span> ProducerBatch&#125; for each node specified with total size less than the requested maxSize.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster,</span><br><span class="line">                                               Set&lt;Node&gt; nodes,</span><br><span class="line">                                               <span class="keyword">int</span> maxSize,</span><br><span class="line">                                               <span class="keyword">long</span> now) &#123;</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</span><br><span class="line">        List&lt;ProducerBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></span><br><span class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            PartitionInfo part = parts.get(drainIndex);</span><br><span class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</span><br><span class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></span><br><span class="line">            <span class="keyword">if</span> (!isMuted(tp, now)) &#123;</span><br><span class="line">                Deque&lt;ProducerBatch&gt; deque = getDeque(tp);</span><br><span class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (deque) &#123; <span class="comment">//note: 先判断有没有数据，然后后面真正处理时再加锁处理</span></span><br><span class="line">                        ProducerBatch first = deque.peekFirst();</span><br><span class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            <span class="keyword">boolean</span> backoff = first.attempts() &gt; <span class="number">0</span> &amp;&amp; first.waitedTimeMs(now) &lt; retryBackoffMs;</span><br><span class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></span><br><span class="line">                            <span class="keyword">if</span> (!backoff) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (size + first.estimatedSizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</span><br><span class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></span><br><span class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></span><br><span class="line">                                    <span class="comment">// request</span></span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">null</span>;</span><br><span class="line">                                    <span class="keyword">boolean</span> isTransactional = <span class="keyword">false</span>;</span><br><span class="line">                                    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123; <span class="comment">//note: 幂等性或事务性时， 做一些检查判断</span></span><br><span class="line">                                        <span class="keyword">if</span> (!transactionManager.isSendToPartitionAllowed(tp))</span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        producerIdAndEpoch = transactionManager.producerIdAndEpoch();</span><br><span class="line">                                        <span class="keyword">if</span> (!producerIdAndEpoch.isValid()) <span class="comment">//note: pid 是否有效</span></span><br><span class="line">                                            <span class="comment">// we cannot send the batch until we have refreshed the producer id</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        isTransactional = transactionManager.isTransactional();</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">if</span> (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))</span><br><span class="line">                                            <span class="comment">//note: 当前这个 topic-partition 的数据出现过超时,不能发送,如果是新的 batch 数据直接跳过（没有 seq  number 信息）</span></span><br><span class="line">                                            <span class="comment">// Don't drain any new batches while the state of previous sequence numbers</span></span><br><span class="line">                                            <span class="comment">// is unknown. The previous batches would be unknown if they were aborted</span></span><br><span class="line">                                            <span class="comment">// on the client after being sent to the broker at least once.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line">                                        <span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">                                                &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">                                            <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">                                            <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">                                            <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">                                            <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">                                            <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">                                            <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">                                            <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">                                            <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">                                            <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">                                            <span class="comment">// in flight request count to 1.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line">                                    &#125;</span><br><span class="line"></span><br><span class="line">                                    ProducerBatch batch = deque.pollFirst();</span><br><span class="line">                                    <span class="keyword">if</span> (producerIdAndEpoch != <span class="keyword">null</span> &amp;&amp; !batch.hasSequence()) &#123;<span class="comment">//note: batch 的相关信息（seq id）是在这里设置的</span></span><br><span class="line">                                        <span class="comment">//note: 这个 batch 还没有 seq number 信息</span></span><br><span class="line">                                        <span class="comment">// If the batch already has an assigned sequence, then we should not change the producer id and</span></span><br><span class="line">                                        <span class="comment">// sequence number, since this may introduce duplicates. In particular,</span></span><br><span class="line">                                        <span class="comment">// the previous attempt may actually have been accepted, and if we change</span></span><br><span class="line">                                        <span class="comment">// the producer id and sequence here, this attempt will also be accepted,</span></span><br><span class="line">                                        <span class="comment">// causing a duplicate.</span></span><br><span class="line">                                        <span class="comment">//</span></span><br><span class="line">                                        <span class="comment">// Additionally, we update the next sequence number bound for the partition,</span></span><br><span class="line">                                        <span class="comment">// and also have the transaction manager track the batch so as to ensure</span></span><br><span class="line">                                        <span class="comment">// that sequence ordering is maintained even if we receive out of order</span></span><br><span class="line">                                        <span class="comment">// responses.</span></span><br><span class="line">                                        <span class="comment">//note: 给这个 batch 设置相应的 pid、seq id 等信息</span></span><br><span class="line">                                        batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);</span><br><span class="line">                                        transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); <span class="comment">//note: 增加 partition 对应的下一个 seq id 值</span></span><br><span class="line">                                        log.debug(<span class="string">"Assigned producerId &#123;&#125; and producerEpoch &#123;&#125; to batch with base sequence "</span> +</span><br><span class="line">                                                        <span class="string">"&#123;&#125; being sent to partition &#123;&#125;"</span>, producerIdAndEpoch.producerId,</span><br><span class="line">                                                producerIdAndEpoch.epoch, batch.baseSequence(), tp);</span><br><span class="line"></span><br><span class="line">                                        transactionManager.addInFlightBatch(batch);</span><br><span class="line">                                    &#125;</span><br><span class="line">                                    batch.close();</span><br><span class="line">                                    size += batch.records().sizeInBytes();</span><br><span class="line">                                    ready.add(batch);</span><br><span class="line">                                    batch.drained(now);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</span><br><span class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="幂等性时-Server-端如何处理-ProduceRequest-请求"><a href="#幂等性时-Server-端如何处理-ProduceRequest-请求" class="headerlink" title="幂等性时 Server 端如何处理 ProduceRequest 请求"></a>幂等性时 Server 端如何处理 ProduceRequest 请求</h3><p>如前面途中所示，当 Broker 收到 ProduceRequest 请求之后，会通过 <code>handleProduceRequest()</code> 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）：</p><ol><li>如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED；</li><li>如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED；</li><li>验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常；</li><li>检查是否有 PID 信息，没有的话走正常的写入流程；</li><li>LOG 对象会在 <code>analyzeAndValidateProducerState()</code> 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）；</li><li>有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验：<ol><li>检查该 PID 是否已经缓存中存在（主要是在 ProducerStateManager 对象中检查）；</li><li>如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）；</li><li>如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同；</li><li>如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常；</li><li>如果不同并且 sequence number 从 0 开始，那么正常写入；</li><li>如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。</li></ol></li><li>下面与正常写入相同。</li></ol><p>幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 <code>analyzeAndValidateProducerState()</code> 方法的实现，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">analyzeAndValidateProducerState</span><span class="params">(records: MemoryRecords, isFromClient: Boolean)</span>: <span class="params">(mutable.Map[Long, ProducerAppendInfo], List[CompletedTxn], Option[BatchMetadata])</span> </span>= &#123;</span><br><span class="line">  val updatedProducers = mutable.Map.empty[Long, ProducerAppendInfo]</span><br><span class="line">  val completedTxns = ListBuffer.empty[CompletedTxn]</span><br><span class="line">  <span class="keyword">for</span> (batch &lt;- records.batches.asScala <span class="keyword">if</span> batch.hasProducerId) &#123; <span class="comment">//note: 有 pid 时,才会做相应的判断</span></span><br><span class="line">    val maybeLastEntry = producerStateManager.lastEntry(batch.producerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if this is a client produce request, there will be up to 5 batches which could have been duplicated.</span></span><br><span class="line">    <span class="comment">// If we find a duplicate, we return the metadata of the appended batch to the client.</span></span><br><span class="line">    <span class="keyword">if</span> (isFromClient) &#123;</span><br><span class="line">      maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt;</span><br><span class="line">        <span class="keyword">return</span> (updatedProducers, completedTxns.toList, Some(duplicate)) <span class="comment">//note: 如果这个 batch 已经收到过，这里直接返回</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient) <span class="comment">//note: 这里</span></span><br><span class="line">    maybeCompletedTxn.foreach(completedTxns += _)</span><br><span class="line">  &#125;</span><br><span class="line">  (updatedProducers, completedTxns.toList, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">findDuplicateBatch</span><span class="params">(batch: RecordBatch)</span>: Option[BatchMetadata] </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (batch.producerEpoch != producerEpoch)</span><br><span class="line">     None</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    batchWithSequenceRange(batch.baseSequence, batch.lastSequence)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Return the batch metadata of the cached batch having the exact sequence range, if any.</span></span><br><span class="line"><span class="function">def <span class="title">batchWithSequenceRange</span><span class="params">(firstSeq: Int, lastSeq: Int)</span>: Option[BatchMetadata] </span>= &#123;</span><br><span class="line">  val duplicate = batchMetadata.filter &#123; metadata =&gt;</span><br><span class="line">    firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq</span><br><span class="line">  &#125;</span><br><span class="line">  duplicate.headOption</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">addBatchMetadata</span><span class="params">(batch: BatchMetadata)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (batchMetadata.size == ProducerStateEntry.NumBatchesToRetain)</span><br><span class="line">    batchMetadata.dequeue() <span class="comment">//note: 只会保留最近 5 个 batch 的记录</span></span><br><span class="line">  batchMetadata.enqueue(batch) <span class="comment">//note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果 batch 不是重复的数据，<code>analyzeAndValidateProducerState()</code> 会通过 <code>updateProducers()</code> 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 检查 seq number</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">checkSequence</span><span class="params">(producerEpoch: Short, appendFirstSeq: Int)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (producerEpoch != updatedEntry.producerEpoch) &#123; <span class="comment">//note: epoch 不同时</span></span><br><span class="line">    <span class="keyword">if</span> (appendFirstSeq != <span class="number">0</span>) &#123; <span class="comment">//note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期）</span></span><br><span class="line">      <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了）</span></span><br><span class="line">      <span class="keyword">if</span> (updatedEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Invalid sequence number for new epoch: $producerEpoch "</span> +</span><br><span class="line">          s<span class="string">"(request epoch), $appendFirstSeq (seq. number)"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据）</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnknownProducerIdException(s<span class="string">"Found no record of producerId=$producerId on the broker. It is possible "</span> +</span><br><span class="line">          s<span class="string">"that the last message with t（）he producerId=$producerId has been removed due to hitting the retention limit."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    val currentLastSeq = <span class="keyword">if</span> (!updatedEntry.isEmpty)</span><br><span class="line">      updatedEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (producerEpoch == currentEntry.producerEpoch)</span><br><span class="line">      currentEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      RecordBatch.NO_SEQUENCE</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (currentLastSeq == RecordBatch.NO_SEQUENCE &amp;&amp; appendFirstSeq != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">//note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据</span></span><br><span class="line">      <span class="comment">// the epoch was bumped by a control record, so we expect the sequence number to be reset</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Out of order sequence number for producerId $producerId: found $appendFirstSeq "</span> +</span><br><span class="line">        s<span class="string">"(incoming seq. number), but expected 0"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inSequence(currentLastSeq, appendFirstSeq)) &#123;</span><br><span class="line">      <span class="comment">//note: 判断是否连续</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Out of order sequence number for producerId $producerId: $appendFirstSeq "</span> +</span><br><span class="line">        s<span class="string">"(incoming seq. number), $currentLastSeq (current end sequence number)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其校验逻辑如前面流程中所述。</p><h2 id="六、小思考"><a href="#六、小思考" class="headerlink" title="六、小思考"></a>六、小思考</h2><p>这里主要思考两个问题：</p><ol><li>Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？</li><li>Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？</li></ol><p>先说一下结论，问题 1 的这个设置要求其实上面分析的时候已经讲述过了，主要跟 server 端只会缓存最近 5 个 batch 的机制有关；问题 2，即使 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，幂等性时依然可以做到有序，下面来详细分析一下这两个问题。</p><h3 id="为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5"><a href="#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5" class="headerlink" title="为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p><p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（相当于client 狂发错误请求）。</p><p>那有没有更好的方案呢？我认为是有的，那就是对于 OutOfOrderSequenceException 异常，再进行细分，区分这个 sequence number 是大于 nextSeq （期望的下次 sequence number 值）还是小于 nextSeq，如果是小于，那么肯定是重复的数据。</p><h3 id="当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序"><a href="#当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序" class="headerlink" title="当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序"></a>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序</h3><p>先来分析一下，在什么情况下 Producer 会出现乱序的问题？没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。</p><p>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个为题，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。</p><p>系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。</p><p>当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Re-enqueue the given record batch in the accumulator to retry</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reenqueue</span><span class="params">(ProducerBatch batch, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    batch.reenqueued(now); <span class="comment">//note: 重试,更新相应的 meta</span></span><br><span class="line">    Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition);</span><br><span class="line">    <span class="keyword">synchronized</span> (deque) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>)</span><br><span class="line">            insertInSequenceOrder(deque, batch); <span class="comment">//note: 将 batch 添加到队列的合适位置（根据 seq num 信息）</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            deque.addFirst(batch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 <code>drain()</code> 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE</span></span><br><span class="line"><span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line"><span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">        &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">    <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">    <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">    <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">    <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">    <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">    <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">    <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">    <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">    <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">    <span class="comment">// in flight request count to 1.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。</p><p>Producer 的 TransactionManager 实例的 inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 <code>addInFlightBatch()</code> 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 <code>removeInFlightBatch()</code> 方法将这个 batch 从 inflightBatchesBySequence 中移除。接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 <code>drain()</code> 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 <code>firstInFlightSequence()</code> 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。</p><p>简单来说，其实现机制概括为：</p><ol><li>Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常；</li><li>Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）；</li><li>Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。</li></ol><hr><p>参考：</p><ol><li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="noopener">Idempotent Producer</a>；</li></ol>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Kafka 事务性之幂等性实现的基本原理。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka ISR设计及水印与leader epoch副本同步机制深入剖析</title>
    <link href="https://gjtmaster.github.io/2018/09/23/kafka%20ISR%E8%AE%BE%E8%AE%A1%E5%8F%8A%E6%B0%B4%E5%8D%B0%E4%B8%8Eleader%20epoch%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2018/09/23/kafka ISR设计及水印与leader epoch副本同步机制深入剖析/</id>
    <published>2018-09-23T07:33:27.000Z</published>
    <updated>2019-10-28T06:33:37.767Z</updated>
    
    <content type="html"><![CDATA[<h2 id="帽子理论（Gilbert-和-Lynch-）"><a href="#帽子理论（Gilbert-和-Lynch-）" class="headerlink" title="帽子理论（Gilbert 和 Lynch ）"></a>帽子理论（Gilbert 和 Lynch ）</h2><ul><li><p>一致性</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">any</span> <span class="built_in">read</span> operation that <span class="keyword">begins</span> <span class="keyword">after</span> <span class="keyword">a</span> <span class="built_in">write</span> operation completes must </span><br><span class="line"><span class="literal">return</span> that <span class="built_in">value</span>, <span class="keyword">or</span> <span class="keyword">the</span> <span class="built_in">result</span> <span class="keyword">of</span> <span class="keyword">a</span> later <span class="built_in">write</span> operation</span><br><span class="line"></span><br><span class="line">通过某个节点的写操作结果对后面通过其它节点的读操作可见</span><br><span class="line"></span><br><span class="line">强一致性：</span><br><span class="line">如果更新数据后，并发访问情况下后续读操作可立即感知该更新，称为强一致性。</span><br><span class="line"></span><br><span class="line">弱一致性：</span><br><span class="line">如果允许之后部分或者全部感知不到该更新，称为弱一致性。</span><br><span class="line"></span><br><span class="line">最终一致性：</span><br><span class="line">若在之后的一段时间（通常该时间不固定）后，一定可以感知到该更新，称为最终一致性。</span><br></pre></td></tr></table></figure></li><li><p>可用性（Availability）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">every request received by a non-failing node <span class="keyword">in</span> the<span class="built_in"> system </span>must result <span class="keyword">in</span> a response</span><br><span class="line"></span><br><span class="line">任何一个没有发生故障的节点必须在有限的时间内返回合理的结果。</span><br></pre></td></tr></table></figure></li><li><p>分区容忍性（Partition Tolerance）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">the<span class="built_in"> network </span>will be allowed <span class="keyword">to</span> lose arbitrarily many messages sent <span class="keyword">from</span> one node <span class="keyword">to</span> another</span><br><span class="line"></span><br><span class="line">部分节点宕机或者无法与其它节点通信时，各分区间还可保持分布式系统的功能。</span><br></pre></td></tr></table></figure></li><li><p>悖论总结：</p><p>可用性限定在无论是否集群节点宕机，只要有活着的节点，就会立即返回请求结果。若要限制返回结果必须是最近一次写的结果，就比较悲剧，若允许分区容忍性 =&gt; 分布式系统分区之间就存在数据同步机制，那么就有可能因为分区心跳切断，导致数据不一致。</p></li></ul><h2 id="partition本质就是为了日志备份（对外最小的存储单元）"><a href="#partition本质就是为了日志备份（对外最小的存储单元）" class="headerlink" title="partition本质就是为了日志备份（对外最小的存储单元）"></a>partition本质就是为了日志备份（对外最小的存储单元）</h2><p>Kafka中topic的每个partition有一个预写式的日志文件，虽然partition可以继续细分为若干个segment文件，但是对于上层应用来说可以将partition看成最小的存储单元（一个有多个segment文件拼接的“巨型”文件），每个partition都由一些列有序的、不可变的消息组成，这些消息被连续的追加到partition中。</p><p><img src="https://user-gold-cdn.xitu.io/2018/11/22/1673bba0c1d92e2a?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>partition本质就是为了日志备份，利用多份日志文件的副本（replica）备份来共同提供冗余机制来保持系统的高可用性。</li><li>kafka会把副本均匀的分配到所有的Broker上。在其中所有的副本中，会挑选一个Leader副本来对外提供服务，其他的副本统称为follower副本，只能被动的向leader副本请求数据。</li></ul><h2 id="Partitioner-三分天下"><a href="#Partitioner-三分天下" class="headerlink" title="Partitioner 三分天下"></a>Partitioner 三分天下</h2><p>下图展示了3个Partition把一个Topic主题数据流分成三份，通过Partioner路由依次追加到分区的末尾中。如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。</p><p><img src="https://user-gold-cdn.xitu.io/2018/11/22/1673bca604b58210?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>config/server.properties可以设置num.partitions参数，实现主题数据分流。</p><h2 id="Leader副本竞选上岗（in-sync-replicas）"><a href="#Leader副本竞选上岗（in-sync-replicas）" class="headerlink" title="Leader副本竞选上岗（in-sync replicas）"></a>Leader副本竞选上岗（in-sync replicas）</h2><ul><li>每一个分区都存在一个in-sync replicas。</li><li>in-sync replicas集合中的每一个副本都与leader保持同步状态，不在里面的保持不了同步状态。</li><li>只有ISR中的副本才有资格被选为leader。</li><li>Producer写入的消息只有被ISR中的副本都接收到，才被视为”已提交”。</li></ul><h2 id="水印HW与末端位移LEO-gt-Leader副本"><a href="#水印HW与末端位移LEO-gt-Leader副本" class="headerlink" title="水印HW与末端位移LEO =&gt; Leader副本"></a>水印HW与末端位移LEO =&gt; Leader副本</h2><ul><li>这里着重强调一下，Leader副本水印HW才真正决定了对外可看到的消息数量。</li><li>所有的副本都有LEO和HW。</li><li>Leader副本水印HW的更新发生在所有的副本都更新了最新的LEO后，Leader副本最终才认为可以更新Leader副本水印。</li></ul><h2 id="ISR设计优化（replica-lag-max-messages废弃）"><a href="#ISR设计优化（replica-lag-max-messages废弃）" class="headerlink" title="ISR设计优化（replica.lag.max.messages废弃）"></a>ISR设计优化（replica.lag.max.messages废弃）</h2><ul><li>解决了producer突然发起一大波消息，从而产生瞬时高峰流量。若设置replica.lag.max.messages=4，则follower副本会被瞬时的拉开距离，从而导致follower副本瞬间被踢出ISR。不过一段时间follower副本同步后，会再次进入ISR。</li><li>同步不同步，同步不同步反复出现，是多大的性能浪费。</li><li>0.9.0.0开始采用 replica. lag. time. max. ms，默认是10s，可谓是明智之举。</li></ul><h2 id="HW同步机制"><a href="#HW同步机制" class="headerlink" title="HW同步机制"></a>HW同步机制</h2><h3 id="HW指向哪里？"><a href="#HW指向哪里？" class="headerlink" title="HW指向哪里？"></a>HW指向哪里？</h3><ul><li>这里重点强调，都是无论HW还是LEO都是指向下一条消息</li><li>举例如下：如果一个普通topic的某个分区副本的LEO是10，那么该副本当前保存了10条消息，位移值范围是[0, 9]。此时若有一个producer向该副本插入一条消息，则该条消息的位移值是10，而副本LEO值则更新成11。</li></ul><h3 id="Leader与follower的HW（两阶段请求定终身）"><a href="#Leader与follower的HW（两阶段请求定终身）" class="headerlink" title="Leader与follower的HW（两阶段请求定终身）"></a>Leader与follower的HW（两阶段请求定终身）</h3><p><img src="https://user-gold-cdn.xitu.io/2018/11/23/1673c6b093d51af0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>follower 副本会不断地向leader副本发送Fetch请求</li></ul><p><strong>（1）follower 副本对象何时更新LEO？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">follower 副本专属线程不断地向<span class="built_in">leader</span>副本所在broker发送FETCH请求。</span><br><span class="line"></span><br><span class="line"><span class="built_in">leader</span> 副本发送 FETCH response 给follower副本。</span><br><span class="line"></span><br><span class="line">Follower 拿到response之后取出位移数据写入到本地底层日志中，在该过程中其LEO值会被更新。</span><br></pre></td></tr></table></figure><p><strong>（2）leader 端非自己副本对象何时更新LEO？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">leader</span> 端非自己副本对象 LEO值是在<span class="built_in">leader</span>端broker处理FETCH请求过程中被更新的。</span><br></pre></td></tr></table></figure><p><strong>（3） follower 副本对象何时更新HW？</strong></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Follower</span> 副本对象更新HW是在其更新本地LEO之后。</span><br><span class="line"></span><br><span class="line">一旦follower向本地日志写完数据后它就会尝试更新其HW值。</span><br><span class="line">算法为取本地LEO与FETCH response中HW值的较小值</span><br></pre></td></tr></table></figure><p><strong>（4）leader 副本对象何时更新HW？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Leader</span> 副本对象处理 Follower FETCH请求时在更新完<span class="built_in">leader</span> 端非自己副本对象的LEO后将尝试更新其自己HW值</span><br><span class="line"></span><br><span class="line">producer 端写入消息会更新<span class="built_in">leader</span> Replica的LEO</span><br><span class="line"></span><br><span class="line">副本被踢出ISR时</span><br><span class="line"></span><br><span class="line">某分区变更为<span class="built_in">leader</span>副本后</span><br></pre></td></tr></table></figure><p><strong>（5）两阶段请求定终身：</strong></p><p>第一次fetch请求仅获得了当前的数据，fetchOffset &lt; Leader LEO, 因为leader 端的非自己的副本leo 是根据fetch请求确定的，因此，只有第二次请求时，fetchOffset才会和Leader LEO相等，进而更新 leader HW ，进而响应为 leader HW，进而更新 Folloer HW。</p><h3 id="HW更新延迟带来的刀山火海"><a href="#HW更新延迟带来的刀山火海" class="headerlink" title="HW更新延迟带来的刀山火海"></a>HW更新延迟带来的刀山火海</h3><p><img src="https://user-gold-cdn.xitu.io/2018/11/23/1673c679cfaadb97?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>因为 fetchOffset是实实在在的需要位移。所以只有第二轮请求时，Follower才会在其现有位移的基础上，加1进行请求，从而连锁更新 会更新Leader非自己remoteLEO 和  Leader HW 和 Follower HW。</li><li>刀山火海就在一轮请求和第二轮请求之间发生了。</li></ul>]]></content>
    
    <summary type="html">
    
      本文深入剖析了kafka ISR设计及水印与leader epoch副本同步机制。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群Controller竞选与责任设计思路架构详解</title>
    <link href="https://gjtmaster.github.io/2018/09/21/kafka%E9%9B%86%E7%BE%A4Controller%E7%AB%9E%E9%80%89%E4%B8%8E%E8%B4%A3%E4%BB%BB%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3/"/>
    <id>https://gjtmaster.github.io/2018/09/21/kafka集群Controller竞选与责任设计思路架构详解/</id>
    <published>2018-09-21T10:47:13.000Z</published>
    <updated>2019-10-28T06:33:37.757Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无所不能的Controller"><a href="#无所不能的Controller" class="headerlink" title="无所不能的Controller"></a>无所不能的Controller</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676dd7505afda5b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>某一个broker被选举出来承担特殊的角色，就是控制器Controller。</p></li><li><p>Leader会向zookeeper上注册Watcher，其他broker几乎不用监听zookeeper的状态变化。</p></li><li><p>Controller集群就是用来管理和协调Kafka集群的，具体就是管理集群中所有分区的状态和分区对应副本的状态。</p></li><li><p>每一个Kafka集群任意时刻都只能有一个controller，当集群启动的时候，所有的broker都会参与到controller的竞选，最终只能有一个broker胜出。</p></li><li><p>Controller维护的状态分为两类：1：管理每一台Broker上对应的分区副本。2：管理每一个Topic分区的状态。</p></li><li><p>KafkaController 核心代码，其中包含副本状态机和分区状态机</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span>(<span class="params">val config : <span class="type">KafkaConfig</span>, zkClient: <span class="type">ZkClient</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">val brokerState: <span class="type">BrokerState</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.logIdent = <span class="string">"[Controller "</span> + config.brokerId + <span class="string">"]: "</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> stateChangeLogger = <span class="type">KafkaController</span>.stateChangeLogger</span><br><span class="line">    <span class="keyword">val</span> controllerContext = <span class="keyword">new</span> <span class="type">ControllerContext</span>(zkClient, config.zkSessionTimeoutMs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> partitionStateMachine = <span class="keyword">new</span> <span class="type">PartitionStateMachine</span>(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">val</span> replicaStateMachine = <span class="keyword">new</span> <span class="type">ReplicaStateMachine</span>(<span class="keyword">this</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controllerElector = <span class="keyword">new</span> <span class="type">ZookeeperLeaderElector</span>(controllerContext, <span class="type">ZkUtils</span>.<span class="type">ControllerPath</span>, onControllerFailover,</span><br><span class="line">    onControllerResignation, config.brokerId)</span><br><span class="line">    <span class="comment">// have a separate scheduler for the controller to be able to start and stop independently of the</span></span><br><span class="line">    <span class="comment">// kafka server</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> autoRebalanceScheduler = <span class="keyword">new</span> <span class="type">KafkaScheduler</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">var</span> deleteTopicManager: <span class="type">TopicDeletionManager</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> offlinePartitionSelector = <span class="keyword">new</span> <span class="type">OfflinePartitionLeaderSelector</span>(controllerContext, config)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> reassignedPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ReassignedPartitionLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">PreferredReplicaPartitionLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controlledShutdownPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ControlledShutdownLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> brokerRequestBatch = <span class="keyword">new</span> <span class="type">ControllerBrokerRequestBatch</span>(<span class="keyword">this</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> partitionReassignedListener = <span class="keyword">new</span> <span class="type">PartitionsReassignedListener</span>(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaElectionListener = <span class="keyword">new</span> <span class="type">PreferredReplicaElectionListener</span>(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure></li><li><p>KafkaController中共定义了五种selector选举器</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、ReassignedPartitionLeaderSelector</span><br><span class="line">从可用的ISR中选取第一个作为<span class="built_in">leader</span>，把当前的ISR作为新的ISR，将重分配的副本集合作为接收LeaderAndIsr请求的副本集合。</span><br><span class="line"><span class="number">2</span>、PreferredReplicaPartitionLeaderSelector</span><br><span class="line">如果从assignedReplicas取出的第一个副本就是分区<span class="built_in">leader</span>的话，则抛出异常，否则将第一个副本设置为分区<span class="built_in">leader</span>。</span><br><span class="line"><span class="number">3</span>、ControlledShutdownLeaderSelector</span><br><span class="line">将ISR中处于关闭状态的副本从集合中去除掉，返回一个新新的ISR集合，然后选取第一个副本作为<span class="built_in">leader</span>，然后令当前AR作为接收LeaderAndIsr请求的副本。</span><br><span class="line"><span class="number">4</span>、NoOpLeaderSelector</span><br><span class="line">原则上不做任何事情，返回当前的<span class="built_in">leader</span>和isr。</span><br><span class="line"><span class="number">5</span>、OfflinePartitionLeaderSelector</span><br><span class="line">从活着的ISR中选择一个broker作为<span class="built_in">leader</span>，如果ISR中没有活着的副本，则从assignedReplicas中选择一个副本作为<span class="built_in">leader</span>，<span class="built_in">leader</span>选举成功后注册到Zookeeper中，并更新所有的缓存。</span><br></pre></td></tr></table></figure></li><li><p>kafka修改分区和副本数</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">..<span class="regexp">/bin/</span>kafka-topics.sh --zookeeper <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">2181</span> --describe  --topic test1</span><br><span class="line"></span><br><span class="line"><span class="string">Topic:</span>test1       <span class="string">PartitionCount:</span><span class="number">3</span>        <span class="string">ReplicationFactor:</span><span class="number">2</span>     <span class="string">Configs:</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">0</span>    <span class="string">Leader:</span> <span class="number">2</span>       <span class="string">Replicas:</span> <span class="number">2</span>,<span class="number">4</span>   <span class="string">Isr:</span> <span class="number">2</span>,<span class="number">4</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">1</span>    <span class="string">Leader:</span> <span class="number">3</span>       <span class="string">Replicas:</span> <span class="number">3</span>,<span class="number">5</span>   <span class="string">Isr:</span> <span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">2</span>    <span class="string">Leader:</span> <span class="number">4</span>       <span class="string">Replicas:</span> <span class="number">4</span>,<span class="number">1</span>   <span class="string">Isr:</span> <span class="number">4</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>topic 分区扩容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  <span class="string">./kafka-topics.sh</span> <span class="params">--zookeeper</span> 127.0.0.1<span class="function">:2181</span> -alter <span class="params">--partitions</span> 4 <span class="params">--topic</span> test1</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure></li></ul><h2 id="ReplicaStateMachine-（ZK持久化副本分配方案）"><a href="#ReplicaStateMachine-（ZK持久化副本分配方案）" class="headerlink" title="ReplicaStateMachine （ZK持久化副本分配方案）"></a>ReplicaStateMachine （ZK持久化副本分配方案）</h2><ul><li><p>Replica有7种状态:</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="string">NewReplica:</span> 在partition reassignment期间KafkaController创建New replica</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> <span class="string">OnlineReplica:</span> 当一个replica变为一个parition的assingned replicas时</span><br><span class="line">其状态变为OnlineReplica, 即一个有效的OnlineReplica</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> Online状态的parition才能转变为leader或isr中的一员</span><br><span class="line"></span><br><span class="line"><span class="number">4</span> <span class="string">OfflineReplica:</span> 当一个broker down时, 上面的replica也随之die, 其状态转变为Onffline;</span><br><span class="line"><span class="string">ReplicaDeletionStarted:</span> 当一个replica的删除操作开始时,其状态转变为ReplicaDeletionStarted</span><br><span class="line"></span><br><span class="line"><span class="number">5</span> <span class="string">ReplicaDeletionSuccessful:</span> Replica成功删除后,其状态转变为ReplicaDeletionSuccessful</span><br><span class="line"></span><br><span class="line"><span class="number">6</span> <span class="string">ReplicaDeletionIneligible:</span> Replica成功失败后,其状态转变为ReplicaDeletionIneligible</span><br><span class="line"></span><br><span class="line"><span class="number">7</span> <span class="string">NonExistentReplica:</span>  Replica成功删除后, 从ReplicaDeletionSuccessful状态转变为NonExistentReplica状态</span><br></pre></td></tr></table></figure></li><li><p>ReplicaStateMachine 所在文件: core/src/main/scala/kafka/controller/ReplicaStateMachine.scala</p></li><li><p>startup: 启动ReplicaStateMachine</p></li><li><p>initializeReplicaState: 初始化每个replica的状态, 如果replica所在的broker是live状态,则此replica的状态为OnlineReplica。</p></li><li><p>处理可以转换到Online状态的Replica, handleStateChanges(controllerContext.allLiveReplicas(), OnlineReplica), 并且发送LeaderAndIsrRequest到各broker nodes: handleStateChanges(controllerContext.allLiveReplicas(), OnlineReplica)</p></li><li><p>当创建某个topic时，该topic下所有分区的所有副本都是NonExistent。</p></li><li><p>当controller加载Zookeeper中该topic每一个分区的所有副本信息到内存中，同时将副本的状态变更为New。</p></li><li><p>之后controller选择该分区副本列表中的第一个副本作为分区的leader副本并设置所有副本进入ISR，然后在Zookeeper中持久化该决定。</p></li><li><p>一旦确定了分区的Leader和ISR之后，controller会将这些消息以请求的方式发送给所有的副本。</p></li><li><p>同时将这些副本状态同步到集群的所有broker上以便让他们知晓。</p></li><li><p>最后controller 会把分区的所有副本状态设置为Online。</p></li></ul><h2 id="partitionStateMachine-（根据副本分配方案创建分区）"><a href="#partitionStateMachine-（根据副本分配方案创建分区）" class="headerlink" title="partitionStateMachine （根据副本分配方案创建分区）"></a>partitionStateMachine （根据副本分配方案创建分区）</h2><ul><li><p>Partition有如下四种状态</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">NonExistentPartition:</span> 这个partition还没有被创建或者是创建后又被删除了<span class="comment">;</span></span><br><span class="line"><span class="symbol">NewPartition:</span> 这个parition已创建, replicas也已分配好,但leader/isr还未就绪<span class="comment">;</span></span><br><span class="line"><span class="symbol">OnlinePartition:</span> 这个partition的leader选好<span class="comment">;</span></span><br><span class="line"><span class="symbol">OfflinePartition:</span> 这个partition的leader挂了,这个parition状态为OfflinePartition<span class="comment">;</span></span><br></pre></td></tr></table></figure></li><li><p>当创建Topic时，controller负责创建分区对象，它首先会短暂的将所有分区状态设置为NonExistent。</p></li><li><p>之后读取Zookeeper副本分配方案，然后令分区状态设置为NewPartion。</p></li><li><p>处于NewPartion状态的分区尚未有leader和ISR，因此Controller会初始化leader和ISR信息并设置分区状态为OnlinePartion，此时分区正常工作。</p></li></ul><h2 id="Controller职责所在-监听znode状态变化做执行"><a href="#Controller职责所在-监听znode状态变化做执行" class="headerlink" title="Controller职责所在(监听znode状态变化做执行)"></a>Controller职责所在(监听znode状态变化做执行)</h2><ul><li>UpdateMetadataRequest：更新元数据请求（比如：topic有多少个分区，每一个分区的leader在哪一台broker上以及分区的副本列表），随着集群的运行，这部分信息随时都可能变更，一旦发生变更，controller会将最新的元数据广播给所有存活的broker。具体方式就是给所有broker发送UpdateMetadataRequest请求</li><li>CreateTopics: 创建topic请求。当前不管是通过API方式、脚本方式（–create）抑或是CreateTopics请求方式来创建topic，做法几乎都是在Zookeeper的/brokers/topics下创建znode来触发创建逻辑，而controller会监听该path下的变更来执行真正的“创建topic”逻辑</li><li>DeleteTopics：删除topic请求。和CreateTopics类似，也是通过创建Zookeeper下的/admin/delete_topics/节点来触发删除topic，主要逻辑有：1：停止所有副本运行。2：删除所有副本的日志数据。3：移除zk上的 /admin/delete_topics/节点。</li><li>分区重分配：即kafka-reassign-partitions脚本做的事情。同样是与Zookeeper结合使用，脚本写入/admin/reassign_partitions节点来触发，controller负责按照方案分配分区。执行过程是：先扩展再伸缩机制（旧副本和新副本集合同时存在）。</li><li>Preferred leader分配：调整分区leader副本，preferred leader选举当前有两种触发方式：1. 自动触发(auto.leader.rebalance.enable = true)，controller会自动调整Preferred leader。2. kafka-preferred-replica-election脚本触发。两者步骤相同，都是向Zookeeper的/admin/preferred_replica_election写数据，controller提取数据执行preferred leader分配</li><li>分区扩展：即增加topic分区数。标准做法也是通过kafka-reassign-partitions脚本完成，不过用户可直接往Zookeeper中写数据来实现，比如直接把新增分区的副本集合写入到/brokers/topics/下，然后controller会为你自动地选出leader并增加分区</li><li>集群扩展：新增broker时Zookeeper中/brokers/ids下会新增znode，controller自动完成服务发现的工作</li><li>broker崩溃：同样地，controller通过Zookeeper可实时侦测broker状态。一旦有broker挂掉了，controller可立即感知并为受影响分区选举新的leader</li><li>ControlledShutdown：broker除了崩溃，还能“优雅”地退出。broker一旦自行终止，controller会接收到一个ControlledShudownRequest请求，然后controller会妥善处理该请求并执行各种收尾工作</li><li>Controller leader选举：controller必然要提供自己的leader选举以防这个全局唯一的组件崩溃宕机导致服务中断。这个功能也是通过Zookeeper的帮助实现的。</li></ul><h2 id="Controller与Broker之间的通信机制（NIO-select）"><a href="#Controller与Broker之间的通信机制（NIO-select）" class="headerlink" title="Controller与Broker之间的通信机制（NIO select）"></a>Controller与Broker之间的通信机制（NIO select）</h2><ul><li>controller启动时会为集群中的所有Broker创建一个专属的Socket连接，假如有100台broker机器，那么controller会创建100个Socket连接。新版本目前统一使用NIO select ，实际上还是要维护100个线程。</li></ul><h2 id="ControllerContext数据组件"><a href="#ControllerContext数据组件" class="headerlink" title="ControllerContext数据组件"></a>ControllerContext数据组件</h2><ul><li>controller的缓存，可谓是最重要的数据组件了，ControllerContext汇总了Zookeeper中关于kafka集群中所有元数据信息，是controller能够正确提供服务的基础。</li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676dd70918f80a5?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>kafka集群Controller主要通过ZK持久化副本分配方案，根据副本分配方案创建分区，监听ZK znode状态变化做执行处理，维护分区和副本ISR机制稳定运行。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述了kafka集群Controller竞选与责任设计思路架构。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka精确一次语义EOS的原理深入剖析</title>
    <link href="https://gjtmaster.github.io/2018/09/20/kafka%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1%E8%AF%AD%E4%B9%89EOS%E7%9A%84%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2018/09/20/kafka精确一次语义EOS的原理深入剖析/</id>
    <published>2018-09-20T11:12:45.000Z</published>
    <updated>2019-10-25T07:49:44.604Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kafka-0-11-0-0版本的逆天之作"><a href="#Kafka-0-11-0-0版本的逆天之作" class="headerlink" title="Kafka 0.11.0.0版本的逆天之作"></a>Kafka 0.11.0.0版本的逆天之作</h2><ul><li>0.11.0.0版本之前默认提供at least once语义，想象这样一种场景，分区的Leader副本所在的Broker成功的将消息写入本地磁盘，然后broker将发送响应给producer，此时假设网络出现故障导致该响应没有发送成功。此种情况下，Producer将认为消息发送请求失败，从而开启重试机制。若此时网络恢复正常，那么同一条消息将会被写入两次。</li><li>基于上述案例：0.11.0.0版本提供幂等性：每个分区中精确一次且有序</li><li>0.11.0.0版本提供事务：跨分区原子写入机制。</li></ul><h2 id="故障类型"><a href="#故障类型" class="headerlink" title="故障类型"></a>故障类型</h2><ul><li>broker可能故障：Kafka是一个高可用、持久化的系统，每一条写入一个分区的消息都会被持久化并且多副本备份（假设有n个副本）。所以，Kafka可以容忍n-1个broker故障，意味着一个分区只要至少有一个broker可用，分区就可用。Kafka的副本协议保证了只要消息被成功写入了主副本，它就会被复制到其他所有的可用副本（ISR）。</li><li>producer到broker的RPC调用可能失败：Kafka的持久性依赖于生产者接收broker的ack响应。没有接收成功ack不代表生产请求本身失败了。broker可能在写入消息后，发送ack给生产者的时候挂了。甚至broker也可能在写入消息前就挂了。由于生产者没有办法知道错误是什么造成的，所以它就只能认为消息没写入成功，并且会重试发送。在一些情况下，这会造成同样的消息在Kafka分区日志中重复，进而造成消费端多次收到这条消息。</li><li>客户端可能会故障：精确一次交付也必须考虑客户端故障。但是我们如何知道一个客户端已经故障而不是暂时和brokers断开，或者经历一个程序短暂的暂停，区分永久性故障和临时故障是很重要的，为了正确性，broker应该丢弃僵住的生产这发送来的消息，同样，也应该不向已经僵住的消费者发送消息。一旦一个新的客户端实例启动，它应该能够从失败的实例留下的任何状态中恢复，从一个安全点开始处理。这意味着，消费的偏移量必须始终与生产的输出保持同步。</li></ul><h2 id="Producer幂等性处理机制"><a href="#Producer幂等性处理机制" class="headerlink" title="Producer幂等性处理机制"></a>Producer幂等性处理机制</h2><ul><li>如果出现导致生产者重试的错误，同样的消息，仍由同样的生产者发送多次，将只被写到kafka broker的日志中一次。对于单个分区，幂等生产者不会因为生产者或broker故障而发送多条重复消息。</li><li>kafka保存序列号仅仅需要几个额外的字段，因此这种机制的开销非常低。</li><li>除了序列号，kafka会为每个Producer实例分配一个Producer id（PID）,每一条消息都会有序列号，并严格递增顺序。若发送的消息的序列号小于或者等于broker端保存的序列号，那么broker会拒绝这条消息的写入操作。</li><li>注意的是：当前的设计只能保证单个producer实例的EOS语义，无法实现多个Producer实例一块提供EOS语义。</li><li>想要开启这个特性，获得每个分区内的精确一次语义，也就是说没有重复，没有丢失，并且有序的语义，只需要设置producer配置中的”enable.idempotence=true”。</li></ul><h2 id="事务：跨分区原子写入"><a href="#事务：跨分区原子写入" class="headerlink" title="事务：跨分区原子写入"></a>事务：跨分区原子写入</h2><ul><li><p>事务：跨分区原子写入</p><p>将允许一个生产者发送一批到不同分区的消息，这些消息要么全部对任何一个消费者可见，要么对任何一个消费者都不可见。这个特性也允许你在一个事务中处理消费数据和提交消费偏移量，从而实现端到端的精确一次语义。</p></li><li><p>主要针对消息经过Partioner分区器到多个分区的情况。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">producer</span><span class="selector-class">.initTransactions</span>();</span><br><span class="line"><span class="selector-tag">try</span> &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.beginTransaction</span>();</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.send</span>(record1);</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.send</span>(record2);</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.commitTransaction</span>();</span><br><span class="line">&#125; <span class="selector-tag">catch</span>(ProducerFencedException e) &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.close</span>();</span><br><span class="line">&#125; <span class="selector-tag">catch</span>(KafkaException e) &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.abortTransaction</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="消费端的事务支持"><a href="#消费端的事务支持" class="headerlink" title="消费端的事务支持"></a>消费端的事务支持</h2><ul><li><p>在消费者方面，有两种选择来读取事务性消息，通过隔离等级“isolation.level”消费者配置表示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read_commited：除了读取不属于事务的消息之外，还可以读取事务提交后的消息。</span><br><span class="line">read_uncommited：按照偏移位置读取所有消息，而不用等事务提交。这个选项类似Kafka消费者的当前语义。</span><br></pre></td></tr></table></figure></li><li><p>为了使用事务，需要配置消费者使用正确的隔离等级。</p></li><li><p>使用新版生产者，并且将生产者的“transactional . id”配置项设置为某个唯一ID。 需要此唯一ID来提供跨越应用程序重新启动的事务状态的连续性。</p></li></ul><h2 id="消费端精确到一次语义实现"><a href="#消费端精确到一次语义实现" class="headerlink" title="消费端精确到一次语义实现"></a>消费端精确到一次语义实现</h2><p>消费端精确到一次语义实现：consumer通过subscribe方法注册到kafka，精确一次的语义要求必须手动管理offset，按照下述步骤进行设置：</p><ul><li><p>1.设置enable.auto.commit = false;</p></li><li><p>2.处理完消息之后不要手动提交offset，</p></li><li><p>3.通过subscribe方法将consumer注册到某个特定topic，</p></li><li><p>4.实现ConsumerRebalanceListener接口和consumer.seek(topicPartition,offset)方法（读取特定topic和partition的offset）</p></li><li><p>5.将offset和消息一块存储，确保原子性，推荐使用事务机制。</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">ExactlyOnceDynamicConsumer</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> OffsetManager offsetManager = <span class="keyword">new</span> OffsetManager(<span class="string">"storage2"</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(<span class="params">String[] str</span>) throws InterruptedException</span> &#123;</span><br><span class="line"></span><br><span class="line">    System.<span class="keyword">out</span>.println(<span class="string">"Starting ManualOffsetGuaranteedExactlyOnceReadingDynamicallyBalancedPartitionConsumer ..."</span>);</span><br><span class="line"></span><br><span class="line">    readMessages();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  private static void readMessages() throws InterruptedException {</p><pre><code>KafkaConsumer&lt;String, String&gt; consumer = createConsumer();// Manually controlling offset but register consumer to topics to get dynamically assigned partitions.// Inside MyConsumerRebalancerListener use consumer.seek(topicPartition,offset) to control offsetconsumer.subscribe(Arrays.asList(&quot;normal-topic&quot;), new MyConsumerRebalancerListener(consumer));processRecords(consumer);</code></pre><p>  }</p></li></ul><pre><code>private static KafkaConsumer&lt;String, String&gt; createConsumer() {    Properties props = new Properties();    props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);    String consumeGroup = &quot;cg3&quot;;    props.put(&quot;group.id&quot;, consumeGroup);    props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);    props.put(&quot;heartbeat.interval.ms&quot;, &quot;2000&quot;);    props.put(&quot;session.timeout.ms&quot;, &quot;6001&quot;);    * Control maximum data on each poll, make sure this value is bigger than the maximum single record size    props.put(&quot;max.partition.fetch.bytes&quot;, &quot;140&quot;);    props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);    props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);    return new KafkaConsumer&lt;String, String&gt;(props);}private static void processRecords(KafkaConsumer&lt;String, String&gt; consumer) {    while (true) {        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);        for (ConsumerRecord&lt;String, String&gt; record : records) {            System.out.printf(&quot;offset = %d, key = %s, value = %s\n&quot;, record.offset(), record.key(), record.value());            offsetManager.saveOffsetInExternalStore(record.topic(), record.partition(), record.offset());        }    }}public class MyConsumerRebalancerListener implements org.apache.kafka.clients.consumer.ConsumerRebalanceListener {private OffsetManager offsetManager = new OffsetManager(&quot;storage2&quot;);private Consumer&lt;String, String&gt; consumer;public MyConsumerRebalancerListener(Consumer&lt;String, String&gt; consumer) {    this.consumer = consumer;}public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {    for (TopicPartition partition : partitions) {        offsetManager.saveOffsetInExternalStore(partition.topic(), partition.partition(), consumer.position(partition));    }}public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {    for (TopicPartition partition : partitions) {        consumer.seek(partition, offsetManager.readOffsetFromExternalStore(partition.topic(), partition.partition()));    }}public class OffsetManager {  private String storagePrefix;  public OffsetManager(String storagePrefix) {      this.storagePrefix = storagePrefix;  }  void saveOffsetInExternalStore(String topic, int partition, long offset) {      try {          FileWriter writer = new FileWriter(storageName(topic, partition), false);          BufferedWriter bufferedWriter = new BufferedWriter(writer);          bufferedWriter.write(offset + &quot;&quot;);          bufferedWriter.flush();          bufferedWriter.close();      } catch (Exception e) {          e.printStackTrace();          throw new RuntimeException(e);      }  }  long readOffsetFromExternalStore(String topic, int partition) {      try {          Stream&lt;String&gt; stream = Files.lines(Paths.get(storageName(topic, partition)));          return Long.parseLong(stream.collect(Collectors.toList()).get(0)) + 1;      } catch (Exception e) {          e.printStackTrace();      }      return 0;  }  private String storageName(String topic, int partition) {      return storagePrefix + &quot;-&quot; + topic + &quot;-&quot; + partition;  }</code></pre>]]></content>
    
    <summary type="html">
    
      本文主要剖析了kafka精确一次语义EOS的原理。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群Broker端基于Reactor模式请求处理流程</title>
    <link href="https://gjtmaster.github.io/2018/09/20/kafka%E9%9B%86%E7%BE%A4Broker%E7%AB%AF%E5%9F%BA%E4%BA%8EReactor%E6%A8%A1%E5%BC%8F%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
    <id>https://gjtmaster.github.io/2018/09/20/kafka集群Broker端基于Reactor模式请求处理流程/</id>
    <published>2018-09-20T11:12:34.000Z</published>
    <updated>2019-10-28T06:33:37.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Reactor单线程案例代码"><a href="#Reactor单线程案例代码" class="headerlink" title="Reactor单线程案例代码"></a>Reactor单线程案例代码</h2><ul><li><p>如下是单线程的JAVA NIO编程模型。</p></li><li><p>首先服务端创建ServerSocketChannel对象，并注册到Select上OP_ACCEPT事件，然后ServerSocketChannel负责监听指定端口上的连接请求。</p></li><li><p>客户端一旦连接上ServerSocketChannel，就会触发Acceptor来处理OP_ACCEPT事件，并为来自客户端的连接创建Socket Channel，并设置为非阻塞模式，并在其Selector上注册OP_READ或者OP_WRITE，最终实现客户端与服务端的连接建立和数据通道打通。</p></li><li><p>当客户端向建立的SocketChannel发送请求时，服务端的Selector就会监听到OP_READ事件，并触发相应的处理逻辑。当服务端向客户端写数据时，会触发服务端Selector的OP_WRITE事件，从而执行响应的处理逻辑。</p></li><li><p>这里有一个明显的问题，就是所有时间的处理逻辑都是在Acceptor单线程完成的，在并发连接数较小，数据量较小的场景下，是没有问题的，但是……</p></li><li><p>Selector 允许一个单一的线程来操作多个 Channel. 如果我们的应用程序中使用了多个 Channel, 那么使用 Selector 很方便的实现这样的目的, 但是因为在一个线程中使用了多个 Channel, 因此也会造成了每个 Channel 传输效率的降低.</p></li><li><p>优化点在于：通道连接|读取或写入|业务处理均采用单线程来处理。通过线程池或者MessageQueue共享队列，进一步优化了高并发的处理要求，这样就解决了同一时间出现大量I/O事件时，单独的Select就可能在分发事件时阻塞（或延时），而成为瓶颈的问题。</p><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676e6d0547f52c4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioEchoServer</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">256</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TIMEOUT = <span class="number">3000</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 打开服务端 Socket</span></span><br><span class="line">      ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 打开 Selector</span></span><br><span class="line">      Selector selector = Selector.open();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 服务端 Socket 监听8080端口, 并配置为非阻塞模式</span></span><br><span class="line">      serverSocketChannel.socket().bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">8080</span>));</span><br><span class="line">      serverSocketChannel.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将 channel 注册到 selector 中.</span></span><br><span class="line">      <span class="comment">// 通常我们都是先注册一个 OP_ACCEPT 事件, 然后在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ</span></span><br><span class="line">      <span class="comment">// 注册到 Selector 中.</span></span><br><span class="line">      serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          <span class="comment">// 通过调用 select 方法, 阻塞地等待 channel I/O 可操作</span></span><br><span class="line">          <span class="keyword">if</span> (selector.select(TIMEOUT) == <span class="number">0</span>) &#123;</span><br><span class="line">              System.out.print(<span class="string">"."</span>);</span><br><span class="line">              <span class="keyword">continue</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 获取 I/O 操作就绪的 SelectionKey, 通过 SelectionKey 可以知道哪些 Channel 的哪类 I/O 操作已经就绪.</span></span><br><span class="line">          Iterator&lt;SelectionKey&gt; keyIterator = selector.selectedKeys().iterator();</span><br><span class="line"></span><br><span class="line">          <span class="keyword">while</span> (keyIterator.hasNext()) &#123;</span><br><span class="line"></span><br><span class="line">              SelectionKey key = keyIterator.next();</span><br><span class="line"></span><br><span class="line">              <span class="comment">// 当获取一个 SelectionKey 后, 就要将它删除, 表示我们已经对这个 IO 事件进行了处理.</span></span><br><span class="line">              keyIterator.remove();</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isAcceptable()) &#123;</span><br><span class="line">                  <span class="comment">// 当 OP_ACCEPT 事件到来时, 我们就有从 ServerSocketChannel 中获取一个 SocketChannel,</span></span><br><span class="line">                  <span class="comment">// 代表客户端的连接</span></span><br><span class="line">                  <span class="comment">// 注意, 在 OP_ACCEPT 事件中, 从 key.channel() 返回的 Channel 是 ServerSocketChannel.</span></span><br><span class="line">                  <span class="comment">// 而在 OP_WRITE 和 OP_READ 中, 从 key.channel() 返回的是 SocketChannel.</span></span><br><span class="line">                  SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept();</span><br><span class="line">                  clientChannel.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">                  <span class="comment">//在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ 注册到 Selector 中.</span></span><br><span class="line">                  <span class="comment">// 注意, 这里我们如果没有设置 OP_READ 的话, 即 interest set 仍然是 OP_CONNECT 的话, 那么 select 方法会一直直接返回.</span></span><br><span class="line">                  clientChannel.register(key.selector(), OP_READ, ByteBuffer.allocate(BUF_SIZE));</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isReadable()) &#123;</span><br><span class="line">                  SocketChannel clientChannel = (SocketChannel) key.channel();</span><br><span class="line">                  ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                  <span class="keyword">long</span> bytesRead = clientChannel.read(buf);</span><br><span class="line">                  <span class="keyword">if</span> (bytesRead == -<span class="number">1</span>) &#123;</span><br><span class="line">                      clientChannel.close();</span><br><span class="line">                  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (bytesRead &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                      key.interestOps(OP_READ | SelectionKey.OP_WRITE);</span><br><span class="line">                      System.out.println(<span class="string">"Get data length: "</span> + bytesRead);</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isValid() &amp;&amp; key.isWritable()) &#123;</span><br><span class="line">                  ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                  buf.flip();</span><br><span class="line">                  SocketChannel clientChannel = (SocketChannel) key.channel();</span><br><span class="line"></span><br><span class="line">                  clientChannel.write(buf);</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">if</span> (!buf.hasRemaining()) &#123;</span><br><span class="line">                      key.interestOps(OP_READ);</span><br><span class="line">                  &#125;</span><br><span class="line">                  buf.compact();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Kafka-Reactor模式设计思路"><a href="#Kafka-Reactor模式设计思路" class="headerlink" title="Kafka Reactor模式设计思路"></a>Kafka Reactor模式设计思路</h2><ul><li><p>SelectionKey.OP_READ：Socket 读事件，以从远程发送过来了相应数据</p></li><li><p>SelectionKey.OP_WRITE：Socket写事件，即向远程发送数据</p></li><li><p>SelectionKey.OP_CONNECT：Socket连接事件，用来客户端同远程Server建立连接的时候注册到Selector，当连接建立以后，即对应的SocketChannel已经准备好了，用户可以从对应的key上取出SocketChannel.</p></li><li><p>SelectionKey.OP_ACCEPT：Socket连接接受事件，用来服务器端通过ServerSocketChannel绑定了对某个端口的监听，然后会让其SocketChannel对应的socket注册到服务端的Selector上，并关注该OP_ACCEPT事件。</p></li><li><p>Kafka的网络层入口类是SocketServer。我们知道，kafka.Kafka是Kafka Broker的入口类，kafka.Kafka.main()是Kafka Server的main()方法，即Kafka Broker的启动入口。我们跟踪代码，即沿着方法调用栈kafka.Kafka.main() -&gt; KafkaServerStartable() -&gt; KafkaServer().startup可以从main()方法入口一直跟踪到SocketServer即网络层对象的创建，这意味着Kafka Server启动的时候会初始化并启动SocketServer。</p></li><li><p>Acceptor的构造方法中，首先通过openServerSocket()打开自己负责的EndPoint的Socket，即打开端口并启动监听。然后，Acceptor会负责构造自己管理的一个或者多个Processor对象。其实，每一个Processor都是一个独立线程。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[kafka] <span class="class"><span class="keyword">class</span> <span class="title">Acceptor</span>(<span class="params">val endPoint: <span class="type">EndPoint</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 val sendBufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 val recvBufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 processors: <span class="type">Array</span>[<span class="type">Processor</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                                 connectionQuotas: <span class="type">ConnectionQuotas</span></span>) <span class="keyword">extends</span> <span class="title">AbstractServerThread</span>(<span class="params">connectionQuotas</span>) <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> nioSelector = <span class="type">NSelector</span>.open()</span><br><span class="line"> <span class="keyword">val</span> serverChannel = openServerSocket(endPoint.host, endPoint.port)<span class="comment">//创建一个ServerSocketChannel，监听endPoint.host, endPoint.port套接字</span></span><br><span class="line">    </span><br><span class="line"> <span class="comment">//Acceptor被构造的时候就会启动所有的processor线程</span></span><br><span class="line"> <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">   <span class="comment">//每个processor创建一个单独线程</span></span><br><span class="line">   processors.foreach &#123; processor =&gt;</span><br><span class="line">     <span class="type">Utils</span>.newThread(<span class="string">"kafka-network-thread-%d-%s-%d"</span>.format(brokerId, endPoint.protocolType.toString, processor.id), processor, <span class="literal">false</span>).start()</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>Acceptor线程的run()方法，是不断监听对应ServerChannel上的连接请求，如果有新的连接请求，就选择出一个Processor，用来处理这个请求，将这个新连接交付给Processor是在方法Acceptor.accept()</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]<span class="comment">//取出channel</span></span><br><span class="line">    <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()<span class="comment">//创建socketChannel，专门负责与这个客户端的连接</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//socketChannel参数设置</span></span><br><span class="line">      processor.accept(socketChannel)<span class="comment">//将SocketChannel交给process进行处理</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">//异常处理</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Processor.accept():</span></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Queue up a new connection for reading</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</span><br><span class="line">    newConnections.add(socketChannel)</span><br><span class="line">    wakeup()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>每一个Processor都维护了一个单独的KSelector对象，这个KSelector只负责这个Processor上所有channel的监听。这样最大程度上保证了不同Processor线程之间的完全并行和业务隔离，尽管，在异步IO情况下，一个Selector负责成百上千个socketChannel的状态监控也不会带来效率问题。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">   startupComplete()<span class="comment">//表示初始化流程已经结束，通过这个CountDownLatch代表初始化已经结束，这个Processor已经开始正常运行了</span></span><br><span class="line">   <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="comment">// setup any new connections that have been queued up</span></span><br><span class="line">       configureNewConnections()<span class="comment">//为已经接受的请求注册OR_READ事件</span></span><br><span class="line">       <span class="comment">// register any new responses for writing</span></span><br><span class="line">       processNewResponses()<span class="comment">//处理响应队列,这个响应队列是Handler线程处理以后的结果，会交付给RequestChannel.responseQueue.同时调用unmute，开始接受请求</span></span><br><span class="line">       poll()  <span class="comment">//调用KSelector.poll(),进行真正的数据读写</span></span><br><span class="line">       processCompletedReceives()<span class="comment">//调用mute，停止接受新的请求</span></span><br><span class="line">       processCompletedSends()</span><br><span class="line">       processDisconnected()</span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">       <span class="comment">//异常处理 略</span></span><br><span class="line">   &#125;</span><br><span class="line">    </span><br><span class="line">   debug(<span class="string">"Closing selector - processor "</span> + id)</span><br><span class="line">   swallowError(closeAll())</span><br><span class="line">   shutdownComplete()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>KSelector.register()方法，开始对远程客户端或者其它服务器的读请求(OP_READ)进行绑定和处理。KSelect.register()方法，会将服务端的SocketChannel注册到服务器端的nioSelector，并关注SelectionKey.OP_READ，即，如果发生读请求，可以取出对应的Channel进行处理。这里的Channel也是Kafka经过封装以后的KafkaChannel对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(String id, SocketChannel socketChannel)</span> <span class="keyword">throws</span> ClosedChannelException </span>&#123;</span><br><span class="line">        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_READ);</span><br><span class="line">        <span class="comment">//如果是SocketServer创建的这个对象并且是纯文本，则channelBuilder是@Code PlainTextChannelBuilder</span></span><br><span class="line">        KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize);<span class="comment">//构造一个KafkaChannel</span></span><br><span class="line">        key.attach(channel);<span class="comment">//将KafkaChannel对象attach到这个registration，以后可以通过调用SelectionKey.attachment()获得这个对象</span></span><br><span class="line">        <span class="keyword">this</span>.channels.put(id, channel);<span class="comment">//记录这个Channel</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>Processor.processCompletedReceives()通过遍历completedReceives，对于每一个已经完成接收的数据，对数据进行解析和封装，交付给RequestChannel，RequestChannel会交付给具体的业务处理层进行处理。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 将completedReceived中的对象进行封装，交付给requestQueue.completRequets</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</span><br><span class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;<span class="comment">//每一个receive是一个NetworkReceivedui'xiagn</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//receive.source代表了这个请求的发送者的身份，KSelector保存了channel另一端的身份和对应的SocketChannel之间的对应关系</span></span><br><span class="line">      <span class="keyword">val</span> channel = selector.channel(receive.source)</span><br><span class="line">      <span class="keyword">val</span> session = <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName),</span><br><span class="line">        channel.socketAddress)</span><br><span class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session, buffer = receive.payload, startTimeMs = time.milliseconds, securityProtocol = protocol)</span><br><span class="line">      requestChannel.sendRequest(req)<span class="comment">//将请求通过RequestChannel.requestQueue交付给Handler</span></span><br><span class="line">      selector.mute(receive.source)<span class="comment">//不再接受Read请求,发送响应之前，不可以再接收任何请求</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">//异常处理 略</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676ed08083c5576?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>详情源码剖析请参考如下博客，讲解非常详细。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/zhanyuanlin/</span>article<span class="regexp">/details/</span><span class="number">76556578</span></span><br><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/zhanyuanlin/</span>article<span class="regexp">/details/</span><span class="number">76906583</span></span><br></pre></td></tr></table></figure></li><li><p>RequestChannel 负责消息从网络层转接到业务层，以及将业务层的处理结果交付给网络层进而返回给客户端。每一个SocketServer只有一个RequestChannel对象，在SocketServer中构造。RequestChannel构造方法中初始化了requestQueue，用来存放网络层接收到的请求，这些请求即将交付给业务层进行处理。同时，初始化了responseQueues，为每一个Processor建立了一个response队列，用来存放这个Processor的一个或者多个Response，这些response即将交付给网络层返回给客户端。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RequestChannel,有totalProcessorThreads个responseQueue队列，</span></span><br><span class="line">    <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></span><br><span class="line">    <span class="comment">//request存放了所有Processor接收到的远程请求，负责把requestQueue中的请求交付给具体业务逻辑进行处理</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</span><br><span class="line">    <span class="comment">//responseQueues存放了所有Processor的带出来的response，即每一个Processor都有一个response queue</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numProcessors) <span class="comment">//初始化responseQueues</span></span><br><span class="line">      responseQueues(i) = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]()</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//一些metrics用来监控request和response的数量，代码略</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>KafkaApis是Kafka的API接口层，可以理解为一个工具类，职责就是解析请求然后获取请求类型，根据请求类型将请求交付给对应的业务层</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            val requestChannel: <span class="type">RequestChannel</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            val apis: <span class="type">KafkaApis</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            numThreads: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">      </span><br><span class="line">        <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></span><br><span class="line">        <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</span><br><span class="line">        <span class="comment">//初始化由KafkaRequestHandler线程构成的线程数组</span></span><br><span class="line">        <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</span><br><span class="line">        <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">          runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis)</span><br><span class="line">          threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</span><br><span class="line">          threads(i).start()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>KafkaRequestHandler.run()方法，就是不断从requestQueue中取出请求，调用API层业务处理逻辑进行处理</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">   <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></span><br><span class="line">       <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="comment">//略</span></span><br><span class="line">       req = requestChannel.receiveRequest(<span class="number">300</span>)<span class="comment">//从RequestChannel.requestQueue中取出请求</span></span><br><span class="line">       <span class="comment">//略</span></span><br><span class="line">       apis.handle(req)<span class="comment">//调用KafkaApi.handle()，将请求交付给业务</span></span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;&#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="参数调优设置"><a href="#参数调优设置" class="headerlink" title="参数调优设置"></a>参数调优设置</h2><ul><li>numProcessorThreads：通过num.network.threads进行配置，单个Acceptor所管理的Processor对象的数量。</li><li>maxQueuedRequests：通过queued.max.requests进行配置，请求队列所允许的最大的未响应请求的数量，用来给ConnectionQuotas进行请求限额控制，避免Kafka Server产生过大的网络负载；</li><li>totalProcessorThreads：计算方式为numProcessorThreads * endpoints.size，即单台机器总的Processor的数量；</li><li>maxConnectionsPerIp：配置项为max.connections.per.ip，单个IP上的最大连接数，用来给ConnectionQuotas控制连接数；</li><li>num.io.threads:表示KafkaRequestHander实际从队列中获取请求进行执行的线程数，默认是8个。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>通过Acceptor、Processor、RequestChannel、KafkaRequestHandler以及KafkaApis多个角色的解析，完成了整个Kafka的消息流通闭环，即从客户端建立连接、发送请求给Kafka Server的Acceptor进行处理，进一步交由Processor、Kafka Server将请求交付给KafkaRequestHandler具体业务进行处理、业务将处理结果返回给网络层、网络层将结果通过NIO返回给客户端。</li><li>由于多Processor线程、以及KafkaRequestHandlerPoll线程池的存在，通过交付-获取的方式而不是阻塞等待的方式，让整个消息处理实现完全的异步化，各个角色各司其职，模块之间无耦合，线程之间或者相互竞争任务，或者被上层安排处理部分任务，整个效率非常高，结构也相当清晰</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述了kafka集群Broker端基于Reactor模式请求处理流程。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka rebalance机制与Consumer多种消费模式案例</title>
    <link href="https://gjtmaster.github.io/2018/09/19/kafka%E9%9B%86%E7%BE%A4Producer%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
    <id>https://gjtmaster.github.io/2018/09/19/kafka集群Producer基本数据结构及工作流程/</id>
    <published>2018-09-19T09:23:45.000Z</published>
    <updated>2019-10-28T06:33:37.737Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Producer端基本数据结构"><a href="#Producer端基本数据结构" class="headerlink" title="Producer端基本数据结构"></a>Producer端基本数据结构</h2><ul><li><p>ProducerRecord: 一个ProducerRecord表示一条待发送的消息记录，主要由5个字段构成：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">topic          所属topic</span><br><span class="line"><span class="built_in">partition</span>      所属分区</span><br><span class="line"><span class="built_in">key</span>            键值</span><br><span class="line">value          消息体</span><br><span class="line">timestamp      时间戳</span><br></pre></td></tr></table></figure></li><li><p>RecordMetadata: Kafka服务器端返回给客户端的消息的元数据信息,前3项相对比较重要，Producer端可以使用这些消息做一些消息发送成功之后的处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">offset                   该条消息的位移</span><br><span class="line">timestamp                消息时间戳</span><br><span class="line">topic + partition        所属topic的分区</span><br><span class="line"><span class="keyword">checksum</span>                 消息<span class="keyword">CRC32</span>码</span><br><span class="line">serializedKeySize        序列化后的消息键字节数</span><br><span class="line">serializedValueSize      序列化后的消息体字节数</span><br></pre></td></tr></table></figure></li></ul><h2 id="Producer端消息发送流程"><a href="#Producer端消息发送流程" class="headerlink" title="Producer端消息发送流程"></a>Producer端消息发送流程</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676fa43d76b5554?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>在send()的发送消息动作触发之前，通过props属性中指定的servers连接到broker集群，从Zookeeper收集集群Metedata信息，从而了解哪些broker掌管哪一个Topic的哪一个partition，以及brokers的健康状态。</p></li><li><p>下面就是流水线操作，ProducerRecord对象携带着topic，partition，message等信息，在Serializer这个“车间”被序列化。</p></li><li><p>序列化过后的ProducerRecord对象进入Partitioner“车间”，按照上文所述的Partitioning 策略决定这个消息将被分配到哪个Partition中。</p></li><li><p>确定partition的ProducerRecord进入一个缓冲区，通过减少IO来提升性能，在这个“车间”，消息被按照TopicPartition信息进行归类整理，相同Topic且相同parition的ProducerRecord被放在同一个RecordBatch中，等待被发送。什么时候发送？都在Producer的props中被指定了，有默认值，显然我们可以自己指定。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>) batch<span class="selector-class">.size</span>:设置每个RecordBatch可以缓存的最大字节数 </span><br><span class="line">(<span class="number">2</span>) buffer<span class="selector-class">.memory</span>:设置所有RecordBatch的总共最大字节数 </span><br><span class="line">(<span class="number">3</span>) linger.ms设置每个RecordBatch的最长延迟发送时间 </span><br><span class="line">(<span class="number">4</span>) max<span class="selector-class">.block</span><span class="selector-class">.ms</span> 设置每个RecordBatch的最长阻塞时间</span><br></pre></td></tr></table></figure></li><li><p>一旦，当单个RecordBatch的linger.ms延迟到达或者batch.size达到上限，这个 RecordBatch会被立即发送。另外，如果所有RecordBatch作为一个整体，达到了buffer.memroy或者max.block.ms上限，所有的RecordBatch都会被发送。</p></li><li><p>ProducerRecord消息按照分配好的Partition发送到具体的broker中,broker接收保存消息，更新Metadata信息，同步给Zookeeper。</p></li><li><p>Producer端其他优化点：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">5</span>) acks：Producer的数据确认阻塞设置，<span class="number">0</span>表示不管任何响应，只管发，发完了立即执行下个任务，这种方式最快，但是很不保险。<span class="number">1</span>表示只确保leader成功响应，接收到数据。<span class="number">2</span>表示确保leader及其所有follwer成功接收保存消息，也可以用”all”。</span><br><span class="line">(<span class="number">6</span>) retries：消息发送失败重试的次数。</span><br><span class="line">(<span class="number">7</span>) retry<span class="selector-class">.backoff</span><span class="selector-class">.ms</span>：失败补偿时间，每次失败重试的时间间隔，不可设置太短，避免第一条消息的响应还没返回，第二条消息又发出去了，造成逻辑错误。</span><br><span class="line">(<span class="number">8</span>) max<span class="selector-class">.in</span><span class="selector-class">.flight</span><span class="selector-class">.request</span><span class="selector-class">.per</span><span class="selector-class">.connection</span>：同一时间，每个Producer能够发送的消息上限。</span><br><span class="line">(<span class="number">9</span>) compression<span class="selector-class">.type</span>  producer所使用的压缩器，目前支持gzip, snappy和lz4。压缩是在用户主线程完成的，通常都需要花费大量的CPU时间，但对于减少网络IO来说确实利器。生产环境中可以结合压力测试进行适当配置</span><br></pre></td></tr></table></figure></li></ul><h2 id="消息缓冲区-accumulator-再剖析"><a href="#消息缓冲区-accumulator-再剖析" class="headerlink" title="消息缓冲区(accumulator)再剖析"></a>消息缓冲区(accumulator)再剖析</h2><ul><li><p>producer创建时会创建一个默认32MB(由buffer.memory参数指定)的accumulator缓冲区，专门保存待发送的消息。</p></li><li><p>该数据结构中还包含了一个特别重要的集合信息：消息批次信息(batches)。该集合本质上是一个HashMap，里面分别保存了每个topic分区下的batch队列，即前面说的批次是按照topic分区进行分组的。这样发往不同分区的消息保存在对应分区下的batch队列中。</p></li><li><p>假设消息M1, M2被发送到test的0分区但属于不同的batch，M3分送到test的1分区，那么batches中包含的信息就是：{“test-0” -&gt; [batch1, batch2], “test-1” -&gt; [batch3]}</p></li><li><p>每个batch中最重要的3个组件包括：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">compressor: 负责执行追加写入操作</span><br><span class="line"><span class="keyword">batch</span>缓冲区：由<span class="keyword">batch</span>.size参数控制，消息被真正追加写入到的地方</span><br><span class="line">thunks：保存消息回调逻辑的集合</span><br></pre></td></tr></table></figure><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fac83237a750?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p></li><li><p>Sender线程自KafkaProducer创建后就一直都在运行着 。它的工作流程基本上是这样的：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)不断轮询缓冲区寻找已做好发送准备的分区 </span><br><span class="line">(<span class="number">2</span>)将轮询获得的各个batch按照目标分区所在的leader broker进行分组</span><br><span class="line">(<span class="number">3</span>)将分组后的batch通过底层创建的Socket连接发送给各个broker</span><br><span class="line">(<span class="number">4</span>)等待服务器端发送response回来</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fb148a5190e4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>Sender线程会发送PRODUCE请求给对应的broker，broker处理完毕之后发送对应的PRODUCE response。一旦Sender线程接收到response将依次(按照消息发送顺序)调用batch中的回调方法</li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fb1b3a94adb8?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Sender线程自KafkaProducer创建后就一直都在运行着，单个RecordBatch的linger.ms延迟到达或者batch.size达到上限，作为后台线程就会检测到立即发送。</li><li>accumulator缓冲器按照Topic partion进行分组，来进行集中向某一个Broker发送。</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述了kafka rebalance机制与Consumer多种消费模式案例。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka rebalance机制与Consumer多种消费模式案例</title>
    <link href="https://gjtmaster.github.io/2018/09/17/kafka%20rebalance%20%E6%9C%BA%E5%88%B6%E4%B8%8EConsumer%E5%A4%9A%E7%A7%8D%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F%E6%A1%88%E4%BE%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/"/>
    <id>https://gjtmaster.github.io/2018/09/17/kafka rebalance 机制与Consumer多种消费模式案例应用实战/</id>
    <published>2018-09-17T10:24:45.000Z</published>
    <updated>2019-10-25T06:50:52.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="rebalance-何时触发？到底干嘛？流程如何？"><a href="#rebalance-何时触发？到底干嘛？流程如何？" class="headerlink" title="rebalance 何时触发？到底干嘛？流程如何？"></a>rebalance 何时触发？到底干嘛？流程如何？</h2><h3 id="reblance-何时触发"><a href="#reblance-何时触发" class="headerlink" title="reblance 何时触发"></a>reblance 何时触发</h3><ul><li>组订阅发生变更，比如基于正则表达式订阅，当匹配到新的topic创建时，组的订阅就会发生变更。</li><li>组的topic分区数发生变更，通过命令行脚本增加了订阅topic的分区数。</li><li>组成员发生变更：新加入组以及离开组。</li></ul><h3 id="reblance-到底干嘛"><a href="#reblance-到底干嘛" class="headerlink" title="reblance 到底干嘛"></a>reblance 到底干嘛</h3><p>一句话：多个Consumer订阅了一个Topic时，根据分区策略进行消费者订阅分区的重分配</p><h3 id="Coordinator-到底在那个Broker"><a href="#Coordinator-到底在那个Broker" class="headerlink" title="Coordinator 到底在那个Broker"></a>Coordinator 到底在那个Broker</h3><p>找到Coordinator的算法 与 找到_consumer_offsets目标分区的算法是一致的。</p><ul><li>第一步：确定目标分区：Math.abs(groupId.hashCode)%50，假设是12。</li><li>第二步：找到_consumer_offsets分区为10的Leader副本所在的Broker，那么该broker即为Group Coordinator。</li></ul><h3 id="reblance-流程如何"><a href="#reblance-流程如何" class="headerlink" title="reblance 流程如何"></a>reblance 流程如何</h3><p>reblance 流程流程整体如下图所示，值得强调的几点如下：</p><ul><li><p>Coordinator的角色由Broker端担任。</p></li><li><p>Group Leader 的角色主要有Consumer担任。</p></li><li><p>加入组请求（JoinGroup）=&gt;作用在于选择Group Leader。</p></li><li><p>同步组请求（SyncGroup）=&gt;作用在于确定分区分配方案给Coordinator，把方案响应给所有Consumer。</p></li></ul><h3 id="reblance-机制的好处"><a href="#reblance-机制的好处" class="headerlink" title="reblance 机制的好处"></a>reblance 机制的好处</h3><ul><li>分区分配权利下放给客户端consumer，因此系统不用重启，既可以实现分区策略的变更。</li><li>用户可以自行实现机架感知分配方案。</li></ul><h3 id="reblance-generation-过滤无用请求"><a href="#reblance-generation-过滤无用请求" class="headerlink" title="reblance generation 过滤无用请求"></a>reblance generation 过滤无用请求</h3><ul><li>kafka引入 reblance generation ，就是为了防止Consumer group的无效Offset提交。若因为某些原因，consumer延迟提交了Offset，而该consumer被踢出了消费组，那么该Consumer再次提交位移时，携带的就是旧的generation了。</li></ul><h2 id="reblance-监听器应用级别实战"><a href="#reblance-监听器应用级别实战" class="headerlink" title="reblance 监听器应用级别实战"></a>reblance 监听器应用级别实战</h2><ul><li><p>reblance 监听器解决用户   把位移提交到外部存储的情况，在监听器中实现位移保存和位移的重定向。</p></li><li><p>onPartitionsRevoked :  rebalance开启新一轮的重平衡前会调用，一般用于手动提交位移，及审计功能</p></li><li><p>onPartitionsAssigned ：rebalance在重平衡结束后会调用，一般用于消费逻辑处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"> props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line"> props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line"> props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"> props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"> props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"> KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"> </span><br><span class="line"> 统计rebalance总时长</span><br><span class="line"> <span class="keyword">final</span> AtomicLong totalRebalanceTimeMs =<span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"> </span><br><span class="line"> 统计rebalance开始时刻</span><br><span class="line"> <span class="keyword">final</span> AtomicLong rebalanceStart =<span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> <span class="number">1</span> 重平衡监听</span><br><span class="line"> consumer.subscribe(Arrays.asList(<span class="string">"test-topic"</span>), <span class="keyword">new</span> ConsumerRebalanceListener()&#123;</span><br><span class="line">     </span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(TopicPartition tp : partitions)&#123;</span><br><span class="line">        </span><br><span class="line">            <span class="number">1</span> 保存到外部存储</span><br><span class="line">            saveToExternalStore(consumer.position(tp)) </span><br><span class="line">            </span><br><span class="line">            <span class="number">2</span> 手动提交位移</span><br><span class="line">            <span class="comment">//consumer.commitSync(toCommit);</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        rebalanceStart.set(System.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    totalRebalanceTimeMs.addAndGet(System.currentTimeMillis()-rebalanceStart.get())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : partitions) &#123;</span><br><span class="line">           </span><br><span class="line">           consumer.seek(tp，readFromExternalStore(tp))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> &#125;);</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> 消息处理</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"> ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line"> <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">     buffer.add(record);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">     insertIntoDb(buffer);</span><br><span class="line">     consumer.commitSync();</span><br><span class="line">     buffer.clear();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Consumer组内消息均衡实战"><a href="#Consumer组内消息均衡实战" class="headerlink" title="Consumer组内消息均衡实战"></a>Consumer组内消息均衡实战</h2><h3 id="Consumer-单线程封装，实现多个消费者来消费（浪费资源）"><a href="#Consumer-单线程封装，实现多个消费者来消费（浪费资源）" class="headerlink" title="Consumer 单线程封装，实现多个消费者来消费（浪费资源）"></a>Consumer 单线程封装，实现多个消费者来消费（浪费资源）</h3><p>实例主题：</p><ul><li>ConsumerGroup 实现组封装</li><li>ConsumerRunnable 每个线程维护私有的KafkaConsumer实例</li></ul><hr><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span>  <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> void main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line">        <span class="keyword">String</span> brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line">        <span class="keyword">String</span> groupId = <span class="string">"testGroup1"</span>;</span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"test-topic"</span>;</span><br><span class="line">        int consumerNum = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">        核心对外封装</span><br><span class="line">        ConsumerGroup consumerGroup = <span class="keyword">new</span> <span class="type">ConsumerGroup</span>(consumerNum, groupId, topic, brokerList);</span><br><span class="line">        consumerGroup.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerGroup</span> </span>&#123;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;ConsumerRunnable&gt; consumers;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> ConsumerGroup(int consumerNum, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic, <span class="keyword">String</span> brokerList) &#123;</span><br><span class="line">        consumers = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(consumerNum);</span><br><span class="line">        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; consumerNum; ++i) &#123;</span><br><span class="line">            ConsumerRunnable consumerThread = <span class="keyword">new</span> <span class="type">ConsumerRunnable</span>(brokerList, groupId, topic);</span><br><span class="line">            consumers.add(consumerThread);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> void execute() &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRunnable task : <span class="type">consumers</span>) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>(task).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class ConsumerRunnable implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> final KafkaConsumer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; consumer;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> ConsumerRunnable(<span class="keyword">String</span> brokerList, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic) &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);        <span class="comment">//本例使用自动提交位移</span></span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="keyword">this</span>.consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));   <span class="comment">// 本例使用分区副本自动分配策略</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">run</span>() &#123;</span><br><span class="line">        <span class="built_in">while</span> (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; records = consumer.poll(<span class="number">200</span>);   </span><br><span class="line">            <span class="built_in">for</span> (ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record : records) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(Thread.currentThread().getName() + <span class="string">" consumed "</span> + record.partition() +</span><br><span class="line">                        <span class="string">"th message with offset: "</span> + record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="一个Consumer，内部实现多线程消费（consumer压力过大）"><a href="#一个Consumer，内部实现多线程消费（consumer压力过大）" class="headerlink" title="一个Consumer，内部实现多线程消费（consumer压力过大）"></a>一个Consumer，内部实现多线程消费（consumer压力过大）</h3><p>实例主题：</p><ul><li>ConsumerHandler 单一的Consumer实例，poll后里面会跑一个线程池，执行多个Processor线程来处理</li><li>Processor 业务逻辑处理方法</li></ul><p>进一步优化建议；</p><ul><li>ConsumerHandler 设置手动提交位移，负责最终位移提交consumer.commitSync();。</li><li>ConsumerHandler设置一个全局的Map&lt;TopicPartion,OffsetAndMetadata&gt; offsets，来管理Processor消费的位移。</li><li>Processor 负责批处理完消息后，得到消息的最大位移，并更新offsets数组</li><li>ConsumerHandler 根据 offsets，位移提交后会清空offsets集合。</li><li>ConsumerHandler设置重平衡监听</li></ul><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> class Main &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">String</span> brokerList = <span class="string">"localhost:9092,localhost:9093,localhost:9094"</span>;</span><br><span class="line">        <span class="keyword">String</span> groupId = <span class="string">"group2"</span>;</span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"test-topic"</span>;</span><br><span class="line">        <span class="keyword">int</span> workerNum = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">        ConsumerHandler consumers = <span class="keyword">new</span> ConsumerHandler(brokerList, groupId, topic);</span><br><span class="line">        consumers.execute(workerNum);</span><br><span class="line">        <span class="built_in">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000000</span>);</span><br><span class="line">        &#125; <span class="built_in">catch</span> (InterruptedException ignored) &#123;&#125;</span><br><span class="line">        consumers.<span class="built_in">shutdown</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ArrayBlockingQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ThreadPoolExecutor;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class ConsumerHandler &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> final KafkaConsumer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; consumer;</span><br><span class="line">    <span class="keyword">private</span> ExecutorService executors;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> ConsumerHandler(<span class="keyword">String</span> brokerList, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic) &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> execute(<span class="keyword">int</span> workerNum) &#123;</span><br><span class="line">    </span><br><span class="line">        executors = <span class="keyword">new</span> ThreadPoolExecutor(workerNum, workerNum, <span class="number">0</span>L, TimeUnit.MILLISECONDS,</span><br><span class="line">                <span class="keyword">new</span> ArrayBlockingQueue&lt;&gt;(<span class="number">1000</span>), <span class="keyword">new</span> ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class="line">                </span><br><span class="line">        <span class="built_in">while</span> (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; records = consumer.poll(<span class="number">200</span>);</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">for</span> (final ConsumerRecord record : records) &#123;</span><br><span class="line">                executors.submit(<span class="keyword">new</span> Processor(record));</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">shutdown</span>() &#123;</span><br><span class="line">        <span class="built_in">if</span> (consumer != null) &#123;</span><br><span class="line">            consumer.<span class="built_in">close</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">if</span> (executors != null) &#123;</span><br><span class="line">            executors.<span class="built_in">shutdown</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">try</span> &#123;</span><br><span class="line">            <span class="built_in">if</span> (!executors.awaitTermination(<span class="number">10</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(<span class="string">"Timeout.... Ignore for this case"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="built_in">catch</span> (InterruptedException ignored) &#123;</span><br><span class="line">            System.out.<span class="built_in">println</span>(<span class="string">"Other thread interrupted this shutdown, ignore for this case."</span>);</span><br><span class="line">            Thread.currentThread().interrupt();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Processor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ConsumerRecord&lt;String, String&gt; consumerRecord;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Processor</span><span class="params">(ConsumerRecord record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.consumerRecord = record;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">" consumed "</span> + consumerRecord.partition()</span><br><span class="line">            + <span class="string">"th message with offset: "</span> + consumerRecord.offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案对比"><a href="#方案对比" class="headerlink" title="方案对比"></a>方案对比</h3><ul><li>第一种方案：建议采用 Consumer 单线程封装，实现多个消费者来消费（浪费资源），这样能很好地保证分区内消费的顺序，同时也没有线程切换的开销。</li><li>第二种方案：实现复杂，问题在于可能无法维护分区内的消息顺序，注意消息处理和消息接收解耦了。</li></ul><h3 id="Consumer指定分区消费案例实战（Standalone-Consumer）"><a href="#Consumer指定分区消费案例实战（Standalone-Consumer）" class="headerlink" title="Consumer指定分区消费案例实战（Standalone  Consumer）"></a>Consumer指定分区消费案例实战（Standalone  Consumer）</h3><ul><li><p>Standalone Consumer  assign 用于接收指定分区列表的消息和Subscribe是矛盾的。只能二选一。</p></li><li><p>多个 Consumer 实例消费一个 Topic 借助于 group reblance可谓是天作之合。</p></li><li><p>若要精准控制，assign逃不了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">poperties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">List&lt;TopicPartion&gt; partitions = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">List&lt;PartitionInfo&gt;  allPartitions = consumer.partitionsFor(<span class="string">"kaiXinTopic"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(allPartitions != <span class="keyword">null</span> &amp;&amp; !allPartitions.isEmpty)&#123;</span><br><span class="line">    <span class="keyword">for</span>(PartitionInfo partitionInfo : allPartitions)&#123;</span><br><span class="line">    </span><br><span class="line">        partitions.add(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(),partitionInfo.partition()))</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    consumer.assign(partitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">   ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">   <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">       buffer.add(record);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">       insertIntoDb(buffer);</span><br><span class="line">       </span><br><span class="line">       consumer.commitSync();</span><br><span class="line">       </span><br><span class="line">       buffer.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      本文主要给出了kafka rebalance 机制与Consumer多种消费模式案例。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka Poll轮询机制与消费者组的重平衡分区策略剖析</title>
    <link href="https://gjtmaster.github.io/2018/09/16/kafka%20Poll%E8%BD%AE%E8%AF%A2%E6%9C%BA%E5%88%B6%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E9%87%8D%E5%B9%B3%E8%A1%A1%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E5%89%96%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2018/09/16/kafka Poll轮询机制与消费者组的重平衡分区策略剖析/</id>
    <published>2018-09-16T09:33:25.000Z</published>
    <updated>2019-10-25T05:57:41.682Z</updated>
    
    <content type="html"><![CDATA[<p>注意本文采用最新版本进行Kafka的内核原理剖析，新版本每一个Consumer通过独立的线程，来管理多个Socket连接，即同时与多个broker通信实现消息的并行读取。这就是新版的技术革新。类似于Linux I/O模型或者Select NIO 模型。</p><h2 id="Poll为什么要设置一个超时参数"><a href="#Poll为什么要设置一个超时参数" class="headerlink" title="Poll为什么要设置一个超时参数"></a>Poll为什么要设置一个超时参数</h2><ul><li>条件：</li><li>1：获取足够多的可用数据</li><li>2：等待时间超过指定的超时时间。</li><li>目的在于让Consumer主线程定期的””苏醒”去做其他事情。比如：定期的执行常规任务，（比如写日志，写库等）。</li><li>获取消息，然后执行业务逻辑。</li></ul><h2 id="位移精度"><a href="#位移精度" class="headerlink" title="位移精度"></a>位移精度</h2><ul><li>最少一次 -&gt;  消息会被重复处理</li><li>最多一次 -&gt;  消息会丢失，但不会被重复处理。</li><li>精确一次 -&gt; 一定会被处理，且也只会处理一次。</li></ul><h2 id="位移角色"><a href="#位移角色" class="headerlink" title="位移角色"></a>位移角色</h2><ul><li>上次提交位移 ：last committed offset</li><li>当前位置 ：current position</li><li>水位 ： High watermark</li><li>日志终端位移： （Log End Offset）</li></ul><h2 id="位移管理"><a href="#位移管理" class="headerlink" title="位移管理"></a>位移管理</h2><p>consumer的位移提交最终会向group coordinator来提交，不过这里重点需要重新说明一下：组协调者coordinator负责管理所有的Consumer实例。而且coordinator运行在broker上（通过选举出某个broker），不过请注意新版本coordinator只负责做组管理。</p><h4 id="但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。"><a href="#但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。" class="headerlink" title="但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。"></a>但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。</h4><p>权利下放的优势：</p><ul><li>如果需要分配就貌似需要重启整个kafka集群。</li><li>在Consumer端可以定制分区分配策略。</li><li>每一个consumer位移提交时，都会向_consumer_offsets对应的分区上追加写入一条消息。如果某一个consumer为同一个group的同一个topic同一个分区提交多次位移，很显然我们只关心最新一次提交的位移。</li></ul><h2 id="reblance的触发条件"><a href="#reblance的触发条件" class="headerlink" title="reblance的触发条件"></a>reblance的触发条件</h2><ul><li>组订阅发生变更，比如基于正则表达式订阅，当匹配到新的topic创建时，组的订阅就会发生变更。</li><li>组的topic分区数发生变更，通过命令行脚本增加了订阅topic的分区数。</li><li>组成员发生变更：新加入组以及离开组。</li></ul><h2 id="reblance-分配策略"><a href="#reblance-分配策略" class="headerlink" title="reblance 分配策略"></a>reblance 分配策略</h2><h3 id="range分区分配策略"><a href="#range分区分配策略" class="headerlink" title="range分区分配策略"></a>range分区分配策略</h3><p>举例如下：一个拥有十个分区（0,1,2…..,9）的topic，相同group拥有三个consumerid为a,b,c的消费者：</p><ul><li><p>consumer a分配对应的分区号为[0,4),即0，1，2，3前面四个分区</p></li><li><p>consumer b 分配对应分区4，5，6中间三个分区</p></li><li><p>consumer c 分配对应分区7，8，9最后三个分区。</p><p>class RangeAssignor() extends PartitionAssignor with Logging {</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign</span></span>(ctx: <span class="type">AssignmentContext</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> valueFactory = (topic: <span class="type">String</span>) =&gt; <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicAndPartition</span>, <span class="type">ConsumerThreadId</span>]</span><br><span class="line">  <span class="keyword">val</span> partitionAssignment =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">ConsumerThreadId</span>]](<span class="type">Some</span>(valueFactory))</span><br><span class="line">  <span class="keyword">for</span> (topic &lt;- ctx.myTopicThreadIds.keySet) &#123;</span><br><span class="line">    <span class="keyword">val</span> curConsumers = ctx.consumersForTopic(topic)</span><br><span class="line">    <span class="keyword">val</span> curPartitions: <span class="type">Seq</span>[<span class="type">Int</span>] = ctx.partitionsForTopic(topic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> nPartsPerConsumer = curPartitions.size / curConsumers.size</span><br><span class="line">    <span class="keyword">val</span> nConsumersWithExtraPart = curPartitions.size % curConsumers.size</span><br><span class="line"></span><br><span class="line">    info(<span class="string">"Consumer "</span> + ctx.consumerId + <span class="string">" rebalancing the following partitions: "</span> + curPartitions +</span><br><span class="line">      <span class="string">" for topic "</span> + topic + <span class="string">" with consumers: "</span> + curConsumers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (consumerThreadId &lt;- curConsumers) &#123;</span><br><span class="line">      <span class="keyword">val</span> myConsumerPosition = curConsumers.indexOf(consumerThreadId)</span><br><span class="line">      assert(myConsumerPosition &gt;= <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> startPart = nPartsPerConsumer * myConsumerPosition + myConsumerPosition.min(nConsumersWithExtraPart)</span><br><span class="line">      <span class="keyword">val</span> nParts = nPartsPerConsumer + (<span class="keyword">if</span> (myConsumerPosition + <span class="number">1</span> &gt; nConsumersWithExtraPart) <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       *   Range-partition the sorted partitions to consumers for better locality.</span></span><br><span class="line"><span class="comment">       *  The first few consumers pick up an extra partition, if any.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">if</span> (nParts &lt;= <span class="number">0</span>)</span><br><span class="line">        warn(<span class="string">"No broker partitions consumed by consumer thread "</span> + consumerThreadId + <span class="string">" for topic "</span> + topic)</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- startPart until startPart + nParts) &#123;</span><br><span class="line">          <span class="keyword">val</span> partition = curPartitions(i)</span><br><span class="line">          info(consumerThreadId + <span class="string">" attempting to claim partition "</span> + partition)</span><br><span class="line">          <span class="comment">// record the partition ownership decision</span></span><br><span class="line">          <span class="keyword">val</span> assignmentForConsumer = partitionAssignment.getAndMaybePut(consumerThreadId.consumer)</span><br><span class="line">          assignmentForConsumer += (<span class="type">TopicAndPartition</span>(topic, partition) -&gt; consumerThreadId)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>源码剖析如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">curConsumers=(a,b,c)</span><br><span class="line">curPartitions=(0,1,2,3,4,5,6,7,8,9)</span><br><span class="line">nPartsPerConsumer=10/3  =3</span><br><span class="line">nConsumersWithExtraPart=10%3  =1</span><br><span class="line"></span><br><span class="line"><span class="section">a:</span></span><br><span class="line">myConsumerPosition= curConsumers.indexof(a) =0</span><br><span class="line">startPart= 3*0+0.min(1) = 0</span><br><span class="line">nParts = 3+(if (0 + 1 &gt; 1) 0 <span class="keyword">else</span> 1)=3+1=4</span><br><span class="line"><span class="section">b:</span></span><br><span class="line">myConsumerPosition=1</span><br><span class="line"><span class="section">c:</span></span><br><span class="line">myConsumerPosition</span><br></pre></td></tr></table></figure></li></ul><h3 id="round-robin分区分配策略"><a href="#round-robin分区分配策略" class="headerlink" title="round-robin分区分配策略"></a>round-robin分区分配策略</h3><p>如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例如下：假设消费组中有2个消费者C0和C1，都订阅了主题topic0 和 topic1，并且每个主题都有3个分区，进行hashCode 排序 后，顺序为：topic0_0、topic0_1、topic0_2、topic1_0、topic1_1、topic1_2。最终的分配结果为：</p><p>消费者consumer0：topic0_0、topic0_2 、 topic1_1</p><p>消费者consumer1：topic0_1、topic1_0、 topic1_2</p><p>使用RoundRobin策略有两个前提条件必须满足：</p><ul><li>同一个Consumer Group里面的所有消费者的num.streams必须相等；</li><li>每个消费者订阅的主题必须相同。</li></ul><p>所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，最后按照round-robin风格将分区分别分配给不同的消费者线程。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> allTopicPartitions = ctx.partitionsForTopic.flatMap &#123; <span class="keyword">case</span>(topic, partitions) =&gt;</span><br><span class="line">  info(<span class="string">"Consumer %s rebalancing the following partitions for topic %s: %s"</span></span><br><span class="line">       .format(ctx.consumerId, topic, partitions))</span><br><span class="line">  partitions.map(partition =&gt; &#123;</span><br><span class="line">    <span class="type">TopicAndPartition</span>(topic, partition)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;.toSeq.sortWith((topicPartition1, topicPartition2) =&gt; &#123;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending</span></span><br><span class="line"><span class="comment">   * up on one consumer (if it has a high enough stream count).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  topicPartition1.toString.hashCode &lt; topicPartition2.toString.hashCode</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="StickyAssignor分区分配策略（摘录）"><a href="#StickyAssignor分区分配策略（摘录）" class="headerlink" title="StickyAssignor分区分配策略（摘录）"></a>StickyAssignor分区分配策略（摘录）</h3><ul><li>分区的分配要尽可能的均匀；</li><li>分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。</li></ul><p>假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p1、t3p0</span><br><span class="line">消费者C1：t0p1、t2p0、t3p1</span><br><span class="line">消费者C2：t1p0、t2p1</span><br></pre></td></tr></table></figure><p>假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p0、t2p0、t3p0</span><br><span class="line">消费者C2：t0p1、t1p1、t2p1、t3p1</span><br></pre></td></tr></table></figure><p>RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p1、t3p0、t2p0</span><br><span class="line">消费者C2：t1p0、t2p1、t0p1、t3p1</span><br></pre></td></tr></table></figure><p>可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配仍然保持了均衡。</p><p>如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。</p><h2 id="reblance-generation-（代代不同）"><a href="#reblance-generation-（代代不同）" class="headerlink" title="reblance generation （代代不同）"></a>reblance generation （代代不同）</h2><p>主要作用在于防止无效的offset提交，原因在于若上一届的consumer成员因为某些原因延迟提交了offset,同时被踢出group组，那么新一届的group组成员分区分配结束后，老一届的consumer再次提交老的offset就会出问题。因此采用reblance generation ，老的请求就会被拒绝。</p><h2 id="reblance-扫尾工作"><a href="#reblance-扫尾工作" class="headerlink" title="reblance 扫尾工作"></a>reblance 扫尾工作</h2><p>每一次reblance操作之前，都会检查用户是否设置了自动提交位移，如果是，则帮助用户提交。如没有设置，会在监听器中回调用户的提交程序。</p>]]></content>
    
    <summary type="html">
    
      本文主要剖析了kafka Poll轮询机制与消费者组的重平衡分区策略。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka之Producer同步与异步消息发送及事务幂等性案例</title>
    <link href="https://gjtmaster.github.io/2018/09/15/kafka%E4%B9%8BProducer%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E5%8F%8A%E4%BA%8B%E5%8A%A1%E5%B9%82%E7%AD%89%E6%80%A7%E6%A1%88%E4%BE%8B/"/>
    <id>https://gjtmaster.github.io/2018/09/15/kafka之Producer同步与异步消息发送及事务幂等性案例/</id>
    <published>2018-09-15T09:25:34.000Z</published>
    <updated>2019-10-25T05:32:50.664Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h2><p>kafka的Producer是线程安全的，用户可以非常非常放心的在多线程中使用。</p><p>但是官方建议：通常情况下，一个线程维护一个kafka 的producer的效率会更高。</p><h2 id="Producer-消息发送流程"><a href="#Producer-消息发送流程" class="headerlink" title="Producer 消息发送流程"></a>Producer 消息发送流程</h2><ul><li>第一步：封装ProducerRecord</li><li>第二步：分区器Partioner进行数据路由，选择某一个Topic分区。如果没有指定key，消息会被均匀的分配到所有分区。</li><li>第三步：确定好分区，就会找分区对应的leader，接下来就是副本同步机制。</li></ul><h2 id="Producer官方实例"><a href="#Producer官方实例" class="headerlink" title="Producer官方实例"></a>Producer官方实例</h2><h3 id="Fire-and-Fogret案例-（无所谓心态）"><a href="#Fire-and-Fogret案例-（无所谓心态）" class="headerlink" title="Fire and Fogret案例 （无所谓心态）"></a>Fire and Fogret案例 （无所谓心态）</h3><ul><li><h5 id="发送之后便不再理会发送结果"><a href="#发送之后便不再理会发送结果" class="headerlink" title="发送之后便不再理会发送结果"></a>发送之后便不再理会发送结果</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"> props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line"> props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"> props.put(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line"> props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line"> props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line"> props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line"> props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"> props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line"> Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">     producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"my-topic"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"></span><br><span class="line"> producer.close();</span><br></pre></td></tr></table></figure></li></ul><h3 id="异步回调官方案例-（不阻塞）"><a href="#异步回调官方案例-（不阻塞）" class="headerlink" title="异步回调官方案例 （不阻塞）"></a>异步回调官方案例 （不阻塞）</h3><ul><li><h5 id="JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。"><a href="#JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。" class="headerlink" title="JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。"></a>JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。</h5></li><li><p>Fully non-blocking usage can make use of the Callback parameter to provide a callback that will be invoked when the request is complete.</p></li><li><p>RecordMetadata 和 Exception 不可能同时为空，消息发送成功时，Exception为null，消息发送失败时，metadata为空。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt;(<span class="string">"the-topic"</span>, key, value);</span><br><span class="line"></span><br><span class="line">producer.send(myRecord,</span><br><span class="line">              <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception e)</span> </span>&#123;</span><br><span class="line">                      <span class="keyword">if</span>(e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                         e.printStackTrace();</span><br><span class="line">                      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                         System.out.println(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset());</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;);</span><br></pre></td></tr></table></figure></li></ul><h3 id="同步发送官方案例-（阻塞）"><a href="#同步发送官方案例-（阻塞）" class="headerlink" title="同步发送官方案例 （阻塞）"></a>同步发送官方案例 （阻塞）</h3><ul><li><p>通过 producer.send（record)返回Future对象，通过调用Future.get()进行无限等待结果返回。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">producer.send（<span class="built_in">record</span>).<span class="keyword">get</span>()</span><br></pre></td></tr></table></figure></li></ul><h3 id="基于事务发送官方案例-（原子性和幂等性）"><a href="#基于事务发送官方案例-（原子性和幂等性）" class="headerlink" title="基于事务发送官方案例 （原子性和幂等性）"></a>基于事务发送官方案例 （原子性和幂等性）</h3><ul><li><p>From Kafka 0.11, the KafkaProducer supports two additional modes: the idempotent producerand the transactional producer. The idempotent producer strengthens Kafka’s deliverysemantics from at least once to exactly once delivery. In particular producer retries willno longer introduce duplicates. The transactional producer allows an application to sendmessages to multiple partitions (and topics!) atomically.</p></li><li><p>To enable idempotence, the enable.idempotence configuration must be set to true. If set,the retries config will default to Integer.MAX_VALUE and the acks config will default to all.There are no API changes for the idempotent producer, so existing applications will not need to be modified to take advantage of this feature.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"my-transactional-id"</span>);</span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props, <span class="keyword">new</span> StringSerializer(), <span class="keyword">new</span> StringSerializer());</span><br><span class="line"></span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">        producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my-topic"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line">    <span class="comment">// We can't recover from these exceptions, so our only option is to close the producer and exit.</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">    <span class="comment">// For all other exceptions, just abort the transaction and try again.</span></span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure></li><li><p>As is hinted at in the example, there can be only one open transaction per producer. All messages sent between the beginTransaction() and commitTransaction() calls will be part of a single transaction. When the <a href="https://link.juejin.im?target=http%3A%2F%2Ftransactional.id" target="_blank" rel="noopener">transactional.id</a> is specified, all messages sent by the producer must be part of a transaction.</p></li></ul><h3 id="可重试异常（继承RetriableException）"><a href="#可重试异常（继承RetriableException）" class="headerlink" title="可重试异常（继承RetriableException）"></a>可重试异常（继承RetriableException）</h3><ul><li>LeaderNotAvailableException :分区的Leader副本不可用，这可能是换届选举导致的瞬时的异常，重试几次就可以恢复</li><li>NotControllerException:Controller主要是用来选择分区副本和每一个分区leader的副本信息，主要负责统一管理分区信息等，也可能是选举所致。</li><li>NetWorkerException :瞬时网络故障异常所致。</li></ul><h3 id="不可重试异常"><a href="#不可重试异常" class="headerlink" title="不可重试异常"></a>不可重试异常</h3><ul><li>SerializationException:序列化失败异常</li><li>RecordToolLargeException:消息尺寸过大导致。</li></ul><h3 id="异常的区别对待"><a href="#异常的区别对待" class="headerlink" title="异常的区别对待"></a>异常的区别对待</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">producer.send(myRecord,</span><br><span class="line">              <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception e)</span> </span>&#123;</span><br><span class="line">                      <span class="keyword">if</span>(e ==<span class="keyword">null</span>)&#123;</span><br><span class="line">                          <span class="comment">//正常处理逻辑</span></span><br><span class="line">                          System.out.println(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset()); </span><br><span class="line">                          </span><br><span class="line">                      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                              </span><br><span class="line">                            <span class="keyword">if</span>(e <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                               <span class="comment">//处理可重试异常</span></span><br><span class="line">                               ......</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                               <span class="comment">//处理不可重试异常</span></span><br><span class="line">                               ......</span><br><span class="line">                            &#125;</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;);</span><br></pre></td></tr></table></figure><h3 id="Producer的绅士关闭"><a href="#Producer的绅士关闭" class="headerlink" title="Producer的绅士关闭"></a>Producer的绅士关闭</h3><ul><li>producer.close()：优先把消息处理完毕，优雅退出。</li><li>producer.close(timeout): 超时时，强制关闭。</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka之Producer同步与异步消息发送及事务幂等性案例。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>利用ogg实现oracle到kafka的增量数据实时同步</title>
    <link href="https://gjtmaster.github.io/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"/>
    <id>https://gjtmaster.github.io/2018/09/13/利用ogg实现oracle到kafka的增量数据实时同步/</id>
    <published>2018-09-13T06:32:58.000Z</published>
    <updated>2019-08-18T05:45:34.748Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。</p><p>ogg即Oracle GoldenGate是Oracle的同步工具，本文讲如何配置ogg以实现Oracle数据库增量数据实时同步到kafka中，其中同步消息格式为json。</p><p>下面是我的源端和目标端的一些配置信息：</p><table><thead><tr><th align="center">-</th><th align="center">版本</th><th align="center">OGG版本</th><th align="center">ip</th><th align="center">主机名</th></tr></thead><tbody><tr><td align="center">源端</td><td align="center">OracleRelease 11.2.0.1.0</td><td align="center">Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64</td><td align="center">192.168.23.167</td><td align="center">cdh01</td></tr><tr><td align="center">目标端</td><td align="center">kafka_2.11-0.11.0.1</td><td align="center">Oracle GoldenGate for Big Data 12.3.0.1.0 on Linux x86-64</td><td align="center">192.168.23.168</td><td align="center">cdh02</td></tr></tbody></table><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>注意：源端和目标端的文件不一样，目标端需要下载Oracle GoldenGate for Big Data,源端需要下载Oracle GoldenGate for Oracle具体下载方法见最后的附录截图。</p><p>目标端在<a href="http://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html" target="_blank" rel="noopener">这里</a>查询下载，源端在<a href="https://edelivery.oracle.com/osdc/faces/SoftwareDelivery" target="_blank" rel="noopener">旧版本</a>查询下载。</p><h2 id="源端（Oracle）配置"><a href="#源端（Oracle）配置" class="headerlink" title="源端（Oracle）配置"></a>源端（Oracle）配置</h2><p>注意：源端是创建了oracle用户且安装了oracle数据库，oracle环境变量之前都配置好了</p><p>（后面只要涉及到源端均在oracle用户下操作）</p><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>先建立ogg目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /oracledata/data/ogg</span><br><span class="line">unzip Oracle GoldenGate_11.2.1.0.3.zip</span><br></pre></td></tr></table></figure><p>解压后得到一个tar包，再解压这个tar</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /oracledata/data/ogg</span><br></pre></td></tr></table></figure><h3 id="配置ogg环境变量"><a href="#配置ogg环境变量" class="headerlink" title="配置ogg环境变量"></a>配置ogg环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export OGG_HOME=/oracledata/data/ogg</span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib</span><br><span class="line">export PATH=$OGG_HOME:$PATH</span><br></pre></td></tr></table></figure><p>使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><p>如果命令成功即可进行下一步，不成功请检查前面的步骤。</p><h3 id="oracle打开归档模式"><a href="#oracle打开归档模式" class="headerlink" title="oracle打开归档模式"></a>oracle打开归档模式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以DBA身份连接数据库</span></span><br><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>执行下面的命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><p>若显示如下，则说明当前未开启归档模式</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Database <span class="built_in">log</span> <span class="built_in">mode</span>       No Archive <span class="built_in">Mode</span></span><br><span class="line">Automatic archival       Disabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online <span class="built_in">log</span> sequence     <span class="number">12</span></span><br><span class="line">Current <span class="built_in">log</span> sequence       <span class="number">14</span></span><br></pre></td></tr></table></figure><p>手动打开即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 立即关闭数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动实例并加载数据库，但不打开</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup mount</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改数据库为归档模式</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database archivelog;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用自动归档</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system archive <span class="built_in">log</span> start;</span></span><br></pre></td></tr></table></figure><p>再执行一下命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Database log mode       Archive Mode</span><br><span class="line">Automatic archival       Enabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online log sequence     12</span><br><span class="line">Next log sequence to archive   14</span><br><span class="line">Current log sequence       14</span><br></pre></td></tr></table></figure><p>可以看到为Enabled，则成功打开归档模式。</p><h3 id="Oracle打开日志相关"><a href="#Oracle打开日志相关" class="headerlink" title="Oracle打开日志相关"></a>Oracle打开日志相关</h3><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">NO</span>     <span class="literal">NO</span></span><br></pre></td></tr></table></figure><p>若为NO，则需要通过命令修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database force logging;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database add supplemental <span class="built_in">log</span> data;</span></span><br></pre></td></tr></table></figure><p>再查看一下为YES即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">YES</span>    <span class="literal">YES</span></span><br></pre></td></tr></table></figure><p>上述操作只是开启了最小补充日志，如果要抽取全部字段需要开启全列补充日志,否则值为null的字段不会在抽取日志中显示！！！</p><p>补充日志开启命令参考：<a href="https://blog.csdn.net/aaron8219/article/details/16825963" target="_blank" rel="noopener">https://blog.csdn.net/aaron8219/article/details/16825963</a></p><p><strong>注：开启全列补充日志会导致磁盘快速增长，LGWR进程繁忙，不建议使用。大家可根据自己的情况使用。</strong></p><p>查看数据库是否开启了全列补充日志</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;  </span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">NO</span></span><br></pre></td></tr></table></figure><p>若未开启可以通过以下命令开启。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; alter database add supplemental log data(all) columns;</span><br><span class="line"></span><br><span class="line">Database altered.</span><br><span class="line"></span><br><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">YES</span></span><br></pre></td></tr></table></figure><h3 id="oracle创建复制用户"><a href="#oracle创建复制用户" class="headerlink" title="oracle创建复制用户"></a>oracle创建复制用户</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="regexp">/oracledata/</span>data<span class="regexp">/tablespace/</span>dbsrv2</span><br></pre></td></tr></table></figure><p>然后执行下面sql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create tablespace oggtbs datafile <span class="string">'/oracledata/data/tablespace/dbsrv2/oggtbs01.dbf'</span> size 1000M autoextend on;</span></span><br><span class="line">控制台显示的内容：Tablespace created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">  create user ogg identified by 123456 default tablespace oggtbs;</span></span><br><span class="line">控制台显示的内容：User created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to ogg;</span></span><br><span class="line">控制台显示的内容：Grant succeeded.</span><br></pre></td></tr></table></figure><h3 id="OGG初始化"><a href="#OGG初始化" class="headerlink" title="OGG初始化"></a>OGG初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">GGSCI (cdh01) 1&gt; create subdirs</span><br></pre></td></tr></table></figure><p>控制台显示的内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Creating subdirectories under current directory /oracledata/data/ogg</span><br><span class="line"></span><br><span class="line">Parameter files                /oracledata/data/ogg/dirprm: created</span><br><span class="line">Report files                   /oracledata/data/ogg/dirrpt: created</span><br><span class="line">Checkpoint files               /oracledata/data/ogg/dirchk: created</span><br><span class="line">Process status files           /oracledata/data/ogg/dirpcs: created</span><br><span class="line">SQL script files               /oracledata/data/ogg/dirsql: created</span><br><span class="line">Database definitions files     /oracledata/data/ogg/dirdef: created</span><br><span class="line">Extract data files             /oracledata/data/ogg/dirdat: created</span><br><span class="line">Temporary files                /oracledata/data/ogg/dirtmp: created</span><br><span class="line">Stdout files                   /oracledata/data/ogg/dirout: created</span><br></pre></td></tr></table></figure><h3 id="Oracle创建测试表"><a href="#Oracle创建测试表" class="headerlink" title="Oracle创建测试表"></a>Oracle创建测试表</h3><p>创建一个用户,在该用户下新建测试表，用户名、密码、表名均为 test_ogg。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create user test_ogg identified by test_ogg default tablespace users;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> conn test_ogg/test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create table test_ogg(id int,name varchar(20),sex varchar(4),primary key(id));</span></span><br></pre></td></tr></table></figure><h2 id="目标端（kafka）配置"><a href="#目标端（kafka）配置" class="headerlink" title="目标端（kafka）配置"></a>目标端（kafka）配置</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -<span class="selector-tag">p</span> /data/apps/ogg</span><br><span class="line">unzip OGG_BigData_12.<span class="number">3.0</span>.<span class="number">1.0</span>_Release.zip</span><br><span class="line">tar xf ggs_Adapters_Linux_x64<span class="selector-class">.tar</span>  -C /data/apps/ogg</span><br></pre></td></tr></table></figure><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> /etc/<span class="keyword">profile</span></span><br></pre></td></tr></table></figure><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/opt/java</span><span class="regexp">/jdk1.8.0_211</span></span><br><span class="line"><span class="regexp">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line"></span><br><span class="line">export OGG_HOME=<span class="regexp">/data/apps</span><span class="regexp">/ogg</span></span><br><span class="line"><span class="regexp">export LD_LIBRARY_PATH=$JAVA_HOME/jre</span><span class="regexp">/lib/amd</span>64:$JAVA_HOME/jre/<span class="class"><span class="keyword">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">libjsig</span>.<span class="title">so</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>/<span class="title">libjvm</span>.<span class="title">so</span>:$<span class="title">OGG_HOME</span>/<span class="title">lib</span></span></span><br><span class="line">export PATH=$<span class="symbol">OGG_HOME:</span>$PATH</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure><p>同样测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><h3 id="初始化目录"><a href="#初始化目录" class="headerlink" title="初始化目录"></a>初始化目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create subdirs</span><br></pre></td></tr></table></figure><h2 id="OGG源端配置"><a href="#OGG源端配置" class="headerlink" title="OGG源端配置"></a>OGG源端配置</h2><p>Oracle实时传输到Hadoop集群（HDFS，<a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">Hive</a>，Kafka等）的基本原理如图：<br><img src="https://mc.qcloudimg.com/static/img/dd548277beb41f51d0e5914dccda9134/image.png" alt="img"><br>根据如上原理，配置大概分为如下步骤：源端目标端配置ogg管理器（mgr）；源端配置extract进程进行Oracle日志抓取；源端配置pump进程传输抓取内容到目标端；目标端配置replicate进程复制日志到Kafka集群。</p><h3 id="配置OGG的全局变量"><a href="#配置OGG的全局变量" class="headerlink" title="配置OGG的全局变量"></a>配置OGG的全局变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 1&gt; dblogin userid ogg password 123456</span><br><span class="line">控制台显示的内容：Successfully logged into database.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 2&gt; edit param ./globals</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">oggschema ogg</span></span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr"><a href="#配置管理器mgr" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">3</span>&gt; edit param mgr</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><p>说明：PORT即mgr的默认监听端口；</p><p>DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；</p><p>AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；</p><p>PURGEOLDEXTRACTS即TRAIL文件的定期清理</p><h3 id="添加复制表"><a href="#添加复制表" class="headerlink" title="添加复制表"></a>添加复制表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 4&gt; add trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo data enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 5&gt; info trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo log data is enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line">控制台显示的内容：Columns supplementally logged for table TEST_OGG.TEST_OGG: ID</span><br></pre></td></tr></table></figure><h3 id="配置extract进程"><a href="#配置extract进程" class="headerlink" title="配置extract进程"></a>配置extract进程</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">6</span>&gt; edit param extkafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">extract</span> extkafka</span><br><span class="line"><span class="attribute">dynamicresolution</span></span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (ORACLE_SID = <span class="string">"dbsrv2"</span>)</span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (NLS_LANG = <span class="string">"american_america.AL32UTF8"</span>)</span><br><span class="line"><span class="attribute">GETUPDATEBEFORES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSDELETES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSUPDATES</span></span><br><span class="line"><span class="attribute">userid</span> ogg,password 123456</span><br><span class="line"><span class="attribute">exttrail</span> /oracledata/data/ogg/dirdat/to</span><br><span class="line"><span class="attribute">table</span> test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>dynamicresolution动态解析；</p><p>SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；</p><p>userid ogg,password 123456即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；</p><p>table即复制表的表名，支持*通配，必须以;结尾</p><p>添加extract进程：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) <span class="number">7</span>&gt; <span class="keyword">add </span><span class="keyword">extract </span><span class="keyword">extkafka,tranlog,begin </span>now</span><br><span class="line">控制台显示的内容：<span class="keyword">EXTRACT </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure><p>(注：若报错</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ERROR: </span>Could not create checkpoint file /opt/ogg/dirchk/EXTKAFKA.cpe (error 2, No such file or directory).</span><br></pre></td></tr></table></figure><p>执行下面的命令再重新添加即可。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="keyword">subdirs</span></span><br></pre></td></tr></table></figure><p>)</p><p>添加trail文件的定义与extract进程绑定：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 8&gt; <span class="builtin-name">add</span> exttrail /oracledata/data/ogg/dirdat/<span class="keyword">to</span>,extract extkafka</span><br><span class="line">控制台显示的内容：EXTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置pump进程"><a href="#配置pump进程" class="headerlink" title="配置pump进程"></a>配置pump进程</h3><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">9</span>&gt; edit param pukafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">extract pukafka</span><br><span class="line">passthru</span><br><span class="line">dynamicresolution</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">rmthost <span class="number">192.168</span><span class="number">.23</span><span class="number">.168</span> mgrport <span class="number">7809</span></span><br><span class="line">rmttrail /data/apps/ogg/dirdat/to</span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；</p><p>dynamicresolution动态解析；</p><p>userid ogg,password ogg即OGG连接Oracle数据库的帐号密码</p><p>rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口；</p><p>rmttrail即目标端trail文件存储位置以及名称。<strong>(注意，这里很容易犯错！！！注意是目标端的路径！！！)</strong></p><p>分别将本地trail文件和目标端的trail文件绑定到extract进程：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 10&gt; <span class="builtin-name">add</span> extract pukafka,exttrailsource /oracledata/data/ogg/dirdat/<span class="keyword">to</span></span><br><span class="line">控制台显示的内容：EXTRACT added.</span><br><span class="line">GGSCI (cdh01) 11&gt; <span class="builtin-name">add</span> rmttrail /data/apps/ogg/dirdat/<span class="keyword">to</span>,extract pukafka</span><br><span class="line">控制台显示的内容：RMTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置defgen文件"><a href="#配置defgen文件" class="headerlink" title="配置defgen文件"></a>配置defgen文件</h3><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">12</span>&gt; edit param test_ogg</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defsfile <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>退出GGSCI</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">13</span>&gt; quit</span><br></pre></td></tr></table></figure><p>进行OGG主目录下执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">./defgen paramfile dirprm/test_ogg.prm</span><br></pre></td></tr></table></figure><p>输出以下内容则执行成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">        Oracle GoldenGate Table Definition Generator for Oracle</span><br><span class="line"> Version 11.2.1.0.3 14400833 OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258</span><br><span class="line">   Linux, x64, 64bit (optimized), Oracle 11g on Aug 23 2012 16:58:29</span><br><span class="line"> </span><br><span class="line">Copyright (C) 1995, 2012, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    Starting at 2018-05-23 05:03:04</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">Operating System Version:</span><br><span class="line">Linux</span><br><span class="line">Version #1 SMP Wed Apr 12 15:04:24 UTC 2017, Release 3.10.0-514.16.1.el7.x86_64</span><br><span class="line">Node: ambari.master.com</span><br><span class="line">Machine: x86_64</span><br><span class="line">                         soft limit   hard limit</span><br><span class="line">Address Space Size   :    unlimited    unlimited</span><br><span class="line">Heap Size            :    unlimited    unlimited</span><br><span class="line">File Size            :    unlimited    unlimited</span><br><span class="line">CPU Time             :    unlimited    unlimited</span><br><span class="line"></span><br><span class="line">Process id: 13126</span><br><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">**            Running with the following parameters                  **</span><br><span class="line">***********************************************************************</span><br><span class="line">defsfile /opt/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password ***</span><br><span class="line">table test_ogg.test_ogg;</span><br><span class="line">Retrieving definition for TEST_OGG.TEST_OGG</span><br><span class="line"></span><br><span class="line">Definitions generated for 1 table in /oracledata/data/ogg/dirdef/test_ogg.test_ogg</span><br></pre></td></tr></table></figure><p>将生成的/oracledata/data/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /oracledata/data/ogg/dirdef/test_ogg.test_ogg root@cdh02:/data/apps/ogg/dirdef/</span><br></pre></td></tr></table></figure><h2 id="OGG目标端配置"><a href="#OGG目标端配置" class="headerlink" title="OGG目标端配置"></a>OGG目标端配置</h2><h3 id="开启kafka服务"><a href="#开启kafka服务" class="headerlink" title="开启kafka服务"></a>开启kafka服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Zookeeper</span></span><br><span class="line">/data/apps/apache-zookeeper-3.5.5-bin/bin/zkServer.sh start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启Kafka</span></span><br><span class="line">/data/apps/kafka_2.11-0.11.0.1/bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr-1"><a href="#配置管理器mgr-1" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt;  edit param mgr</span><br><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><h3 id="配置checkpoint"><a href="#配置checkpoint" class="headerlink" title="配置checkpoint"></a>配置checkpoint</h3><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 2&gt; edit param ./GLOBALS</span><br><span class="line">CHECKPOINTTABLE test_ogg.checkpoint</span><br></pre></td></tr></table></figure><h3 id="配置replicate进程"><a href="#配置replicate进程" class="headerlink" title="配置replicate进程"></a>配置replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 3&gt; edit param rekafka</span><br><span class="line">REPLICAT rekafka</span><br><span class="line">sourcedefs /data/apps/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props</span><br><span class="line">REPORTCOUNT EVERY 1 MINUTES, RATE </span><br><span class="line">GROUPTRANSOPS 10000</span><br><span class="line">MAP test_ogg.test_ogg, TARGET test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>REPLICATE rekafka定义rep进程名称；</p><p>sourcedefs即在4.6中在源服务器上做的表映射文件；</p><p>TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props；</p><p>REPORTCOUNT即复制任务的报告生成频率；</p><p>GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；</p><p>MAP即源端与目标端的映射关系</p><h3 id="配置kafka-props"><a href="#配置kafka-props" class="headerlink" title="配置kafka.props"></a>配置kafka.props</h3><p><strong>本环节配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirprm/</span><br><span class="line">vim kafka.props</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> handler类型</span></span><br><span class="line">gg.handlerlist=kafkahandler</span><br><span class="line">gg.handler.kafkahandler.type=kafka</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kafka生产者配置文件</span></span><br><span class="line">gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的topic名称，无需手动创建</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gg.handler.kafkahandler.topicMappingTemplate=test_ogg（新版topicName属性的设置方式）</span></span><br><span class="line">gg.handler.kafkahandler.topicName=test_ogg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 传输文件的格式，支持json，xml等</span></span><br><span class="line">gg.handler.kafkahandler.format=json</span><br><span class="line">gg.handler.kafkahandler.format.insertOpKey = I  </span><br><span class="line">gg.handler.kafkahandler.format.updateOpKey = U  </span><br><span class="line">gg.handler.kafkahandler.format.deleteOpKey = D</span><br><span class="line">gg.handler.kafkahandler.format.truncateOpKey=T</span><br><span class="line">gg.handler.kafkahandler.format.includePrimaryKeys=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> OGG <span class="keyword">for</span> Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次</span></span><br><span class="line">gg.handler.kafkahandler.mode=op</span><br><span class="line"><span class="meta">#</span><span class="bash"> 类路径</span></span><br><span class="line">gg.classpath=dirprm/:/data/apps/kafka_2.11-0.11.0.1/libs/*:/data/apps/ogg/:/data/apps/ogg/lib/*</span><br></pre></td></tr></table></figure><p>紧接着创建Kafka生产者配置文件：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim custom_kafk<span class="built_in">a_producer</span>.properties</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kafkabroker的地址</span></span><br><span class="line">bootstrap.servers=cdh01:9092,cdh02:9092,cdh03:9092</span><br><span class="line">acks=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 压缩类型</span></span><br><span class="line">compression.type=gzip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重连延时</span></span><br><span class="line">reconnect.backoff.ms=1000</span><br><span class="line">value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">batch.size=102400</span><br><span class="line">linger.ms=10000</span><br></pre></td></tr></table></figure><p><strong>配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><h3 id="添加trail文件到replicate进程"><a href="#添加trail文件到replicate进程" class="headerlink" title="添加trail文件到replicate进程"></a>添加trail文件到replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt; add replicat rekafka exttrail /data/apps/ogg/dirdat/to,checkpointtable test_ogg.checkpoint</span><br><span class="line">控制台显示的内容：REPLICAT added.</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="启动所有进程"><a href="#启动所有进程" class="headerlink" title="启动所有进程"></a>启动所有进程</h3><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。<br>启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。<br>全部需要在ogg目录下执行ggsci目录进入ogg命令行。<br>源端依次是</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> extkafka</span><br><span class="line"><span class="literal">start</span> pukafka</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> rekafka</span><br></pre></td></tr></table></figure><p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功<br>源端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.master.com) 5&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">EXTRACT     RUNNING     EXTKAFKA    04:50:21      00:00:03    </span><br><span class="line">EXTRACT     RUNNING     PUKAFKA     00:00:00      00:00:03</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.slave1.com) 3&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">REPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</span><br></pre></td></tr></table></figure><h3 id="异常解决"><a href="#异常解决" class="headerlink" title="异常解决"></a>异常解决</h3><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ggser.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>或者ogg命令行,以rekafka进程为例</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh02</span>) <span class="number">2</span>&gt; view report rekafka</span><br></pre></td></tr></table></figure><h3 id="测试同步更新效果"><a href="#测试同步更新效果" class="headerlink" title="测试同步更新效果"></a>测试同步更新效果</h3><p>现在源端执行sql语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conn test_ogg/test_ogg</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_ogg <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'test'</span>,<span class="literal">null</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">update</span> test_ogg <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'zhangsan'</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">delete</span> test_ogg <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p>查看源端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-rw-rw- <span class="number">1</span> oracle oinstall <span class="number">1464</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看目标端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-r----- <span class="number">1</span> root root <span class="number">1504</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看kafka是否自动建立对应的主题</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.<span class="keyword">sh</span> --<span class="keyword">list</span> --zookeeper localhos<span class="variable">t:2181</span></span><br></pre></td></tr></table></figure><p>在列表中显示有test_ogg则表示没问题<br>通过消费者看是否有同步消息</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.44</span><span class="number">.129</span>:<span class="number">9092</span> --topic test_ogg --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"I"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:04:39.001362"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:04:44.610000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001246"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"U"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:05:44.000411"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:05:50.764000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001541"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;,<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"D"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:06:33.000312"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:06:39.845000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001670"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>before代表操作之前的数据，after代表操作后的数据，现在已经可以从kafka获取到同步的json数据了，后面可以用SparkStreaming和Storm等解析然后存到hadoop等大数据平台里</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想通配整个库的话，只需要把上面的配置所有表名改为*，如test_ogg<span class="selector-class">.test_ogg</span> 改为 test_ogg.*,但是kafka的topic不能通配，所以需要把所有表的数据放在一个topic，后面再用程序解析表名即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">若后期因业务需要导致表结构发生改变，需要重新生成源端表结构的defgen定义文件，再把定义文件通过scp放到目标端。defgen文件的作用是，记录了源端的表结构，然后我们再把这个文件放到目标端，在目标端应用SQL时就能根据defgen文件与目标端表结构，来做一定的转换。</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/purpleraintear/p/6071038.html" target="_blank" rel="noopener">基于OGG的Oracle与Hadoop集群准实时同步介绍</a></p><p><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">Fusion Middleware Integrating Oracle GoldenGate for Big Data</a></p>]]></content>
    
    <summary type="html">
    
      Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。
    
    </summary>
    
      <category term="数据的导入导出" scheme="https://gjtmaster.github.io/categories/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/"/>
    
    
      <category term="Oracle" scheme="https://gjtmaster.github.io/tags/Oracle/"/>
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="ogg" scheme="https://gjtmaster.github.io/tags/ogg/"/>
    
  </entry>
  
</feed>
