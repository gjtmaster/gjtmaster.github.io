<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Joker&#39;s Blog</title>
  
  <subtitle>高金涛</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://gjtmaster.github.io/"/>
  <updated>2019-09-25T08:53:04.587Z</updated>
  <id>https://gjtmaster.github.io/</id>
  
  <author>
    <name>高金涛</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 进阶：Time 深度解析</title>
    <link href="https://gjtmaster.github.io/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2019/08/19/Flink 进阶：Time 深度解析/</id>
    <published>2019-08-19T11:18:56.000Z</published>
    <updated>2019-09-25T08:53:04.587Z</updated>
    
    <content type="html"><![CDATA[<p>原  作  者 | 崔星灿</p><p>原整理者 | 沙晟阳（成阳）</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/001-1024x449.png" alt="img"></p><h3 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h3><p>在不同的应用场景中时间语义是各不相同的，Flink 作为一个先进的分布式流处理引擎，它本身支持不同的时间语义。其核心是 Processing Time 和 Event Time（Row Time），这两类时间主要的不同点如下表所示：</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/002-1024x415.png" alt="img"></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time 则是根据每一条处理记录所携带的时间戳来判定。</p><p>这两种时间在 Flink 内部的处理以及还是用户的实际使用方面，难易程度都是不同的。相对而言的 Processing Time 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用 Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，使用 Event Time 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同。如果希望结果完全相同，就只能用 Event Time；如果接受结果不同，则可以用 Processing Time。Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用 Processing Time。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/003-1024x499.png" alt="img"></p><h4 id="时间的特性"><a href="#时间的特性" class="headerlink" title="时间的特性"></a>时间的特性</h4><p><strong>时间的一个重要特性是：时间只能递增，不会来回穿越。</strong> 在使用时间的时候我们要充分利用这个特性。假设我们有这么一些记录，然后我们来分别看一下 Processing Time 还有 Event Time 对于时间的处理。</p><ul><li>对于 Processing Time，因为我们是使用的是本地节点的时间（假设这个节点的时钟同步没有问题），我们每一次取到的 Processing Time 肯定都是递增的，递增就代表着有序，所以说我们相当于拿到的是一个有序的数据流。</li><li>而在用 Event Time 的时候因为时间是绑定在每一条的记录上的，由于网络延迟、程序内部逻辑、或者其他一些分布式系统的原因，数据的时间可能会存在一定程度的乱序，比如上图的例子。在 Event Time 场景下，我们把每一个记录所包含的时间称作 Record Timestamp。如果 Record Timestamp 所得到的时间序列存在乱序，我们就需要去处理这种情况。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/004.png" alt="img"></p><p>如果单条数据之间是乱序，我们就考虑对于整个序列进行更大程度的离散化。简单地讲，就是把数据按照一定的条数组成一些小批次，但这里的小批次并不是攒够多少条就要去处理，而是为了对他们进行时间上的划分。经过这种更高层次的离散化之后，我们会发现最右边方框里的时间就是一定会小于中间方框里的时间，中间框里的时间也一定会小于最左边方框里的时间。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/005.png" alt="img"></p><p>这个时候我们在整个时间序列里插入一些类似于标志位的一些特殊的处理数据，这些特殊的处理数据叫做 watermark。一个 watermark 本质上就代表了这个 watermark 所包含的 timestamp 数值，表示以后到来的数据已经再也没有小于或等于这个时间的了。</p><h3 id="Timestamp-和-Watermark-行为概览"><a href="#Timestamp-和-Watermark-行为概览" class="headerlink" title="Timestamp 和 Watermark 行为概览"></a>Timestamp 和 Watermark 行为概览</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/006-1024x442.png" alt="img"></p><p>接下来我们重点看一下 Event Time 里的 Record Timestamp（简写成 timestamp）和 watermark 的一些基本信息。绝大多数的分布式流计算引擎对于数据都是进行了 DAG 图的抽象，它有自己的数据源，有处理算子，还有一些数据汇。数据在不同的逻辑算子之间进行流动。watermark 和 timestamp 有自己的生命周期，接下来我会从 watermark 和 timestamp 的产生、他们在不同的节点之间的传播、以及在每一个节点上的处理，这三个方面来展开介绍。</p><h4 id="Timestamp-分配和-Watermark-生成"><a href="#Timestamp-分配和-Watermark-生成" class="headerlink" title="Timestamp 分配和 Watermark 生成"></a>Timestamp 分配和 Watermark 生成</h4><p>Flink 支持两种 watermark 生成方式。第一种是在 SourceFunction 中产生，相当于把整个的 timestamp 分配和 watermark 生成的逻辑放在流处理应用的源头。我们可以在 SourceFunction 里面通过这两个方法产生 watermark：</p><ul><li>通过 collectWithTimestamp 方法发送一条数据，其中第一个参数就是我们要发送的数据，第二个参数就是这个数据所对应的时间戳；也可以调用 emitWatermark 方法去产生一条 watermark，表示接下来不会再有时间戳小于等于这个数值记录。</li><li>另外，有时候我们不想在 SourceFunction 里生成 timestamp 或者 watermark，或者说使用的 SourceFunction 本身不支持，我们还可以在使用 DataStream API 的时候指定，调用的 DataStream.assignTimestampsAndWatermarks 这个方法，能够接收不同的 timestamp 和 watermark 的生成器。</li></ul><p>总体上而言生成器可以分为两类：第一类是定期生成器；第二类是根据一些在流处理数据流中遇到的一些特殊记录生成的。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/007.png" alt="img"></p><p>两者的区别主要有三个方面，首先定期生成是现实时间驱动的，这里的“定期生成”主要是指 watermark（因为 timestamp 是每一条数据都需要有的），即定期会调用生成逻辑去产生一个 watermark。而根据特殊记录生成是数据驱动的，即是否生成 watermark 不是由现实时间来决定，而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了，这个时候相当于每一次分配 Timestamp 之后都会调用用户实现的 watermark 生成方法，用户需要在生成方法中去实现 watermark 的生成逻辑。</p><p>大家要注意的是就是我们在分配 timestamp 和生成 watermark 的过程，虽然在 SourceFunction 和 DataStream 中都可以指定，但是还是建议生成的工作越靠近 DataSource 越好。这样会方便让程序逻辑里面更多的 operator 去判断某些数据是否乱序。Flink 内部提供了很好的机制去保证这些 timestamp 和 watermark 被正确地传递到下游的节点。</p><h4 id="Watermark-传播"><a href="#Watermark-传播" class="headerlink" title="Watermark 传播"></a>Watermark 传播</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/008-1024x474.png" alt="img"></p><p>具体的传播策略基本上遵循这三点。</p><ul><li>首先，watermark 会以广播的形式在算子之间进行传播。比如说上游的算子，它连接了三个下游的任务，它会把自己当前的收到的 watermark 以广播的形式传到下游。</li><li>第二，如果在程序里面收到了一个 Long.MAX_VALUE 这个数值的 watermark，就表示对应的那一条流的一个部分不会再有数据发过来了，它相当于就是一个终止的一个标志。</li><li>第三，对于单流而言，这个策略比较好理解，而对于有多个输入的算子，watermark 的计算就有讲究了，一个原则是：单输入取其大，多输入取小。</li></ul><p>举个例子，假设这边蓝色的块代表一个算子的一个任务，然后它有三个输入，分别是 W1、W2、W3，这三个输入可以理解成任何的输入，这三个输入可能是属于同一个流，也可能是属于不同的流。然后在计算 watermark 的时候，对于单个输入而言是取他们的最大值，因为我们都知道 watermark 应该遵循一个单调递增的一个原则。对于多输入，它要统计整个算子任务的 watermark 时，就会取这三个计算出来的 watermark 的最小值。即一个多个输入的任务，它的 watermark 受制于最慢的那条输入流。这一点类似于木桶效应，整个木桶中装的水会就是受制于最矮的那块板。</p><p>watermark 在传播的时候有一个特点是，它的传播是幂等的。多次收到相同的 watermark，甚至收到之前的 watermark 都不会对最后的数值产生影响，因为对于单个输入永远是取最大的，而对于整个任务永远是取一个最小的。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在它没有区分你这个输入是一条流多个 partition 还是来自于不同的逻辑上的流的 JOIN。对于同一个流的不同 partition，我们对他做这种强制的时钟同步是没有问题的，因为一开始就是把一条流拆散成不同的部分，但每一个部分之间共享相同的时钟。但是如果算子的任务是在做类似于 JOIN 操作，那么要求你两个输入的时钟强制同步其实没有什么道理的，因为完全有可能是把一条离现在时间很近的数据流和一个离当前时间很远的数据流进行 JOIN，这个时候对于快的那条流，因为它要等慢的那条流，所以说它可能就要在状态中去缓存非常多的数据，这对于整个集群来说是一个很大的性能开销。</p><h4 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h4><p>在正式介绍 watermark 的处理之前，先简单介绍 ProcessFunction，因为 watermark 在任务里的处理逻辑分为内部逻辑和外部逻辑。外部逻辑其实就是通过 ProcessFunction 来体现的，如果你需要使用 Flink 提供的时间相关的 API 的话就只能写在 ProcessFunction 里。</p><p>ProcessFunction 和时间相关的功能主要有三点：</p><ul><li>第一点就是根据你当前系统使用的时间语义不同，你可以去获取当前你正在处理这条记录的 Record Timestamp，或者当前的 Processing Time。</li><li>第二点就是它可以获取当前算子的时间，可以把它理解成当前的 watermark。</li><li>第三点就是为了在 ProcessFunction 中去实现一些相对复杂的功能，允许注册一些 timer（定时器）。比如说在 watermark 达到某一个时间点的时候就触发定时器，所有的这些回调逻辑也都是由用户来提供，涉及到如下三个方法，registerEventTimeTimer、registerProcessingTimeTimer 和 onTimer。在 onTimer 方法中就需要去实现自己的回调逻辑，当条件满足时回调逻辑就会被触发。</li></ul><p>一个简单的应用是，我们在做一些时间相关的处理的时候，可能需要缓存一部分数据，但这些数据不能一直去缓存下去，所以需要有一些过期的机制，我们可以通过 timer 去设定这么一个时间，指定某一些数据可能在将来的某一个时间点过期，从而把它从状态里删除掉。所有的这些和时间相关的逻辑在 Flink 内部都是由自己的 Time Service（时间服务）完成的。</p><h4 id="Watermark-处理"><a href="#Watermark-处理" class="headerlink" title="Watermark 处理"></a>Watermark 处理</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/009-1024x376.png" alt="img"></p><p>一个算子的实例在收到 watermark 的时候，首先要更新当前的算子时间，这样的话在 ProcessFunction 里方法查询这个算子时间的时候，就能获取到最新的时间。第二步它会遍历计时器队列，这个计时器队列就是我们刚刚说到的 timer，你可以同时注册很多 timer，Flink 会把这些 Timer 按照触发时间放到一个优先队列中。第三步 Flink 得到一个时间之后就会遍历计时器的队列，然后逐一触发用户的回调逻辑。 通过这种方式，Flink 的某一个任务就会将当前的 watermark 发送到下游的其他任务实例上，从而完成整个 watermark 的传播，从而形成一个闭环。</p><h3 id="Table-API-中的时间"><a href="#Table-API-中的时间" class="headerlink" title="Table API 中的时间"></a>Table API 中的时间</h3><p>下面我们来看一看 Table/SQL API 中的时间。为了让时间参与到 Table/SQL 这一层的运算中，我们需要提前把时间属性放到表的 schema 中，这样的话我们才能够在 SQL 语句或者 Table 的一些逻辑表达式里面去使用这些时间去完成需求。</p><h4 id="Table-中指定时间列"><a href="#Table-中指定时间列" class="headerlink" title="Table 中指定时间列"></a>Table 中指定时间列</h4><p>其实之前社区就怎么在 Table/SQL 中去使用时间这个问题做过一定的讨论，是把获取当前 Processing Time 的方法是作为一个特殊的 UDF，还是把这一个列物化到整个的 schema 里面，最终采用了后者。我们这里就分开来讲一讲 Processing Time 和 Event Time 在使用的时候怎么在 Table 中指定。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/010-1024x487.png" alt="img"></p><p>对于 Processing Time，我们知道要得到一个 Table 对象（或者注册一个 Table）有两种手段：</p><ol><li>可以从一个 DataStream 转化成一个 Table；</li><li>直接通过 TableSource 去生成这么一个 Table；</li></ol><p>对于第一种方法而言，我们只需要在你已有的这些列中（例子中 f1 和 f2 就是两个已有的列），在最后用“列名.proctime”这种写法就可以把最后的这一列注册为一个 Processing Time，以后在写查询的时候就可以去直接使用这一列。如果 Table 是通过 TableSource 生成的，就可以通过实现这一个 DefinedRowtimeAttributes 接口，然后就会自动根据你提供的逻辑去生成对应的 Processing Time。</p><p>相对而言，在使用 Event Time 时则有一个限制，因为 Event Time 不像 Processing Time 那样是随拿随用。如果你要从 DataStream 去转化得到一个 Table，必须要提前保证原始的 DataStream 里面已经存在了 Record Timestamp 和 watermark。如果你想通过 TableSource 生成的，也一定要保证你要接入的一个数据里面存在一个类型为 long 或者 timestamp 的这么一个时间字段。</p><p>具体来说，如果你要从 DataStream 去注册一个表，和 proctime 类似，你只需要加上“列名.rowtime”就可以。需要注意的是，如果你要用 Processing Time，必须保证你要新加的字段是整个 schema 中的最后一个字段，而 Event Time 的时候你其实可以去替换某一个已有的列，然后 Flink 会自动的把这一列转化成需要的 rowtime 这个类型。 如果是通过 TableSource 生成的，只需要实现 DefinedRowtimeAttributes 接口就可以了。需要说明的一点是，在 DataStream API 这一侧其实不支持同时存在多个 Event Time（rowtime），但是在 Table 这一层理论上可以同时存在多个 rowtime。因为 DefinedRowtimeAttributes 接口的返回值是一个对于 rowtime 描述的 List，即其实可以同时存在多个 rowtime 列，在将来可能会进行一些其他的改进，或者基于去做一些相应的优化。</p><h4 id="时间列和-Table-操作"><a href="#时间列和-Table-操作" class="headerlink" title="时间列和 Table 操作"></a>时间列和 Table 操作</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/011-1024x475.png" alt="img"></p><p>指定完了时间列之后，当我们要真正去查询时就会涉及到一些具体的操作。这里我列举的这些操作都是和时间列紧密相关，或者说必须在这个时间列上才能进行的。比如说“Over 窗口聚合”和“Group by 窗口聚合”这两种窗口聚合，在写 SQL 提供参数的时候只能允许你在这个时间列上进行这种聚合。第三个就是时间窗口聚合，你在写条件的时候只支持对应的时间列。最后就是排序，我们知道在一个无尽的数据流上对数据做排序几乎是不可能的事情，但因为这个数据本身到来的顺序已经是按照时间属性来进行排序，所以说我们如果要对一个 DataStream 转化成 Table 进行排序的话，你只能是按照时间列进行排序，当然同时你也可以指定一些其他的列，但是时间列这个是必须的，并且必须放在第一位。</p><p>为什么说这些操作只能在时间列上进行？因为我们有的时候可以把到来的数据流就看成是一张按照时间排列好的一张表，而我们任何对于表的操作，其实都是必须在对它进行一次顺序扫描的前提下完成的。因为大家都知道数据流的特性之一就是一过性，某一条数据处理过去之后，将来其实不太好去访问它。当然因为 Flink 中内部提供了一些状态机制，我们可以在一定程度上去弱化这个特性，但是最终还是不能超越的限制状态不能太大。所有这些操作为什么只能在时间列上进行，因为这个时间列能够保证我们内部产生的状态不会无限的增长下去，这是一个最终的前提。</p>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Time机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 进阶：增量 Checkpoint 详解</title>
    <link href="https://gjtmaster.github.io/2019/08/16/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9A%E5%A2%9E%E9%87%8F%20Checkpoint%20%E8%AF%A6%E8%A7%A3/"/>
    <id>https://gjtmaster.github.io/2019/08/16/Flink 进阶：增量 Checkpoint 详解/</id>
    <published>2019-08-16T10:52:50.000Z</published>
    <updated>2019-09-25T08:53:04.610Z</updated>
    
    <content type="html"><![CDATA[<p>原作者 | Stefan Ricther &amp; Chris Ward<br>原翻译 | 邱从贤（山智）</p><p>via | <a href="https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html" target="_blank" rel="noopener">https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html</a></p><p>Apache Flink 是一个有状态的流计算框架，状态是作业算子中已经处理过的内存状态，供后续处理时使用。状态在流计算很多复杂场景中非常重要，比如：</p><ul><li>保存所有历史记录，用来寻找某种记录模式</li><li>保存最近一分钟的所有记录，用于对每分钟的记录进行聚合统计</li><li>保存当前的模型参数，用于进行模型训练</li></ul><p>有状态的流计算框架必须有很好的容错性，才能在生产环境中发挥用处。这里的容错性是指，不管是发生硬件故障，还是程序异常，最终的结果不丢也不重。</p><p><strong>Flink 的容错性从一开始就是一个非常强大的特性，在遇到故障时，能够保证不丢不重，且对正常逻辑处理的性能影响很小。</strong></p><p>这里面的核心就是 checkpoint 机制，Flink 使用 checkpoint 机制来进行状态保证，在 Flink 中 checkpoint 是一个定时触发的全局异步快照，并持久化到持久存储系统上（通常是分布式文件系统）。发生故障后，Flink 选择从最近的一个快照进行恢复。有用户的作业状态达到 GB 甚至 TB 级别，对这么大的作业状态做一次 checkpoint 会非常耗时，耗资源，因此我们在 Flink 1.3 中引入了增量 checkpoint 机制。</p><p>在增量 checkpoint 之前，Flink 的每个 checkpoint 都包含作业的所有状态。我们在观察到状态在 checkpoint 之间的变化并没有那么大之后，支持了增量 checkpoint。增量 checkpoint 仅包含上次 checkpoint 和本次 checkpoint 之间状态的差异（也就是“增量”）。</p><p>对于状态非常大的作业，增量 checkpoint 对性能的提升非常明显。<strong>有生产用户反馈对于 TB 级别的作业，使用增量 checkpoint 后能将 checkpoint 的整体时间从 3 分钟降到 30 秒。</strong>这些时间节省主要归功于不需要在每次 checkpoint 都将所有状态写到持久化存储系统。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>当前，仅能够在 RocksDB StateBackend 上使用增量 checkpoint 机制，Flink 依赖 RocksDB 内部的备份机制来生成 checkpoint 文件。Flink 会自动清理掉之前的 checkpoint 文件, 因此增量 checkpoint 的历史记录不会无限增长。</p><p>为了在作业中开启增量 checkpoint，建议详细阅读 Apache Flink 的 checkpoint 文档，简单的说，你可以像之前一样开启 checkpoint，然后将构造函数的第二个参数设置为 true 来启用增量 checkpoint。</p><h3 id="Java-示例"><a href="#Java-示例" class="headerlink" title="Java 示例"></a>Java 示例</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(filebackend, <span class="literal">true</span>));</span><br></pre></td></tr></table></figure><h3 id="Scala-示例"><a href="#Scala-示例" class="headerlink" title="Scala 示例"></a>Scala 示例</h3><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(filebackend, <span class="keyword">true</span>))</span><br></pre></td></tr></table></figure><p>Flink 默认保留一个成功的 checkpoint，如果你需要保留多个的话，可以通过下面的配置进行设置：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">state</span>.checkpoints.num-retained</span><br></pre></td></tr></table></figure><h3 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h3><p>Flink 的增量 checkpoint 以 RocksDB 的 checkpoint 为基础。RocksDB 是一个 LSM 结构的 KV 数据库，把所有的修改保存在内存的可变缓存中（称为 memtable），所有对 memtable 中 key 的修改，会覆盖之前的 value，当前 memtable 满了之后，RocksDB 会将所有数据以有序的写到磁盘。当 RocksDB 将 memtable 写到磁盘后，整个文件就不再可变，称为有序字符串表（sstable）。</p><p>RocksDB 的后台压缩线程会将 sstable 进行合并，就重复的键进行合并，合并后的 sstable 包含所有的键值对，RocksDB 会删除合并前的 sstable。</p><p>在这个基础上，Flink 会记录上次 checkpoint 之后所有新生成和删除的 sstable，另外因为 sstable 是不可变的，Flink 用 sstable 来记录状态的变化。为此，<strong>Flink 调用 RocksDB 的 flush，强制将 memtable 的数据全部写到 sstable，并硬链到一个临时目录中。这个步骤是在同步阶段完成，其他剩下的部分都在异步阶段完成，不会阻塞正常的数据处理。</strong></p><p>Flink 将所有新生成的 sstable 备份到持久化存储（比如 HDFS，S3），并在新的 checkpoint 中引用。Flink 并不备份前一个 checkpoint 中已经存在的 sstable，而是引用他们。Flink 还能够保证所有的 checkpoint 都不会引用已经删除的文件，因为 RocksDB 中文件删除是由压缩完成的，压缩后会将原来的内容合并写成一个新的 sstable。因此，Flink 增量 checkpoint 能够切断 checkpoint 历史。</p><p>为了追踪 checkpoint 间的差距，备份合并后的 sstable 是一个相对冗余的操作。但是 Flink 会增量的处理，增加的开销通常很小，并且可以保持一个更短的 checkpoint 历史，恢复时从更少的 checkpoint 进行读取文件，因此我们认为这是值得的。</p><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/Checkpoint-.jpg" alt="img"></p><p>上图以一个有状态的算子为例，checkpoint 最多保留 2 个，上图从左到右分别记录每次 checkpoint 时本地的 RocksDB 状态文件，引用的持久化存储上的文件，以及当前 checkpoint 完成后文件的引用计数情况。</p><ul><li><strong>Checkpoint 1 的时候</strong>，本地 RocksDB 包含两个 sstable 文件，该 checkpoint 会把这两个文件备份到持久化存储，当 checkpoint 完成后，对这两个文件的引用计数进行加 1，引用计数使用键值对的方式保存，其中键由算子的当前并发以及文件名所组成。我们同时会维护一个引用计数中键到对应文件的隐射关系。</li><li><strong>Checkpoint 2 的时候</strong>，RocksDB 生成两个新的 sstable 文件，并且两个旧的文件还存在。Flink 会把两个新的文件进行备份，然后引用两个旧的文件，当 checkpoint 完成时，Flink 对这 4 个文件都进行引用计数 +1 操作。</li><li><strong>Checkpoint 3 的时候</strong>，RocksDB 将 sstable-(1)，sstable-(2) 以及 sstable-(3) 合并成 sstable-(1,2,3)，并且删除了三个旧文件，新生成的文件包含了三个删除文件的所有键值对。sstable-(4) 还继续存在，生成一个新的 sstable-(5) 文件。Flink 会将 sstable-(1,2,3) 和 sstable-(5) 备份到持久化存储，然后增加 sstable-4 的引用计数。由于保存的 checkpoint 数达到上限（2 个），因此会删除 checkpoint 1，然后对 checkpoint 1 中引用的所有文件（sstable-(1) 和 sstable-(2)）的引用计数进行 -1 操作。</li><li><strong>Checkpoint 4 的时候</strong>，RocksDB 将 sstable-(4)，sstable-(5) 以及新生成的 sstable-(6) 合并成一个新的 sstable-(4,5,6)。Flink 将 sstable-(4,5,6) 备份到持久化存储，并对 sstabe-(1,2,3) 和 sstable-(4,5,6) 进行引用计数 +1 操作，然后删除 checkpoint 2，并对 checkpoint 引用的文件进行引用计数 -1 操作。这个时候 sstable-(1)，sstable-(2) 以及 sstable-(3) 的引用计数变为 0，Flink 会从持久化存储删除这三个文件。</li></ul><h3 id="竞争问题以及并发-checkpoint"><a href="#竞争问题以及并发-checkpoint" class="headerlink" title="竞争问题以及并发 checkpoint"></a>竞争问题以及并发 checkpoint</h3><p>Flink 支持并发 checkpoint，有时晚触发的 checkpoint 会先完成，因此增量 checkpoint 需要选择一个正确的基准。Flink 仅会引用成功的 checkpoint 文件，从而防止引用一些被删除的文件。</p><h3 id="从-checkpoint-恢复以及性能"><a href="#从-checkpoint-恢复以及性能" class="headerlink" title="从 checkpoint 恢复以及性能"></a>从 checkpoint 恢复以及性能</h3><p>开启增量 checkpoint 之后，不需要再进行其他额外的配置。如果 Job 异常，Flink 的 JobMaster 会通知所有 task 从上一个成功的 checkpoint 进行恢复，不管是全量 checkpoint 还是增量 checkpoint。每个 TaskManager 会从持久化存储下载他们需要的状态文件。</p><p>尽管增量 checkpoint 能减少大状态下的 checkpoint 时间，但是天下没有免费的午餐，我们需要在其他方面进行舍弃。增量 checkpoint 可以减少 checkpoint 的总时间，但是也可能导致恢复的时候需要更长的时间<strong>。</strong>如果集群的故障频繁，Flink 的 TaskManager 需要从多个 checkpoint 中下载需要的状态文件（这些文件中包含一些已经被删除的状态），作业恢复的整体时间可能比不使用增量 checkpoint 更长。</p><p>另外在增量 checkpoint 情况下，我们不能删除旧 checkpoint 生成的文件，因为新的 checkpoint 会继续引用它们，这可能导致需要更多的存储空间，并且恢复的时候可能消耗更多的带宽。</p><p>关于控制便捷性与性能之间平衡的策略可以参考此文档：</p><p><strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html</a></strong></p>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Checkpoint机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 进阶：Runtime 核心机制剖析</title>
    <link href="https://gjtmaster.github.io/2019/08/15/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ARuntime%20%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2019/08/15/Flink 进阶：Runtime 核心机制剖析/</id>
    <published>2019-08-15T10:22:32.000Z</published>
    <updated>2019-09-25T08:53:04.571Z</updated>
    
    <content type="html"><![CDATA[<p>原作者：高赟（云骞）</p><h3 id="1-综述"><a href="#1-综述" class="headerlink" title="1. 综述"></a>1. 综述</h3><p>本文主要介绍 Flink Runtime 的作业执行的核心机制。本文将首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍在这个过程，Flink 是怎么进行资源管理、作业调度以及错误恢复的。最后，本文还将简要介绍 Flink Runtime 层当前正在进行的一些工作。</p><h3 id="2-Flink-Runtime-整体架构"><a href="#2-Flink-Runtime-整体架构" class="headerlink" title="2. Flink Runtime 整体架构"></a>2. Flink Runtime 整体架构</h3><p>Flink 的整体架构如图 1 所示。Flink 是可以运行在多种不同的环境中的，例如，它可以通过单进程多线程的方式直接运行，从而提供调试的能力。它也可以运行在 Yarn 或者 K8S 这种资源管理系统上面，也可以在各种云环境中执行。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-1.png" alt="img"></p><p>​                            图1. Flink 的整体架构，其中 Runtime 层对不同的执行环境提供了一套统一的分布式执行引擎。</p><p>针对不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。Flink 在 Runtime 层之上提供了 DataStream 和 DataSet 两套 API，分别用来编写流作业与批作业，以及一组更高级的 API 来简化特定作业的编写。本文主要介绍 Flink Runtime 层的整体架构。</p><p>Flink Runtime 层的主要架构如图 2 所示，它展示了一个 Flink 集群的基本结构。Flink Runtime 层的整个架构主要是在 FLIP-6 中实现的，整体来说，它采用了标准 master-slave 的结构，其中左侧白色圈中的部分即是 master，它负责管理整个集群中的资源和作业；而右侧的两个 TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-2.png" alt="img">                         </p><p>​                                     图2. Flink 集群的基本结构。Flink Runtime 层采用了标准的 master-slave 架构。</p><p>其中，Master 部分又包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程中。</p><p>基于上述结构，当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。</p><p>当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。这时根据 Session 和 Per-Job 模式的区别， TaskExecutor 可能已经启动或者尚未启动。如果是前者，此时 ResourceManager 中已有记录了 TaskExecutor 注册的资源，可以直接选取空闲资源进行分配。否则，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配。目前 Flink 中 TaskExecutor 的资源是通过 Slot 来描述的，一个 Slot 一般可以执行一个具体的 Task，但在一些情况下也可以执行多个相关联的 Task，这部分内容将在下文进行详述。ResourceManager 选择到空闲的 Slot 之后，就会通知相应的 TM “将该 Slot 分配分 JobManager XX ”，然后 TaskExecutor 进行相应的记录后，会向 JobManager 进行注册。JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。</p><p>TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><p>以上就是 Flink Runtime 层执行作业的基本流程。可以看出，Flink 支持两种不同的模式，即 Per-job 模式与 Session 模式。如图 3 所示，Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。与之对应，在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-3.png" alt="img">  </p><p>​                                                                   图3. Flink Runtime 支持两种作业执行的模式。</p><h3 id="3-资源管理与作业调度"><a href="#3-资源管理与作业调度" class="headerlink" title="3. 资源管理与作业调度"></a>3. 资源管理与作业调度</h3><p>本节对 Flink 中资源管理与作业调度的功能进行更深入的说明。实际上，作业调度可以看做是对资源和任务进行匹配的过程。如上节所述，在 Flink 中，资源是通过 Slot 来表示的，每个 Slot 可以用来执行不同的 Task。而在另一端，任务即 Job 中实际的 Task，它包含了待执行的用户逻辑。调度的主要目的就是为了给 Task 找到匹配的 Slot。逻辑上来说，每个 Slot 都应该有一个向量来描述它所能提供的各种资源的量，每个 Task 也需要相应的说明它所需要的各种资源的量。但是实际上在 1.9 之前，Flink 是不支持细粒度的资源描述的，而是统一的认为每个 Slot 提供的资源和 Task 需要的资源都是相同的。从 1.9 开始，Flink 开始增加对细粒度的资源匹配的支持的实现，但这部分功能目前仍在完善中。</p><p>作业调度的基础是首先提供对资源的管理，因此我们首先来看下 Flink 中资源管理的实现。如上文所述，Flink 中的资源是由 TaskExecutor 上的 Slot 来表示的。如图 4 所示，在 ResourceManager 中，有一个子组件叫做 SlotManager，它维护了当前集群中所有 TaskExecutor 上的 Slot 的信息与状态，如该 Slot 在哪个 TaskExecutor 中，该 Slot 当前是否空闲等。当 JobManger 来为特定 Task 申请资源的时候，根据当前是 Per-job 还是 Session 模式，ResourceManager 可能会去申请资源来启动新的 TaskExecutor。当 TaskExecutor 启动之后，它会通过服务发现找到当前活跃的 ResourceManager 并进行注册。在注册信息中，会包含该 TaskExecutor中所有 Slot 的信息。 ResourceManager 收到注册信息后，其中的 SlotManager 就会记录下相应的 Slot 信息。当 JobManager 为某个 Task 来申请资源时， SlotManager 就会从当前空闲的 Slot 中按一定规则选择一个空闲的 Slot 进行分配。当分配完成后，如第 2 节所述，RM 会首先向 TaskManager 发送 RPC 要求将选定的 Slot 分配给特定的 JobManager。TaskManager 如果还没有执行过该 JobManager 的 Task 的话，它需要首先向相应的 JobManager 建立连接，然后发送提供 Slot 的 RPC 请求。在 JobManager 中，所有 Task 的请求会缓存到 SlotPool 中。当有 Slot 被提供之后，SlotPool 会从缓存的请求中选择相应的请求并结束相应的请求过程。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-4.png" alt="img"></p><p>​                                                                   图4. Flink 中资源管理功能各模块交互关系。</p><p>当 Task 结束之后，无论是正常结束还是异常结束，都会通知 JobManager 相应的结束状态，然后在 TaskManager 端将 Slot 标记为已占用但未执行任务的状态。JobManager 会首先将相应的 Slot 缓存到 SlotPool 中，但不会立即释放。这种方式避免了如果将 Slot 直接还给 ResourceManager，在任务异常结束之后需要重启时，需要立刻重新申请 Slot 的问题。通过延时释放，Failover 的 Task 可以尽快调度回原来的 TaskManager，从而加快 Failover 的速度。当 SlotPool 中缓存的 Slot 超过指定的时间仍未使用时，SlotPool 就会发起释放该 Slot 的过程。与申请 Slot 的过程对应，SlotPool 会首先通知 TaskManager 来释放该 Slot，然后 TaskExecutor 通知 ResourceManager 该 Slot 已经被释放，从而最终完成释放的逻辑。</p><p>除了正常的通信逻辑外，在 ResourceManager 和 TaskExecutor 之间还存在定时的心跳消息来同步 Slot 的状态。在分布式系统中，消息的丢失、错乱不可避免，这些问题会在分布式系统的组件中引入不一致状态，如果没有定时消息，那么组件无法从这些不一致状态中恢复。此外，当组件之间长时间未收到对方的心跳时，就会认为对应的组件已经失效，并进入到 Failover 的流程。</p><p>在 Slot 管理基础上，Flink 可以将 Task 调度到相应的 Slot 当中。如上文所述，Flink 尚未完全引入细粒度的资源匹配，默认情况下，每个 Slot 可以分配给一个 Task。但是，这种方式在某些情况下会导致资源利用率不高。如图 5 所示，假如 A、B、C 依次执行计算逻辑，那么给 A、B、C 分配分配单独的 Slot 就会导致资源利用率不高。为了解决这一问题，Flink 提供了 Share Slot 的机制。如图 5 所示，基于 Share Slot，每个 Slot 中可以部署来自不同 JobVertex 的多个任务，但是不能部署来自同一个 JobVertex 的 Task。如图5所示，每个 Slot 中最多可以部署同一个 A、B 或 C 的 Task，但是可以同时部署 A、B 和 C 的各一个 Task。当单个 Task 占用资源较少时，Share Slot 可以提高资源利用率。 此外，Share Slot 也提供了一种简单的保持负载均衡的方式。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-5.png" alt="img"></p><p>​                                                                       图5.Flink Share Slot 示例。<br>使用 Share Slot 可以在每个 Slot 中部署来自不同 JobVertex 的多个 Task。</p><p>基于上述 Slot 管理和分配的逻辑，JobManager 负责维护作业中 Task执行的状态。如上文所述，Client 端会向 JobManager 提交一个 JobGraph，它代表了作业的逻辑结构。JobManager 会根据 JobGraph 按并发展开，从而得到 JobManager 中关键的 ExecutionGraph。ExecutionGraph 的结构如图 5 所示，与 JobGraph 相比，ExecutionGraph 中对于每个 Task 与中间结果等均创建了对应的对象，从而可以维护这些实体的信息与状态。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-6.png" alt="img"></p><p>​                                                               图6.Flink 中的 JobGraph 与 ExecutionGraph。<br>ExecutionGraph 是 JobGraph 按并发展开所形成的，它是 JobMaster 中的核心数据结构。</p><p>在一个 Flink Job 中是包含多个 Task 的，因此另一个关键的问题是在 Flink 中按什么顺序来调度 Task。如图 7 所示，目前 Flink 提供了两种基本的调度逻辑，即 Eager 调度与 Lazy From Source。Eager 调度如其名子所示，它会在作业启动时申请资源将所有的 Task 调度起来。这种调度算法主要用来调度可能没有终止的流作业。与之对应，Lazy From Source 则是从 Source 开始，按拓扑顺序来进行调度。简单来说，Lazy From Source 会先调度没有上游任务的 Source 任务，当这些任务执行完成时，它会将输出数据缓存到内存或者写入到磁盘中。然后，对于后续的任务，当它的前驱任务全部执行完成后，Flink 就会将这些任务调度起来。这些任务会从读取上游缓存的输出数据进行自己的计算。这一过程继续进行直到所有的任务完成计算。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-7.png" alt="img"></p><p>​                                                                        图7. Flink 中两种基本的调度策略。<br>其中 Eager 调度适用于流作业，而Lazy From Source 适用于批作业。</p><h3 id="4-错误恢复"><a href="#4-错误恢复" class="headerlink" title="4. 错误恢复"></a>4. 错误恢复</h3><p>在 Flink 作业的执行过程中，除正常执行的流程外，还有可能由于环境等原因导致各种类型的错误。整体上来说，错误可能分为两大类：Task 执行出现错误或 Flink 集群的 Master 出现错误。由于错误不可避免，为了提高可用性，Flink 需要提供自动错误恢复机制来进行重试。</p><p>对于第一类 Task 执行错误，Flink 提供了多种不同的错误恢复策略。如图 8 所示，第一种策略是 Restart-all，即直接重启所有的 Task。对于 Flink 的流任务，由于 Flink 提供了 Checkpoint 机制，因此当任务重启后可以直接从上次的 Checkpoint 开始继续执行。因此这种方式更适合于流作业。第二类错误恢复策略是 Restart-individual，它只适用于 Task 之间没有数据传输的情况。这种情况下，我们可以直接重启出错的任务。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-8.png" alt="img"></p><p>​                                                                           图8.Restart-all 错误恢复策略示例。<br>​                                                                            该策略会直接重启所有的 Task。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-9.png" alt="img"></p><p>​                                                                              图9.Restart-individual 错误恢复策略示例。<br>​                                     该策略只适用于 Task之间不需要数据传输的作业，对于这种作业可以只重启出现错误的 Task。</p><p>由于 Flink 的批作业没有 Checkpoint 机制，因此对于需要数据传输的作业，直接重启所有 Task 会导致作业从头计算，从而导致一定的性能问题。为了增强对 Batch 作业，Flink 在1.9中引入了一种新的Region-Based的Failover策略。在一个 Flink 的 Batch 作业中 Task 之间存在两种数据传输方式，一种是 Pipeline 类型的方式，这种方式上下游 Task 之间直接通过网络传输数据，因此需要上下游同时运行；另外一种是 Blocking 类型的试，如上节所述，这种方式下，上游的 Task 会首先将数据进行缓存，因此上下游的 Task 可以单独执行。基于这两种类型的传输，Flink 将 ExecutionGraph 中使用 Pipeline 方式传输数据的 Task 的子图叫做 Region，从而将整个 ExecutionGraph 划分为多个子图。可以看出，Region 内的 Task 必须同时重启，而不同 Region 的 Task 由于在 Region 边界存在 Blocking 的边，因此，可以单独重启下游 Region 中的 Task。</p><p>基于这一思路,如果某个 Region 中的某个 Task 执行出现错误，可以分两种情况进行考虑。如图 8 所示，如果是由于 Task 本身的问题发生错误，那么可以只重启该 Task 所属的 Region 中的 Task，这些 Task 重启之后，可以直接拉取上游 Region 缓存的输出结果继续进行计算。</p><p>另一方面，如图如果错误是由于读取上游结果出现问题，如网络连接中断、缓存上游输出数据的 TaskExecutor 异常退出等，那么还需要重启上游 Region 来重新产生相应的数据。在这种情况下，如果上游 Region 输出的数据分发方式不是确定性的（如 KeyBy、Broadcast 是确定性的分发方式，而 Rebalance、Random 则不是，因为每次执行会产生不同的分发结果），为保证结果正确性，还需要同时重启上游 Region 所有的下游 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-10.png" alt="img"></p><p>​                                                                   图10.Region-based 错误恢复策略示例一。<br>​                                             如果是由于下游任务本身导致的错误，可以只重启下游对应的 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-11.png" alt="img"></p><p>​                                                                       图11.Region-based 错误恢复策略示例二。<br>​                                                     如果是由于上游失败导致的错误，那么需要同时重启上游的 Region 和下游的 Region。实际上，如果下游的输出使用了非确定的数据分割方式，为了保持数据一致性，还需要同时重启所有上游 Region 的下游 Region。</p><p>除了 Task 本身执行的异常外，另一类异常是 Flink 集群的 Master 进行发生异常。目前 Flink 支持启动多个 Master 作为备份，这些 Master 可以通过 ZK 来进行选主，从而保证某一时刻只有一个 Master 在运行。当前活路的 Master 发生异常时,某个备份的 Master 可以接管协调的工作。为了保证 Master 可以准确维护作业的状态，Flink 目前采用了一种最简单的实现方式，即直接重启整个作业。实际上，由于作业本身可能仍在正常运行，因此这种方式存在一定的改进空间。</p><h3 id="5-未来展望"><a href="#5-未来展望" class="headerlink" title="5. 未来展望"></a>5. 未来展望</h3><p>Flink目前仍然在Runtime部分进行不断的迭代和更新。目前来看，Flink未来可能会在以下几个方式继续进行优化和扩展：</p><ul><li><strong>更完善的资源管理</strong>：从 1.9 开始 Flink 开始了对细粒度资源匹配的支持。基于细粒度的资源匹配，用户可以为 TaskExecutor 和 Task 设置实际提供和使用的 CPU、内存等资源的数量，Flink 可以按照资源的使用情况进行调度。这一机制允许用户更大范围的控制作业的调度，从而为进一步提高资源利用率提供了基础。</li><li><strong>统一的 Stream 与 Batch</strong>：Flink 目前为流和批分别提供了 DataStream 和 DataSet 两套接口，在一些场景下会导致重复实现逻辑的问题。未来 Flink 会将流和批的接口都统一到 DataStream 之上。</li><li><strong>更灵活的调度策略</strong>：Flink 从 1.9 开始引入调度插件的支持，从而允许用户来扩展实现自己的调度逻辑。未来 Flink 也会提供更高性能的调度策略的实现。</li><li><strong>Master Failover 的优化</strong>：如上节所述，目前 Flink 在 Master Failover 时需要重启整个作业，而实际上重启作业并不是必须的逻辑。Flink 未来会对 Master failover 进行进一步的优化来避免不必要的作业重启。</li></ul>]]></content>
    
    <summary type="html">
    
      本文围绕Flink的Runtime 核心机制进行了深度解析。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink1.7.1与Kafka0.11.0.1</title>
    <link href="https://gjtmaster.github.io/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/"/>
    <id>https://gjtmaster.github.io/2018/11/15/Flink1.7.1与Kafka0.11.0.1/</id>
    <published>2018-11-15T02:17:01.000Z</published>
    <updated>2019-07-27T05:59:18.146Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink的Checkpoint"><a href="#Flink的Checkpoint" class="headerlink" title="Flink的Checkpoint"></a>Flink的Checkpoint</h1><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><ul><li>使用StreamExecutionEnvironment.enableCheckpointing方法来设置开启checkpoint；具体可以使用enableCheckpointing(long interval)，或者enableCheckpointing(long interval, CheckpointingMode mode)；interval用于指定checkpoint的触发间隔(单位milliseconds)，而CheckpointingMode默认是CheckpointingMode.EXACTLY_ONCE，也可以指定为CheckpointingMode.AT_LEAST_ONCE</li><li>也可以通过StreamExecutionEnvironment.getCheckpointConfig().setCheckpointingMode来设置CheckpointingMode，一般对于超低延迟的应用(大概几毫秒)可以使用CheckpointingMode.AT_LEAST_ONCE，其他大部分应用使用CheckpointingMode.EXACTLY_ONCE就可以</li><li>checkpointTimeout用于指定checkpoint执行的超时时间(单位milliseconds)，超时没完成就会被abort掉</li><li>minPauseBetweenCheckpoints用于指定checkpoint coordinator上一个checkpoint完成之后最小等多久可以出发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</li><li>maxConcurrentCheckpoints用于指定运行中的checkpoint最多可以有多少个，用于包装topology不会花太多的时间在checkpoints上面；如果有设置了minPauseBetweenCheckpoints，则maxConcurrentCheckpoints这个参数就不起作用了(大于1的值不起作用)</li><li>enableExternalizedCheckpoints用于开启checkpoints的外部持久化，但是在job失败的时候不会自动清理，需要自己手工清理state；ExternalizedCheckpointCleanup用于指定当job canceled的时候externalized checkpoint该如何清理，DELETE_ON_CANCELLATION的话，在job canceled的时候会自动删除externalized state，但是如果是FAILED的状态则会保留；RETAIN_ON_CANCELLATION则在job canceled的时候会保留externalized checkpoint state</li><li>failOnCheckpointingErrors用于指定在checkpoint发生异常的时候，是否应该fail该task，默认为true，如果设置为false，则task会拒绝checkpoint然后继续运行</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.env.getCheckpointConfig().setFailOnCheckpointingErrors(true);</code></pre><h1 id="FlinkKafkaConsumer011"><a href="#FlinkKafkaConsumer011" class="headerlink" title="FlinkKafkaConsumer011"></a>FlinkKafkaConsumer011</h1><h2 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h2><ul><li>setStartFromGroupOffsets()【默认消费策略】<br>默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</li><li>setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息</li><li>setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息</li><li>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)</li></ul><ul><li>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</li><li>为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次</li><li>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:<br>(1) Checkpoint关闭时： 可以通过下面两个参数配置<br>enable.auto.commit<br>auto.commit.interval.ms<br>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置<br>setCommitOffsetsOnCheckpoints(boolean)<br>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略</li></ul><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; public class StreamingKafkaSource {    public static void main(String[] args) throws Exception {        //获取Flink的运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //checkpoint配置        env.enableCheckpointing(5000);        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);        env.getCheckpointConfig().setCheckpointTimeout(60000);        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);        //设置statebackend        //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://hadoop100:9000/flink/checkpoints&quot;,true));        String topic = &quot;kafkaConsumer&quot;;        Properties prop = new Properties();        prop.setProperty(&quot;bootstrap.servers&quot;,&quot;SparkMaster:9092&quot;);        prop.setProperty(&quot;group.id&quot;,&quot;kafkaConsumerGroup&quot;);        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;&gt;(topic, new SimpleStringSchema(), prop);        myConsumer.setStartFromGroupOffsets();//默认消费策略        DataStreamSource&lt;String&gt; text = env.addSource(myConsumer);        text.print().setParallelism(1);        env.execute(&quot;StreamingFromCollection&quot;);    }}</code></pre><h1 id="FlinkKafkaProducer011"><a href="#FlinkKafkaProducer011" class="headerlink" title="FlinkKafkaProducer011"></a>FlinkKafkaProducer011</h1><h2 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h2><ul><li>Kafka Producer的容错-Kafka 0.9 and 0.10</li><li>如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数:<br>setLogFailuresOnly(false)<br>setFlushOnCheckpoint(true)</li><li>注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是0】</li><li>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义<br>Semantic.NONE<br>Semantic.AT_LEAST_ONCE【默认】<br>Semantic.EXACTLY_ONCE</li></ul><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><pre><code>public class StreamingKafkaSink {    public static void main(String[] args) throws Exception {    //获取Flink的运行环境    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    //checkpoint配置    env.enableCheckpointing(5000);    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);    env.getCheckpointConfig().setCheckpointTimeout(60000);    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);    env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);    //设置statebackend    //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://SparkMaster:9000/flink/checkpoints&quot;,true));    DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;SparkMaster&quot;, 9001, &quot;\n&quot;);    String brokerList = &quot;SparkMaster:9092&quot;;    String topic = &quot;kafkaProducer&quot;;    Properties prop = new Properties();    prop.setProperty(&quot;bootstrap.servers&quot;,brokerList);    //第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间    //设置事务超时时间    //prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);    //第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。    //FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());    //使用EXACTLY_ONCE语义的kafkaProducer    FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(topic, new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);    text.addSink(myProducer);    env.execute(&quot;StreamingFromCollection&quot;);  }}</code></pre>]]></content>
    
    <summary type="html">
    
      本文主要讲述Flink1.7.1与Kafka0.11.0.1交互相关API的使用与案例。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Flink On Yarn HA</title>
    <link href="https://gjtmaster.github.io/2018/11/10/Flink%20on%20Yarn%20HA/"/>
    <id>https://gjtmaster.github.io/2018/11/10/Flink on Yarn HA/</id>
    <published>2018-11-10T05:15:07.000Z</published>
    <updated>2019-08-25T08:49:04.146Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配好主机映射</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置免密登录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 准备好安装包 hadoop-2.8.5.tar.gz、flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建flink用户，后续操作均在flink用户下操作</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将Hadoop安装包解压至flink01节点的/data/apps路径下</span></span><br><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将flink安装包解压至flink01节点的/data/apps路径下</span></span><br><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点配置如下：</span></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">192.168.23.51</td><td align="center">flink01</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、NameNode、DFSZKFailoverController、DataNode</td></tr><tr><td align="center">192.168.23.52</td><td align="center">flink02</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、 NameNode、DFSZKFailoverController、DataNode、ResourceManager</td></tr><tr><td align="center">192.168.23.53</td><td align="center">flink03</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、ResourceManager、DataNode</td></tr></tbody></table><h2 id="Hadoop-HA配置"><a href="#Hadoop-HA配置" class="headerlink" title="Hadoop HA配置"></a>Hadoop HA配置</h2><h3 id="进入hadoop配置目录"><a href="#进入hadoop配置目录" class="headerlink" title="进入hadoop配置目录"></a>进入hadoop配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入hadoop配置目录</span></span><br><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改Java环境配置"><a href="#修改Java环境配置" class="headerlink" title="修改Java环境配置"></a>修改Java环境配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 修改hadoop-env.sh中的JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置yarn-env.sh中的JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置mapred-env.sh中的JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/name1,/data/apps/hadoop-2.8.5/tmp/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/data1,/data/apps/hadoop-2.8.5/tmp/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://flink01:8485;flink02:8485;flink03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行</span></span><br><span class="line"><span class="comment">sshfence:当Active出问题后，standby切换成Active，此时，原Active又没有停止服务，这种情况下会被强制杀死进程。</span></span><br><span class="line"><span class="comment">shell(/bin/true)：NN Active和它的ZKFC一起挂了，没有人通知ZK，ZK长期没有接到通知，standby要切换，此时，standby调一个shell（脚本内容），这个脚本返回true则切换成功。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/flink/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rmcluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群的地址--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--启用自动恢复--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置capacity-scheduler-xml"><a href="#配置capacity-scheduler-xml" class="headerlink" title="配置capacity-scheduler.xml"></a>配置capacity-scheduler.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>集群中可用于运行application master的资源比例上限.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="启动Zookeeper集群"><a href="#启动Zookeeper集群" class="headerlink" title="启动Zookeeper集群"></a>启动Zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="初始化Hadoop环境"><a href="#初始化Hadoop环境" class="headerlink" title="初始化Hadoop环境"></a>初始化Hadoop环境</h3><p>启动journalnode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>格式化zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行完成后，会在zookeeper 上创建一个目录，查看是否创建成功：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入zookeeper家目录，执行bin/zkCli.sh客户端连接ZK。在ZK客户端的shell命令行查看：ls /</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 出现hadoop-ha即表示成功。</span></span><br></pre></td></tr></table></figure><p>启动主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>备用NN 同步主NN信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>关闭已启动的所有journalnode和主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="启动hadoop集群"><a href="#启动hadoop集群" class="headerlink" title="启动hadoop集群"></a>启动hadoop集群</h3><p>启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令（建议先启动所有journalnode以防出现namenode连接journalnode超时）</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个namenode的状态</span></span><br><span class="line">bin/hdfs haadmin -getServiceState nn1     #查看nn1状态</span><br><span class="line">bin/hdfs haadmin -getServiceState nn2     #查看nn2状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动切换namenode状态（此处禁用，有需要再执行）</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1##切换成active</span><br><span class="line">bin/hdfs haadmin -transitionToStandby nn1##切换成standby</span><br></pre></td></tr></table></figure><p>启动Yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在flink03执行以下命令</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个Resourcemanager的状态</span></span><br><span class="line">bin/yarn rmadmin -getServiceState rm1      ##查看rm1的状态</span><br><span class="line">bin/yarn rmadmin -getServiceState rm2      ##查看rm2的状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当flink02的ResourceManager是Active状态的时候，访问flink03的ResourceManager会自动跳转到flink02的web页面</span></span><br></pre></td></tr></table></figure><h2 id="Flink-HA配置"><a href="#Flink-HA配置" class="headerlink" title="Flink HA配置"></a>Flink HA配置</h2><h3 id="进入flink配置目录"><a href="#进入flink配置目录" class="headerlink" title="进入flink配置目录"></a>进入flink配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html" target="_blank" rel="noopener">点此查看flink配置说明</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The config parameter defining the network address to connect to <span class="keyword">for</span> communication with the job manager. This value is only interpreted <span class="keyword">in</span> setups <span class="built_in">where</span> a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used <span class="keyword">in</span> many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</span></span><br><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the JobManager.</span></span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the TaskManagers, <span class="built_in">which</span> are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager<span class="string">'s YARN container, minus a certain tolerance value.</span></span></span><br><span class="line">taskmanager.heap.size: 2048m</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of parallel operator or user <span class="keyword">function</span> instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a <span class="keyword">function</span> or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or <span class="keyword">function</span> instances. This value is typically proportional to the number of physical CPU cores that the TaskManager<span class="string">'s machine has (e.g., equal to the number of cores, or half the number of cores).</span></span></span><br><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line"><span class="meta">#</span><span class="bash"> Default parallelism <span class="keyword">for</span> <span class="built_in">jobs</span>.</span></span><br><span class="line">parallelism.default: 2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defines high-availability mode used <span class="keyword">for</span> the cluster execution. To <span class="built_in">enable</span> high-availability, <span class="built_in">set</span> this mode to <span class="string">"ZOOKEEPER"</span> or specify FQN of factory class.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> high-availability mode (required): The high-availability mode has to be <span class="built_in">set</span> <span class="keyword">in</span> conf/flink-conf.yaml to zookeeper <span class="keyword">in</span> order to <span class="built_in">enable</span> high availability mode. Alternatively this option can be <span class="built_in">set</span> to FQN of factory class Flink should use to create HighAvailabilityServices instance.</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash"> File system path (URI) <span class="built_in">where</span> Flink persists metadata <span class="keyword">in</span> high-availability setups.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Storage directory (required): JobManager metadata is persisted <span class="keyword">in</span> the file system storageDir and only a pointer to this state is stored <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The storageDir stores all metadata needed to recover a JobManager failure.</span></span><br><span class="line">high-availability.storageDir: hdfs://ns1/flink/recovery</span><br><span class="line"><span class="meta">#</span><span class="bash"> The ZooKeeper quorum to use, when running Flink <span class="keyword">in</span> a high-availability mode with ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper quorum (required): A ZooKeeper quorum is a replicated group of ZooKeeper servers, <span class="built_in">which</span> provide the distributed coordination service.</span></span><br><span class="line">high-availability.zookeeper.quorum: flink01:2181,flink02:2181,flink03:2181</span><br><span class="line"><span class="meta">#</span><span class="bash"> The root path under <span class="built_in">which</span> Flink stores its entries <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper root (recommended): The root ZooKeeper node, under <span class="built_in">which</span> all cluster nodes are placed.</span></span><br><span class="line">high-availability.zookeeper.path.root: /flink</span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts: The number of ApplicationMaster (+ its TaskManager containers) attempts. If this value is <span class="built_in">set</span> to 1 (default), the entire YARN session will fail when the Application master fails. Higher values specify the number of restarts of the ApplicationMaster by YARN.</span></span><br><span class="line">yarn.application-attempts: 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The state backend to be used to store and checkpoint state.</span></span><br><span class="line">state.backend: rocksdb</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory used <span class="keyword">for</span> storing the data files and meta data of checkpoints <span class="keyword">in</span> a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).</span></span><br><span class="line">state.checkpoints.dir: hdfs://ns1/flink/flink-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory <span class="keyword">for</span> savepoints. Used by the state backends that write savepoints to file systems (MemoryStateBackend, FsStateBackend, RocksDBStateBackend).</span></span><br><span class="line">state.savepoints.dir: hdfs://ns1/flink/save-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> Option whether the state backend should create incremental checkpoints, <span class="keyword">if</span> possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Some state backends may not support incremental checkpoints and ignore this option.</span></span><br><span class="line">state.backend.incremental: true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Directories <span class="keyword">for</span> temporary files, separated by<span class="string">","</span>, <span class="string">"|"</span>, or the system<span class="string">'s java.io.File.pathSeparator.</span></span></span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br></pre></td></tr></table></figure><p>切记：Flink On Yarn HA一定不要手动配置high-availability.cluster-id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper cluster-id (recommended): The cluster-id ZooKeeper node, under <span class="built_in">which</span> all required coordination data <span class="keyword">for</span> a cluster is placed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Important: You should not <span class="built_in">set</span> this value manually when running a YARN cluster, a per-job YARN session, or on another cluster manager. In those cases a cluster-id is automatically being generated based on the application id. Manually setting a cluster-id overrides this behaviour <span class="keyword">in</span> YARN. Specifying a cluster-id with the -z CLI option, <span class="keyword">in</span> turn, overrides manual configuration. If you are running multiple Flink HA clusters on bare metal, you have to manually configure separate cluster-ids <span class="keyword">for</span> each cluster.</span></span><br><span class="line">high-availability.cluster-id: /default</span><br></pre></td></tr></table></figure><h3 id="替换日志框架为logback"><a href="#替换日志框架为logback" class="headerlink" title="替换日志框架为logback"></a>替换日志框架为logback</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><p><strong>logback-yarn.xml配置示例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn-HA测试说明"><a href="#Flink-on-Yarn-HA测试说明" class="headerlink" title="Flink on Yarn HA测试说明"></a>Flink on Yarn HA测试说明</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开始一个yarn-session（命名为FlinkTestCluster）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager内存2048M</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个TaskManager内存2048M且分配4个slot（The session cluster will automatically allocate additional containers <span class="built_in">which</span> run the Task Managers when <span class="built_in">jobs</span> are submitted to the cluster.）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式启动</span></span><br><span class="line">yarn-session.sh -jm 2048 -tm 2048 -s 4 -nm FlinkTestCluster -d</span><br></pre></td></tr></table></figure><table><thead><tr><th>配置</th><th>测试方案</th><th>现象</th><th>备注</th></tr></thead><tbody><tr><td>Job本身配置了Flink的重启策略</td><td>提供bug程序，导致Job失败</td><td>重启失败的Job</td><td>保证Job HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnSessionClusterEntrypoint进程（<em>JobManager</em>和AM的共同进程）</td><td>重启JobManager和AM，该进程会迁移到其它节点（非必须）且进程号改变，全部Job重启</td><td>保证JobManager HA</td></tr><tr><td>Job本身配置了Flink的重启策略、Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnTaskExecutorRunner进程（TaskManager进程）</td><td>重启TaskManager，该进程会迁移到其它节点（非必须）且进程号改变，被Kill掉的TaskManager包含的Job重启</td><td>保证TaskManager HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts</td><td>未主动Cancel掉Flink集群中的Job，但不小心kill掉对应的yarn-session(对应Yarn队列中的一个Application)、之后在命令行重新提交yarn-session</td><td>启动新的yarn-session、之前未Cancel掉的Job自动迁移到当前yarn-session、JobManager和TaskManager自动创建</td><td>保证 YarnSessionHA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts、配置了Yarn的HA</td><td>Kill掉Resourcemanager</td><td>ResourceManager迁移到另一台节点，yarn-session重启，所有Job重启</td><td>保证Yarn HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts（也可在yarn-session提交时通过-D动态配置）、配置了HDFS的HA</td><td>Kill掉NameNode</td><td>NameNode迁移到另一台节点</td><td>保证HDFS HA</td></tr></tbody></table><h2 id="Yarn的基本思想"><a href="#Yarn的基本思想" class="headerlink" title="Yarn的基本思想"></a>Yarn的基本思想</h2><p>YARN的基本思想是将资源管理和作业调度/监视的功能分解为单独的守护进程。我们的想法是拥有一个全局ResourceManager（<em>RM</em>）和每个应用程序ApplicationMaster（<em>AM</em>）。应用程序可以是单个作业，也可以是作业的DAG。</p><p>ResourceManager和NodeManager构成了数据计算框架。ResourceManager是在系统中的所有应用程序之间仲裁资源的最终权限。NodeManager是每台机器上负责Containers的代理框架，监视其资源使用情况（CPU，内存，磁盘，网络）并将其报告给ResourceManager / Scheduler。</p><p>每个应用程序ApplicationMaster实际上是一个含具体库的框架，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</p><p><img src="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="MapReduce NextGen架构"></p><p>ResourceManager有两个主要组件：Scheduler和ApplicationsManager。</p><p>Scheduler负责根据熟悉的容量，队列等约束将资源分配给各种正在运行的应用程序。Scheduler是纯调度程序，因为它不执行应用程序状态的监视或跟踪。此外，当出现应用程序故障或硬件故障，它无法保证重新启动失败的任务。Scheduler根据应用程序的资源需求执行其调度功能; 它是基于资源<em>Container</em>的抽象概念，它包含内存，CPU，磁盘，网络等元素。</p><p>Scheduler具有可插入策略，该策略负责在各种队列，应用程序等之间对集群资源进行分区。当前的调度程序（如<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler）</a>将是插件的一些示例。</p><p>ApplicationsManager负责接受作业提交，协商第一个容器以执行特定于应用程序的ApplicationMaster，并提供在失败时重新启动ApplicationMaster容器的服务。每个应用程序ApplicationMaster负责从Scheduler协调适当的资源容器，跟踪其状态并监视进度。</p><h2 id="Flink-on-Yarn的基本思想"><a href="#Flink-on-Yarn的基本思想" class="headerlink" title="Flink on Yarn的基本思想"></a>Flink on Yarn的基本思想</h2><p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.7/fig/FlinkOnYarn.svg" alt="img"></p><p>YARN客户端需要访问Hadoop配置以连接到YARN资源管理器和HDFS。它使用以下策略确定Hadoop配置：</p><ul><li>按顺序测试是否配置<code>YARN_CONF_DIR</code>，<code>HADOOP_CONF_DIR</code>或<code>HADOOP_CONF_PATH</code>。如果设置了其中一个变量，则用于读取配置。</li><li>如果上述策略失败（在正确的YARN设置中不应该这样），则客户端使用配置的<code>HADOOP_HOME</code>环境变量。如果<code>HADOOP_HOME</code>环境变量已配置，则客户端尝试访问<code>$HADOOP_HOME/etc/hadoop</code>（Hadoop 2）或<code>$HADOOP_HOME/conf</code>（Hadoop 1）。</li></ul><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（ApplicationMaster的memory和vcores）是否可用。之后，它将包含Flink的jar包和配置信息上传到HDFS（步骤1）。</p><p>客户端的下一步是请求（步骤2）YARN容器以启动<em>ApplicationMaster</em>（步骤3）。客户端将配置信息和jar文件注册为容器的资源，在特定机器上运行的NodeManager将负责准备容器（例如下载文件的工作）。完成后，将启动<em>ApplicationMaster</em>（AM）。</p><p>该<em>JobManager</em>和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（Flink主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager），该文件也上传到HDFS。此外，<em>AM</em>容器还提供Flink的Web界面。YARN代码分配的所有端口都是<em>临时端口</em>。这允许用户并行执行多个Flink YARN会话。</p><p>之后，AM开始为Flink的TaskManagers分配容器（步骤4），这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a> </p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html#yarn-cluster-high-availability" target="_blank" rel="noopener">JobManager High Availability (HA)</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Flink on Yarn</a></p>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink on Yarn 高可用的集群部署方案。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="Yarn" scheme="https://gjtmaster.github.io/tags/Yarn/"/>
    
      <category term="Flink on Yarn" scheme="https://gjtmaster.github.io/tags/Flink-on-Yarn/"/>
    
  </entry>
  
  <entry>
    <title>Flink On Yarn集群部署</title>
    <link href="https://gjtmaster.github.io/2018/10/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://gjtmaster.github.io/2018/10/15/Flink On Yan集群部署/</id>
    <published>2018-10-15T05:15:07.000Z</published>
    <updated>2019-08-03T14:26:05.960Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配好主机映射</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建flink用户</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置免密登录</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 准备好相关资源：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.8.5.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> flink-1.7.1-bin-hadoop28-scala_2.11</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 节点配置如下：(建议每台NM节点预留2G内存给系统)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">hostname</th><th align="center">资源配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">flink01</td><td align="center">16G/16cores</td><td align="center">NameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink02</td><td align="center">16G/16cores</td><td align="center">ResourceManager/DataNode/NodeManager</td></tr><tr><td align="center">flink03</td><td align="center">16G/16cores</td><td align="center">SecondaryNameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink04</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink05</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink06</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr></tbody></table><h2 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h2><h3 id="将Hadoop安装包解压至flink01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至flink01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至flink01节点的/data/apps路径下"></a>将Hadoop安装包解压至flink01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录"><a href="#进入配置目录" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改hadoop-env-sh中的JAVA-HOME"><a href="#修改hadoop-env-sh中的JAVA-HOME" class="headerlink" title="修改hadoop-env.sh中的JAVA_HOME"></a>修改hadoop-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置yarn-env-sh中的JAVA-HOME"><a href="#配置yarn-env-sh中的JAVA-HOME" class="headerlink" title="配置yarn-env.sh中的JAVA_HOME"></a>配置yarn-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置mapred-env-sh中的JAVA-HOME"><a href="#配置mapred-env-sh中的JAVA-HOME" class="headerlink" title="配置mapred-env.sh中的JAVA_HOME"></a>配置mapred-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br><span class="line">fli<span class="symbol">nk04</span></span><br><span class="line">fli<span class="symbol">nk05</span></span><br><span class="line">fli<span class="symbol">nk06</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS的路径的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该属性定义了 HDFS WEB访问服务器的主机名和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 定义secondarynamenode 外部地址 访问的主机和端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager 的主机位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>7168<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>14336<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>含义</th><th>值</th><th>备注</th></tr></thead><tbody><tr><td>yarn.nodemanager.aux-services</td><td>设置yarn中的服务类</td><td>mapreduce_shuffle</td><td></td></tr><tr><td>yarn.resourcemanager.hostname</td><td>配置resourcemanager 的主机位置</td><td>flink02</td><td></td></tr><tr><td>yarn.resourcemanager.am.max-attempts</td><td>AM重启最大尝试次数</td><td>4</td><td></td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>开启物理内存限制</td><td>true</td><td>检测物理内存的使用是否超出分配值，若任务超出分配值，则将其杀掉，默认true。</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>关闭虚拟内存限制</td><td>false</td><td>检测虚拟内存的使用是否超出；若任务超出分配值，则将其杀掉，默认true。在确定内存不会泄漏的情况下可以设置此项为 False；</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>每个Container请求的最小内存</td><td>1024</td><td>单个容器/调度器可申请的最少物理内存量，默认是1024（MB）；一般每个contain都分配这个值；即：capacity memory:3072, vCores:1，如果提示物理内存溢出，提高这个值即可；</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>每个Container请求的最大内存</td><td>7168</td><td>单个容器/调度器可申请的最大物理内存量</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container请求的最小virtual CPU cores</td><td>1</td><td></td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container请求的最大virtual CPU cores</td><td>16</td><td></td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>限制 NodeManager 能够使用的最大物理内存</td><td>14336</td><td>该节点上YARN可使用的物理内存总量，【向操作系统申请的总量】默认是8192（MB）</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>限制 NodeManager 能够使用的最大virtual CPU cores</td><td>16</td><td>该节点上YARN可使用的总核心数；一般设为cat /proc/cpuinfo| grep “processor”| wc -l 的值。默认是8个</td></tr><tr><td>yarn.log-aggregation-enable</td><td>启用日志聚集功能</td><td>true</td><td></td></tr><tr><td>yarn.nodemanager.log.retain-seconds</td><td>设置HDFS上日志的保存时间,默认设置为7天</td><td>10800</td><td></td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>虚拟内存率</td><td>5</td><td>任务每使用1MB物理内存，最多可使用虚拟内存量比率，默认2.1；关闭虚拟内存限制的情况下，配置此项就无意义了</td></tr></tbody></table><h3 id="修改capacity-scheduler-xml"><a href="#修改capacity-scheduler-xml" class="headerlink" title="修改capacity-scheduler.xml"></a>修改capacity-scheduler.xml</h3><p><strong>（flink yarn session启用的jobmanager占用的资源总量受此参数限制）</strong></p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;    &lt;value&gt;0.3&lt;/value&gt;    &lt;description&gt;集群中可用于运行application master的资源比例上限.&lt;/description&gt;&lt;/property&gt;</code></pre><h3 id="快速安装Hadoop"><a href="#快速安装Hadoop" class="headerlink" title="快速安装Hadoop"></a>快速安装Hadoop</h3><p><strong>（使用此脚本安装完后需要单独修改capacity-scheduler.xml）</strong></p><p><strong>将安装脚本和安装包放在相同路径下并执行以下命令可快速完成上述配置步骤！</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认相关资源已放在当前用户的~路径下</span></span><br><span class="line">sh ~/install-hadoop.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">加入以下内容（这里提前加上了flink的环境变量）：</span><br><span class="line">export FLINK_HOME = /data/apps/flink-1.7.1</span><br><span class="line">export HADOOP_HOME=/data/apps/hadoop-2.8.5</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLINK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h3><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>在NameNode所在节点启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>在ResourceManager所在节点启动YARN</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="Flink集群"><a href="#Flink集群" class="headerlink" title="Flink集群"></a>Flink集群</h2><h3 id="将Hadoop安装包解压至kafka01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至kafka01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至kafka01节点的/data/apps路径下"></a>将Hadoop安装包解压至kafka01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录-1"><a href="#进入配置目录-1" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line">parallelism.default: 2</span><br><span class="line">taskmanager.numberOfTaskSlots: 8</span><br><span class="line">state.backend: rocksdb</span><br><span class="line">state.checkpoints.dir: hdfs://flink01:9000/flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs://flink01:9000/flink-savepoints</span><br><span class="line">state.backend.incremental: true</span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br><span class="line">yarn.application-attempts: 4</span><br></pre></td></tr></table></figure><h3 id="删除Flink原先使用的日志框架log4j相关资源"><a href="#删除Flink原先使用的日志框架log4j相关资源" class="headerlink" title="删除Flink原先使用的日志框架log4j相关资源"></a>删除Flink原先使用的日志框架log4j相关资源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br></pre></td></tr></table></figure><h3 id="更换Flink的日志框架为logback"><a href="#更换Flink的日志框架为logback" class="headerlink" title="更换Flink的日志框架为logback"></a>更换Flink的日志框架为logback</h3><p>（1）添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</p><p>（2）自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn的两种运行模式"><a href="#Flink-on-Yarn的两种运行模式" class="headerlink" title="Flink on Yarn的两种运行模式"></a>Flink on Yarn的两种运行模式</h2><h3 id="Start-a-long-running-Flink-cluster-on-YARN"><a href="#Start-a-long-running-Flink-cluster-on-YARN" class="headerlink" title="Start a long-running Flink cluster on YARN"></a>Start a long-running Flink cluster on YARN</h3><p>​    这种方式需要先启动集群，然后在提交Flink-Job（同一个Session中可以提交多个Flink-Job，可以在Flink的WebUI上submit，也可以使用Flink run命令提交）。启动集群时会向yarn申请一块空间，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成，释放了资源，那下一个作业才会正常提交.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认配置启动flink on yarn（默认启动资源如下）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> &#123;masterMemoryMB=1024, taskManagerMemoryMB=1024,numberTaskManagers=1, slotsPerTaskManager=1&#125;</span></span><br><span class="line">yarn-session.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 系统默认使用con/flink-conf.yaml里的配置，Flink on yarn将会覆盖掉几个参数：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jobmanager.rpc.address因为jobmanager的在集群的运行位置并不是事先确定的，其实就是AM的地址；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.tmp.dirs使用yarn给定的临时目录;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism.default也会被覆盖掉，如果在命令行里指定了slot数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 自定义配置可选参数如下 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Required     </span></span><br><span class="line"> -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)   </span><br><span class="line"><span class="meta">#</span><span class="bash"> Optional     </span></span><br><span class="line"> -D &lt;arg&gt;                        Dynamic properties     </span><br><span class="line"> -d,--detached                   Start detached     </span><br><span class="line"> -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)     </span><br><span class="line"> -nm,--name                      Set a custom name for the application on YARN     </span><br><span class="line"> -q,--query                      Display available YARN resources (memory, cores)     </span><br><span class="line"> -qu,--queue &lt;arg&gt;               Specify YARN queue.     </span><br><span class="line"> -s,--slots &lt;arg&gt;                Number of slots per TaskManager     </span><br><span class="line"> -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)     </span><br><span class="line"> -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例：启动15个TaskManager，1个JobManager，JobManager内存1024M，每个TaskManager内存1024M且含有8个slot，自定义该应用的名称为FlinkOnYarnSession，-d以分离式模式执行（不指定-d则以客户端模式执行）</span></span><br><span class="line">yarn-session.sh -n 15 -jm 1024 -tm 1024 -s 8 -nm FlinkOnYarnSession -d</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 客户端模式指的是在终端启动一个客户端，这种方式是不能断开终端的，断开即相当于<span class="built_in">kill</span>掉Flink集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式指的是启动Flink on Yarn后，Flink YARN客户端将仅向Yarn提交Flink，然后自行关闭。，要<span class="built_in">kill</span>掉Flink集群需要使用如下命令：</span></span><br><span class="line">yarn application -kill &lt;appId&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;appId&gt;指的是发布在Yarn上的作业ID，在Yarn集群上可以查到对应的ID</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于Flink On Yarn来说，一个JobManager占用一个Container，一个TaskManager占用一个Container</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager的数量+TaskManager的数量 = 申请的Container的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下以6台16核，16G内存的机器举例说明（每台节点预留2G内存给系统）</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcores=16 每台NodeManager节点为YARN集群分配的cpu为16核</span><br><span class="line">yarn.nodemanager.resource.memory-mb=14336 每台NodeManager节点为YARN集群分配的物理内存为14G</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores=1 每台NodeManager节点上每个Contaniner最小使用1核cpu</span><br><span class="line">yarn.scheduler.minimum-allocation-mb=1024 每台NodeManager节点上每个Contaniner最小使用1G的物理内存</span><br><span class="line"><span class="meta">#</span><span class="bash"> 若所有节点全部用于Flink作业,推荐提供的Flink集群：</span></span><br><span class="line">（总的资源为14*6=84G内存，16*6=96核）</span><br><span class="line">yarn-session.sh -n 8 -jm 4096 -tm 3584 -s 16 -nm FlinkOnYarnSession -d</span><br><span class="line">一共占用32G内存，9cores，申请了1个4G/1cores的JobManager和8个3.5G/1cores/16slots的TaskManager</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############## Recovery behavior of Flink on YARN</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Flink’s YARN client has the following configuration parameters to control how to behave <span class="keyword">in</span> <span class="keyword">case</span> of container failures. These parameters can be <span class="built_in">set</span> either from the conf/flink-conf.yaml or when starting the YARN session, using -D parameters</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.reallocate-failed : 控制 Flink是否应该重新分配失败的TaskManager容器，默认<span class="literal">true</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.maximum-failed-containers : ApplicationMaster接收container失败的最大次数，默认是TaskManager的次数（-n的值）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts : ApplicationMaster尝试次数。如果这个值为1（默认），那么当Application Master失败时，整个YARN session就会失败。更高的值是指ApplicationMaster重新启动的次数</span></span><br></pre></td></tr></table></figure><h3 id="Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）"><a href="#Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）" class="headerlink" title="Run a Flink job on YARN（Flink per-job cluster模式）"></a>Run a Flink job on YARN（Flink per-job cluster模式）</h3><p>这种方式不需要先启动集群，每提交一个Flink-Job都会在Yarn上启动一个Flink集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> TaskManager slots number配置</span></span><br><span class="line">这个参数是配置一个TaskManager有多少个并发的slot数。有两种配置方式：</span><br><span class="line">- taskmanager.numberOfTaskSlots. 在conf/flink-conf.yaml中更改，默认值为1，表示默认一个TaskManager只有1个task slot.</span><br><span class="line">- 提交作业时通过参数配置。--yarnslots 1，表示TaskManager的slot数为1.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TaskManager的个数</span></span><br><span class="line">注意： Per job模式提交作业时并不像session模式能够指定拉起多少个TaskManager，TaskManager的数量是在提交作业时根据并发度动态计算。</span><br><span class="line">首先，根据设定的operator的最大并发度计算，例如，如果作业中operator的最大并发度为10，则 Parallelism/numberOfTaskSlots为向YARN申请的TaskManager数。</span><br></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#######################示例</span><br><span class="line"># flink run -m yarn-cluster 必须指定</span><br><span class="line"># -d 分离模式启动（不指定则以客户端模式启动）</span><br><span class="line"># 启动<span class="number">1</span>个JobManager，内存占用<span class="number">1024</span>M</span><br><span class="line"># 每台TaskManager指定<span class="number">4</span>个slot、内存占用<span class="number">1024</span>M</span><br><span class="line"># 假设abc.jar所有operator中最大并发度为<span class="number">8</span>，则会启动<span class="number">8</span>/<span class="number">4</span>=<span class="number">2</span>台TaskManager</span><br><span class="line">flink run -m yarn-cluster -d --yarnslots <span class="number">4</span> -yjm <span class="number">1024</span> -ytm <span class="number">1024</span> /data/abc.jar</span><br></pre></td></tr></table></figure><h2 id="Log-Files"><a href="#Log-Files" class="headerlink" title="Log Files"></a>Log Files</h2><p>In cases where the Flink YARN session fails during the deployment itself, users have to rely on the logging capabilities of Hadoop YARN. The most useful feature for that is the <a href="http://hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="noopener">YARN log aggregation</a>. To enable it, users have to set the <code>yarn.log-aggregation-enable</code>property to <code>true</code> in the <code>yarn-site.xml</code> file. Once that is enabled, users can use the following command to retrieve all log files of a (failed) YARN session.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt;</span><br></pre></td></tr></table></figure><p>Note that it takes a few seconds after the session has finished until the logs show up.</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink On Yarn的集群部署流程以及两种运行模式。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="Yarn" scheme="https://gjtmaster.github.io/tags/Yarn/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink使用Logback作为日志框架的相关配置</title>
    <link href="https://gjtmaster.github.io/2018/10/13/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
    <id>https://gjtmaster.github.io/2018/10/13/Flink使用Logback作为日志框架的相关配置/</id>
    <published>2018-10-13T08:13:17.000Z</published>
    <updated>2019-07-27T11:44:48.836Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink切换日志框架为Logback"><a href="#Flink切换日志框架为Logback" class="headerlink" title="Flink切换日志框架为Logback"></a>Flink切换日志框架为Logback</h1><h2 id="client端pom文件配置"><a href="#client端pom文件配置" class="headerlink" title="client端pom文件配置"></a>client端pom文件配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Add the two required logback dependencies --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add the log4j -&gt; sfl4j (-&gt; logback) bridge into the classpath</span></span><br><span class="line"><span class="comment">     Hadoop is logging to log4j! --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>添加logback-core、logback-classic及log4j-over-slf4j依赖，</li><li>之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；</li><li><strong>最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</strong></li></ul><h2 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h2><ul><li><p>添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)</p><p>相关jar包在logback官网上都有，<a href="https://download.csdn.net/download/qq_36643786/11190626" target="_blank" rel="noopener">嫌麻烦的可以点此链接直接下载！</a></p></li><li><p>移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)</p></li><li><p>如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</p></li></ul><h3 id="flink-daemon-sh"><a href="#flink-daemon-sh" class="headerlink" title="flink-daemon.sh"></a>flink-daemon.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-daemon.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#  Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment">#  or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment">#  distributed with this work for additional information</span></span><br><span class="line"><span class="comment">#  regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment">#  to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment">#  "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment">#  with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">#  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">#  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start/stop a Flink daemon.</span></span><br><span class="line">USAGE=<span class="string">"Usage: flink-daemon.sh (start|stop|stop-all) (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">STARTSTOP=<span class="variable">$1</span></span><br><span class="line">DAEMON=<span class="variable">$2</span></span><br><span class="line">ARGS=(<span class="string">"<span class="variable">$&#123;@:3&#125;</span>"</span>) <span class="comment"># get remaining arguments as array</span></span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"<span class="variable">$0</span>"</span>`</span><br><span class="line">bin=`<span class="built_in">cd</span> <span class="string">"<span class="variable">$bin</span>"</span>; <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">. <span class="string">"<span class="variable">$bin</span>"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$DAEMON</span> <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unknown daemon '<span class="variable">$&#123;DAEMON&#125;</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$FLINK_IDENT_STRING</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_IDENT_STRING=<span class="string">"<span class="variable">$USER</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">pid=<span class="variable">$FLINK_PID_DIR</span>/flink-<span class="variable">$FLINK_IDENT_STRING</span>-<span class="variable">$DAEMON</span>.pid</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Log files for daemons are indexed from the process ID's position in the PID</span></span><br><span class="line"><span class="comment"># file. The following lock prevents a race condition during daemon startup</span></span><br><span class="line"><span class="comment"># when multiple daemons read, index, and write to the PID file concurrently.</span></span><br><span class="line"><span class="comment"># The lock is created on the PID directory since a lock file cannot be safely</span></span><br><span class="line"><span class="comment"># removed. The daemon is started with the lock closed and the lock remains</span></span><br><span class="line"><span class="comment"># active in this script until the script exits.</span></span><br><span class="line"><span class="built_in">command</span> -v flock &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">exec</span> 200&lt;<span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line">    flock 200</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ascending ID depending on number of lines in pid file.</span></span><br><span class="line"><span class="comment"># This allows us to start multiple daemon of each type.</span></span><br><span class="line">id=$([ -f <span class="string">"<span class="variable">$pid</span>"</span> ] &amp;&amp; <span class="built_in">echo</span> $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>"</span>) || <span class="built_in">echo</span> <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">FLINK_LOG_PREFIX=<span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>/flink-<span class="variable">$&#123;FLINK_IDENT_STRING&#125;</span>-<span class="variable">$&#123;DAEMON&#125;</span>-<span class="variable">$&#123;id&#125;</span>-<span class="variable">$&#123;HOSTNAME&#125;</span>"</span></span><br><span class="line"><span class="built_in">log</span>=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.log"</span></span><br><span class="line">out=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.out"</span></span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog.file=<span class="variable">$&#123;log&#125;</span>"</span> <span class="string">"-Dlog4j.configuration=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/log4j.properties"</span> <span class="string">"-Dlogback.configurationFile=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/logback.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$(<span class="variable">$&#123;JAVA_RUN&#125;</span> -version 2&gt;&amp;1 | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only set JVM 8 arguments if we have correctly extracted the version</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$&#123;JAVA_VERSION&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_VERSION</span>"</span> -lt 18 ]; <span class="keyword">then</span></span><br><span class="line">        JVM_ARGS=<span class="string">"<span class="variable">$JVM_ARGS</span> -XX:MaxPermSize=256m"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STARTSTOP</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">    (start)</span><br><span class="line">        <span class="comment"># Rotate log files</span></span><br><span class="line">        rotateLogFilesWithPrefix <span class="string">"<span class="variable">$FLINK_LOG_DIR</span>"</span> <span class="string">"<span class="variable">$FLINK_LOG_PREFIX</span>"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print a warning if daemons are already running on host</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          active=()</span><br><span class="line">          <span class="keyword">while</span> IFS=<span class="string">''</span> <span class="built_in">read</span> -r p || [[ -n <span class="string">"<span class="variable">$p</span>"</span> ]]; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">kill</span> -0 <span class="variable">$p</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">              active+=(<span class="variable">$p</span>)</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">          <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          count=<span class="string">"<span class="variable">$&#123;#active[@]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> [ <span class="variable">$&#123;count&#125;</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"[INFO] <span class="variable">$count</span> instance(s) of <span class="variable">$DAEMON</span> are already running on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">          <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Evaluate user options for local variable expansion</span></span><br><span class="line">        FLINK_ENV_JAVA_OPTS=$(<span class="built_in">eval</span> <span class="built_in">echo</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Starting <span class="variable">$DAEMON</span> daemon on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="variable">$JAVA_RUN</span> <span class="variable">$JVM_ARGS</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span> <span class="string">"<span class="variable">$&#123;log_setting[@]&#125;</span>"</span> -classpath <span class="string">"`manglePathList "</span><span class="variable">$FLINK_TM_CLASSPATH</span>:<span class="variable">$INTERNAL_HADOOP_CLASSPATHS</span><span class="string">"`"</span> <span class="variable">$&#123;CLASS_TO_RUN&#125;</span> <span class="string">"<span class="variable">$&#123;ARGS[@]&#125;</span>"</span> &gt; <span class="string">"<span class="variable">$out</span>"</span> 200&lt;&amp;- 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line">        mypid=$!</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to pid file if successful start</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="variable">$&#123;mypid&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]] &amp;&amp; <span class="built_in">kill</span> -0 <span class="variable">$mypid</span> &gt; /dev/null 2&gt;&amp;1 ; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="variable">$mypid</span> &gt;&gt; <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Error starting <span class="variable">$DAEMON</span> daemon."</span></span><br><span class="line">            <span class="built_in">exit</span> 1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Remove last in pid file</span></span><br><span class="line">            to_stop=$(tail -n 1 <span class="string">"<span class="variable">$pid</span>"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> [ -z <span class="variable">$to_stop</span> ]; <span class="keyword">then</span></span><br><span class="line">                rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                sed \<span class="variable">$d</span> <span class="string">"<span class="variable">$pid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="comment"># all but last line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                [ $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>.tmp"</span>) -eq 0 ] &amp;&amp; rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$pid</span>.tmp"</span> || mv <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) is running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop-all)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            mv <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">read</span> to_stop; <span class="keyword">do</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Skipping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>), because it is not running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">            rm <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unexpected argument '<span class="variable">$STARTSTOP</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml</li></ul><h3 id="flink-console-sh"><a href="#flink-console-sh" class="headerlink" title="flink-console.sh"></a>flink-console.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-console.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line"># Start a Flink service <span class="keyword">as</span> a console application. Must be stopped <span class="keyword">with</span> Ctrl-C</span><br><span class="line"># or <span class="keyword">with</span> SIGTERM by kill or the controlling process.</span><br><span class="line">USAGE=<span class="string">"Usage: flink-console.sh (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">SERVICE=$<span class="number">1</span></span><br><span class="line">ARGS=(<span class="string">"$&#123;@:2&#125;"</span>) # get remaining arguments <span class="keyword">as</span> array</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> $SERVICE <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        echo <span class="string">"Unknown service '$&#123;SERVICE&#125;'. $USAGE."</span></span><br><span class="line">        exit <span class="number">1</span></span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog4j.configuration=file:$&#123;FLINK_CONF_DIR&#125;/log4j-console.properties"</span> <span class="string">"-Dlogback.configurationFile=file:$&#123;FLINK_CONF_DIR&#125;/logback-console.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$($&#123;JAVA_RUN&#125; -version <span class="number">2</span>&gt;&amp;<span class="number">1</span> | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"># Only set JVM <span class="number">8</span> arguments <span class="keyword">if</span> we have correctly extracted the version</span><br><span class="line"><span class="keyword">if</span> [[ $&#123;JAVA_VERSION&#125; =~ $&#123;IS_NUMBER&#125; ]]; then</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"$JAVA_VERSION"</span> -lt <span class="number">18</span> ]; then</span><br><span class="line">        JVM_ARGS=<span class="string">"$JVM_ARGS -XX:MaxPermSize=256m"</span></span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"Starting $SERVICE as a console application on host $HOSTNAME."</span></span><br><span class="line">exec $JAVA_RUN $JVM_ARGS $&#123;FLINK_ENV_JAVA_OPTS&#125; <span class="string">"$&#123;log_setting[@]&#125;"</span> -classpath <span class="string">"`manglePathList "</span>$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS<span class="string">"`"</span> $&#123;CLASS_TO_RUN&#125; <span class="string">"$&#123;ARGS[@]&#125;"</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml</li></ul><h3 id="yarn-session-sh"><a href="#yarn-session-sh" class="headerlink" title="yarn-session.sh"></a>yarn-session.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line"># get Flink config</span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$FLINK_IDENT_STRING"</span> = <span class="string">""</span> ]; then</span><br><span class="line">        FLINK_IDENT_STRING=<span class="string">"$USER"</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">JVM_ARGS=<span class="string">"$JVM_ARGS -Xmx512m"</span></span><br><span class="line"></span><br><span class="line">CC_CLASSPATH=`manglePathList $(constructFlinkClassPath):$INTERNAL_HADOOP_CLASSPATHS`</span><br><span class="line"></span><br><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-yarn-session-$HOSTNAME.log</span><br><span class="line">log_setting=<span class="string">"-Dlog.file="</span>$log<span class="string">" -Dlog4j.configuration=file:"</span>$FLINK_CONF_DIR<span class="string">"/log4j-yarn-session.properties -Dlogback.configurationFile=file:"</span>$FLINK_CONF_DIR<span class="string">"/logback-yarn.xml"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> FLINK_CONF_DIR</span><br><span class="line"></span><br><span class="line">$JAVA_RUN $JVM_ARGS -classpath <span class="string">"$CC_CLASSPATH"</span> $log_setting org.apache.flink.yarn.cli.FlinkYarnSessionCli -j <span class="string">"$FLINK_LIB_DIR"</span>/flink-dist*.jar <span class="string">"$@"</span></span><br></pre></td></tr></table></figure><ul><li>使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h2 id="doc"><a href="#doc" class="headerlink" title="doc"></a>doc</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/best_practices.html#using-logback-instead-of-log4j" target="_blank" rel="noopener">Using Logback instead of Log4j</a></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>client端使用logback的话，要在pom文件添加logback-core、logback-classic及log4j-over-slf4j依赖，之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</li><li>服务端使用logback的话，要在添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)；移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)；如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</li><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h1 id="Logback配置文件详解"><a href="#Logback配置文件详解" class="headerlink" title="Logback配置文件详解"></a>Logback配置文件详解</h1><p>Logback，Java 日志框架。</p><p>Logback 如何加载配置的</p><ol><li>logback 首先会查找 logback.groovy 文件</li><li>当没有找到，继续试着查找 logback-test.xml 文件</li><li>当没有找到时，继续试着查找 logback.xml 文件</li><li>如果仍然没有找到，则使用默认配置（打印到控制台）</li></ol><h2 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h2><p>configuration 是配置文件的根节点，他包含的属性：</p><ul><li>scan<br>　　当此属性设置为 true 时，配置文件如果发生改变，将会被重新加载，默认值为 true</li><li>scanPeriod<br>　　设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。但 scan 为 true 时，此属性生效，默认的时间间隔为 1 分钟</li><li>debug<br>　　当此属性设置为 true 时，将打印出 logback 内部日志信息，实时查看 logback 运行状态，默认值为 false。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="configuration-的子节点"><a href="#configuration-的子节点" class="headerlink" title="configuration 的子节点"></a>configuration 的子节点</h2><h4 id="设置上下文名称：contextName"><a href="#设置上下文名称：contextName" class="headerlink" title="设置上下文名称：contextName"></a>设置上下文名称：contextName</h4><p>每个 logger 度关联到 logger 上下文，默认上下文名称为 “default”。可以通过设置 contextName 修改上下文名称，用于区分不同应该程序的记录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>myAppName<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置变量：property"><a href="#设置变量：property" class="headerlink" title="设置变量：property"></a>设置变量：property</h4><p>用于定义键值对的变量， property 有两个属性 name 和 value，name 是键，value 是值，通过 property 定义的键值对会保存到logger 上下文的 map 集合内。定义变量后，可以使用 “${}” 来使用变量</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"APP_Name"</span> <span class="attr">value</span>=<span class="string">"myAppName"</span> /&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;APP_Name&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="获取时间戳字符串：timestamp"><a href="#获取时间戳字符串：timestamp" class="headerlink" title="获取时间戳字符串：timestamp"></a>获取时间戳字符串：timestamp</h4><p>timestamp 有两个属性，key：标识此 timestamp 的名字；datePattern：时间输出格式，遵循SimpleDateFormat 的格式</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">timestamp</span> <span class="attr">key</span>=<span class="string">"bySecond"</span> <span class="attr">datePattern</span>=<span class="string">"yyyyMMdd'T'HHmmss"</span>/&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;bySecond&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="logger"><a href="#logger" class="headerlink" title="logger"></a>logger</h2><p>logger 有两种级别，一种是 root，一种是普通的 logger，logger 是用来设置某一个包或者具体的某一个类的日志打印机级别，以及制定的 appender。<br>logger 有三个属性</p><ul><li>name：用来指定此 logger 约束的某一个包或者具体的某一个类</li><li>level：用来设置打印机别，</li><li>addtivity：是否向上级 logger 传递打印信息。默认是 true</li></ul><p>每个 logger 都有对应的父级关系，它通过包名来决定父级关系，root 是最高级的父元素。<br>下面定义了四个 logger，他们的父子关系从小到大为：<br>com.lwc.qg.test.logbackDemo → com.lwc.qg.tes → com.lwc.qg → root</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 根 logger --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    普通的 logger</span></span><br><span class="line"><span class="comment">    name：类名或包名，标志该 logger 与哪个包或哪个类绑定</span></span><br><span class="line"><span class="comment">    level：该 logger 的日志级别</span></span><br><span class="line"><span class="comment">    additivity：是否将日志信息传递给上一级</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test.logbackDemo"</span> <span class="attr">level</span>=<span class="string">"debug"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br></pre></td></tr></table></figure><p>　　从该种级别来看，如果此时在最低层的 logger 输出日志信息，以该配置作为基础，它将会向父级的所有 logger 依次传递，所以按理来说一个打印信息将会打印四次</p><p>　　从控制台上看，的确每条日志信息都被打印出了四次，但是细心从配置文件上来看，root 的日志级别配置的为 info，但是却输出<br>debug 级别的日志信息，所以从测试结果可以看出，向上传递的日志信息的日志级别将由最底层的子元素决定（最初传递信息的<br>logger），因为子元素设置的日志级别为 debug，所以也输出了 debug 级别的信息。<br>　　因此，从理论上来说，如果子元素日志级别设置高一点，那么也将会只输出高级别的日志信息。实际上也是如此，如果我们把 com.lwc.qg.test.logbackDemo 对应的 logger 日志级别设为 warn，那么将只会输出 warn 及其以上的信息</p><h2 id="root"><a href="#root" class="headerlink" title="root"></a>root</h2><p>root 也是 logger 元素，但它是根 logger。只有一个 level 属性</p><h2 id="appender"><a href="#appender" class="headerlink" title="appender"></a>appender</h2><p>appender 是负责写日志的组件，常用的组件有：</p><ul><li>ConsoleAppender</li><li>FileAppender</li><li>RollingFileAppender</li></ul><h2 id="ConsoleAppender"><a href="#ConsoleAppender" class="headerlink" title="ConsoleAppender"></a>ConsoleAppender</h2><p>控制台日志组件，该组件将日志信息输出到控制台,该组件有以下节点</p><ul><li>encoder：对日志进行格式化</li><li>target：System.out 或者 System.err，默认是 System.out</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="FileAppender"><a href="#FileAppender" class="headerlink" title="FileAppender"></a>FileAppender</h2><p>文件日志组件，该组件将日志信息输出到日志文件中，该组件有以下节点</p><ul><li>file：被写入的文件名，可以是相对路径，也可以是绝对路径。如果上级目录不存在会自动创建，没有默认值</li><li>append：如果是 true，日志被追加到文件结尾；如果是 false，清空现存文件，默认是 true。</li><li>encoder：格式化</li><li>prudent：如果是 true，日志会被安全的写入文件，即使其他的 FileAppender 也在向此文件做写入操作，效率低，默认是 false。</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>testFile.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>true<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">prudent</span>&gt;</span>true<span class="tag">&lt;/<span class="name">prudent</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="RollingFileAppender"><a href="#RollingFileAppender" class="headerlink" title="RollingFileAppender"></a>RollingFileAppender</h2><p>滚动记录文件日志组件，先将日志记录记录到指定文件，当符合某个条件时，将日志记录到其他文件，该组件有以下节点</p><ul><li>file：文件名</li><li>encoder：格式化</li><li>rollingPolicy：当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名</li><li>triggeringPolicy：告知 RollingFileAppender 合适激活滚动</li><li>prudent：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。</li></ul><p>#### </p><h3 id="rollingPolicy"><a href="#rollingPolicy" class="headerlink" title="rollingPolicy"></a>rollingPolicy</h3><p>滚动策略</p><ol><li>TimeBasedRollingPolicy：最常用的滚动策略，它根据时间来制定滚动策略，即负责滚动也负责触发滚动，包含节点：<ul><li>fileNamePattern：文件名模式</li><li>maxHistoury：控制文件的最大数量，超过数量则删除旧文件</li></ul></li><li>FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略，包含节点<ul><li>minInedx：窗口索引最小值</li><li>maxIndex：串口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12</li><li>fileNamePattern：文件名模式，必须包含%i，命名模式为 log%i.log，会产生 log1.log，log2.log 这样的文件</li></ul></li><li>triggeringPolicy：根据文件大小的滚动策略，包含节点<ul><li>maxFileSize：日志文件最大大小</li></ul></li></ol><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>logFile.%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="filter-过滤器"><a href="#filter-过滤器" class="headerlink" title="filter 过滤器"></a>filter 过滤器</h2><p>过滤器是用于日志组件中的，每经过一个过滤器都会返回一个确切的枚举值，分别是</p><ul><li>DENY：返回 DENY，日志将立即被抛弃不再经过其他过滤器</li><li>NEUTRAL：有序列表的下个过滤器接着处理日志</li><li>ACCEPT：日志会被立即处理，不再经过剩余过滤器</li></ul><h3 id="常用过滤器"><a href="#常用过滤器" class="headerlink" title="常用过滤器"></a>常用过滤器</h3><p>常用的过滤器有以下：</p><ul><li>LevelFilter<br>级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 omMatch 和 omMismatch 接受或拒绝日志。他有以下节点<br>　　level：过滤级别<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作<br>例：该组件设置一个 INFO 级别的过滤器，那么所有非 INFO 级别的日志都会被过滤掉　　</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>ThresholdFilter<br>临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器会返回 NEUTRAL；当日志级别低于临界值时，日志会被拒绝<br>例：过滤掉所有低于 INFO 级别的日志</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span> </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>EvaluatorFilter<br>求值过滤器，评估、鉴别日志是否符合指定条件，包含节点：<br>　　evaluator：鉴别器，通过子标签 expression 配置求值条件<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述了Flink切换日志框架为Logback的详细步骤，并对Logback框架的配置文件进行了详细的介绍。
    
    </summary>
    
      <category term="日志框架" scheme="https://gjtmaster.github.io/categories/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Logback" scheme="https://gjtmaster.github.io/tags/Logback/"/>
    
  </entry>
  
  <entry>
    <title>Flink SQL 深度解析</title>
    <link href="https://gjtmaster.github.io/2018/09/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
    <id>https://gjtmaster.github.io/2018/09/18/FlinkSQL深度解析/</id>
    <published>2018-09-18T10:19:05.000Z</published>
    <updated>2019-09-08T13:36:12.726Z</updated>
    
    <content type="html"><![CDATA[<h1 id="大数据计算领域对SQL的应用"><a href="#大数据计算领域对SQL的应用" class="headerlink" title="大数据计算领域对SQL的应用"></a>大数据计算领域对SQL的应用</h1><h2 id="离线计算（批计算）"><a href="#离线计算（批计算）" class="headerlink" title="离线计算（批计算）"></a>离线计算（批计算）</h2><p>提及大数据计算领域不得不说MapReduce计算模型，MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法，并发于2004年发表了论文Simplified Data Processing on Large Clusters。论文发表之后Apache 开源社区参考Google MapReduce，基于Java设计开发了一个称为Hadoop的开源MapReduce并行计算框架。很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并了解MapReduce的运行原理，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛，进而也成功的将SQL应用到了大数据计算领域当中来。</p><h2 id="实时计算（流计算）"><a href="#实时计算（流计算）" class="headerlink" title="实时计算（流计算）"></a>实时计算（流计算）</h2><p>SQL不仅仅被成功的应用到了离线计算，SQL的易用性也吸引了流计算产品，目前最热的Spark，Flink也纷纷支持了SQL，尤其是Flink支持的更加彻底，集成了Calcite，完全遵循ANSI-SQL标准。Apache Flink在low-level API上面用DataSet支持批计算，用DataStream支持流计算，但在High-Level API上面利用SQL将流与批进行了统一，使得用户编写一次SQL既可以在流计算中使用，又可以在批计算中使用，为既有流计算业务，又有批计算业务的用户节省了大量开发成本。</p><h1 id="SQL高性能与简洁性"><a href="#SQL高性能与简洁性" class="headerlink" title="SQL高性能与简洁性"></a>SQL高性能与简洁性</h1><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>SQL经过传统数据库领域几十年的不断打磨，查询优化器已经能够极大的优化SQL的查询性能，Apache Flink 应用Calcite进行查询优化，复用了大量数据库查询优化规则，在性能上不断追求极致，能够让用户关心但不用担心性能问题。如下图(Alibaba 对 Apache Flink 进行架构优化后的组件栈)</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN43HQMdZty0IxMowiaBs1oaPZwyEeVpLvkLakk4V51uz6iaMbz9toslicw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>相对于DataStream而言，SQL会经过Optimization模块透明的为用户进行查询优化，用户专心编写自己的业务逻辑，不用担心性能，却能得到最优的查询性能!</p><h2 id="简洁"><a href="#简洁" class="headerlink" title="简洁"></a>简洁</h2><p>就简洁性而言，SQL与DataSet和DataStream相比具有很大的优越性，我们先用一个WordCount示例来直观的查看用户的代码量：</p><p>DataStream/DataSetAPI</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">... <span class="comment">//省略初始化代码</span></span><br><span class="line"><span class="comment">// 核心逻辑</span></span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> WordCount.Tokenizer()).keyBy(<span class="keyword">new</span> <span class="built_in">int</span>[]&#123;<span class="number">0</span>&#125;).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// flatmap 代码定义</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> class Tokenizer implements FlatMapFunction&lt;<span class="keyword">String</span>, Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> Tokenizer() &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> flatMap(<span class="keyword">String</span> value, Collector&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; out) &#123;</span><br><span class="line"><span class="keyword">String</span>[] tokens = value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>);</span><br><span class="line"><span class="keyword">String</span>[] var4 = tokens;</span><br><span class="line"><span class="built_in">int</span> var5 = tokens.length;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">int</span> var6 = <span class="number">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class="line"><span class="keyword">String</span> token = var4[var6];</span><br><span class="line"><span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">out.collect(<span class="keyword">new</span> Tuple2(token, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SQL</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...//省略初始化代码</span><br><span class="line"><span class="keyword">SELECT</span> word, <span class="built_in">COUNT</span>(word) <span class="keyword">FROM</span> tab <span class="keyword">GROUP</span> <span class="keyword">BY</span> word;</span><br></pre></td></tr></table></figure><p>我们直观的体会到相同的统计功能使用SQL的简洁性。</p><h1 id="Flink-SQL-Job的组成"><a href="#Flink-SQL-Job的组成" class="headerlink" title="Flink SQL Job的组成"></a>Flink SQL Job的组成</h1><p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这三个部分，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNmwGyZgbFaPfs2bjXzSGdh9jSTKnxrYlSbLzwMUn95uVLOuHcueGLnw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p><ul><li>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</li><li>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</li><li>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</li></ul><h1 id="Flink-SQL-核心算子"><a href="#Flink-SQL-核心算子" class="headerlink" title="Flink SQL 核心算子"></a>Flink SQL 核心算子</h1><p>目前Flink SQL支持Union，Join，Projection,Difference, Intersection以及Window等大多数传统数据库支持的操作，接下来为大家分别进行简单直观的介绍。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>为了很好的体验和理解Apache Flink SQL算子我们需要先准备一下测试环境，我们选择IDEA，以ITCase测试方式来进行体验。IDEA 安装这里不占篇幅介绍了，相信大家能轻松搞定！我们进行功能体验有两种方式，具体如下：</p><h2 id="源码方式"><a href="#源码方式" class="headerlink" title="源码方式"></a>源码方式</h2><p>对于开源爱好者可能更喜欢源代码方式理解和体验Apache Flink SQL功能，那么我们需要下载源代码并导入到IDEA中：</p><ul><li>下载源码：</li></ul><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载源代码</span></span><br><span class="line">git clone https:<span class="comment">//github.com/apache/flink.git study</span></span><br><span class="line"><span class="comment">// 进入源码目录</span></span><br><span class="line">cd study</span><br><span class="line"><span class="comment">// 拉取稳定版release-1.6</span></span><br><span class="line">git fetch origin <span class="built_in">release</span><span class="number">-1.6</span>:<span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//切换到稳定版</span></span><br><span class="line">git checkout <span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//将依赖安装到本地mvn仓库，耐心等待需要一段时间</span></span><br><span class="line">mvn clean install -DskipTests</span><br></pre></td></tr></table></figure><ul><li>导入到IDEA<br>将Flink源码导入到IDEA过程这里不再占用篇幅，导入后确保在IDEA中可以运行 <code>org.apache.flink.table.runtime.stream.sql.SqlITCase</code> 并测试全部通过，即证明体验环境已经完成，即证明体验环境已经完成。如下图所示：</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNo09iaFxmhAfdNGPSjCc6qnjDUWyZaCO8UBSkyUJy1EEcicoSv4qa8wzg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上图运行测试后显示测试通过，我们就可以继续下面的Apache Flink SQL功能体验了。</p><h2 id="依赖Flink包方式"><a href="#依赖Flink包方式" class="headerlink" title="依赖Flink包方式"></a>依赖Flink包方式</h2><p>我们还有一种更简单直接的方式，就是新建一个mvn项目，并在pom中添加如下依赖：</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">table.version</span>&gt;</span>1.6-SNAPSHOT<span class="tag">&lt;/<span class="name">table.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>完成环境准备后，我们开始准备测试数据和写一个简单的测试类。</p><h2 id="示例数据及测试类"><a href="#示例数据及测试类" class="headerlink" title="示例数据及测试类"></a>示例数据及测试类</h2><h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><ul><li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li></ul><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><ul><li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li></ul><table><thead><tr><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr></tbody></table><ul><li>Item_tab<br> 商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li></ul><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>30</td></tr><tr><td>ITEM004</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td></tr></tbody></table><ul><li>PageAccess_tab<br>页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><ul><li>PageAccessCount_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userCount</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>100</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>86</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>210</td><td>2017.11.11 10:06:00</td></tr><tr><td>BeiJing</td><td>33</td><td>2017.11.11 10:10:00</td></tr><tr><td>ShangHai</td><td>129</td><td>2017.11.11 12:10:00</td></tr></tbody></table><ul><li>PageAccessSession_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:01:00</td></tr><tr><td>ShangHai</td><td>U0012</td><td>2017-11-11 10:02:00</td></tr><tr><td>ShangHai</td><td>U0013</td><td>2017-11-11 10:03:00</td></tr><tr><td>ShangHai</td><td>U0015</td><td>2017-11-11 10:05:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U0110</td><td>2017-11-11 10:10:00</td></tr><tr><td>ShangHai</td><td>U2010</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0410</td><td>2017-11-11 12:16:00</td></tr></tbody></table><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><p>我们创建一个<code>SqlOverviewITCase.scala</code> 用于接下来介绍Flink SQL算子的功能体验。代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.<span class="type">StateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.<span class="type">MemoryStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.<span class="type">RichSinkFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">TableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.junit.rules.<span class="type">TemporaryFolder</span></span><br><span class="line"><span class="keyword">import</span> org.junit.&#123;<span class="type">Rule</span>, <span class="type">Test</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqlOverviewITCase</span> </span>&#123;</span><br><span class="line"><span class="keyword">val</span> _tempFolder = <span class="keyword">new</span> <span class="type">TemporaryFolder</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Rule</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFolder</span></span>: <span class="type">TemporaryFolder</span> = _tempFolder</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStateBackend</span></span>: <span class="type">StateBackend</span> = &#123;</span><br><span class="line"><span class="keyword">new</span> <span class="type">MemoryStateBackend</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 客户表数据</span></span><br><span class="line"><span class="keyword">val</span> customer_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">customer_data.+=((<span class="string">"c_001"</span>, <span class="string">"Kevin"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_002"</span>, <span class="string">"Sunny"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_003"</span>, <span class="string">"JinCheng"</span>, <span class="string">"from HeBei"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 订单表数据</span></span><br><span class="line"><span class="keyword">val</span> order_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">order_data.+=((<span class="string">"o_001"</span>, <span class="string">"c_002"</span>, <span class="string">"2018-11-05 10:01:01"</span>, <span class="string">"iphone"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_002"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:01:55"</span>, <span class="string">"ipad"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_003"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:03:44"</span>, <span class="string">"flink book"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 商品销售表数据</span></span><br><span class="line"><span class="keyword">val</span> item_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="number">20</span>, <span class="string">"ITEM001"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="number">50</span>, <span class="string">"ITEM002"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">30</span>, <span class="string">"ITEM003"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">60</span>, <span class="string">"ITEM004"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365780000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="number">40</span>, <span class="string">"ITEM005"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="number">20</span>, <span class="string">"ITEM006"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365960000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366020000</span>L, (<span class="number">1510366020000</span>L, <span class="number">70</span>, <span class="string">"ITEM007"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366020000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366080000</span>L, (<span class="number">1510366080000</span>L, <span class="number">20</span>, <span class="string">"ITEM008"</span>, <span class="string">"Clothes"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">151036608000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据</span></span><br><span class="line"><span class="keyword">val</span> pageAccess_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问量表数据2</span></span><br><span class="line"><span class="keyword">val</span> pageAccessCount_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="number">100</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="number">86</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="string">"BeiJing"</span>, <span class="number">210</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="number">33</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="number">129</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据3</span></span><br><span class="line"><span class="keyword">val</span> pageAccessSession_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0012"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0013"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0015"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373760000</span>L, (<span class="number">1510373760000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0410"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373760000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">procTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将order_tab, customer_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> customer = env.fromCollection(customer_data).toTable(tEnv).as(<span class="symbol">'c_id</span>, <span class="symbol">'c_name</span>, <span class="symbol">'c_desc</span>)</span><br><span class="line">    <span class="keyword">val</span> order = env.fromCollection(order_data).toTable(tEnv).as(<span class="symbol">'o_id</span>, <span class="symbol">'c_id</span>, <span class="symbol">'o_time</span>, <span class="symbol">'o_desc</span>)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"order_tab"</span>, order)</span><br><span class="line">    tEnv.registerTable(<span class="string">"customer_tab"</span>, customer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rowTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setStateBackend(getStateBackend)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将item_tab, pageAccess_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> item =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">String</span>)](item_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'onSellTime</span>, <span class="symbol">'price</span>, <span class="symbol">'itemID</span>, <span class="symbol">'itemType</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccess =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccess_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessCount =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">Int</span>)](pageAccessCount_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'accessCount</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessSession =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccessSession_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"item_tab"</span>, item)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccess_tab"</span>, pageAccess)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessCount_tab"</span>, pageAccessCount)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessSession_tab"</span>, pageAccessSession)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testSelect</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">"替换想要测试的SQL"</span></span><br><span class="line">    <span class="comment">// 非window 相关用 procTimePrint(sql)</span></span><br><span class="line">    <span class="comment">// Window 相关用 rowTimePrint(sql)</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义Sink</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RetractingSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> retractedResults: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(v: (<span class="type">Boolean</span>, <span class="type">Row</span>)) &#123;</span><br><span class="line">    retractedResults.synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> value = v._2.toString</span><br><span class="line">    <span class="keyword">if</span> (v._1) &#123;</span><br><span class="line">    retractedResults += value</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = retractedResults.indexOf(value)</span><br><span class="line">    <span class="keyword">if</span> (idx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    retractedResults.remove(idx)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Tried to retract a value that wasn't added first. "</span> +</span><br><span class="line">    <span class="string">"This is probably an incorrectly implemented test. "</span> +</span><br><span class="line">    <span class="string">"Try to set the parallelism of the sink to 1."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    retractedResults.sorted.foreach(println(_))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Water mark 生成器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventTimeSourceFunction</span>[<span class="type">T</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">dataWithTimestampList: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span></span>), <span class="title">Long</span>]]) <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(w) =&gt; ctx.emitWatermark(<span class="keyword">new</span> <span class="type">Watermark</span>(w))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h2><p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某</p><p>些列, 如下图所示:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNibZ25Mic3yIbEcG8icTWkkJiaMcTr5oq0wTkT7rdZ5EkUpXEp26ZKVTrKw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_name, <span class="keyword">CONCAT</span>(c_name, <span class="string">' come '</span>, c_desc) <span class="keyword">as</span> <span class="keyword">desc</span> <span class="keyword">FROM</span> customer_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><table><thead><tr><th>c_name</th><th>desc</th></tr></thead><tbody><tr><td>Kevin</td><td>Kevin come from JinLin</td></tr><tr><td>Sunny</td><td>Sunny come from JinLin</td></tr><tr><td>Jincheng</td><td>Jincheng come from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是携带 <code>DISTINCT</code> 关键字，示例如下：</p><p><strong>SQL 示例</strong></p><p>在订单表查询所有的客户id，消除重复客户id, 如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c_id <span class="keyword">FROM</span> order_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th></tr></thead><tbody><tr><td>c_001</td></tr><tr><td>c_002</td></tr></tbody></table><h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> 用于从数据集/流中过滤数据，与<span class="keyword">SELECT</span>一起使用，语法遵循<span class="keyword">ANSI</span>-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</span><br></pre></td></tr></table></figure><p><strong>SQL 示例</strong></p><p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id = <span class="string">'c_001'</span> <span class="keyword">OR</span> c_id = <span class="string">'c_003'</span>;</span><br></pre></td></tr></table></figure><p><strong>Result</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>AND</code>， <code>OR</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 <code>WHERE</code> 可以结合<code>IN</code>,<code>NOT IN</code>联合使用，具体如下：</p><p><strong>SQL 示例 (IN 常量)</strong></p><p>使用 <code>IN</code> 在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="string">'c_001'</span>, <span class="string">'c_003'</span>);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>SQL 示例 (IN 子查询)</strong></p><p>使用 <code>IN</code>和 子查询 在<code>customer_tab</code>查询已经下过订单的客户信息，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="keyword">SELECT</span> c_id <span class="keyword">FROM</span> order_tab);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr></tbody></table><p><strong>IN/NOT IN 与关系代数</strong></p><p>如上介绍IN是关系代数中的Intersection， NOT IN是关系代数的Difference， 如下图示意：</p><ul><li>IN(Intersection</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblp6icardxtCKeaf7RHrFbN6TVbyqyGOGuSWYY7uY3DJb5ODYsOqvv1mWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li><li>NOT IN(Difference）</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpcofHeFia7icQorYjiaGmHO9yiclrFaMCk3l6sBuQa2sm5QlrtepLOrdIMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li></ul><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpoeicHXKPbhnpAKEe8cMRzf4WHDQiagwAHRIlH6icqn107hHkiaeJh2CWDQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>SQL 示例</strong></p><p>将order_tab信息按customer_tab分组统计订单数量，简单示例如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT c_id, count(o_id) as o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY c_id;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>o_count</th></tr></thead><tbody><tr><td>c_001</td><td>2</td></tr><tr><td>c_002</td><td>1</td></tr></tbody></table><p><strong>特别说明</strong></p><p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p><p><strong>SQL 示例</strong></p><p>按时间进行分组，查询每分钟的订单数量，如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT SUBSTRING(o_time, 1, 16) AS o_time_min, count(o_id) AS o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY SUBSTRING(o_time, 1, 16)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>o_time_min</th><th>o_count</th></tr></thead><tbody><tr><td>2018-11-05 10:01</td><td>2</td></tr><tr><td>2018-11-05 10:03</td><td>1</td></tr></tbody></table><p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p><h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNpaHuv6VYq4P9Zyke2cuHCIwibTbicJpXicRJWemZsJN6Y1Nq3vKVNzpNg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p><h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNALmfI36VBMGEontFaDkRleLsSbErPHtRYvT0dBQ4ic6kwQD3AEJIhfQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p><h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p><ul><li>JOIN - INNER JOIN</li><li>LEFT JOIN - LEFT OUTER JOIN</li><li>RIGHT JOIN - RIGHT OUTER JOIN</li><li>FULL JOIN - FULL OUTER JOIN</li></ul><p>JOIN与关系代数的Join语义相同，具体如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN8qxel16siciaMAH8x3aQCcZ6q0ic8QrtZtco3D9frZFjHfZYj4q33hszg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例 (JOIN)</strong></p><p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr></tbody></table><p><strong>SQL 示例 (LEFT JOIN)</strong></p><p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNLcrI9iar3vKlgxMwRceIAMCZm2uNbhUWINhK1yRAllPdkwVJ1PcHhqQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>对应的SQL语句如下(LEFT JOIN)：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, <span class="built_in">T2</span>.ColC, ColE FROM TI LEFT <span class="keyword">JOIN </span><span class="built_in">T2</span> ON <span class="built_in">T1</span>.ColC = <span class="built_in">T2</span>.ColC <span class="comment">;</span></span><br></pre></td></tr></table></figure><ul><li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li></ul><p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr></tbody></table><p><strong>特别说明</strong></p><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p><h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p><h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p><ul><li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li><li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li></ul><h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p><p><strong>语义</strong></p><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN6yotUibfmTVgbnFd7dvC4tgfFEddh0xJ6PzC9wzLDgiaemZoCCjVNxaw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p><p><strong>语法</strong></p><p>Bounded ROWS OVER Window 语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">ROWS</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | rowCount) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>rowCount - 是定义根据当前行开始向前追溯几行元素。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> onSellTime </span><br><span class="line">        <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>50</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>60</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p><p><strong>语义</strong></p><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNTtvBlDvT0wfxJvTOL8e9CbVJg6YVxAfLMKskjXibicrCeOGgIZxAJxdw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p><p><strong>语法</strong></p><p>Bounded RANGE OVER Window的语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">RANGE</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | timeInterval) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li></ul><p><strong>SQL 示例</strong></p><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下（Bounded RANGE OVER Windo</strong>w）</p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>60</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>40</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><p><strong>特别说明</strong></p><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p><ul><li>GROUP BY - <code>SELECT d, MAX(c) FROM table GROUP BY d</code></li><li>OVER Window = <code>SELECT a, b, c, d, MAX(c) OVER(PARTITION BY d, ORDER BY ProcTime())</code><br>如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li></ul><h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p><ul><li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li><li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li><li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li></ul><p>说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p><h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><p><strong>语义</strong></p><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN9PPeHiaOUQib8BG2xs3YPxpN8EYibnRNkFxgicW1kPrNeicE8vpcUB7tspA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Tumble 滚动窗口对应的语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)], </span><br><span class="line">    [TUMBLE_END(timeCol, size)], </span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] - 决定了流是Keyed还是/Non-Keyed;</li><li>TUMBLE_START - 窗口开始时间;</li><li>TUMBLE_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>size - 表示窗口的大小，如 秒，分钟，小时，天。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region,</span><br><span class="line">    TUMBLE_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    TUMBLE_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv</span><br><span class="line"><span class="keyword">FROM</span> pageAccess_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, TUMBLE(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p><p><strong>语义</strong></p><p>Hop 滑动窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNCwyBicMTKEicSxibebwTfwvImiaA2TlN0FuM0wuG6zAibYyk5JrfBTmrwEA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Hop 滑动窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,  </span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggN(colN) </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>HOP_START - 窗口开始时间;</li><li>HOP_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>slide - 是滑动步伐的大小；</li><li>size - 是窗口的大小，如 秒，分钟，小时，天；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  HOP_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">  HOP_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">  <span class="keyword">SUM</span>(accessCount) <span class="keyword">AS</span> accessCount  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessCount_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>winStart</th><th>winEnd</th><th>accessCount</th></tr></thead><tbody><tr><td>2017-11-11 01:55:00.0</td><td>2017-11-11 02:05:00.0</td><td>186</td></tr><tr><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:10:00.0</td><td>396</td></tr><tr><td>2017-11-11 02:05:00.0</td><td>2017-11-11 02:15:00.0</td><td>243</td></tr><tr><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:20:00.0</td><td>33</td></tr><tr><td>2017-11-11 04:05:00.0</td><td>2017-11-11 04:15:00.0</td><td>129</td></tr><tr><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:20:00.0</td><td>129</td></tr></tbody></table><h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p><p>语义</p><p>Session 会话窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNfhx9EQvp4OyBYHue50QEzW3qZfxeRV5DCb8CkcneoGjadj7NqNHq9w/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Seeeion 会话窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    SESSION_START(timeCol, gap) AS winStart,  </span><br><span class="line">    SESSION_END(timeCol, gap) AS winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>SESSION_START - 窗口开始时间；</li><li>SESSION_END - 窗口结束时间；</li><li>timeCol - 是流表中表示时间字段；</li><li>gap - 是窗口数据非活跃周期的时长；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessSession_tab</code>测试数据，我们按地域统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region, </span><br><span class="line">    SESSION_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd, </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessSession_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, <span class="keyword">SESSION</span>(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:13:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:01:00.0</td><td>2017-11-11 02:08:00.0</td><td>4</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:14:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:16:00.0</td><td>2017-11-11 04:19:00.0</td><td>1</td></tr></tbody></table><h2 id="UDX"><a href="#UDX" class="headerlink" title="UDX"></a>UDX</h2><p>Apache Flink 除了提供了大部分ANSI-SQL的核心算子，也为用户提供了自己编写业务代码的机会，那就是User-Defined Function,目前支持如下三种 User-Defined Function：</p><ul><li>UDF - User-Defined Scalar Function</li><li>UDTF - User-Defined Table Function</li><li>UDAF - User-Defined Aggregate Funciton</li></ul><p>UDX都是用户自定义的函数，那么Apache Flink框架为啥将自定义的函数分成三类呢？是根据什么划分的呢？Apache Flink对自定义函数进行分类的依据是根据函数语义的不同，函数的输入和输出不同来分类的，具体如下：</p><table><thead><tr><th>UDX</th><th>INPUT</th><th>OUTPUT</th><th>INPUT:OUTPUT</th></tr></thead><tbody><tr><td>UDF</td><td>单行中的N(N&gt;=0)列</td><td>单行中的1列</td><td>1:1</td></tr><tr><td>UDTF</td><td>单行中的N(N&gt;=0)列</td><td>M(M&gt;=0)行</td><td>1:N(N&gt;=0)</td></tr><tr><td>UDAF</td><td>M(M&gt;=0)行中的每行的N(N&gt;=0)列</td><td>单行中的1列</td><td>M：1(M&gt;=0)</td></tr></tbody></table><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ul><li>定义<br>用户想自己编写一个字符串联接的UDF，我们只需要实现<code>ScalarFunction#eval()</code>方法即可，简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyConnect</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="meta">@varargs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(args: <span class="type">String</span>*): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; args.length) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args(i) == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      sb.append(args(i))</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    sb.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> <span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = MyConnect</span></span><br><span class="line"> tEnv.registerFunction(<span class="string">"myConnect"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"> <span class="keyword">val</span> sql = <span class="string">"SELECT myConnect(a, b) as str FROM tab"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><ul><li>定义<br>用户想自己编写一个字符串切分的UDTF，我们只需要实现<code>TableFunction#eval()</code>方法即可，简单实现如下：</li></ul><p>ScalarFunction#eval()`</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>))&#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(collect)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>, prefix: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>)) &#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(s =&gt; collect(prefix + s))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MySplit</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"mySplit"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT c, s FROM MyTable, LATERAL TABLE(mySplit(c)) AS T(s)"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><ul><li>定义<br>UDAF 要实现的接口比较多，我们以一个简单的CountAGG为例，做简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The initial accumulator for count aggregate function */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span> <span class="keyword">extends</span> <span class="title">JTuple1</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  f0 = <span class="number">0</span>L <span class="comment">//count</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * User-defined count aggregate function</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCount</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">JLong</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 += <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 -= <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 -= <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">JLong</span> = &#123;</span><br><span class="line">    acc.f0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">CountAccumulator</span>, its: <span class="type">JIterable</span>[<span class="type">CountAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = its.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      acc.f0 += iter.next().f0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CountAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getAccumulatorType</span></span>: <span class="type">TypeInformation</span>[<span class="type">CountAccumulator</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TupleTypeInfo</span>(classOf[<span class="type">CountAccumulator</span>], <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResultType</span></span>: <span class="type">TypeInformation</span>[<span class="type">JLong</span>] =</span><br><span class="line">    <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MyCount</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"myCount"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT myCount(c) FROM MyTable GROUP BY  a"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="Source-amp-Sink"><a href="#Source-amp-Sink" class="headerlink" title="Source&amp;Sink"></a>Source&amp;Sink</h1><p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p><h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MySourceFunction</span>[<span class="type">T</span>](<span class="title">dataWithTimestampList</span>: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span>), <span class="type">Long</span>]]) </span></span><br><span class="line"><span class="class">  extends <span class="type">SourceFunction</span>[<span class="type">T</span>] &#123;</span></span><br><span class="line"><span class="class">  override def run(<span class="title">ctx</span>: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span></span><br><span class="line"><span class="class">    dataWithTimestampList.foreach &#123;</span></span><br><span class="line"><span class="class">      case <span class="type">Left</span>(<span class="title">t</span>) =&gt; ctx.collectWithTimestamp(<span class="title">t</span>.<span class="title">_2</span>, <span class="title">t</span>.<span class="title">_1</span>)</span></span><br><span class="line"><span class="class">      case <span class="type">Right</span>(<span class="title">w</span>) =&gt; ctx.emitWatermark(<span class="title">new</span> <span class="type">Watermark(w)</span>)</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">  override def cancel(): <span class="type">Unit</span> = ???</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTableSource</span> <span class="keyword">extends</span> <span class="title">StreamTableSource</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fieldNames = <span class="type">Array</span>(<span class="string">"accessTime"</span>, <span class="string">"region"</span>, <span class="string">"userId"</span>)</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">TableSchema</span>(fieldNames, <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">SQL_TIMESTAMP</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">  <span class="keyword">val</span> rowType = <span class="keyword">new</span> <span class="type">RowTypeInfo</span>(</span><br><span class="line">    <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">LONG</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>).asInstanceOf[<span class="type">Array</span>[<span class="type">TypeInformation</span>[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 页面访问表数据 rows with timestamps and watermarks</span></span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366200000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366200000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366200000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366260000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366260000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366260000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510373400000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510373400000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510373400000</span>L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRowtimeAttributeDescriptors</span></span>: util.<span class="type">List</span>[<span class="type">RowtimeAttributeDescriptor</span>] = &#123;</span><br><span class="line">    <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">RowtimeAttributeDescriptor</span>(</span><br><span class="line">      <span class="string">"accessTime"</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExistingField</span>(<span class="string">"accessTime"</span>),</span><br><span class="line">      <span class="type">PreserveWatermarks</span>.<span class="type">INSTANCE</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDataStream</span></span>(execEnv: <span class="type">StreamExecutionEnvironment</span>): <span class="type">DataStream</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    execEnv.addSource(<span class="keyword">new</span> <span class="type">MySourceFunction</span>[<span class="type">Row</span>](data)).setParallelism(<span class="number">1</span>).returns(rowType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReturnType</span></span>: <span class="type">TypeInformation</span>[<span class="type">Row</span>] = rowType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getTableSchema</span></span>: <span class="type">TableSchema</span> = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = <span class="built_in">File</span>.createTempFile(<span class="string">"csv_sink_"</span>, <span class="string">"tem"</span>)</span><br><span class="line">    <span class="comment">// 打印sink的文件路径，方便我们查看运行结果</span></span><br><span class="line">    <span class="built_in">println</span>(<span class="string">"Sink path : "</span> + tempFile)</span><br><span class="line">    <span class="built_in">if</span> (tempFile.<span class="built_in">exists</span>()) &#123;</span><br><span class="line">      tempFile.<span class="keyword">delete</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[<span class="keyword">String</span>](<span class="string">"region"</span>, <span class="string">"winStart"</span>, <span class="string">"winEnd"</span>, <span class="string">"pv"</span>),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//方便我们查出输出数据</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sourceTableName = <span class="string">"mySource"</span></span><br><span class="line">    <span class="comment">// 创建自定义source数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSource = <span class="keyword">new</span> <span class="type">MyTableSource</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sinkTableName = <span class="string">"csvSink"</span></span><br><span class="line">    <span class="comment">// 创建CSV sink 数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册source</span></span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    <span class="comment">// 注册sink</span></span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">"SELECT  "</span> +</span><br><span class="line">      <span class="string">"  region, "</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,"</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv "</span> +</span><br><span class="line">      <span class="string">" FROM mySource "</span> +</span><br><span class="line">      <span class="string">" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region"</span></span><br><span class="line"></span><br><span class="line">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink <span class="string">path :</span> <span class="regexp">/var/</span>folders<span class="regexp">/88/</span><span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn<span class="regexp">/T/</span>csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure><p>Cat 方式查看计算结果，如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/<span class="number">88</span>/<span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:12:00.0</span>,<span class="number">2</span></span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">04:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">04:12:00.0</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure><p>表格化如上结果：</p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job收尾。</p>]]></content>
    
    <summary type="html">
    
      本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="FlinkSQL" scheme="https://gjtmaster.github.io/tags/FlinkSQL/"/>
    
  </entry>
  
  <entry>
    <title>利用ogg实现oracle到kafka的增量数据实时同步</title>
    <link href="https://gjtmaster.github.io/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"/>
    <id>https://gjtmaster.github.io/2018/09/13/利用ogg实现oracle到kafka的增量数据实时同步/</id>
    <published>2018-09-13T06:32:58.000Z</published>
    <updated>2019-08-18T05:45:34.748Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。</p><p>ogg即Oracle GoldenGate是Oracle的同步工具，本文讲如何配置ogg以实现Oracle数据库增量数据实时同步到kafka中，其中同步消息格式为json。</p><p>下面是我的源端和目标端的一些配置信息：</p><table><thead><tr><th align="center">-</th><th align="center">版本</th><th align="center">OGG版本</th><th align="center">ip</th><th align="center">主机名</th></tr></thead><tbody><tr><td align="center">源端</td><td align="center">OracleRelease 11.2.0.1.0</td><td align="center">Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64</td><td align="center">192.168.23.167</td><td align="center">cdh01</td></tr><tr><td align="center">目标端</td><td align="center">kafka_2.11-0.11.0.1</td><td align="center">Oracle GoldenGate for Big Data 12.3.0.1.0 on Linux x86-64</td><td align="center">192.168.23.168</td><td align="center">cdh02</td></tr></tbody></table><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>注意：源端和目标端的文件不一样，目标端需要下载Oracle GoldenGate for Big Data,源端需要下载Oracle GoldenGate for Oracle具体下载方法见最后的附录截图。</p><p>目标端在<a href="http://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html" target="_blank" rel="noopener">这里</a>查询下载，源端在<a href="https://edelivery.oracle.com/osdc/faces/SoftwareDelivery" target="_blank" rel="noopener">旧版本</a>查询下载。</p><h2 id="源端（Oracle）配置"><a href="#源端（Oracle）配置" class="headerlink" title="源端（Oracle）配置"></a>源端（Oracle）配置</h2><p>注意：源端是创建了oracle用户且安装了oracle数据库，oracle环境变量之前都配置好了</p><p>（后面只要涉及到源端均在oracle用户下操作）</p><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>先建立ogg目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /oracledata/data/ogg</span><br><span class="line">unzip Oracle GoldenGate_11.2.1.0.3.zip</span><br></pre></td></tr></table></figure><p>解压后得到一个tar包，再解压这个tar</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /oracledata/data/ogg</span><br></pre></td></tr></table></figure><h3 id="配置ogg环境变量"><a href="#配置ogg环境变量" class="headerlink" title="配置ogg环境变量"></a>配置ogg环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export OGG_HOME=/oracledata/data/ogg</span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib</span><br><span class="line">export PATH=$OGG_HOME:$PATH</span><br></pre></td></tr></table></figure><p>使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><p>如果命令成功即可进行下一步，不成功请检查前面的步骤。</p><h3 id="oracle打开归档模式"><a href="#oracle打开归档模式" class="headerlink" title="oracle打开归档模式"></a>oracle打开归档模式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以DBA身份连接数据库</span></span><br><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>执行下面的命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><p>若显示如下，则说明当前未开启归档模式</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Database <span class="built_in">log</span> <span class="built_in">mode</span>       No Archive <span class="built_in">Mode</span></span><br><span class="line">Automatic archival       Disabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online <span class="built_in">log</span> sequence     <span class="number">12</span></span><br><span class="line">Current <span class="built_in">log</span> sequence       <span class="number">14</span></span><br></pre></td></tr></table></figure><p>手动打开即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 立即关闭数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动实例并加载数据库，但不打开</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup mount</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改数据库为归档模式</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database archivelog;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用自动归档</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system archive <span class="built_in">log</span> start;</span></span><br></pre></td></tr></table></figure><p>再执行一下命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Database log mode       Archive Mode</span><br><span class="line">Automatic archival       Enabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online log sequence     12</span><br><span class="line">Next log sequence to archive   14</span><br><span class="line">Current log sequence       14</span><br></pre></td></tr></table></figure><p>可以看到为Enabled，则成功打开归档模式。</p><h3 id="Oracle打开日志相关"><a href="#Oracle打开日志相关" class="headerlink" title="Oracle打开日志相关"></a>Oracle打开日志相关</h3><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">NO</span>     <span class="literal">NO</span></span><br></pre></td></tr></table></figure><p>若为NO，则需要通过命令修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database force logging;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database add supplemental <span class="built_in">log</span> data;</span></span><br></pre></td></tr></table></figure><p>再查看一下为YES即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">YES</span>    <span class="literal">YES</span></span><br></pre></td></tr></table></figure><p>上述操作只是开启了最小补充日志，如果要抽取全部字段需要开启全列补充日志,否则值为null的字段不会在抽取日志中显示！！！</p><p>补充日志开启命令参考：<a href="https://blog.csdn.net/aaron8219/article/details/16825963" target="_blank" rel="noopener">https://blog.csdn.net/aaron8219/article/details/16825963</a></p><p><strong>注：开启全列补充日志会导致磁盘快速增长，LGWR进程繁忙，不建议使用。大家可根据自己的情况使用。</strong></p><p>查看数据库是否开启了全列补充日志</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;  </span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">NO</span></span><br></pre></td></tr></table></figure><p>若未开启可以通过以下命令开启。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; alter database add supplemental log data(all) columns;</span><br><span class="line"></span><br><span class="line">Database altered.</span><br><span class="line"></span><br><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">YES</span></span><br></pre></td></tr></table></figure><h3 id="oracle创建复制用户"><a href="#oracle创建复制用户" class="headerlink" title="oracle创建复制用户"></a>oracle创建复制用户</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="regexp">/oracledata/</span>data<span class="regexp">/tablespace/</span>dbsrv2</span><br></pre></td></tr></table></figure><p>然后执行下面sql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create tablespace oggtbs datafile <span class="string">'/oracledata/data/tablespace/dbsrv2/oggtbs01.dbf'</span> size 1000M autoextend on;</span></span><br><span class="line">控制台显示的内容：Tablespace created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">  create user ogg identified by 123456 default tablespace oggtbs;</span></span><br><span class="line">控制台显示的内容：User created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to ogg;</span></span><br><span class="line">控制台显示的内容：Grant succeeded.</span><br></pre></td></tr></table></figure><h3 id="OGG初始化"><a href="#OGG初始化" class="headerlink" title="OGG初始化"></a>OGG初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">GGSCI (cdh01) 1&gt; create subdirs</span><br></pre></td></tr></table></figure><p>控制台显示的内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Creating subdirectories under current directory /oracledata/data/ogg</span><br><span class="line"></span><br><span class="line">Parameter files                /oracledata/data/ogg/dirprm: created</span><br><span class="line">Report files                   /oracledata/data/ogg/dirrpt: created</span><br><span class="line">Checkpoint files               /oracledata/data/ogg/dirchk: created</span><br><span class="line">Process status files           /oracledata/data/ogg/dirpcs: created</span><br><span class="line">SQL script files               /oracledata/data/ogg/dirsql: created</span><br><span class="line">Database definitions files     /oracledata/data/ogg/dirdef: created</span><br><span class="line">Extract data files             /oracledata/data/ogg/dirdat: created</span><br><span class="line">Temporary files                /oracledata/data/ogg/dirtmp: created</span><br><span class="line">Stdout files                   /oracledata/data/ogg/dirout: created</span><br></pre></td></tr></table></figure><h3 id="Oracle创建测试表"><a href="#Oracle创建测试表" class="headerlink" title="Oracle创建测试表"></a>Oracle创建测试表</h3><p>创建一个用户,在该用户下新建测试表，用户名、密码、表名均为 test_ogg。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create user test_ogg identified by test_ogg default tablespace users;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> conn test_ogg/test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create table test_ogg(id int,name varchar(20),sex varchar(4),primary key(id));</span></span><br></pre></td></tr></table></figure><h2 id="目标端（kafka）配置"><a href="#目标端（kafka）配置" class="headerlink" title="目标端（kafka）配置"></a>目标端（kafka）配置</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -<span class="selector-tag">p</span> /data/apps/ogg</span><br><span class="line">unzip OGG_BigData_12.<span class="number">3.0</span>.<span class="number">1.0</span>_Release.zip</span><br><span class="line">tar xf ggs_Adapters_Linux_x64<span class="selector-class">.tar</span>  -C /data/apps/ogg</span><br></pre></td></tr></table></figure><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> /etc/<span class="keyword">profile</span></span><br></pre></td></tr></table></figure><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/opt/java</span><span class="regexp">/jdk1.8.0_211</span></span><br><span class="line"><span class="regexp">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line"></span><br><span class="line">export OGG_HOME=<span class="regexp">/data/apps</span><span class="regexp">/ogg</span></span><br><span class="line"><span class="regexp">export LD_LIBRARY_PATH=$JAVA_HOME/jre</span><span class="regexp">/lib/amd</span>64:$JAVA_HOME/jre/<span class="class"><span class="keyword">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">libjsig</span>.<span class="title">so</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>/<span class="title">libjvm</span>.<span class="title">so</span>:$<span class="title">OGG_HOME</span>/<span class="title">lib</span></span></span><br><span class="line">export PATH=$<span class="symbol">OGG_HOME:</span>$PATH</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure><p>同样测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><h3 id="初始化目录"><a href="#初始化目录" class="headerlink" title="初始化目录"></a>初始化目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create subdirs</span><br></pre></td></tr></table></figure><h2 id="OGG源端配置"><a href="#OGG源端配置" class="headerlink" title="OGG源端配置"></a>OGG源端配置</h2><p>Oracle实时传输到Hadoop集群（HDFS，<a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">Hive</a>，Kafka等）的基本原理如图：<br><img src="https://mc.qcloudimg.com/static/img/dd548277beb41f51d0e5914dccda9134/image.png" alt="img"><br>根据如上原理，配置大概分为如下步骤：源端目标端配置ogg管理器（mgr）；源端配置extract进程进行Oracle日志抓取；源端配置pump进程传输抓取内容到目标端；目标端配置replicate进程复制日志到Kafka集群。</p><h3 id="配置OGG的全局变量"><a href="#配置OGG的全局变量" class="headerlink" title="配置OGG的全局变量"></a>配置OGG的全局变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 1&gt; dblogin userid ogg password 123456</span><br><span class="line">控制台显示的内容：Successfully logged into database.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 2&gt; edit param ./globals</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">oggschema ogg</span></span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr"><a href="#配置管理器mgr" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">3</span>&gt; edit param mgr</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><p>说明：PORT即mgr的默认监听端口；</p><p>DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；</p><p>AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；</p><p>PURGEOLDEXTRACTS即TRAIL文件的定期清理</p><h3 id="添加复制表"><a href="#添加复制表" class="headerlink" title="添加复制表"></a>添加复制表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 4&gt; add trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo data enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 5&gt; info trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo log data is enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line">控制台显示的内容：Columns supplementally logged for table TEST_OGG.TEST_OGG: ID</span><br></pre></td></tr></table></figure><h3 id="配置extract进程"><a href="#配置extract进程" class="headerlink" title="配置extract进程"></a>配置extract进程</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">6</span>&gt; edit param extkafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">extract</span> extkafka</span><br><span class="line"><span class="attribute">dynamicresolution</span></span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (ORACLE_SID = <span class="string">"dbsrv2"</span>)</span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (NLS_LANG = <span class="string">"american_america.AL32UTF8"</span>)</span><br><span class="line"><span class="attribute">GETUPDATEBEFORES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSDELETES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSUPDATES</span></span><br><span class="line"><span class="attribute">userid</span> ogg,password 123456</span><br><span class="line"><span class="attribute">exttrail</span> /oracledata/data/ogg/dirdat/to</span><br><span class="line"><span class="attribute">table</span> test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>dynamicresolution动态解析；</p><p>SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；</p><p>userid ogg,password 123456即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；</p><p>table即复制表的表名，支持*通配，必须以;结尾</p><p>添加extract进程：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) <span class="number">7</span>&gt; <span class="keyword">add </span><span class="keyword">extract </span><span class="keyword">extkafka,tranlog,begin </span>now</span><br><span class="line">控制台显示的内容：<span class="keyword">EXTRACT </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure><p>(注：若报错</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ERROR: </span>Could not create checkpoint file /opt/ogg/dirchk/EXTKAFKA.cpe (error 2, No such file or directory).</span><br></pre></td></tr></table></figure><p>执行下面的命令再重新添加即可。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="keyword">subdirs</span></span><br></pre></td></tr></table></figure><p>)</p><p>添加trail文件的定义与extract进程绑定：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 8&gt; <span class="builtin-name">add</span> exttrail /oracledata/data/ogg/dirdat/<span class="keyword">to</span>,extract extkafka</span><br><span class="line">控制台显示的内容：EXTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置pump进程"><a href="#配置pump进程" class="headerlink" title="配置pump进程"></a>配置pump进程</h3><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">9</span>&gt; edit param pukafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">extract pukafka</span><br><span class="line">passthru</span><br><span class="line">dynamicresolution</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">rmthost <span class="number">192.168</span><span class="number">.23</span><span class="number">.168</span> mgrport <span class="number">7809</span></span><br><span class="line">rmttrail /data/apps/ogg/dirdat/to</span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；</p><p>dynamicresolution动态解析；</p><p>userid ogg,password ogg即OGG连接Oracle数据库的帐号密码</p><p>rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口；</p><p>rmttrail即目标端trail文件存储位置以及名称。<strong>(注意，这里很容易犯错！！！注意是目标端的路径！！！)</strong></p><p>分别将本地trail文件和目标端的trail文件绑定到extract进程：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 10&gt; <span class="builtin-name">add</span> extract pukafka,exttrailsource /oracledata/data/ogg/dirdat/<span class="keyword">to</span></span><br><span class="line">控制台显示的内容：EXTRACT added.</span><br><span class="line">GGSCI (cdh01) 11&gt; <span class="builtin-name">add</span> rmttrail /data/apps/ogg/dirdat/<span class="keyword">to</span>,extract pukafka</span><br><span class="line">控制台显示的内容：RMTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置defgen文件"><a href="#配置defgen文件" class="headerlink" title="配置defgen文件"></a>配置defgen文件</h3><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">12</span>&gt; edit param test_ogg</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defsfile <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>退出GGSCI</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">13</span>&gt; quit</span><br></pre></td></tr></table></figure><p>进行OGG主目录下执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">./defgen paramfile dirprm/test_ogg.prm</span><br></pre></td></tr></table></figure><p>输出以下内容则执行成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">        Oracle GoldenGate Table Definition Generator for Oracle</span><br><span class="line"> Version 11.2.1.0.3 14400833 OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258</span><br><span class="line">   Linux, x64, 64bit (optimized), Oracle 11g on Aug 23 2012 16:58:29</span><br><span class="line"> </span><br><span class="line">Copyright (C) 1995, 2012, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    Starting at 2018-05-23 05:03:04</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">Operating System Version:</span><br><span class="line">Linux</span><br><span class="line">Version #1 SMP Wed Apr 12 15:04:24 UTC 2017, Release 3.10.0-514.16.1.el7.x86_64</span><br><span class="line">Node: ambari.master.com</span><br><span class="line">Machine: x86_64</span><br><span class="line">                         soft limit   hard limit</span><br><span class="line">Address Space Size   :    unlimited    unlimited</span><br><span class="line">Heap Size            :    unlimited    unlimited</span><br><span class="line">File Size            :    unlimited    unlimited</span><br><span class="line">CPU Time             :    unlimited    unlimited</span><br><span class="line"></span><br><span class="line">Process id: 13126</span><br><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">**            Running with the following parameters                  **</span><br><span class="line">***********************************************************************</span><br><span class="line">defsfile /opt/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password ***</span><br><span class="line">table test_ogg.test_ogg;</span><br><span class="line">Retrieving definition for TEST_OGG.TEST_OGG</span><br><span class="line"></span><br><span class="line">Definitions generated for 1 table in /oracledata/data/ogg/dirdef/test_ogg.test_ogg</span><br></pre></td></tr></table></figure><p>将生成的/oracledata/data/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /oracledata/data/ogg/dirdef/test_ogg.test_ogg root@cdh02:/data/apps/ogg/dirdef/</span><br></pre></td></tr></table></figure><h2 id="OGG目标端配置"><a href="#OGG目标端配置" class="headerlink" title="OGG目标端配置"></a>OGG目标端配置</h2><h3 id="开启kafka服务"><a href="#开启kafka服务" class="headerlink" title="开启kafka服务"></a>开启kafka服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Zookeeper</span></span><br><span class="line">/data/apps/apache-zookeeper-3.5.5-bin/bin/zkServer.sh start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启Kafka</span></span><br><span class="line">/data/apps/kafka_2.11-0.11.0.1/bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr-1"><a href="#配置管理器mgr-1" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt;  edit param mgr</span><br><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><h3 id="配置checkpoint"><a href="#配置checkpoint" class="headerlink" title="配置checkpoint"></a>配置checkpoint</h3><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 2&gt; edit param ./GLOBALS</span><br><span class="line">CHECKPOINTTABLE test_ogg.checkpoint</span><br></pre></td></tr></table></figure><h3 id="配置replicate进程"><a href="#配置replicate进程" class="headerlink" title="配置replicate进程"></a>配置replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 3&gt; edit param rekafka</span><br><span class="line">REPLICAT rekafka</span><br><span class="line">sourcedefs /data/apps/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props</span><br><span class="line">REPORTCOUNT EVERY 1 MINUTES, RATE </span><br><span class="line">GROUPTRANSOPS 10000</span><br><span class="line">MAP test_ogg.test_ogg, TARGET test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>REPLICATE rekafka定义rep进程名称；</p><p>sourcedefs即在4.6中在源服务器上做的表映射文件；</p><p>TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props；</p><p>REPORTCOUNT即复制任务的报告生成频率；</p><p>GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；</p><p>MAP即源端与目标端的映射关系</p><h3 id="配置kafka-props"><a href="#配置kafka-props" class="headerlink" title="配置kafka.props"></a>配置kafka.props</h3><p><strong>本环节配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirprm/</span><br><span class="line">vim kafka.props</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> handler类型</span></span><br><span class="line">gg.handlerlist=kafkahandler</span><br><span class="line">gg.handler.kafkahandler.type=kafka</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kafka生产者配置文件</span></span><br><span class="line">gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的topic名称，无需手动创建</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gg.handler.kafkahandler.topicMappingTemplate=test_ogg（新版topicName属性的设置方式）</span></span><br><span class="line">gg.handler.kafkahandler.topicName=test_ogg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 传输文件的格式，支持json，xml等</span></span><br><span class="line">gg.handler.kafkahandler.format=json</span><br><span class="line">gg.handler.kafkahandler.format.insertOpKey = I  </span><br><span class="line">gg.handler.kafkahandler.format.updateOpKey = U  </span><br><span class="line">gg.handler.kafkahandler.format.deleteOpKey = D</span><br><span class="line">gg.handler.kafkahandler.format.truncateOpKey=T</span><br><span class="line">gg.handler.kafkahandler.format.includePrimaryKeys=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> OGG <span class="keyword">for</span> Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次</span></span><br><span class="line">gg.handler.kafkahandler.mode=op</span><br><span class="line"><span class="meta">#</span><span class="bash"> 类路径</span></span><br><span class="line">gg.classpath=dirprm/:/data/apps/kafka_2.11-0.11.0.1/libs/*:/data/apps/ogg/:/data/apps/ogg/lib/*</span><br></pre></td></tr></table></figure><p>紧接着创建Kafka生产者配置文件：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim custom_kafk<span class="built_in">a_producer</span>.properties</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kafkabroker的地址</span></span><br><span class="line">bootstrap.servers=cdh01:9092,cdh02:9092,cdh03:9092</span><br><span class="line">acks=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 压缩类型</span></span><br><span class="line">compression.type=gzip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重连延时</span></span><br><span class="line">reconnect.backoff.ms=1000</span><br><span class="line">value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">batch.size=102400</span><br><span class="line">linger.ms=10000</span><br></pre></td></tr></table></figure><p><strong>配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><h3 id="添加trail文件到replicate进程"><a href="#添加trail文件到replicate进程" class="headerlink" title="添加trail文件到replicate进程"></a>添加trail文件到replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt; add replicat rekafka exttrail /data/apps/ogg/dirdat/to,checkpointtable test_ogg.checkpoint</span><br><span class="line">控制台显示的内容：REPLICAT added.</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="启动所有进程"><a href="#启动所有进程" class="headerlink" title="启动所有进程"></a>启动所有进程</h3><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。<br>启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。<br>全部需要在ogg目录下执行ggsci目录进入ogg命令行。<br>源端依次是</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> extkafka</span><br><span class="line"><span class="literal">start</span> pukafka</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> rekafka</span><br></pre></td></tr></table></figure><p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功<br>源端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.master.com) 5&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">EXTRACT     RUNNING     EXTKAFKA    04:50:21      00:00:03    </span><br><span class="line">EXTRACT     RUNNING     PUKAFKA     00:00:00      00:00:03</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.slave1.com) 3&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">REPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</span><br></pre></td></tr></table></figure><h3 id="异常解决"><a href="#异常解决" class="headerlink" title="异常解决"></a>异常解决</h3><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ggser.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>或者ogg命令行,以rekafka进程为例</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh02</span>) <span class="number">2</span>&gt; view report rekafka</span><br></pre></td></tr></table></figure><h3 id="测试同步更新效果"><a href="#测试同步更新效果" class="headerlink" title="测试同步更新效果"></a>测试同步更新效果</h3><p>现在源端执行sql语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conn test_ogg/test_ogg</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_ogg <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'test'</span>,<span class="literal">null</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">update</span> test_ogg <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'zhangsan'</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">delete</span> test_ogg <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p>查看源端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-rw-rw- <span class="number">1</span> oracle oinstall <span class="number">1464</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看目标端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-r----- <span class="number">1</span> root root <span class="number">1504</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看kafka是否自动建立对应的主题</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.<span class="keyword">sh</span> --<span class="keyword">list</span> --zookeeper localhos<span class="variable">t:2181</span></span><br></pre></td></tr></table></figure><p>在列表中显示有test_ogg则表示没问题<br>通过消费者看是否有同步消息</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.44</span><span class="number">.129</span>:<span class="number">9092</span> --topic test_ogg --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"I"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:04:39.001362"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:04:44.610000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001246"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"U"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:05:44.000411"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:05:50.764000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001541"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;,<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"D"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:06:33.000312"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:06:39.845000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001670"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>before代表操作之前的数据，after代表操作后的数据，现在已经可以从kafka获取到同步的json数据了，后面可以用SparkStreaming和Storm等解析然后存到hadoop等大数据平台里</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想通配整个库的话，只需要把上面的配置所有表名改为*，如test_ogg<span class="selector-class">.test_ogg</span> 改为 test_ogg.*,但是kafka的topic不能通配，所以需要把所有表的数据放在一个topic，后面再用程序解析表名即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">若后期因业务需要导致表结构发生改变，需要重新生成源端表结构的defgen定义文件，再把定义文件通过scp放到目标端。defgen文件的作用是，记录了源端的表结构，然后我们再把这个文件放到目标端，在目标端应用SQL时就能根据defgen文件与目标端表结构，来做一定的转换。</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/purpleraintear/p/6071038.html" target="_blank" rel="noopener">基于OGG的Oracle与Hadoop集群准实时同步介绍</a></p><p><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">Fusion Middleware Integrating Oracle GoldenGate for Big Data</a></p>]]></content>
    
    <summary type="html">
    
      Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。
    
    </summary>
    
      <category term="数据的导入导出" scheme="https://gjtmaster.github.io/categories/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/"/>
    
    
      <category term="Oracle" scheme="https://gjtmaster.github.io/tags/Oracle/"/>
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="ogg" scheme="https://gjtmaster.github.io/tags/ogg/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群基于延时指标进行性能调优</title>
    <link href="https://gjtmaster.github.io/2018/09/10/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%BB%B6%E6%97%B6%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/10/kafka集群基于延时指标进行性能调优/</id>
    <published>2018-09-10T10:35:54.000Z</published>
    <updated>2019-09-24T05:27:35.492Z</updated>
    
    <content type="html"><![CDATA[<h2 id="缓解延时症状"><a href="#缓解延时症状" class="headerlink" title="缓解延时症状"></a>缓解延时症状</h2><ul><li>不要创建具备超多分区数的topic，因为适当增加分区数的确可以提升TPS，但是大量的分区的存在对于延时确实损害，分区数越多，broker端就需要越长的时间实现follower与leader的同步。</li><li>适当增加Broker数来分散分区数，从而限制了单台Broker上的总分区数，减轻了单台Broker端分区访问压力。</li><li>增加num.replica.fetchers参数提升broker端的I/O并行度。该值增加了broker端follower副本从leader副本处获取的最大线程数。默认值是1。</li><li>和调节吞吐量相反，调优延时要求producer端尽量不要缓存消息，而是尽快地把消息发送出去。</li></ul><h2 id="实际可行性调优"><a href="#实际可行性调优" class="headerlink" title="实际可行性调优"></a>实际可行性调优</h2><ul><li>producer端尽量不要缓存消息，而是尽快的将消息发送出去，又重复了一遍。</li><li>设置linger.ms参数设置为0，不要让producer花费额外的时间去缓存待发送的消息。</li><li>压缩是一种时间换空间的一种优化方式，为了减少网络I/O传输量，推荐关闭。compression.type=none。</li><li>Producer端的acks参数也是优化延时的重要手段之一，leader.broker越快的发送response，producer端就能越快地发送下一批消息。该参数默认值是1，实际上已经是非常好的设置了。</li><li>调整leader副本返回的最小数据量来间接的影响Consumer端的延时，即fetch.min.bytes参数值。默认值是1，已经是非常好的选择。</li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p>broker端</p><ul><li>适当增加num.replica.fetchers(broker端follower副本从leader副本处获取的最大线程数)</li><li>避免创建过多的topic分区。</li></ul><p>producer端</p><ul><li>设置linger.ms=0</li><li>设置compression.type=none</li><li>设置acks=1或者0。</li></ul><p>consumer端</p><ul><li>设置fetch.min.bytes=1</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记住，调优延时要求producer端尽量不要缓存消息，而是尽快地把消息发送出去。又重复了一遍。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka集群基于延时指标进行性能调优的方案。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群基于吞吐量指标进行性能调优</title>
    <link href="https://gjtmaster.github.io/2018/09/09/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%90%9E%E5%90%90%E9%87%8F%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/09/kafka集群基于吞吐量指标进行性能调优/</id>
    <published>2018-09-09T11:28:24.000Z</published>
    <updated>2019-09-24T05:27:35.480Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><ul><li>性能<ul><li>吞吐量：broker或者clients应用程序每秒能处理多少字节（或消息）</li><li>延时：通常指Producer发送到broker端持久化保存消息之间的时间间隔。</li></ul></li><li>可用性：系统和组件正常运行的概率或时间比率，业界一般N个9来量化可用性，比如年度4个9表示53分钟（365* 24 * 60 * 0.01%=53分钟）</li><li>持久性：已提交的消息需要被持久化到Broker端底层的文件系统的物理日志而不能丢失。</li></ul><h2 id="kafka-基础设施优化"><a href="#kafka-基础设施优化" class="headerlink" title="kafka 基础设施优化"></a>kafka 基础设施优化</h2><ul><li><p>磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10TB数据，单个broker能存储 2TB，那么我们需要的最小Kafka集群大小5 个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p></li><li><p>文件系统在文件被访问、创建、修改等的时候会记录文件的一些时间戳，比如：文件创建时间（ctime）、最近一次修改时间（mtime）和最近一次访问时间（atime）。默认情况下，atime的更新会有一次读操作，这会产生大量的磁盘读写，然而atime</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mount -o noatime</span></span><br></pre></td></tr></table></figure></li><li><p>绝大多数运行在Linux上的软件都是基于EXT4构建和测试的，因此兼容性上EXT4要优于其他文件系统。</p></li><li><p>作为高性能的64位日志文件系统（journaling file system），XFS表现出高性能，高伸缩性，特别适应于生产服务器，特别是大文件（30+GB）操作。很多存储类的应用都适合选择XFS作为底层文件系统。</p></li><li><p>计算机的内存分为虚拟内存和物理内存。物理内存是真实的内存，虚拟内存是用磁盘来代替内存。并通过swap机制实现磁盘到物理内存的加载和替换,这里面用到的磁盘我们称为swap磁盘。在写文件的时候，Linux首先将数据写入没有被使用的内存中，这些内存被叫做内存页（page cache）。然后读的时候，Linux会优先从page cache中查找，如果找不到就会从硬盘中查找。当物理内存使用达到一定的比例后，Linux就会使用进行swap，使用磁盘作为虚拟内存。通过cat /proc/sys/vm/swappiness可以看到swap参数。这个参数表示虚拟内存中swap磁盘占了多少百分比。0表示最大限度的使用内存，100表示尽量使用swap磁盘。系统默认的参数是60，当物理内存使用率达到40%，就会频繁进行swap，影响系统性能，推荐将vm.swappiness 设置为较低的值1。最终我设置为10，因为我们的机器的内存还是比较小的，只有40G，设置的太小，可能会影响到虚拟内存的使用吧。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">临时修改：sudo sysctl vm.<span class="attribute">swappiness</span>=N</span><br><span class="line">永久修改（/etc/sysctl.conf）：vm.<span class="attribute">swappiness</span>=N</span><br></pre></td></tr></table></figure></li></ul><h2 id="kafka-JVM设置"><a href="#kafka-JVM设置" class="headerlink" title="kafka JVM设置"></a>kafka JVM设置</h2><ul><li><p>PermGen space : 全称是Permanent Generation  space，是指内存的永久保存区域，为什么会发生内存溢出？这一部分用于存放Class和Meta的信息, Class在被 Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,所以如果你的APP会LOAD很多CLASS的话,就很可能出现PermGen space错误。</p></li><li><p>G1算法将堆划分为若干个区域（Region），它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。</p></li><li><p>在G1中，还有一种特殊的区域，叫Humongous区域。 如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。这些巨型对象，默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。</p><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1677483bb9496fd9?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p></li><li><p>G1采用内存分区(Region)的思路，将内存划分为一个个相等大小的内存分区，回收时则以分区为单位进行回收，存活的对象复制到另一个空闲分区中。由于都是以相等大小的分区为单位进行操作，因此G1天然就是一种压缩方案(局部压缩)；</p></li><li><p>G1虽然也是分代收集器，但整个内存分区不存在物理上的年轻代与老年代的区别，也不需要完全独立的survivor(to space)堆做复制准备。G1只有逻辑上的分代概念，或者说每个分区都可能随G1的运行在不同代之间前后切换；</p></li><li><p>G1的收集都是STW的，但年轻代和老年代的收集界限比较模糊，采用了混合(mixed)收集的方式。即每次收集既可能只收集年轻代分区(年轻代收集)，也可能在收集年轻代的同时，包含部分老年代分区(混合收集)，这样即使堆内存很大时，也可以限制收集范围，从而降低停顿。</p></li><li><p>堆内存中一个Region的大小可以通过-XX:G1HeapRegionSize参数指定，大小区间只能是1M、2M、4M、8M、16M和32M，总之是2的幂次方，如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实践大小，默认把堆内存按照2048份均分，最后得到一个合理的大小。</p></li><li><p>JVM 8 metaSpace 诞生了: 不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小：-XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。</p></li><li><p>-XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集-XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集</p></li><li><p>XX:MaxGCPauseMillis=n : 设置最大GC停顿时间(GC pause time)指标(target). 这是一个软性指标(soft goal)， JVM 会尽量去达成这个目标。</p></li><li><p>InitiatingHeapOccupancyPercent： 整个堆栈使用达到百分之多少的时候，启动GC周期. 基于整个堆，不仅仅是其中的某个代的占用情况，G1根据这个值来判断是否要触发GC周期, 0表示一直都在GC，默认值是45（即45%慢了，或者说占用了)</p><p>   -XX:G1NewSizePercent    新生代最小值，默认值5%</p></li><li><p>-XX:G1MaxNewSizePercent 新生代最大值，默认值60%</p></li><li><p>MetaspaceSize: 这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># export JAVA_HOME=/usr/java/jdk1.8.0_51</span></span><br><span class="line"><span class="comment"># export KAFKA_HEAP_OPTS="</span></span><br><span class="line">-Xmx6g -Xms6g -XX:<span class="attribute">MetaspaceSize</span>=128m </span><br><span class="line">-XX:<span class="attribute">MaxMetaspaceSize</span>=128m -XX:+UseG1GC -XX:<span class="attribute">MaxGCPauseMillis</span>=20</span><br><span class="line">-XX:<span class="attribute">InitiatingHeapOccupancyPercent</span>=35 -XX:+<span class="attribute">G1HeapRegionSize</span>=16M</span><br><span class="line">-XX:<span class="attribute">MinMetaspaceFreeRatio</span>=50 <span class="string">"</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="文件调优"><a href="#文件调优" class="headerlink" title="文件调优"></a>文件调优</h2><ul><li><p>若出现”too many files open”错误时，就需要为Broker所在的机器调优最大文件部署符上限。文件句柄个数计算方法如下：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker上可能最大分区数*（每个分区平均数据量/平均的日志段大小+<span class="number">3</span>）其中<span class="number">3</span> 表示索引文件个数，</span><br><span class="line">假设<span class="number">20</span>个分区，分割分区总数据量为<span class="number">100</span>GB，每一个日志段大小是<span class="number">1</span>GB，那么这台机器最大文件部署符大小应该是：</span><br><span class="line">            <span class="number">20</span>*（<span class="number">100</span>/<span class="number">1</span>+<span class="number">3</span>）=<span class="number">2060</span></span><br><span class="line">因此该参数一定要设置足够大。比如<span class="number">100000</span></span><br></pre></td></tr></table></figure></li><li><p>若出现”java.lang.OutOfMemoryError:Map failed的严重错误，主要原因是大量创建topic将极大消耗操作系统内存，用户可以适当调整vm.max.map.count参数，具体方法如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sbin/sysctl -w vm<span class="selector-class">.max_map_count</span> = N ，该参数默认值是<span class="number">65535</span>，可以考虑线上环境设置更大的值。</span><br></pre></td></tr></table></figure></li></ul><h2 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h2><p>Broker端；</p><ul><li>适当增加num.replica.fetchers，但不要超过CPU核数，该值控制了broker端follower副本从leader副本处获取消息的最大线程数。默认值是1，表明follower副本只使用一个线程实时拉取leader处的最新消息。对于设置了acks=all的producer而言，主要延时可能耽误在follower与leader同步过程，所以增加该值可以缩短同步的时间，从而间接的提升Producer端的TPS。</li><li>调优GC避免经常性的Full GC。老版本过渡依赖Zookeeper来表征Consumer还活着，若GC时间过长，会导致Zookeeper会话过期，kafka会立即对group进行rebalance。新版本上已经弃用了对Zookeeper的依赖。</li></ul><p>Producer端：</p><ul><li>适当增加batch.size，比如100-512KB。</li><li><a href="https://link.juejin.im?target=http%3A%2F%2Fxn--linger-2e4j090brvo308i.ms" target="_blank" rel="noopener">适当增加linger.ms</a>，比如10-100毫秒Producer端是批量发送消息的，更大的batch size可以令更多的消息封装进同一个请求，故发送给broker端的总请求数就会减少，此举可以减轻Producer的负载，也降低了broker端的CPU请求处理开销。而更大的linger.ms使producer等待更长的时间才发送消息，这样就能够缓存更多的消息填满batch，从而从整体上提升TPS。但是延时肯定增加了。</li><li>设置压缩类型compression.type=lz4，目前支持的压缩方式有：GZIP，Snappy，LZ4，在CPU资源丰富的情况下compression.type=lz4的效果是最好的。</li><li>acks=0或1</li><li>retries=0</li><li>若多线程共享Produer或分区数很多时，增加buffer.memory。因为每一个分区都会占用一个batch.size。</li></ul><p>consumer端</p><ul><li>采用多Consumer实例，共同消费多分区数据，这些实例共享相同的group.id</li><li>增加fetch.min.bytes，比如10000，表征每次leader副本所在的broker所返回的最小数据量来间接影响TPS,通过增加该值，Kafka会为每一个FETCH请求的response填入更多的数据。</li></ul><h2 id="悖论存在（分区数越多，TPS越高）"><a href="#悖论存在（分区数越多，TPS越高）" class="headerlink" title="悖论存在（分区数越多，TPS越高）"></a>悖论存在（分区数越多，TPS越高）</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/16774b2833e5a82e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>Kafka基本的并行单元就是分区，producer在设计时实现了能够同时向多个分区发送消息，从而这些消息最终写入到多个broker上，最终可以被多个consumer同时消费。通常来说，分区数越多，TPS越高</li><li>分区数越多，占用的缓冲区越多，因为缓冲区是以分区为粒度的，所以Server/clients端将占用更多的内存。</li><li>每一个分区在底层文件系统中都有专属目录，除了3个索引文件外还保存日志段文件，会占用大量的文件句柄。</li><li>每一个分区都有若干个副本保存在不同的broker上，当broker挂掉后，因为需要Controller进行处理leader变更请求，该处理线程是单线程，疯了吧，亚历山大。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>num.replica.fetchers倒是一个新颖的选手，可以好好试试，acks重点关注，其他都中规中矩。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka集群基于吞吐量指标进行性能调优的方案。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群基于可用性指标进行性能调优</title>
    <link href="https://gjtmaster.github.io/2018/09/07/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%8F%AF%E7%94%A8%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/07/kafka集群基于可用性指标进行性能调优/</id>
    <published>2018-09-07T10:38:54.000Z</published>
    <updated>2019-09-23T10:20:16.023Z</updated>
    
    <content type="html"><![CDATA[<h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><ul><li>可用性反映了kafka集群应对崩溃的能力，调优可用性就是为了让Kafka更快的从崩溃中恢复过来。</li></ul><h2 id="Topic角度看问题"><a href="#Topic角度看问题" class="headerlink" title="Topic角度看问题"></a>Topic角度看问题</h2><ul><li>Controller就是broker，主要负责副本分配方案和分区上线的工作（根据副本的分配方案）。</li><li>副本状态机制</li><li>分区状态机制</li><li>注意Topic的分区数越多，一旦分区的leader副本所在broker发生崩溃，就需要进行leader的选举，虽然Leader的选举时间很短，大概几毫秒，但是Controller处理请求时单线程的，controller通过Zookeeper可实时侦测broker状态。一旦有broker挂掉了，controller可立即感知并为受影响分区选举新的leader，但是在新的分配方案出来后，发布到各个broker进行元数据更新就要浪费网络I/O了。</li><li>建议分区不要过大，可能会影响可用性。</li></ul><h2 id="Producer角度看问题"><a href="#Producer角度看问题" class="headerlink" title="Producer角度看问题"></a>Producer角度看问题</h2><ul><li>首推参数是acks，当acks设置为all时，broker端参数min.insync.replicas的效果会影响Producer端的可用性。该参数越大，kafka会强制进行越多的follower副本同步写入日志，当出现ISR缩减到min.insync.replicas值时，producer会停止对特定分区发送消息，从而影响了可用性。</li></ul><h2 id="Broker角度看问题"><a href="#Broker角度看问题" class="headerlink" title="Broker角度看问题"></a>Broker角度看问题</h2><ul><li>Broker端高可用性直接的表现形式就是broker崩溃，崩溃之后leader选举有两种：1：从ISR中选择。2：通过unclean.leader.election.enable值决定是否从ISR中选择。第二种情况会出现数据丢失的风险。</li><li>另一个参数是broker崩溃恢复调优，即num.recovery.threads.per.data.dir。场景是这样的，当broker从崩溃中重启恢复后，broker会扫描并加载底层的分区数据执行清理和与其他broker保持同步。这一工程被称为日志加载和日志恢复。默认情况下是单线程的。假设某个Broker的log.dirs配置了10个日志目录，那么单线程可能就吭哧吭哧扫描加载，太慢了。实际使用中，建议配置为日志的log.dirs磁盘数量。</li></ul><h2 id="Consumer角度看问题"><a href="#Consumer角度看问题" class="headerlink" title="Consumer角度看问题"></a>Consumer角度看问题</h2><ul><li><p>对于Consumer而言，高可用性主要是基于组管理的consumer group来体现的，当group下某个或某些consumer实例“挂了”，group的coordinator能够自动检测出这种崩溃并及时的开启rebalance，进而将崩溃的消费分区分配到其他存活的consumer上。</p></li><li><p>consumer端参数session.timeout.ms就是定义了能检测出failure的时间间隔。若要实现高可用，必须设置较低的值，比如5-10秒。一旦超过该值，就会开启新一轮的reblance。</p></li><li><p>消息处理时间参数很牛，<code>max.poll.interval.ms</code>，参数设置了consumer实例所需要的消息处理时间，一旦超过该值，就是高负荷状态，此时consumer将停止发送心跳，并告知coordinator要离开group。<code>消费者在创建时会有一个属性max.poll.interval.ms</code>，该属性意思为kafka消费者在每一轮poll()调用之间的最大延迟,消费者在获取更多记录之前可以空闲的时间量的上限。如果此超时时间期满之前poll()没有被再次调用，则消费者被视为失败，并且分组将重新平衡，以便将分区重新分配给别的成员</p></li><li><p><code>hearbeat.interval.ms</code> 当coordinator决定开启新一轮的reblance时，他会把这个决定以REBALANCE_IN_PROCESS异常的形式塞进consumer心跳的请求响应中，这样就可以飞快的感知到新的分区分配方案。</p></li><li><p>kafka调优经典的独白：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">上线两个月都没有问题，为什么最近突然出现问题了。我想肯定是业务系统有什么动作，我就去问了一个下，</span><br><span class="line">果然头一天风控系统kafka挂掉了，并进行了数据重推，导致了数据阻塞。但是我又想即使阻塞了也会慢慢消费掉啊，不应</span><br><span class="line">该报错呀。后来我看了一下kafka官网上的参数介绍，发现max.poll.records默认是<span class="number">2147483647</span> </span><br><span class="line">（<span class="number">0.10</span><span class="number">.0</span><span class="number">.1</span>版本），也就是kafka里面有多少poll多少，如果消费者拿到的这些数据在制定时间内消费不完，就会手动提交</span><br><span class="line">失败，数据就会回滚到kafka中，会发生重复消费的情况。如此循环，数据就会越堆越多。后来咨询了公司的kafka大神，他</span><br><span class="line">说我的kafka版本跟他的集群版本不一样让我升级kafka版本。于是我就升级到了<span class="number">0.10</span><span class="number">.2</span><span class="number">.1</span>，查阅官网发现这个版本的max.po</span><br><span class="line">ll.records默认是<span class="number">500</span>，可能kafka开发团队也意识到了这个问题。并且这个版本多了一个max.poll.interval.ms这个参数，</span><br><span class="line">默认是<span class="number">300</span>s。这个参数的大概意思就是kafka消费者在一次poll内，业务处理时间不能超过这个时间。后来升级了kafka版本</span><br><span class="line">，把max.poll.records改成了<span class="number">50</span>个之后，上了一次线，准备观察一下。上完线已经晚上<span class="number">9</span>点了，于是就打卡回家了，明天看</span><br><span class="line">结果。第二天早起满心欢喜准备看结果，以为会解决这个问题，谁曾想还是堆积。我的天，思来想去，也想不出哪里有问题</span><br><span class="line">。于是就把处理各个业务的代码前后执行时间打印出来看一下，添加代码，提交上线。然后观察结果，发现大部分时间都用</span><br><span class="line">在数据库IO上了，并且执行时间很慢，大部分都是<span class="number">2</span>s。于是想可能刚上线的时候数据量比较小，查询比较快，现在数据量大</span><br><span class="line">了，就比较慢了。当时脑子里第一想法就是看了一下常用查询字段有没有添加索引，一看没有，然后马上添加索引。加完索</span><br><span class="line">引观察了一下，处理速度提高了好几倍。虽然单条业务处理的快了， </span><br><span class="line">但是堆积还存在，后来发现，业务系统大概<span class="number">1</span>s推送<span class="number">3</span>、<span class="number">4</span>条数据，但是我kafka现在是单线程消费，速度大概也是这么多。再</span><br><span class="line">加上之前的堆积，所以消费还是很慢。于是业务改成多线程消费，利用线程池，开启了<span class="number">10</span>个线程，上线观察。几分钟就消费</span><br><span class="line">完了。大功告成，此时此刻，心里舒坦了好多。不容易呀！</span><br></pre></td></tr></table></figure></li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p><strong>Broker端</strong></p><ul><li>避免过多分区</li><li>设置unclean.leader.election.enable=true（为了可用性，数据丢失不考虑）</li><li>设置min.insync.replicas=1（减轻同步压力）</li><li>设置num.recovery.threads.per.data.dir=broker 日志的log.dirs磁盘数</li></ul><p><strong>Producer端</strong></p><ul><li>设置acks=1，设置为all时，遵循min.insync.replicas=1</li></ul><p><strong>consumer端</strong></p><ul><li>设置session.timeout.ms为较低的值，比如100000</li><li>设置max.poll.interval.ms消息平均处理时间，可适当调大。</li><li>设置max.poll.records和max.partition.fetch.bytes减少消息处理总时长，避免频繁的rebalance。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>num.recovery.threads.per.data.dir以及max.partition.fetch.bytes以及max.poll.records重点关注一下。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka集群基于可用性指标进行性能调优的方案。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka集群基于持久性指标进行性能调优</title>
    <link href="https://gjtmaster.github.io/2018/09/06/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E6%8C%81%E4%B9%85%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/06/kafka集群基于持久性指标进行性能调优/</id>
    <published>2018-09-06T10:24:22.000Z</published>
    <updated>2019-09-23T10:20:16.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h2><ul><li>持久性定义了kafka集群中消息不容易丢失的程度，持久性越高表明Kafka越不会丢失消息。</li><li>持久性通常是通过冗余来实现，而kafka实现冗余的手段就是备份机制，该备份机制保证了Kafka的每一个消息都会被保存到多台Broker上。</li></ul><h2 id="实际可行性调优"><a href="#实际可行性调优" class="headerlink" title="实际可行性调优"></a>实际可行性调优</h2><ul><li>分区数和副本因子由broker端参数num.partitions和default.replication.factor指定，这两个参数默认值都是1。</li><li>禁止kafka自动创建topic，可以参数设置auto.create.topics.enable=false。</li><li>新版本已经把consumer端位移信息的存储从Zookeeper转移到了内部的topic <strong>consumer_offsets中，注意这个也同样会受到default.replication.factor参数影响。0.11.0.0要求</strong>consumer_offsets创建时必须满足default.replication.factor，若Broker小于default.replication.factor会直接报错。</li><li>若要达到最高的持久性，必须设置acks=all(或者acks=-1)，即强制leader副本等待的ISR中所有副本都响应了某条消息后发送response给producer。</li><li>producer发送失败后，会进行重试机制，比如网络临时抖动。</li><li>当Producer重试后，待发送的消息可能已经成功，假如数据已经落盘写入日志，在发送response给Producer时出现了故障，重试时就会再次发送同一条消息，也就出现了重复的消息。自从0.11.0.0版本开始，kafka提供了幂等性Producer，实现了精确一次的处理语义。幂等性Producer保证同一条消息只会被broker写入一次。启动这样设置：enable.idempotence=true。</li><li>消息乱序处理机制，producer依次发送消息m1和m2，若m1写入失败但m2写入成功，一旦重试m1，那么m1就位于m2的后面了。如果对消息的顺序要求比较严格的话，max.in.flight.requests.per.connection=1来规避这个问题。该参数目的保证单个producer连接上在任意某个时刻只处理一个请求。</li><li>当某个Broker出现出现崩溃时，controller会自动检测出宕机的broker并为该Broker上的所有分区重新选举leader，选择的范围是从ISR副本中挑选。若ISR副本所在的broker全部宕机，Kafka就会从非ISR副本中选举leader。这种情况下就会造成不同步和消息丢失。是否允许Unclean选举，借助unclean.leader.election.enable参数控制，0.11.0.0版本默认是false。</li><li>min.insync.replicas，若成功写入消息时则必须等待响应完成的最少ISR副本，该参数是在acks=all时才会生效。</li><li>自0.10.0.0版本开始，kafka仿照Hadoop为broker引入机架属性信息（rack）,该参数由broker.rack设定。Kafka集群在后台会收集所有的机架信息仔仔创建分区时将分区分散到不同的机架上。</li><li>日志持久化：两个重要参数log.flush.interval.ms和log.flush.interval.message。前者指定了Kafka多长时间执行一次消息“落盘”，后者是写入多少消息后执行一次消息“落盘”。默认情况下，log.flush.interval.ms是空而将log.flush.interval.message设置为Long.MAX_VALUE，这个表示Kafka实际上不会自动执行消息的“冲刷”操作，事实上这也是kafka的设计初衷。即把消息同步到磁盘的工作交由操作系统来完成，由操作系统控制页缓存数据到物理文件的同步。但是若用户希望每一条消息都冲刷一次，及时持久化，可以设置log.flush.interval.message=1。</li><li>提交位移的持久化，实际应用中设置auto.commit.enable=false，同时用户需要使用commitSync方法来提交位移，而非commitAsync方法。</li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p>​         <strong>broker端</strong></p><ul><li><p>设置unclean.leader.election.enable=false(0.11.0.0之前版本)</p></li><li><p>设置auto.create.topics.enable=false</p></li><li><p>设置replication.factor=3，min.insync.replicas=1</p></li><li><p>设置default.replication.factor=3</p></li><li><p>设置broker.rack属性分散分区数据到不同的机架。</p></li><li><p>设置log.flush.interval.ms和log.flush.interval.message为一个较小的值。</p><p>​</p><p><strong>producer端</strong></p></li><li><p>设置acks=all</p></li><li><p>设置retries为一个较大的值，比如10-30。</p></li><li><p>设置max.in.flight.request.per.connection=1</p></li><li><p>设置enable.idempotence=true</p><p>​</p></li></ul><p>​         <strong>consumer端</strong></p><ul><li>设置auto.commit.enable=false</li><li>消息消费成功后，调用commitSync提交位移。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>enable.idempotence=true 和max.in.flight.request.per.connection=1以及broker.rack属性比较新颖，值得一试。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka集群基于持久性指标进行性能调优的方案。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka消费者Consumer参数设置及调优</title>
    <link href="https://gjtmaster.github.io/2018/09/04/kafka%E6%B6%88%E8%B4%B9%E8%80%85Consumer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/04/kafka消费者Consumer参数设置及调优/</id>
    <published>2018-09-04T06:11:12.000Z</published>
    <updated>2019-09-20T09:57:46.534Z</updated>
    
    <content type="html"><![CDATA[<h2 id="消息的接收-gt-基于Consumer-Group"><a href="#消息的接收-gt-基于Consumer-Group" class="headerlink" title="消息的接收-&gt;基于Consumer Group"></a>消息的接收-&gt;基于Consumer Group</h2><p>Consumer Group 主要用于实现高伸缩性，高容错性的Consumer机制。因此，消息的接收是基于Consumer Group 的。组内多个Consumer实例可以同时读取Kafka消息，同一时刻一条消息只能被一个消费者消费，而且一旦某一个consumer “挂了”， Consumer Group 会立即将已经崩溃的Consumer负责的分区转交给其他Consumer来负责。从而保证 Consumer Group 能够正常工作。</p><h2 id="位移保存-gt-基于Consumer-Group"><a href="#位移保存-gt-基于Consumer-Group" class="headerlink" title="位移保存-&gt;基于Consumer Group"></a>位移保存-&gt;基于Consumer Group</h2><p>说来奇怪，位移保存是基于Consumer Group，同时引入检查点模式，定期实现offset的持久化。</p><h2 id="位移提交-gt-抛弃ZooKeeper"><a href="#位移提交-gt-抛弃ZooKeeper" class="headerlink" title="位移提交-&gt;抛弃ZooKeeper"></a>位移提交-&gt;抛弃ZooKeeper</h2><p>Consumer会定期向kafka集群汇报自己消费数据的进度，这一过程叫做位移的提交。这一过程已经抛弃Zookeeper，因为Zookeeper只是一个协调服务组件，不能作为存储组件，高并发的读取势必造成Zk的压力。</p><ul><li>新版本位移提交是在kafka内部维护了一个内部Topic(_consumer_offsets)。</li><li>在kafka内部日志目录下面，总共有50个文件夹，每一个文件夹包含日志文件和索引文件。日志文件主要是K-V结构，（group.id,topic,分区号）。</li><li>假设线上有很多的consumer和ConsumerGroup，通过对group.id做Hash求模运算，这50个文件夹就可以分散同时位移提交的压力。</li></ul><h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><h3 id="自动提交位移"><a href="#自动提交位移" class="headerlink" title="自动提交位移"></a>自动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="手动提交位移"><a href="#手动提交位移" class="headerlink" title="手动提交位移"></a>手动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">200</span>;</span><br><span class="line">List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        buffer.add(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">        insertIntoDb(buffer);</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">        buffer.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="kafka-Consumer参数设置"><a href="#kafka-Consumer参数设置" class="headerlink" title="kafka Consumer参数设置"></a>kafka Consumer参数设置</h2><ul><li>consumer.poll(1000) 重要参数</li><li>新版本的Consumer的Poll方法使用了类似于Select I/O机制，因此所有相关事件（包括reblance，消息获取等）都发生在一个事件循环之中。</li><li>1000是一个超时时间，一旦拿到足够多的数据（参数设置），consumer.poll(1000)会立即返回 ConsumerRecords&lt;String, String&gt; records。</li><li>如果没有拿到足够多的数据，会阻塞1000ms，但不会超过1000ms就会返回。</li></ul><hr><ul><li>session. timeout. ms &lt;=  coordinator检测失败的时间</li><li>默认值是10s</li><li>该参数是 Consumer Group 主动检测 (组内成员consummer)崩溃的时间间隔。若设置10min，那么Consumer Group的管理者（group coordinator）可能需要10分钟才能感受到。太漫长了是吧。</li></ul><hr><ul><li>max. poll. interval. ms &lt;= 处理逻辑最大时间</li><li>这个参数是0.10.1.0版本后新增的，可能很多地方看不到喔。这个参数需要根据实际业务处理时间进行设置，一旦Consumer处理不过来，就会被踢出Consumer Group。</li><li>注意：如果业务平均处理逻辑为1分钟，那么max. poll. interval. ms需要设置稍微大于1分钟即可，但是session. timeout. ms可以设置小一点（如10s），用于快速检测Consumer崩溃。</li></ul><hr><ul><li>auto.offset.reset</li><li>该属性指定了消费者在读取一个没有偏移量或者偏移量无效（消费者长时间失效当前的偏移量已经过时并且被删除了）的分区的情况下，应该作何处理，默认值是latest，也就是从最新记录读取数据（消费者启动之后生成的记录），另一个值是earliest，意思是在偏移量无效的情况下，消费者从起始位置开始读取数据。</li></ul><hr><ul><li>enable.auto.commit</li><li>对于精确到一次的语义，最好手动提交位移</li></ul><hr><ul><li>fetch.max.bytes</li><li>单次获取数据的最大消息数。</li></ul><hr><ul><li>max.poll.records  &lt;=  吞吐量</li><li>单次poll调用返回的最大消息数，如果处理逻辑很轻量，可以适当提高该值。</li><li>一次从kafka中poll出来的数据条数,max.poll.records条数据需要在在session.timeout.ms这个时间内处理完</li><li>默认值为500</li></ul><hr><ul><li>heartbeat. interval. ms &lt;= 居然拖家带口</li><li>heartbeat心跳主要用于沟通交流，及时返回请求响应。这个时间间隔真是越快越好。因为一旦出现reblance,那么就会将新的分配方案或者通知重新加入group的命令放进心跳响应中。</li></ul><hr><ul><li>connection. max. idle. ms &lt;= socket连接</li><li>kafka会定期的关闭空闲Socket连接。默认是9分钟。如果不在乎这些资源开销，推荐把这些参数值为-1，即不关闭这些空闲连接。</li></ul><hr><ul><li>request. timeout. ms</li><li>这个配置控制一次请求响应的最长等待时间。如果在超时时间内未得到响应，kafka要么重发这条消息，要么超过重试次数的情况下直接置为失败。</li><li>消息发送的最长等待时间.需大于session.timeout.ms这个时间</li></ul><hr><ul><li>fetch.min.bytes</li><li>server发送到消费端的最小数据，若是不满足这个数值则会等待直到满足指定大小。默认为1表示立即接收。</li></ul><hr><ul><li><p>fetch.wait.max.ms</p></li><li><p>若是不满足fetch.min.bytes时，等待消费端请求的最长等待时间</p></li></ul><hr><ul><li>0.11 新功能</li><li>空消费组延时rebalance，主要在server.properties文件配置</li><li>group.initial.rebalance.delay.ms&lt;= ，防止成员加入请求后本应立即开启的rebalance</li><li>对于用户来说，这个改进最直接的效果就是新增了一个broker配置：group.initial.rebalance.delay.ms，</li><li>默认是3秒钟。</li><li>主要作用是让coordinator推迟空消费组接收到成员加入请求后本应立即开启的rebalance。在实际使用时，假设你预估你的所有consumer组成员加入需要在10s内完成，那么你就可以设置该参数=10000。</li></ul><h2 id="线上采坑"><a href="#线上采坑" class="headerlink" title="线上采坑"></a>线上采坑</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.consumer</span><span class="selector-class">.CommitFailedException</span>:</span><br><span class="line"> Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. </span><br><span class="line">This means that the <span class="selector-tag">time</span> between subsequent calls to poll() was longer than the configured session<span class="selector-class">.timeout</span><span class="selector-class">.ms</span>, which typically implies that the poll loop is spending too much <span class="selector-tag">time</span> message processing. </span><br><span class="line">You can <span class="selector-tag">address</span> this either by increasing the session timeout or by reducing the maximum size of batches returned <span class="keyword">in</span> poll() with max<span class="selector-class">.poll</span><span class="selector-class">.records</span>. [com<span class="selector-class">.bonc</span><span class="selector-class">.framework</span><span class="selector-class">.server</span><span class="selector-class">.kafka</span><span class="selector-class">.consumer</span><span class="selector-class">.ConsumerLoop</span>]</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure><p>基于最新版本10，注意此版本session. timeout. ms 与max.poll.interval.ms进行功能分离了。</p><ul><li>可以发现频繁reblance，并伴随者重复性消费，这是一个很严重的问题，就是处理逻辑过重,max.poll. interval.ms过小导致。发生的原因就是 poll（）的循环调用时间过长，出现了处理超时。此时只用调大max.poll. interval.ms ，调小max.poll.records即可，同时要把request. timeout. ms设置大于max.poll. interval.ms</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化会继续，暂时把核心放在request. timeout. ms, max. poll. interval. ms，max.poll.records 上，避免因为处理逻辑过重，导致Consumer被频繁的踢出Consumer group。</p>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka消费者Consumer参数设置及调优。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka生产者Producer参数设置及调优</title>
    <link href="https://gjtmaster.github.io/2018/09/03/kafka%E7%94%9F%E4%BA%A7%E8%80%85Producer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
    <id>https://gjtmaster.github.io/2018/09/03/kafka生产者Producer参数设置及调优/</id>
    <published>2018-09-03T07:21:55.000Z</published>
    <updated>2019-09-20T09:57:46.509Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Producer核心工作流程"><a href="#Producer核心工作流程" class="headerlink" title="Producer核心工作流程"></a>Producer核心工作流程</h2><ul><li>Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。</li><li>进行序列化后，发送给Partioner，由Partioner确定目标分区后，发送到Producer程序中的一块内存缓冲区中。</li><li>Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次，统一发送给对应的broker中。</li></ul><h2 id="producer-主要参数设置"><a href="#producer-主要参数设置" class="headerlink" title="producer 主要参数设置"></a>producer 主要参数设置</h2><h3 id="producer-参数acks-设置（无数据丢失）"><a href="#producer-参数acks-设置（无数据丢失）" class="headerlink" title="producer 参数acks 设置（无数据丢失）"></a>producer 参数acks 设置（无数据丢失）</h3><p>在消息被认为是“已提交”之前，producer需要leader确认的produce请求的应答数。该参数用于控制消息的持久性，目前提供了3个取值：</p><p>acks = 0: 表示produce请求立即返回，不需要等待leader的任何确认。这种方案有最高的吞吐率，但是不保证消息是否真的发送成功。</p><p>acks = -1: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为produce请求成功。这种方案提供最高的消息持久性保证，但是理论上吞吐率也是最差的。</p><p>acks = 1: 表示leader副本必须应答此produce请求并写入消息到本地日志，之后produce请求被认为成功。如果此时leader副本应答请求之后挂掉了，消息会丢失。这是个较好的方案，提供了不错的持久性保证和吞吐。</p><p><strong>商业环境推荐：</strong></p><p>如果要较高的持久性要求以及无数据丢失的需求，设置acks = -1。其他情况下设置acks = 1</p><h3 id="producer参数-buffer-memory-设置（吞吐量）"><a href="#producer参数-buffer-memory-设置（吞吐量）" class="headerlink" title="producer参数 buffer.memory 设置（吞吐量）"></a>producer参数 buffer.memory 设置（吞吐量）</h3><p>该参数用于指定Producer端用于缓存消息的缓冲区大小，单位为字节，默认值为：33554432合计为32M。kafka采用的是异步发送的消息架构，prducer启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由一个专属线程负责从缓冲区读取消息进行真正的发送。</p><p><strong>商业环境推荐：</strong></p><ul><li>消息持续发送过程中，当缓冲区被填满后，producer立即进入阻塞状态直到空闲内存被释放出来，这段时间不能超过max.blocks.ms设置的值，一旦超过，producer则会抛出TimeoutException 异常，因为Producer是线程安全的，若一直报TimeoutException，需要考虑调高buffer.memory 了。</li><li>用户在使用多个线程共享kafka producer时，很容易把 buffer.memory 打满。</li></ul><h3 id="producer参数-compression-type-设置（lZ4）"><a href="#producer参数-compression-type-设置（lZ4）" class="headerlink" title="producer参数 compression.type 设置（lZ4）"></a>producer参数 compression.type 设置（lZ4）</h3><p>producer压缩器，目前支持none（不压缩），gzip，snappy和lz4。</p><p><strong>商业环境推荐：</strong></p><p>基于公司的大数据平台，试验过目前lz4的效果最好。当然2016年8月，FaceBook开源了Ztandard。官网测试： Ztandard压缩率为2.8，snappy为2.091，LZ4 为2.101 。</p><h3 id="producer参数-retries设置-注意消息乱序-EOS"><a href="#producer参数-retries设置-注意消息乱序-EOS" class="headerlink" title="producer参数 retries设置(注意消息乱序,EOS)"></a>producer参数 retries设置(注意消息乱序,EOS)</h3><p>producer重试的次数设置。重试时producer会重新发送之前由于瞬时原因出现失败的消息。瞬时失败的原因可能包括：元数据信息失效、副本数量不足、超时、位移越界或未知分区等。倘若设置了retries &gt; 0，那么这些情况下producer会尝试重试。</p><p><strong>商业环境推荐：</strong></p><ul><li>producer还有个参数：max.in.flight.requests.per.connection。如果设置该参数大约1，那么设置retries就有可能造成发送消息的乱序。</li><li>版本为0.11.0.1的kafka已经支持”精确到一次的语义”，因此消息的重试不会造成消息的重复发送。</li></ul><h3 id="producer参数batch-size设置-吞吐量和延时性能"><a href="#producer参数batch-size设置-吞吐量和延时性能" class="headerlink" title="producer参数batch.size设置(吞吐量和延时性能)"></a>producer参数batch.size设置(吞吐量和延时性能)</h3><p>producer都是按照batch进行发送的，因此batch大小的选择对于producer性能至关重要。producer会把发往同一分区的多条消息封装进一个batch中，当batch满了后，producer才会把消息发送出去。但是也不一定等到满了，这和另外一个参数linger.ms有关。默认值为16K，合计为16384.</p><p><strong>商业环境推荐：</strong></p><ul><li>batch 越小，producer的吞吐量越低，越大，吞吐量越大。</li></ul><h3 id="producer参数linger-ms设置-吞吐量和延时性能"><a href="#producer参数linger-ms设置-吞吐量和延时性能" class="headerlink" title="producer参数linger.ms设置(吞吐量和延时性能)"></a>producer参数linger.ms设置(吞吐量和延时性能)</h3><p>producer是按照batch进行发送的，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。</p><p><strong>商业环境推荐：</strong></p><ul><li>为了减少了网络IO，提升了整体的TPS。假设设置linger.ms=5，表示producer请求可能会延时5ms才会被发送。</li></ul><h3 id="producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能"><a href="#producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能" class="headerlink" title="producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)"></a>producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)</h3><p>producer的IO线程在单个Socket连接上能够发送未应答produce请求的最大数量。增加此值应该可以增加IO线程的吞吐量，从而整体上提升producer的性能。不过就像之前说的如果开启了重试机制，那么设置该参数大于1的话有可能造成消息的乱序。</p><p><strong>商业环境推荐：</strong></p><ul><li>默认值5是一个比较好的起始点,如果发现producer的瓶颈在IO线程，同时各个broker端负载不高，那么可以尝试适当增加该值.</li></ul><ul><li>过大增加该参数会造成producer的整体内存负担，同时还可能造成不必要的锁竞争反而会降低TPS</li></ul><h2 id="Java客户端"><a href="#Java客户端" class="headerlink" title="Java客户端"></a>Java客户端</h2><p>KafkaProducer(org.apache.kafka.clients.producer.KafkaProducer)是一个用于向kafka集群发送数据的Java客户端。该Java客户端是线程安全的，多个线程可以共享同一个producer实例，而且这通常比在多个线程中每个线程创建一个实例速度要快些。本文介绍的内容来自于kafka官方文档，详情参见<a href="http://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="noopener">KafkaProducer</a> </p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">package com.test.kafkaProducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.PartitionInfo;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class TestProducer &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, <span class="string">"192.168.137.200:9092"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="comment">//生产者发送消息 </span></span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"mytopic"</span>;</span><br><span class="line">        Producer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; procuder = <span class="keyword">new</span> KafkaProducer&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(props);</span><br><span class="line">        <span class="built_in">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">String</span> value = <span class="string">"value_"</span> + i;</span><br><span class="line">            ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; msg = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(topic, value);</span><br><span class="line">            procuder.send(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//列出topic的相关信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = <span class="keyword">new</span> ArrayList&lt;PartitionInfo&gt;() ;</span><br><span class="line">        partitions = procuder.partitionsFor(topic);</span><br><span class="line">        <span class="built_in">for</span>(PartitionInfo p:partitions)</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.<span class="built_in">println</span>(p);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.<span class="built_in">println</span>(<span class="string">"send message over."</span>);</span><br><span class="line">        procuder.<span class="built_in">close</span>(<span class="number">100</span>,TimeUnit.MILLISECONDS);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>producer包含一个用于保存待发送消息的缓冲池，缓冲池中消息是还没来得及传输到kafka集群的消息。位于底层的kafka I/O线程负责将缓冲池中的消息转换成请求发送到集群。如果在结束produce时，没有调用close()方法，那么这些资源会发生泄露。<br>用于建立消费者的相关参数说明及其默认值参见producerconfigs，此处对代码中用到的几个参数进行解释：<br>bootstrap.servers:用于初始化时建立链接到kafka集群，以host:port形式，多个以逗号分隔host1:port1,host2:port2；<br>acks:生产者需要server端在接收到消息后，进行反馈确认的尺度，主要用于消息的可靠性传输；acks=0表示生产者不需要来自server的确认；acks=1表示server端将消息保存后即可发送ack，而不必等到其他follower角色的都收到了该消息；acks=all(or acks=-1)意味着server端将等待所有的副本都被接收后才发送确认。<br>retries:生产者发送失败后，重试的次数<br>batch.size:当多条消息发送到同一个partition时，该值控制生产者批量发送消息的大小，批量发送可以减少生产者到服务端的请求数，有助于提高客户端和服务端的性能。<br>linger.ms:默认情况下缓冲区的消息会被立即发送到服务端，即使缓冲区的空间并没有被用完。可以将该值设置为大于0的值，这样发送者将等待一段时间后，再向服务端发送请求，以实现每次请求可以尽可能多的发送批量消息。<br>batch.size和linger.ms是两种实现让客户端每次请求尽可能多的发送消息的机制，它们可以并存使用，并不冲突。<br>buffer.memory:生产者缓冲区的大小，保存的是还未来得及发送到server端的消息，如果生产者的发送速度大于消息被提交到server端的速度，该缓冲区将被耗尽。<br>key.serializer,value.serializer说明了使用何种序列化方式将用户提供的key和vaule值序列化成字节。</p></blockquote><p><strong>kafka客户端的API</strong>  </p><p><strong>KafkaProducer对象实例化方法</strong>,可以使用map形式的键值对或者Properties对象来配置客户端的属性</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keySerializer:发送数据key值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> *valueSerializer:发送数据value值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>消息发送方法send()</strong></p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *record:key-value形式的待发送数据</span></span><br><span class="line"><span class="comment"> *callback:到发送的消息被borker端确认后的回调函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record</span>)</span>; <span class="comment">// Equivalent to send(record, null)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record,Callback callback</span>)</span>;</span><br></pre></td></tr></table></figure><p>send方法负责将缓冲池中的消息异步的发送到broker的指定topic中。异步发送是指，方法将消息存储到底层待发送的I/O缓存后，将立即返回，这可以实现并行无阻塞的发送更多消息。send方法的返回值是RecordMetadata类型，它含有消息将被投递的partition信息，该条消息的offset，以及时间戳。<br>因为send返回的是Future对象，因此在该对象上调用get()方法将阻塞，直到相关的发送请求完成并返回元数据信息；或者在发送时抛出异常而退出。<br>阻塞发送的方法如下：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">String</span> <span class="built_in">key</span> = <span class="string">"Key"</span>;</span><br><span class="line"><span class="keyword">String</span> value = <span class="string">"Value"</span>;</span><br><span class="line">ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(<span class="built_in">key</span>, value);</span><br><span class="line">producer.send(record).<span class="built_in">get</span>();</span><br></pre></td></tr></table></figure><p>可以充分利用回调函数和异步发送方式来确认消息发送的进度:</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(<span class="string">"the-topic"</span>, <span class="built_in">key</span>, value);</span><br><span class="line">producer.send(myRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> onCompletion(RecordMetadata metadata, Exception e) &#123;</span><br><span class="line">                        <span class="keyword">if</span>(e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            System.out.<span class="built_in">println</span>(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset());</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure><p><strong>flush</strong> </p><p>立即发送缓存数据</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure><p>调用该方法将使得缓冲区的所有消息被立即发送（即使linger.ms参数被设置为大于0），且会阻塞直到这些相关消息的发送请求完成。flush方法的前置条件是：之前发送的所有消息请求已经完成。一个请求被视为完成是指：根据acks参数配置项收到了相应的确认，或者发送中抛出异常失败了。<br>下面的例子展示了从一个topic消费后发到另一个topic，flush方法在此非常有用，它提供了一种方便的方法来确保之前发送的消息确实已经完成了：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">for</span>(ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record: consumer.poll(<span class="number">100</span>))</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"my-topic"</span>, record.key(), record.value());</span><br><span class="line">producer.<span class="built_in">flush</span>();  <span class="comment">//将缓冲区的消息立即发送</span></span><br><span class="line">consumer.commit(); <span class="comment">//消费者手动确认消费进度</span></span><br></pre></td></tr></table></figure><p><strong>partitionsFor</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取指定topic的partition元数据信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="built_in">List</span>&lt;PartitionInfo&gt; partitionsFor(<span class="built_in">String</span> topic);</span><br></pre></td></tr></table></figure><p><strong>close</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//关闭producer，方法将被阻塞，直到之前的发送请求已经完成</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;<span class="comment">//  equivalent to close(Long.MAX_VALUE, TimeUnit.MILLISECONDS)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout,TimeUnit timeUnit)</span></span>; <span class="comment">//同上，方法将等待timeout时长，以让未完成的请求完成发送</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka生产者Producer参数设置及调优。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka生产者和消费者吞吐量测试</title>
    <link href="https://gjtmaster.github.io/2018/09/02/kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%9E%E5%90%90%E9%87%8F%E6%B5%8B%E8%AF%95/"/>
    <id>https://gjtmaster.github.io/2018/09/02/kafka生产者和消费者吞吐量测试/</id>
    <published>2018-09-02T07:11:47.000Z</published>
    <updated>2019-09-18T10:27:41.305Z</updated>
    
    <content type="html"><![CDATA[<h2 id="测试集群配置："><a href="#测试集群配置：" class="headerlink" title="测试集群配置："></a>测试集群配置：</h2><blockquote><p><strong>三台4核cpu/8G内存/50G硬盘的CentOS7机器</strong></p><p><strong>Kafka版本：Kafka_2.11-0.11.0.1</strong></p></blockquote><h2 id="kafka生产者吞吐量测试"><a href="#kafka生产者吞吐量测试" class="headerlink" title="kafka生产者吞吐量测试"></a>kafka生产者吞吐量测试</h2><p><strong>使用kafka-producer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic test --num-records 500000 --record-size 200 --throughput -1 --producer-props bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092 acks=1</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">500000</span> records sent ,<span class="number">180701.120347</span> records/sec (<span class="number">34.47</span> MB/sec),<span class="number">315.17</span> ms/avg latency ,<span class="number">982.00</span> ms max latency ,<span class="number">167</span>ms <span class="number">50</span>h ,<span class="number">902</span>ms <span class="number">95</span>th ,<span class="number">969</span> ms <span class="number">99</span>h, <span class="number">981</span>ms <span class="number">99.9</span>th</span><br></pre></td></tr></table></figure><blockquote><p>测试结果显示：</p><p>发送了500000条消息，kafka 的平均吞吐量是34.47 MB/sec ，即占用275Mb/s左右的带宽，平均每秒发送180701条消息。</p><p>平均延时为315 ms，最大延时为982 ms，</p><p>发送50%的消息需要167ms，发送95%的消息需要902ms，发送99%的消息需要969ms，发送99.9%的消息需要981ms。</p></blockquote><h2 id="kafka消费者吞吐量测试"><a href="#kafka消费者吞吐量测试" class="headerlink" title="kafka消费者吞吐量测试"></a>kafka消费者吞吐量测试</h2><p><strong>使用kafka-consumer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --message-size 200 --messages 500000 --topic test</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:41</span><span class="selector-pseudo">:388</span>, 2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:43</span><span class="selector-pseudo">:906</span>, 95<span class="selector-class">.3674</span>, 37<span class="selector-class">.8743</span>, 500000, 198570<span class="selector-class">.2939</span></span><br></pre></td></tr></table></figure><blockquote><p>看测试结果显示：</p><p>消费了95.3674MB消息，吞吐量为37.8743MB/s,也即303Mb/s，</p><p>消费了500000条消息，吞吐量为198570条/s</p></blockquote>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka测试集群上生产者和消费者吞吐量测试的整个流程。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>kafka生产环境规划</title>
    <link href="https://gjtmaster.github.io/2018/09/01/kafka%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92/"/>
    <id>https://gjtmaster.github.io/2018/09/01/kafka生产环境规划/</id>
    <published>2018-09-01T10:17:52.000Z</published>
    <updated>2019-09-19T10:01:01.083Z</updated>
    
    <content type="html"><![CDATA[<h2 id="操作系统选型"><a href="#操作系统选型" class="headerlink" title="操作系统选型"></a>操作系统选型</h2><p>因为kafka服务端代码是Scala语言开发的，因此属于JVM系的大数据框架，目前部署最多的3类操作系统主要由Linux ，OS X 和Windows,但是部署在Linux数量最多，为什么呢？因为I/O模型的使用和数据网络传输效率两点。</p><ul><li>第一：Kafka新版本的Clients在设计底层网络库时采用了Java的Select模型，而在Linux实现机制是epoll,感兴趣的读者可以查询一下epoll和select的区别，明确一点就是：kafka跑在Linux上效率更高，因为epoll取消了轮询机制，换成了回调机制，当底层连接socket数较多时，可以避免CPU的时间浪费。</li><li>第二：网络传输效率上。kafka需要通过网络和磁盘进行数据传输，而大部分操作系统都是通过Java的FileChannel.transferTo方法实现，而Linux操作系统则会调用sendFile系统调用，也即零拷贝（Zero Copy 技术），避免了数据在内核地址空间和用户程序空间进行重复拷贝。</li></ul><h2 id="磁盘类型规划"><a href="#磁盘类型规划" class="headerlink" title="磁盘类型规划"></a>磁盘类型规划</h2><ul><li>机械磁盘（HDD）一般机械磁盘寻道时间是毫秒级的，若有大量随机I/O，则将会出现指数级的延迟，但是kafka是顺序读写的，因此对于机械磁盘的性能也是不弱的，所以，基于成本问题可以考虑。</li><li>固态硬盘（SSD）读写速度可观，没有成本问题可以考虑。</li><li>JBOD (Just Bunch Of Disks ) 经济实惠的方案，对数据安全级别不是非常非常高的情况下可以采用，建议用户在Broker服务器上设置多个日志路径，每个路径挂载在不同磁盘上，可以极大提升并发的日志写入速度。</li><li>RAID 磁盘阵列常见的RAID是RAID10，或者称为（RAID 1+0） 这种磁盘阵列结合了磁盘镜像和磁盘带化技术来保护数据，因为使用了磁盘镜像技术，使用率只有50%，注意，LinkedIn公司采用的就是RAID作为存储来提供服务的。那么弊端在什么地方呢？如果Kafka副本数量设置为3，那么实际上数据将存在6倍的冗余数据，利用率实在太低。因此，LinkedIn正在计划更改方案为JBOD.</li></ul><h2 id="磁盘容量规划"><a href="#磁盘容量规划" class="headerlink" title="磁盘容量规划"></a>磁盘容量规划</h2><p>假设每天大约能够产生一亿条消息，假设副本replica设置为2 （其实我们设置为3），数据留存时间为1周，平均每条上报事件消息为1K左右，那么每天产生的消息总量为：1亿 乘 2 乘  1K 除以 1000 除以 1000 =200G磁盘。预留10%的磁盘空间，为210G。一周大约为1.5T。采用压缩，平均压缩比为0.5，整体磁盘容量为0.75T。关联因素主要有：</p><ul><li>新增消息数</li><li>副本数</li><li>是否启用压缩</li><li>消息大小</li><li>消息保留时间</li></ul><h2 id="内存容量规划"><a href="#内存容量规划" class="headerlink" title="内存容量规划"></a>内存容量规划</h2><p>kafka对于内存的使用，并不过多依赖JVM 内存,而是更多的依赖操作系统的页缓存，consumer若命中页缓存，则不用消耗物理I/O操作。一般情况下，java堆内存的使用属于朝生夕灭的，很快会被GC,一般情况下，不会超过6G，对于16G内存的机器，文件系统page cache 可以达到10-14GB。</p><ul><li>怎么设计page cache，可以设置为单个日志段文件大小，若日志段为10G,那么页缓存应该至少设计为10G以上。</li><li>堆内存最好不要超过6G。</li></ul><h2 id="CPU选择规划"><a href="#CPU选择规划" class="headerlink" title="CPU选择规划"></a>CPU选择规划</h2><p>kafka不属于计算密集型系统，因此CPU核数够多就可以，而不必追求时钟频率，因此核数选择最好大于8。</p><h2 id="网络带宽决定Broker数量"><a href="#网络带宽决定Broker数量" class="headerlink" title="网络带宽决定Broker数量"></a>网络带宽决定Broker数量</h2><p>带宽主要有1Gb/s 和10 Gb/s 。我们可以称为千兆位网络和万兆位网络。举例如下：我们的系统一天每小时都要处理1Tb的数据，我们选择1Gb/b带宽，那么需要选择多少机器呢？</p><ul><li>假设网络带宽kafka专用，且分配给kafka服务器70%带宽，那么单台Borker带宽就是710Mb/s，但是万一出现突发流量问题，很容易把网卡打满，因此在降低1/3,也即240Mb/s。因为1小时处理1TTB数据，每秒需要处理292MB,1MB=8Mb，也就是2336Mb数据，那么一小时处理1TB数据至少需要2336/240=10台Broker数据。冗余设计，最终可以定为20台机器。</li></ul><h2 id="典型推荐"><a href="#典型推荐" class="headerlink" title="典型推荐"></a>典型推荐</h2><ul><li>cpu 核数 32</li><li>内存 32GB</li><li>磁盘 3TB 7200转 SAS盘三块</li><li>带宽 1Gb/s</li></ul>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka生产环境规划。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>Kafka0.11版本+Zookeeper-3.4.10集群部署</title>
    <link href="https://gjtmaster.github.io/2018/09/01/Kafka0.11%E7%89%88%E6%9C%AC+Zookeeper-3.4.10%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
    <id>https://gjtmaster.github.io/2018/09/01/Kafka0.11版本+Zookeeper-3.4.10集群部署/</id>
    <published>2018-09-01T05:32:44.000Z</published>
    <updated>2019-09-01T05:03:10.556Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img.shields.io/badge/Author-Joker-green" alt></p><p><img src="https://img.shields.io/badge/Email-gaojintao999%40163.com-blue" alt></p><h2 id="集群环境："><a href="#集群环境：" class="headerlink" title="集群环境："></a>集群环境：</h2><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th></tr></thead><tbody><tr><td align="center">192.168.23.167</td><td align="center">kafka01</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.168</td><td align="center">kafka02</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.169</td><td align="center">kafka03</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr></tbody></table><p><strong>集群安装目录：/data/apps</strong></p><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><h3 id="root用户配置主机映射"><a href="#root用户配置主机映射" class="headerlink" title="root用户配置主机映射"></a>root用户配置主机映射</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.23.167  kafka01</span><br><span class="line">192.168.23.168  kafka02</span><br><span class="line">192.168.23.169  kafka03</span><br></pre></td></tr></table></figure><h3 id="在root用户下新建kafka用户（kafka-admin）"><a href="#在root用户下新建kafka用户（kafka-admin）" class="headerlink" title="在root用户下新建kafka用户（kafka/admin）"></a>在root用户下新建kafka用户（kafka/admin）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">adduser kafka -g kafka</span><br><span class="line">passwd kafka</span><br></pre></td></tr></table></figure><h3 id="在root用户下将apps目录下的用户及用户组均更改为kafka"><a href="#在root用户下将apps目录下的用户及用户组均更改为kafka" class="headerlink" title="在root用户下将apps目录下的用户及用户组均更改为kafka"></a>在root用户下将apps目录下的用户及用户组均更改为kafka</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R kafka:kafka /data/apps/</span><br></pre></td></tr></table></figure><h3 id="切换到kafka用户（之后的操作均在kafka用户下）"><a href="#切换到kafka用户（之后的操作均在kafka用户下）" class="headerlink" title="切换到kafka用户（之后的操作均在kafka用户下）"></a>切换到kafka用户（之后的操作均在kafka用户下）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su kafka</span><br></pre></td></tr></table></figure><h3 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id kafka@kafka01</span><br><span class="line">ssh-copy-id kafka@kafka02</span><br><span class="line">ssh-copy-id kafka@kafka03</span><br></pre></td></tr></table></figure><h3 id="将所有安装资源上传到kafka01节点的kafka用户的家目录下"><a href="#将所有安装资源上传到kafka01节点的kafka用户的家目录下" class="headerlink" title="将所有安装资源上传到kafka01节点的kafka用户的家目录下"></a>将所有安装资源上传到kafka01节点的kafka用户的家目录下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper-3.4.10.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka_2.11-0.11.0.1.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka-manager-1.3.3.22 (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> influxdb-1.7.5.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jmxtrans-270.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grafana-6.0.2-1.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-kafka.sh (文件内容见附录)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-zookeeper.sh (文件内容见附录)</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行zookeeper集群安装脚本"><a href="#在kafka01节点上执行zookeeper集群安装脚本" class="headerlink" title="在kafka01节点上执行zookeeper集群安装脚本"></a>在kafka01节点上执行zookeeper集群安装脚本</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sh</span> ~/install-zookeeper.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行kafka集群安装脚本"><a href="#在kafka01节点上执行kafka集群安装脚本" class="headerlink" title="在kafka01节点上执行kafka集群安装脚本"></a>在kafka01节点上执行kafka集群安装脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ~/install-kafka.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 内容如下：</span></span><br><span class="line">export ZOOKEEPER_HOME=/data/apps/zookeeper-3.4.10</span><br><span class="line">export KAFKA_HOME=/data/apps/kafka_0.11.0.1</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使环境变量生效</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动zookeeper集群"><a href="#在kafka01、02、03节点上执行命令启动zookeeper集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动zookeeper集群"></a>在kafka01、02、03节点上执行命令启动zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动kafka集群"><a href="#在kafka01、02、03节点上执行命令启动kafka集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动kafka集群"></a>在kafka01、02、03节点上执行命令启动kafka集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JMX_PORT=9999 kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure><h3 id="Zookeeper常用命令"><a href="#Zookeeper常用命令" class="headerlink" title="Zookeeper常用命令"></a>Zookeeper常用命令</h3><p>查看znode中的内容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ls</span>  /</span><br></pre></td></tr></table></figure><p>创建普通的节点 </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create</span></span><br></pre></td></tr></table></figure><p>获得节点的信息</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">get</span></span><br></pre></td></tr></table></figure><p>创建临时节点  </p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> -<span class="built_in">e</span></span><br></pre></td></tr></table></figure><p>编号节点： </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create -s</span></span><br></pre></td></tr></table></figure><p> 删除一个节点 </p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span></span><br></pre></td></tr></table></figure><p>递归删除节点</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rmr</span></span><br></pre></td></tr></table></figure><p>修改节点内容</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span></span><br></pre></td></tr></table></figure><p>监听节点  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">get</span> /test watch</span><br></pre></td></tr></table></figure><p>在其他节点进行修改  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> /<span class="built_in">test</span> 555</span><br></pre></td></tr></table></figure><p>监听节点上收到WatchedEvent state:SyncConnected type:NodeDataChanged path:/test</p><h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>关闭Kafka（关闭Kafka之前禁止关闭Zookeeper）</p><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">server</span>-<span class="keyword">stop</span>.sh</span><br></pre></td></tr></table></figure><p>创建Topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --replication-factor 3 --partitions 3 --topic test</span><br></pre></td></tr></table></figure><p>查看Topic列表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper kafka01:2181,kafka02:2181,kafka03:2181</span><br></pre></td></tr></table></figure><p>查看Topic详细信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><p>建立发布者console-producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --topic test</span><br></pre></td></tr></table></figure><p>建立订阅者console-consumer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafka01:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure><p>删除topic(需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><h2 id="使用kafka-manager管理kafka集群"><a href="#使用kafka-manager管理kafka集群" class="headerlink" title="使用kafka-manager管理kafka集群"></a>使用kafka-manager管理kafka集群</h2><p><strong>注：以下均在kafka用户下搭建，仅在kafka01节点上安装kafka-manager</strong></p><h3 id="将kafka-manager的安装包放到-data-apps目录下"><a href="#将kafka-manager的安装包放到-data-apps目录下" class="headerlink" title="将kafka-manager的安装包放到/data/apps目录下"></a>将kafka-manager的安装包放到/data/apps目录下</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv ~/kafka-manager<span class="number">-1.3</span><span class="number">.3</span><span class="number">.22</span> /data/apps</span><br></pre></td></tr></table></figure><h3 id="配置kafka-manager"><a href="#配置kafka-manager" class="headerlink" title="配置kafka-manager"></a>配置kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd conf</span><br><span class="line">vim application.conf</span><br><span class="line">修改kafka-manager.zkhosts="kafka01:2181,kafka02:2181,kafka03:2181"</span><br></pre></td></tr></table></figure><h3 id="启动kafka-manager"><a href="#启动kafka-manager" class="headerlink" title="启动kafka-manager"></a>启动kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</span><br></pre></td></tr></table></figure><h3 id="kafka-manager的WebUI"><a href="#kafka-manager的WebUI" class="headerlink" title="kafka-manager的WebUI"></a>kafka-manager的WebUI</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka01:8080</span><br></pre></td></tr></table></figure><h2 id="使用jmxtrans-influxdb-grafana监控JMX指标"><a href="#使用jmxtrans-influxdb-grafana监控JMX指标" class="headerlink" title="使用jmxtrans+influxdb+grafana监控JMX指标"></a>使用jmxtrans+influxdb+grafana监控JMX指标</h2><p><strong>注：以下均在root用户下搭建，除了jmxtrans，其他组件仅在kafka01节点上安装</strong></p><h3 id="开启Kafka-JMX端口"><a href="#开启Kafka-JMX端口" class="headerlink" title="开启Kafka JMX端口"></a>开启Kafka JMX端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd bin</span><br><span class="line">vim kafka-run-class.sh</span><br></pre></td></tr></table></figure><p><strong>第一行增加<code>JMX_PORT=9999</code>即可</strong></p><p><strong>修改好后重启kafka，查看Kafka以及JMX端口状态</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ps</span> -ef | <span class="keyword">grep</span> kafka</span><br><span class="line">netstat -anop | <span class="keyword">grep</span> <span class="number">9999</span></span><br></pre></td></tr></table></figure><h3 id="安装InfluxDB"><a href="#安装InfluxDB" class="headerlink" title="安装InfluxDB"></a>安装InfluxDB</h3><p><strong>下载InfluxDB rpm安装包</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http<span class="variable">s:</span>//<span class="keyword">dl</span>.influxdata.<span class="keyword">com</span>/influxdb/releases/influxdb-<span class="number">1.7</span>.<span class="number">5</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">influxdb-1</span><span class="selector-class">.7</span><span class="selector-class">.5</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动InfluxDB</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service <span class="literal">inf</span>luxdb <span class="literal">start</span>（systemctl <span class="literal">start</span> <span class="literal">inf</span>luxdb）</span><br></pre></td></tr></table></figure><p><strong>查看InfluxDB状态</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep influxdb</span><br><span class="line">service influxdb status（systemctl status influxdb）</span><br></pre></td></tr></table></figure><p><strong>使用InfluxDB客户端</strong></p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">influx</span></span><br></pre></td></tr></table></figure><p><strong>创建用户和数据库</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">"admin"</span> <span class="keyword">WITH</span> <span class="keyword">PASSWORD</span> <span class="string">'admin'</span> <span class="keyword">WITH</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="string">"jmxDB"</span>;</span><br></pre></td></tr></table></figure><p><strong>创建完成InfluxDB的用户和数据库暂时就够用了，其它简单操作如下，后面会用到</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建数据库</span></span><br><span class="line">create database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示所有的数据库</span></span><br><span class="line">show databases</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除数据库</span></span><br><span class="line">drop database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用数据库</span></span><br><span class="line">use db_name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示该数据库中所有的表</span></span><br><span class="line">show measurements</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建表，直接在插入数据的时候指定表名</span></span><br><span class="line">insert test,host=127.0.0.1,monitor_name=test count=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop measurement "measurement_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出</span></span><br><span class="line">quit</span><br></pre></td></tr></table></figure><h3 id="安装jmxtrans（所有kafka节点均安装）"><a href="#安装jmxtrans（所有kafka节点均安装）" class="headerlink" title="安装jmxtrans（所有kafka节点均安装）"></a>安装jmxtrans（所有kafka节点均安装）</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http:<span class="regexp">//</span>central.maven.org<span class="regexp">/maven2/</span>org<span class="regexp">/jmxtrans/</span>jmxtrans<span class="regexp">/270/</span>jmxtrans-<span class="number">270</span>.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">jmxtrans-270</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>jmxtrans相关路径</strong></p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jmxtrans安装目录：/usr/share/jmxtrans</span><br><span class="line">json文件默认目录：/var/<span class="class"><span class="keyword">lib</span>/<span class="title">jmxtrans</span>/</span></span><br><span class="line">日志路径：/var/log/jmxtrans/jmxtrans.log</span><br></pre></td></tr></table></figure><p><strong>配置json，jmxtrans的github上有一段示例配置</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"servers"</span> : [ &#123;</span><br><span class="line">    <span class="attr">"port"</span> : <span class="string">"9999"</span>,</span><br><span class="line">    <span class="attr">"host"</span> : <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="attr">"queries"</span> : [ &#123;</span><br><span class="line">      <span class="attr">"obj"</span> : <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line">      <span class="attr">"attr"</span> : [ <span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span> ],</span><br><span class="line">      <span class="attr">"resultAlias"</span>:<span class="string">"jvmMemory"</span>,</span><br><span class="line">      <span class="attr">"outputWriters"</span> : [ &#123;</span><br><span class="line">        <span class="attr">"@class"</span> : <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"http://127.0.0.1:8086/"</span>,</span><br><span class="line">        <span class="attr">"username"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"password"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"database"</span> : <span class="string">"jmxDB"</span>,</span><br><span class="line">        <span class="attr">"tags"</span>     : &#123;<span class="attr">"application"</span> : <span class="string">"kafka"</span>&#125;</span><br><span class="line">      &#125; ]</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>host：监控服务器</li><li>port：jmx端口</li><li>obj：对应jmx的ObjectName，就是我们要监控的指标</li><li>attr：对应ObjectName的属性，可以理解为我们要监控的指标的值</li><li>resultAlias：对应metric 的名称，在InfluxDB里面就是MEASUREMENTS名</li><li>tags：对应InfluxDB的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</li></ul><p><strong>附上两段配置的json示例文件（完整的均放在了三台节点的/var/lib/jmxtrans/目录下）</strong></p><p>base_10.164.204.248.json</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesRejectedPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchFollower"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MemoryUsage"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MemoryUsage"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=GarbageCollector,name=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"CollectionCount"</span>, <span class="string">"CollectionTime"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"GC"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"GC"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Threading"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"PeakThreadCount"</span>, <span class="string">"ThreadCount"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"Thread"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Thread"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaFetcherManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MaxLag"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=PartitionCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"PartitionCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"UnderReplicatedPartitions"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=LeaderCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LeaderCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=IsrShrinksPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"IsrShrinksPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>flink_sf_lx_248.json</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.log:type=Log,name=LogEndOffset,topic=FLINK_SF_LX,partition=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LogEndOffset"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>配置说明：</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">1、全局指标</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesOutPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesRejectedPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesRejectedPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒的消息写入总量</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"MessagesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchFollower的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchConsumer的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">每秒Produce的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line">内存使用的使用情况</span><br><span class="line">"obj" : "java.lang:type=Memory"</span><br><span class="line">"attr" : [ "HeapMemoryUsage", "NonHeapMemoryUsage" ]</span><br><span class="line">"resultAlias":"MemoryUsage"</span><br><span class="line">"tags" : &#123;"application" : "MemoryUsage"&#125;</span><br><span class="line"></span><br><span class="line">GC的耗时和次数</span><br><span class="line">"obj" : "java.lang:type=GarbageCollector,name=*"</span><br><span class="line">"attr" : [ "CollectionCount","CollectionTime" ]</span><br><span class="line">"resultAlias":"GC"</span><br><span class="line">"tags" : &#123;"application" : "GC"&#125;</span><br><span class="line"></span><br><span class="line">线程的使用情况</span><br><span class="line">"obj" : "java.lang:type=Threading"</span><br><span class="line">"attr" : [ "PeakThreadCount","ThreadCount" ]</span><br><span class="line">"resultAlias":"Thread"</span><br><span class="line">"tags" : &#123;"application" : "Thread"&#125;</span><br><span class="line"></span><br><span class="line">副本落后主分片的最大消息数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaFetcherManager"</span><br><span class="line">"tags" : &#123;"application" : "MaxLag"&#125;</span><br><span class="line"></span><br><span class="line">该broker上的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=PartitionCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "PartitionCount"&#125;</span><br><span class="line"></span><br><span class="line">正在做复制的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "UnderReplicatedPartitions"&#125;</span><br><span class="line"></span><br><span class="line">Leader的replica的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=LeaderCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "LeaderCount"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchConsumer耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchFollower耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">一个请求Produce耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2、topic的监控指标</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的写入流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的输出流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒写入消息的数量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=falcon_monitor_us"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us在每个分区最后的Offset</span><br><span class="line">"obj" : "kafka.log:type=Log,name=LogEndOffset,topic=falcon_monitor_us,partition=*"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "LogEndOffset"&#125;</span><br><span class="line"></span><br><span class="line">PS：</span><br><span class="line">1、参数说明</span><br><span class="line">"obj"对应jmx的ObjectName，就是我们要监控的指标</span><br><span class="line">"attr"对应ObjectName的属性，可以理解为我们要监控的指标的值</span><br><span class="line">"resultAlias"对应metric 的名称，在InfluxDb里面就是MEASUREMENTS名</span><br><span class="line">"tags" 对应InfluxDb的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</span><br><span class="line"></span><br><span class="line">2、对于全局监控，每一个监控指标对应一个MEASUREMENTS，所有的kafka节点同一个监控指标数据写同一个MEASUREMENTS ，对于topc监控的监控指标，同一个topic所有kafka节点写到同一个MEASUREMENTS，并且以topic名称命名</span><br></pre></td></tr></table></figure><p><strong>启动jmxtrans</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service jmxtrans <span class="literal">start</span>(systemctl <span class="literal">start</span> jmxtrans)</span><br></pre></td></tr></table></figure><p><strong>查看日志没有报错即为成功</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail /<span class="built_in">var</span>/<span class="keyword">log</span>/jmxtrans/jmxtrans.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><h3 id="安装Grafana"><a href="#安装Grafana" class="headerlink" title="安装Grafana"></a>安装Grafana</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://s3-us-west<span class="string">-2</span>.amazonaws.com/grafana-releases/release/grafana<span class="string">-6</span>.0.2<span class="string">-1</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包（如果缺少依赖，下载依赖）</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">grafana-6</span><span class="selector-class">.0</span><span class="selector-class">.2-1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install --downloadonly --downloaddir=./ fontconfig</span><br><span class="line"></span><br><span class="line">yum localinstall fontconfig-<span class="number">2.13</span>.<span class="number">0</span>-<span class="number">4.3</span><span class="selector-class">.el7</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br><span class="line"></span><br><span class="line">yum install --downloadonly --downloaddir=./ urw-fonts</span><br><span class="line"></span><br><span class="line">yum localinstall urw-fonts-<span class="number">2.4</span>-<span class="number">11</span><span class="selector-class">.el6</span><span class="selector-class">.noarch</span><span class="selector-class">.rpm</span> </span><br><span class="line"></span><br><span class="line">rpm -ivh grafana-<span class="number">6.0</span>.<span class="number">2</span>-<span class="number">1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动Grafana</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service grafana-server <span class="keyword">start</span>（systemctl <span class="keyword">start</span> grafana-<span class="keyword">server</span>）</span><br></pre></td></tr></table></figure><p><strong>打开浏览器</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//127.0.0.1:3000</span></span><br></pre></td></tr></table></figure><p><strong>先输入默认用户名密码admin/admin</strong></p><p><strong>点击Add data source，选择InfluxDB</strong></p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Url、Database、User、Password需要和jmxtrans采集数据配置文件里面的写一致，然后点击<span class="keyword">Save</span>&amp;<span class="keyword">Test</span>，提示成功就正常了</span><br></pre></td></tr></table></figure><p><strong>通过后点击Back返回</strong></p><p><strong>左侧 + 可以创建或引入仪表盘，创建一个dashboard，然后在这里配置每一个监控指标的图</strong></p><p><strong>主要配置项说明：</strong></p><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>DataSource</td><td>选择Grafana已配置的数据源</td></tr><tr><td>FROM-Default</td><td>默认Schema，保持不变即可</td></tr><tr><td>FROM-measurement</td><td>对应的InfluxDB的表名</td></tr><tr><td>WHERE</td><td>WHERE条件，根据自己需求选择</td></tr><tr><td>SELECT-Field</td><td>对应选的字段，可根据需求增减</td></tr><tr><td>SELECT-mean()</td><td>选择的字段对应的InfluxDB的函数</td></tr><tr><td>GroupBY-time()</td><td>根据时间分组</td></tr><tr><td>GROUPBY-fill()</td><td>当不存在数据时，以null为默认值填充</td></tr></tbody></table><p><strong>要点说明：</strong></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、对于监控指标为Count的监控项，需要通过Grafana做计算得到我们想要的监控，比如BytesInPerSec这个指标，它的监控值是一个累计值，我们想要取到每秒的流量，肯定需要计算，(本次采集的值-上次采集的值)/<span class="number">60</span> ,jmxtrans是一分钟采集一次数据，具体配置参考下面截图：</span><br><span class="line"></span><br><span class="line">因为我们是一分钟采集一次数据，所以group <span class="keyword">by</span> 和derivative选<span class="number">1</span>分钟；因为我们要每秒的流量，所以math这里除以<span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、X轴的单位选择，比如流量的单位、时间的单位、每秒消息的个数无单位等等，下面分布举一个例子介绍说明</span><br><span class="line"></span><br><span class="line">设置流量的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》data（Metric）--》bytes</span></span><br><span class="line"></span><br><span class="line">设置时间的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》time--》milliseconds(ms)</span></span><br><span class="line"></span><br><span class="line">设置按原始值展示，无单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》none--》none</span></span><br></pre></td></tr></table></figure><h2 id="附录（kafka配置说明）"><a href="#附录（kafka配置说明）" class="headerlink" title="附录（kafka配置说明）"></a>附录（kafka配置说明）</h2><h3 id="server-properties"><a href="#server-properties" class="headerlink" title="server.properties"></a>server.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.server.KafkaConfig <span class="keyword">for</span> additional details and defaults</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Server Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的ID，必须与其它节点不同</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Switch to <span class="built_in">enable</span> topic deletion or not, default value is <span class="literal">false</span></span></span><br><span class="line">delete.topic.enable=false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> java.net.InetAddress.getCanonicalHostName() <span class="keyword">if</span> not configured.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器监听的地址。如果没有配置，就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   FORMAT:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   EXAMPLE:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line">listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Hostname and port the broker will advertise to producers and consumers. If not <span class="built_in">set</span>, </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> it uses the value <span class="keyword">for</span> <span class="string">"listeners"</span> <span class="keyword">if</span> configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的主机名会通知给生产者和消费者。如果没有设置，如果配置了<span class="string">"listeners"</span>就使用<span class="string">"listeners"</span>的值。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line">advertised.listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> Maps listener names to security protocols, the default is <span class="keyword">for</span> them to be the same. See the config documentation <span class="keyword">for</span> more details</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将侦听器的名称映射到安全协议，默认情况下它们是相同的。有关详细信息，请参阅配置文档</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来接受请求或者发送响应的线程数</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> processing requests, <span class="built_in">which</span> may include disk I/O</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来处理请求的线程数，可能包括磁盘IO</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的发送缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的接收缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单个请求最大能接收的数据量</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A comma seperated list of directories under <span class="built_in">which</span> to store <span class="built_in">log</span> files</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个逗号分隔的目录列表，用来存储日志文件</span></span><br><span class="line">log.dirs=/data/apps/kafkaapp/logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default number of <span class="built_in">log</span> partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism <span class="keyword">for</span> consumption, but this will also result <span class="keyword">in</span> more files across</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the brokers.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个主题的日志分区的默认数量。更多的分区允许更大的并行操作，但是它会导致节点产生更多的文件</span></span><br><span class="line">num.partitions=6</span><br><span class="line">default.replication.factor=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads per data directory to be used <span class="keyword">for</span> <span class="built_in">log</span> recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This value is recommended to be increased <span class="keyword">for</span> installations with data <span class="built_in">dirs</span> located <span class="keyword">in</span> RAID array.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个数据目录中的线程数，用于在启动时日志恢复，并在关闭时刷新。</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Internal Topic Settings  #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The replication factor <span class="keyword">for</span> the group metadata internal topics <span class="string">"__consumer_offsets"</span> and <span class="string">"__transaction_state"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For anything other than development testing, a value greater than 1 is recommended <span class="keyword">for</span> to ensure availability such as 3.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于除了开发测试之外的其他任何东西，group元数据内部主题的复制因子“__consumer_offsets”和“__transaction_state”，建议值大于1，以确保可用性(如3)。</span></span><br><span class="line">offsets.topic.replication.factor=3</span><br><span class="line">transaction.state.log.replication.factor=3</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Flush Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Messages are immediately written to the filesystem but by default we only fsync() to sync</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the OS cache lazily. The following configurations control the flush of data to disk.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息直接被写入文件系统，但是默认情况下我们仅仅调用fsync()以延迟的同步系统缓存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There are a few important trade-offs here:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这些有一些重要的权衡</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    1. Durability: Unflushed data may be lost <span class="keyword">if</span> you are not using replication.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. 持久性:如果不使用复制，未刷新的数据可能会丢失。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 延迟:非常大的刷新间隔可能会在刷新时导致延迟，因为将会有大量数据刷新。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 吞吐量:刷新通常是最昂贵的操作，而一个小的刷新间隔可能会导致过多的搜索。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The settings below allow one to configure the flush policy to flush data after a period of time or</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> every N messages (or both). This can be <span class="keyword">done</span> globally and overridden on a per-topic basis.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的设置允许你去配置刷新策略，每隔一段时间刷新或者一次N个消息（或者两个都配置）。这可以在全局范围内完成，并在每个主题的基础上重写。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of messages to accept before forcing a flush of data to disk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.messages=10000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum amount of time a message can sit <span class="keyword">in</span> a <span class="built_in">log</span> before we force a flush</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.ms=1000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Retention Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configurations control the disposal of <span class="built_in">log</span> segments. The policy can</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> be <span class="built_in">set</span> to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下的配置控制了日志段的处理。策略可以配置为每隔一段时间删除片段或者到达一定大小之后。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> from the end of the <span class="built_in">log</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当满足这些条件时，将会删除一个片段。删除总是发生在日志的末尾。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The minimum age of a <span class="built_in">log</span> file to be eligible <span class="keyword">for</span> deletion due to age</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个日志的最小存活时间，可以被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A size-based retention policy <span class="keyword">for</span> logs. Segments are pruned from the <span class="built_in">log</span> as long as the remaining</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> segments don<span class="string">'t drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.retention.bytes=1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a <span class="built_in">log</span> segment file. When this size is reached a new <span class="built_in">log</span> segment will be created.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每一个日志段大小的最大值。当到达这个大小时，会生成一个新的片段。</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The interval at <span class="built_in">which</span> <span class="built_in">log</span> segments are checked to see <span class="keyword">if</span> they can be deleted according</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to the retention policies</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查日志段的时间间隔，看是否可以根据保留策略删除它们</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string (see zookeeper docs <span class="keyword">for</span> details).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper连接字符串（具体见Zookeeper文档）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这是一个以逗号为分割的部分，每一个都匹配一个Zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 您还可以将一个可选的chroot字符串附加到url，以指定所有kafka znode的根目录。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> root directory <span class="keyword">for</span> all kafka znodes.</span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接到Zookeeper的超时时间</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Group Coordinator Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configuration specifies the time, <span class="keyword">in</span> milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default value <span class="keyword">for</span> this is 3 seconds.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> We override this to 0 here as it makes <span class="keyword">for</span> a better out-of-the-box experience <span class="keyword">for</span> development and testing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> However, <span class="keyword">in</span> production environments the default value of 3 seconds is more suitable as this will <span class="built_in">help</span> to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br></pre></td></tr></table></figure><h3 id="producer-properties"><a href="#producer-properties" class="headerlink" title="producer.properties"></a>producer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.producer.ProducerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Producer Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> list of brokers used <span class="keyword">for</span> bootstrapping knowledge about the rest of the cluster</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> format: host1:port1,host2:port2 ...</span></span><br><span class="line">bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> specify the compression codec <span class="keyword">for</span> all data generated: none, gzip, snappy, lz4</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。</span></span><br><span class="line">compression.type=none</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> name of the partitioner class <span class="keyword">for</span> partitioning events; default partition spreads data randomly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区</span></span><br><span class="line">partitioner.class=kafka.producer.DefaultPartitioner</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum amount of time the client will <span class="built_in">wait</span> <span class="keyword">for</span> the response of a request</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功)</span></span><br><span class="line">request.timeout.ms=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> how long `KafkaProducer.send` and `KafkaProducer.partitionsFor` will block <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.block.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the producer will <span class="built_in">wait</span> <span class="keyword">for</span> up to the given delay to allow other records to be sent so that the sends can be batched together</span></span><br><span class="line"><span class="meta">#</span><span class="bash">linger.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum size of a request <span class="keyword">in</span> bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.request.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the default batch size <span class="keyword">in</span> bytes when batching multiple records sent to a partition</span></span><br><span class="line"><span class="meta">#</span><span class="bash">batch.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the total bytes of memory the producer can use to buffer records waiting to be sent to the server</span></span><br><span class="line"><span class="meta">#</span><span class="bash">buffer.memory=</span></span><br></pre></td></tr></table></figure><h3 id="consumer-properties"><a href="#consumer-properties" class="headerlink" title="consumer.properties"></a>consumer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.consumer.ConsumerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span></span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer group id</span></span><br><span class="line">group.id=test-consumer-group</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer timeout</span></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer.timeout.ms=500</span></span><br></pre></td></tr></table></figure><h3 id="install-kafka-sh"><a href="#install-kafka-sh" class="headerlink" title="install-kafka.sh"></a>install-kafka.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置KAFKA的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入kafka的安装目录,不存在脚本自动创建,最后一个/不要写,如 /data/apps"</span><br><span class="line">read kafkainstallpath</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建kafka安装的目录</span></span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">   mkdir -p $kafkainstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">  echo "创建目录$kafkainstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'kafka.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   # 当前目录没有kafka的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现kafka*.tar.gz,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   # 解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'kafka.*[gz]$') -C $kafkainstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka版本全称</span></span><br><span class="line">kafkaversion=`ls $kafkainstallpath| grep 'kafka_2.*'`</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka配置文件存储路径</span></span><br><span class="line">confpath=$kafkainstallpath/$kafkaversion/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改配置文件</span></span><br><span class="line">echo -e "请输入当前kafka节点的broker.id：唯一值 例如 0"</span><br><span class="line">read brokerid</span><br><span class="line">sed -i "s/^broker.id=0/broker.id=$&#123;brokerid&#125;/g" $confpath/server.properties</span><br><span class="line"></span><br><span class="line">echo -e "请输入当前kafka节点的hostname: 例如kafka01"</span><br><span class="line">read hostname</span><br><span class="line">sed -i "s/^#listeners=PLAINTEXT:\/\/:9092/listeners=PLAINTEXT:\/\/$hostname:9092/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e "请输入kafka消息存储目录：例如 /data/apps/kafkaapp/log"</span><br><span class="line">read kafkalogspath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建KAFKA日志存储目录</span></span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">   mkdir -p $kafkalogspath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">  echo "创建目录$kafkalogspath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='log.dirs=/tmp/kafka-logs'</span><br><span class="line">new_dir='log.dirs='$kafkalogspath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e '请输入zookeeper集群的所有节点：（严格按照示例格式） 例如kafka01:2181,kafka02:2181,kafka03:2181'</span><br><span class="line">read allhosts</span><br><span class="line">sed -i "s/^zookeeper.connect=localhost:2181/zookeeper.connect=$allhosts/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭删除topic的权限</span></span><br><span class="line">sed -i 's/^#delete.topic.enable=true/delete.topic.enable=false/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置topic的默认分区数量为6</span></span><br><span class="line">sed -i 's/^num.partitions=1/num.partitions=6/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line">sed -i 's/^#log.retention.bytes=1073741824/log.retention.bytes=1073741824/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line">sed -i 's/^#log.flush.interval.messages=10000/log.flush.interval.messages=10000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line">sed -i 's/^#log.flush.interval.ms=1000/log.flush.interval.ms=1000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置偏移量topic的复制因子为3</span></span><br><span class="line">sed -i 's/^offsets.topic.replication.factor=1/offsets.topic.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置事务topic的复制因子为3</span></span><br><span class="line">sed -i 's/^transaction.state.log.replication.factor=1/transaction.state.log.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认副本因子为3</span></span><br><span class="line">echo ""&gt;&gt;$confpath/server.properties</span><br><span class="line">echo "default.replication.factor=3" &gt;&gt;$confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"log.cleanup.policy=delete"</span> &gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka参数优化</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">'s/^log.retention.hours=16/log.retention.hours=72/g'</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> param=`cat /proc/cpuinfo | grep <span class="string">"cpu cores"</span>| uniq`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_count=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_count=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+1))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_count&#125;</span>!<span class="variable">$&#123;new_count&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_io=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_io=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+<span class="variable">$&#123;param:0-1:1&#125;</span>))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_io&#125;</span>!<span class="variable">$&#123;new_io&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#KAFKA <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$KAFKA_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"> </span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">kafkapath=$kafkainstallpath/$kafkaversion</span><br><span class="line">kafkapathtemp=$kafkainstallpath/$kafkaversion-temp</span><br><span class="line">cp -r $kafkapath $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$&#123;hostname&#125;),严格符合以下格式hostname:brokerid,空格隔开， 如kafka02:1 kafka03:2'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> kafkahostname=$&#123;array3[0]&#125;</span><br><span class="line"> kafkabrokerid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $kafkahostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $kafkahostname "rm -rf $kafkapath $kafkalogspath"</span><br><span class="line"> ssh $kafkahostname "mkdir -p $kafkapath $kafkalogspath"</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改broker.id</span></span><br><span class="line"> old_brokerid="broker.id=$brokerid"</span><br><span class="line"> new_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"><span class="meta"> #</span><span class="bash">修改listeners</span></span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$&#123;hostname&#125;:9092"</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$&#123;kafkahostname&#125;:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> scp -r $kafkapathtemp/* $&#123;user&#125;@$kafkahostname:$kafkapath/</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo '#KAFKA <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo 'export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">'echo "export PATH=\$PATH:\$KAFKA_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">再次修改回来 防止修改错误</span></span><br><span class="line"> new_brokerid="broker.id=$brokerid"</span><br><span class="line"> old_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$hostname:9092"</span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$kafkahostname:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> echo ======= $kafkahostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="install-zookeeper-sh"><a href="#install-zookeeper-sh" class="headerlink" title="install-zookeeper.sh"></a>install-zookeeper.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置zk的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入zk的安装目录,不存在脚本自动创建,最后一个/不要写 如/data/apps"</span><br><span class="line">read zkinstallpath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk安装的目录</span></span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">   mkdir -p $zkinstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">  echo "创建目录$zkinstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit:</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'zookeeper-.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   #当前目录没有zk的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现zookeeper的gz压缩包,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   #解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'zookeeper-.*[gz]$') -C $zkinstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">zkversion=`ls $zkinstallpath| grep 'zookeeper-.*'`</span><br><span class="line"></span><br><span class="line">confpath=$zkinstallpath/$zkversion/conf</span><br><span class="line"></span><br><span class="line">cp $confpath/zoo_sample.cfg  $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line">echo -e "请输入zk数据存储目录：例如 /data/apps/zookeeperapp"</span><br><span class="line">read zkdatapath</span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk数据的目录</span></span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">   mkdir -p $zkdatapath</span><br><span class="line">fi</span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">  echo "创建目录$zkdatapath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='dataDir=/tmp/zookeeper'</span><br><span class="line">new_dir='dataDir='$zkdatapath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo  "请输入所有的zk集群节点：（按照空格分割） 例如 zk01 zk02 zk03"</span><br><span class="line">read zkNodes</span><br><span class="line">array=(`echo $zkNodes | tr ' ' ' '` )</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line">echo ""&gt;&gt;$confpath/zoo.cfg</span><br><span class="line">for i in `seq 0 $(($&#123;#array[@]&#125;-1))`</span><br><span class="line">do</span><br><span class="line"> echo "server.$(($&#123;i&#125;+1))=$&#123;array[$&#123;i&#125;]&#125;:2888:3888" &gt;&gt;$confpath/zoo.cfg</span><br><span class="line">done </span><br><span class="line"></span><br><span class="line">echo  "请输入zk的myid,不能重复,唯一值 例如 1" </span><br><span class="line">read myid</span><br><span class="line">echo $myid &gt; $zkdatapath/myid</span><br><span class="line"></span><br><span class="line">binpath=$zkinstallpath/$zkversion/bin</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG_DIR=\".\"/ZOO_LOG_DIR=\"$&#123;ZOOKEEPER_PREFIX&#125;\/logs\"/g' $binpath/zkEnv.sh</span><br><span class="line"></span><br><span class="line">echo "ZOO_LOG_DIR修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG4J_PROP=\"INFO,CONSOLE\"/ZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\"/g' $binpath/zkEnv.sh</span><br><span class="line">echo "ZOO_LOG4J_PROP修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.out\"/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.log\"/g' $binpath/zkServer.sh</span><br><span class="line">echo "_ZOO_DAEMON_OUT修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/zookeeper.root.logger=INFO, CONSOLE/zookeeper.root.logger=INFO, ROLLINGFILE/g' $confpath/log4j.properties</span><br><span class="line">echo "zookeeper.root.logger修改成功"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#zookeeper <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$ZK_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"></span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">zkpath=$zkinstallpath/$zkversion</span><br><span class="line">zkpathtemp=$zkinstallpath/$zkversion-temp</span><br><span class="line">cp -r $zkpath $zkpathtemp</span><br><span class="line"></span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">currentnode=`hostname`</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$currentnode),严格符合以下格式hostname:zkID,空格隔开， 如zk02:2 zk03:3 zk04:4 zk05:5 zk06:6'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> hostname=$&#123;array3[0]&#125;</span><br><span class="line"> zkid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $hostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $hostname "mkdir -p $zkpath"</span><br><span class="line"> ssh $hostname "mkdir -p $zkdatapath"</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">修改zk的myid唯一值</span></span><br><span class="line"> ssh $hostname "echo $zkid &gt; $zkdatapath/myid"</span><br><span class="line"></span><br><span class="line"> scp -r $zkpathtemp/* $&#123;user&#125;@$hostname:$zkpath/</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo '#zk <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo 'export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">'echo "export PATH=\$PATH:\$ZK_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"></span><br><span class="line"> echo ======= $hostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $zkpathtemp</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      本文主要讲述kafka_2.11-0.11.0.1和Zookeeper-3.4.10整合搭建Kafka集群以及监控平台的全套流程。
    
    </summary>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
      <category term="Kafka" scheme="https://gjtmaster.github.io/tags/Kafka/"/>
    
      <category term="消息中间件" scheme="https://gjtmaster.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>JsonSchema全套解决方案</title>
    <link href="https://gjtmaster.github.io/2018/08/17/JsonSchema%E5%85%A8%E5%A5%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://gjtmaster.github.io/2018/08/17/JsonSchema全套解决方案/</id>
    <published>2018-08-17T03:34:28.000Z</published>
    <updated>2019-07-26T15:36:32.110Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Json-Schema说明"><a href="#Json-Schema说明" class="headerlink" title="Json Schema说明"></a>Json Schema说明</h1><ol><li>json schema 本身也是一个json串；</li><li>每个schema可以描述一个json实例，并且该json实例里每一个节点都可以用一个schema来描述，因此schema与json一样，本身也是一个层级结构，一个schema中可能嵌套着另外若干层schema；</li><li>json schema 定义的检查规则以数据格式验证为主（字段存在性、字段类型），并可以支持一些简单的数据正确性验证（例如数值范围、字符串的模式等），但不能进行复杂的逻辑校验（例如进价必须小于售价等）；</li></ol><h1 id="Json-Schema-格式"><a href="#Json-Schema-格式" class="headerlink" title="Json Schema 格式"></a>Json Schema 格式</h1><p>Json schema 本身遵循Json规范，本身就是一个Json字符串，先来看一个例子</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来看一下json schema 最外层包含以下几个字段</p><table><thead><tr><th>$schema</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>$schema</td><td>$schema 关键字状态，表示这个模式与 v4 规范草案书写一致。</td><td></td></tr><tr><td>title</td><td>标题，用来描述结构</td><td></td></tr><tr><td>description</td><td>描述</td><td></td></tr><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr></tbody></table><p>上面只是一个简单的例子，从上面可以看出Json schema 本身是一个JSON字符串，由通过key-value的形式进行标示。<br>type 和 properties 用来定义json 属性的类型。required 是对Object字段的必段性进行约束。事实上,json Schema定义了json所支持的类型，每种类型都有0-N种约束方式。下一节我们来，细致介绍一下。</p><hr><h1 id="Json-Schema-类型"><a href="#Json-Schema-类型" class="headerlink" title="Json Schema 类型"></a>Json Schema 类型</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>object类型有三个关键字:type（限定类型）,properties(定义object的各个字段),required（限定必需字段）,如下：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr><tr><td>maxProperties</td><td>最大属性个数</td><td></td></tr><tr><td>minProperties</td><td>最小属性个数</td><td></td></tr><tr><td>additionalProperties</td><td>true or false or object</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html" target="_blank" rel="noopener">参考</a></td></tr></tbody></table><p>properties 定义每个属性的名字和类型，方式如上例。</p><h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">     &#125;,</span><br><span class="line">     <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">     <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>array有三个单独的属性:items,minItems,uniqueItems:</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>items</td><td>array 每个元素的类型</td><td>.</td></tr><tr><td>minItems</td><td>约束属性，数组最小的元素个数</td><td></td></tr><tr><td>maxItems</td><td>约束属性，数组最大的元素个数</td><td></td></tr><tr><td>uniqueItems</td><td>约束属性，每个元素都不相同</td><td></td></tr><tr><td>additionalProperties</td><td>约束items的类型，不建议使用</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/array.html" target="_blank" rel="noopener">示例</a></td></tr><tr><td>Dependencies</td><td>属性依赖</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=additionalproperties" target="_blank" rel="noopener">用法</a></td></tr><tr><td>patternProperties</td><td></td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=patternproperties" target="_blank" rel="noopener">用法</a></td></tr></tbody></table><h2 id="string"><a href="#string" class="headerlink" title="string"></a>string</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"ip"</span>: &#123;</span><br><span class="line">            <span class="attr">"mail"</span>: <span class="string">"string"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"w+([-+.]w+)*@w+([-.]w+)*.w+([-.]w+)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"host"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"phoneNumber"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"((d&#123;3,4&#125;)|d&#123;3,4&#125;-)?d&#123;7,8&#125;(-d&#123;3&#125;)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"ip"</span>, <span class="string">"host"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>maxLength</td><td>定义字符串的最大长度，&gt;=0</td><td>.</td></tr><tr><td>minLength</td><td>定义字符串的最小长度，&gt;=0</td><td></td></tr><tr><td>pattern</td><td>用正则表达式约束字符串</td><td></td></tr></tbody></table><h2 id="integer"><a href="#integer" class="headerlink" title="integer"></a>integer</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr><tr><td>multipleOf</td><td>是某数的倍数，必须大于0的整数</td><td></td></tr></tbody></table><h2 id="number"><a href="#number" class="headerlink" title="number"></a>number</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>number 关键字可以描述任意长度，任意小数点的数字。number类型的约束有以下几个：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr></tbody></table><h2 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"boolean"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>true or false</p><h2 id="enum"><a href="#enum" class="headerlink" title="enum"></a>enum</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以这么做</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]                   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="null"><a href="#null" class="headerlink" title="null"></a>null</h2><h1 id="Json-Schema进阶"><a href="#Json-Schema进阶" class="headerlink" title="Json Schema进阶"></a>Json Schema进阶</h1><p>了解了上面的各个类型的定义及约定条件，就可以满足大部分情况了。但为了写出更好的json schema,我们再学习几个关键字</p><h2 id="ref"><a href="#ref" class="headerlink" title="$ref"></a>$ref</h2><p>$ref 用来引用其它schema,<br>示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product set"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">        <span class="attr">"properties"</span>: &#123;</span><br><span class="line">            <span class="attr">"id"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"name"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"price"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">                <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"tags"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">                <span class="attr">"items"</span>: &#123;</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">                <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"dimensions"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">                <span class="attr">"properties"</span>: &#123;</span><br><span class="line">                    <span class="attr">"length"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"width"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"height"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"required"</span>: [<span class="string">"length"</span>, <span class="string">"width"</span>, <span class="string">"height"</span>]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"warehouseLocation"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"Coordinates of the warehouse with the product"</span>,</span><br><span class="line">                <span class="attr">"$ref"</span>: <span class="string">"http://json-schema.org/geo"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="definitions"><a href="#definitions" class="headerlink" title="definitions"></a>definitions</h2><p>当一个schema写的很大的时候，可能需要创建内部结构体，再使用$ref进行引用，示列如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/positiveInteger"</span> &#125;,</span><br><span class="line">    <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">        <span class="attr">"positiveInteger"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="allOf"><a href="#allOf" class="headerlink" title="allOf"></a>allOf</h2><p>意思是展示全部属性，建议用requires替代</p><p>不建议使用，示例如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">    <span class="attr">"address"</span>: &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">      <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"street_address"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"city"</span>:           &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"state"</span>:          &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"required"</span>: [<span class="string">"street_address"</span>, <span class="string">"city"</span>, <span class="string">"state"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"allOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/address"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: &#123; <span class="attr">"enum"</span>: [ <span class="string">"residential"</span>, <span class="string">"business"</span> ] &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="anyOf"><a href="#anyOf" class="headerlink" title="anyOf"></a>anyOf</h2><p>意思是展示任意属性，建议用requires替代和minProperties替代，示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"anyOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="oneOf"><a href="#oneOf" class="headerlink" title="oneOf"></a>oneOf</h2><p>其中之一</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"oneOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">5</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">3</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="not"><a href="#not" class="headerlink" title="not"></a>not</h2><p>非 * 类型<br>示例</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">"not"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125; &#125;</span><br></pre></td></tr></table></figure><h1 id="Java-Json-Schema库"><a href="#Java-Json-Schema库" class="headerlink" title="Java Json Schema库"></a>Java Json Schema库</h1><p>表中给出了两种java中使用的JSON Schema库</p><table><thead><tr><th>库名称</th><th>地址</th><th>支持草案</th></tr></thead><tbody><tr><td>fge</td><td><a href="https://github.com/daveclayton/json-schema-validator" target="_blank" rel="noopener">https://github.com/daveclayton/json-schema-validator</a></td><td>draft-04 draft-03</td></tr><tr><td>everit</td><td><a href="https://github.com/everit-org/json-schema" target="_blank" rel="noopener">https://github.com/everit-org/json-schema</a></td><td>draft-04</td></tr></tbody></table><p>建议：</p><ol><li><p>如果在项目中使用了jackson json，那么使用fge是一个好的选择，因为fge就是使用的jackson json。</p></li><li><p>如果项目中使用的是org.json API，那么使用everit会更好。</p></li><li><p>如果是使用以上两个库以外的库，那么就使用everit，因为everit会比fge的性能好上两倍。</p></li></ol><h2 id="fge的使用："><a href="#fge的使用：" class="headerlink" title="fge的使用："></a>fge的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.github.fge&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;json-schema-validator&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.2.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  JsonNode schema = readJsonFile(<span class="string">"src/main/resources/Schema.json"</span>);</span><br><span class="line">  JsonNode data = readJsonFile(<span class="string">"src/main/resources/failure.json"</span>);</span><br><span class="line">  ProcessingReport report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  Assert.assertTrue(report.isSuccess());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> JsonNode <span class="title">readJsonFile</span><span class="params">(String filePath)</span> </span>&#123;</span><br><span class="line">  JsonNode instance = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">instance = <span class="keyword">new</span> JsonNodeReader().fromReader(<span class="keyword">new</span> FileReader(filePath));</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> instance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真正的调用只有一行代码，需要传入验证规则和数据。分别有validate和validateUnchecked两种方法，区别在于validateUnchecked方法不会抛出ProcessingException异常。</p><p>还可以从字符串中读取json，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  String failure = <span class="keyword">new</span> String(<span class="string">"&#123;\"foo\":1234&#125;"</span>);</span><br><span class="line">  String Schema = <span class="string">"&#123;\"type\": \"object\", \"properties\" : &#123;\"foo\" : &#123;\"type\" : \"string\"&#125;&#125;&#125;"</span>;</span><br><span class="line">  ProcessingReport report = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">JsonNode data = JsonLoader.fromString(failure);</span><br><span class="line">JsonNode schema = JsonLoader.fromString(Schema);</span><br><span class="line">report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//Assert.assertTrue(report.isSuccess());</span></span><br><span class="line">  Iterator&lt;ProcessingMessage&gt; it = report.iterator();</span><br><span class="line">  <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">System.out.println(it.next());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中ProcessingReport对象中维护了一共迭代器，如果执行失败（执行成功时没有信息），其提供了一些高级故障信息。每个错误可能包含以下属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">level: 错误级别（应该就是error）</span><br><span class="line">schema：引起故障的模式的所在位置的 URI</span><br><span class="line">instance：错误对象</span><br><span class="line">domain：验证域</span><br><span class="line">keyword：引起错误的约束key</span><br><span class="line">found：现在类型</span><br><span class="line">expected：期望类型</span><br></pre></td></tr></table></figure><p>以上代码的json信息为：</p><p>failure.json ：  </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"foo"</span> : <span class="number">1234</span>&#125;</span><br></pre></td></tr></table></figure><p>Schema.json ：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="string">"properties"</span> : &#123;</span><br><span class="line">    <span class="string">"foo"</span> : &#123;</span><br><span class="line">      <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行错误信息为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">error: <span class="function">instance <span class="title">type</span> <span class="params">(integer)</span> does not match any allowed primitive <span class="title">type</span> <span class="params">(allowed: [<span class="string">"string"</span>])</span></span></span><br><span class="line"><span class="function">level: "error"</span></span><br><span class="line"><span class="function">schema: </span>&#123;<span class="string">"loadingURI"</span>:<span class="string">"#"</span>,<span class="string">"pointer"</span>:<span class="string">"/properties/foo"</span>&#125;</span><br><span class="line">instance: &#123;<span class="string">"pointer"</span>:<span class="string">"/foo"</span>&#125;</span><br><span class="line">domain: <span class="string">"validation"</span></span><br><span class="line">keyword: <span class="string">"type"</span></span><br><span class="line">found: <span class="string">"integer"</span></span><br><span class="line">expected: [<span class="string">"string"</span>]</span><br></pre></td></tr></table></figure><h2 id="everit的使用："><a href="#everit的使用：" class="headerlink" title="everit的使用："></a>everit的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.everit.json&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;org.everit.json.schema&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  InputStream inputStream = getClass().getResourceAsStream(<span class="string">"/Schema.json"</span>);</span><br><span class="line">  JSONObject Schema = <span class="keyword">new</span> JSONObject(<span class="keyword">new</span> JSONTokener(inputStream));</span><br><span class="line">  JSONObject data = <span class="keyword">new</span> JSONObject(<span class="string">"&#123;\"foo\" : 1234&#125;"</span>);</span><br><span class="line">  Schema schema = SchemaLoader.load(Schema);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">schema.validate(data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ValidationException e) &#123;</span><br><span class="line">System.out.println(e.getMessage());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果验证失败会抛出一个ValidationException异常，然后在catch块中打印出错误信息。everit中的错误信息想比fge来说比较简单，相同的json测试文件，打印的信息如下：</p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#/foo: expected <span class="keyword">type</span>: <span class="built_in">String</span>, found: <span class="built_in">Integer</span></span><br></pre></td></tr></table></figure><p>此外everit提供了一个format关键字，可以自定义validator来校验json中一些复杂数据，比如IP地址，电话号码等。具体请参考官方文档。</p><h2 id="性能测试："><a href="#性能测试：" class="headerlink" title="性能测试："></a>性能测试：</h2><p>1、一共执行1000次，成功和失败分开执行，每种情况执行250次。然后记录下每次的执行时间，执行10次，取平均值。</p><p>fge每1000次的执行时间(ms)：1158, 1122, 1120, 1042, 1180, 1254, 1198，1126，1177，1192<br>everit每1000次的执行时间(ms)：33, 49, 54, 57, 51, 47, 48, 52, 53, 44</p><p>2、一共执行10000次，成功和失败分开执行，每种情况执行2500次。</p><table><thead><tr><th>方法/场景</th><th>每次执行时间(ms)</th></tr></thead><tbody><tr><td>fge/场景1</td><td>1.1569</td></tr><tr><td>fge/场景2</td><td>0.3407</td></tr><tr><td>everit/场景1</td><td>0.0488</td></tr><tr><td>everit/场景2</td><td>0.0206</td></tr></tbody></table><p><strong>使用对比：</strong></p><p>​    从性能上来说everit完全是碾压fge，官方说的至少两倍，实际测试过程中，差不多有20倍的差距。虽然fge使用的是jackson json，相对来说学习成本可能较低，但是使用下来发现everit的使用也并不复杂，需要注意的是包需要导入正确（org.json）。fge唯一的优势在于错误信息比较详细。还有一点区别在于，everit验证失败是抛出异常，而fge是判断返回一个boolean类型的值。</p>]]></content>
    
    <summary type="html">
    
      JSON schema是一个帮助你定义、校验甚至是修复json数据格式的解决方案。它定义了一整套规则，允许我们通过定义一个schema(本身也是JSON)来描述一个JSON串的数据格式。
    
    </summary>
    
      <category term="数据存储格式" scheme="https://gjtmaster.github.io/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/"/>
    
    
      <category term="Json" scheme="https://gjtmaster.github.io/tags/Json/"/>
    
  </entry>
  
  <entry>
    <title>Flink 底层原理：Session Window</title>
    <link href="https://gjtmaster.github.io/2018/05/21/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9ASession%20Window/"/>
    <id>https://gjtmaster.github.io/2018/05/21/Flink 底层原理：Session Window/</id>
    <published>2018-05-21T02:13:21.000Z</published>
    <updated>2019-09-19T08:57:58.361Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章《Window机制》中，我们介绍了窗口的概念和底层实现，以及 Flink 一些内建的窗口，包括滑动窗口、翻滚窗口。本文将深入讲解一种较为特殊的窗口：会话窗口（session window）。建议您在阅读完上一篇文章的基础上再阅读本文。</p><p>当我们需要分析用户的一段交互的行为事件时，通常的想法是将用户的事件流按照“session”来分组。session 是指一段持续活跃的期间，由活跃间隙分隔开。通俗一点说，消息之间的间隔小于超时阈值（sessionGap）的，则被分配到同一个窗口，间隔大于阈值的，则被分配到不同的窗口。目前开源领域大部分的流计算引擎都有窗口的概念，但是没有对 session window 的支持，要实现 session window，需要用户自己去做完大部分事情。而当 Flink 1.1.0 版本正式发布时，Flink 将会是开源流计算领域第一个内建支持 session window 的引擎。</p><p>在 Flink 1.1.0 之前，Flink 也可以通过自定义的window assigner和trigger来实现一个基本能用的session window。<code>release-1.0</code> 版本中提供了一个实现 session window 的 example：<a href="https://github.com/apache/flink/blob/release-1.0/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/SessionWindowing.java" target="_blank" rel="noopener">SessionWindowing</a>。这个session window范例的实现原理是，基于GlobleWindow这个window assigner，将所有元素都分配到同一个窗口中，然后指定一个自定义的trigger来触发执行窗口。这个trigger的触发机制是，对于每个到达的元素都会根据其时间戳（timestamp）注册一个会话超时的定时器（timestamp+sessionTimeout），并移除上一次注册的定时器。最新一个元素到达后，如果超过 sessionTimeout 的时间还没有新元素到达，那么trigger就会触发，当前窗口就会是一个session window。处理完窗口后，窗口中的数据会清空，用来缓存下一个session window的数据。</p><p>但是这种session window的实现是非常弱的，无法应用到实际生产环境中的。因为它无法处理乱序 event time 的消息。 而在即将到来的 Flink 1.1.0 版本中，Flink 提供了对 session window 的直接支持，用户可以通过<code>SessionWindows.withGap()</code>来轻松地定义 session widnow，而且能够处理乱序消息。Flink 对 session window 的支持主要借鉴自 Google 的 DataFlow 。</p><h2 id="Session-Window-in-Flink"><a href="#Session-Window-in-Flink" class="headerlink" title="Session Window in Flink"></a>Session Window in Flink</h2><p>假设有这么个场景，用户点开手机淘宝后会进行一系列的操作（点击、浏览、搜索、购买、切换tab等），这些操作以及对应发生的时间都会发送到服务器上进行用户行为分析。那么用户的操作行为流的样例可能会长下面这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" alt="img"></a></p><p>通过上图，我们可以很直观地观察到，用户的行为是一段一段的，每一段内的行为都是连续紧凑的，段内行为的关联度要远大于段之间行为的关联度。我们把每一段用户行为称之为“session”，段之间的空档我们称之为“session gap”。所以，理所当然地，我们应该按照 session window 对用户的行为流进行切分，并计算每个session的结果。如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" alt="img"></a></p><p>为了定义上述的窗口切分规则，我们可以使用 Flink 提供的 <code>SessionWindows</code> 这个 widnow assigner API。如果你用过 <code>SlidingEventTimeWindows</code>、<code>TumlingProcessingTimeWindows</code>等，你会对这个很熟悉。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStream <span class="built_in">input</span> = …</span><br><span class="line">DataStream result = <span class="built_in">input</span></span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .window(SessionWindows.withGap(Time.seconds(&lt;seconds&gt;))</span><br><span class="line">  .apply(&lt;window <span class="function"><span class="keyword">function</span>&gt;)</span> // <span class="keyword">or</span> reduce() <span class="keyword">or</span> fold()</span><br></pre></td></tr></table></figure><p>这样，Flink 就会基于元素的时间戳，自动地将元素放到不同的session window中。如果两个元素的时间戳间隔小于 session gap，则会在同一个session中。如果两个元素之间的间隔大于session gap，且没有元素能够填补上这个gap，那么它们会被放到不同的session中。</p><h2 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h2><p>为了实现 session window，我们需要扩展 Flink 中的窗口机制，使得能够支持窗口合并。要理解其原因，我们需要先了解窗口的现状。在上一篇文章中，我们谈到了 Flink 中 WindowAssigner 负责将元素分配到哪个/哪些窗口中去，Trigger 决定了一个窗口何时能够被计算或清除。当元素被分配到窗口之后，这些窗口是固定的不会改变的，而且窗口之间不会相互作用。</p><p>对于session window来说，我们需要窗口变得更灵活。基本的思想是这样的：<code>SessionWindows</code> assigner 会为每个进入的元素分配一个窗口，该窗口以元素的时间戳作为起始点，时间戳加会话超时时间为结束点，也就是该窗口为<code>[timestamp, timestamp+sessionGap)</code>。比如我们现在到了两个元素，它们被分配到两个独立的窗口中，两个窗口目前不相交，如图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" alt="img"></a></p><p>当第三个元素进入时，分配到的窗口与现有的两个窗口发生了叠加，情况变成了这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" alt="img"></a></p><p>由于我们支持了窗口的合并，<code>WindowAssigner</code>可以合并这些窗口。它会遍历现有的窗口，并告诉系统哪些窗口需要合并成新的窗口。Flink 会将这些窗口进行合并，合并的主要内容有两部分：</p><ol><li>需要合并的窗口的底层状态的合并（也就是窗口中缓存的数据，或者对于聚合窗口来说是一个聚合值）</li><li>需要合并的窗口的Trigger的合并（比如对于EventTime来说，会删除旧窗口注册的定时器，并注册新窗口的定时器）</li></ol><p>总之，结果是三个元素现在在同一个窗口中了：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" alt="img"></a></p><p>需要注意的是，对于每一个新进入的元素，都会分配一个属于该元素的窗口，都会检查并合并现有的窗口。在触发窗口计算之前，每一次都会检查该窗口是否可以和其他窗口合并，直到trigger触发后，会将该窗口从窗口列表中移除。对于 event time 来说，窗口的触发是要等到大于窗口结束时间的 watermark 到达，当watermark没有到，窗口会一直缓存着。所以基于这种机制，可以做到对乱序消息的支持。</p><p>这里有一个优化点可以做，因为每一个新进入的元素都会创建属于该元素的窗口，然后合并。如果新元素连续不断地进来，并且新元素的窗口一直都是可以和之前的窗口重叠合并的，那么其实这里多了很多不必要的创建窗口、合并窗口的操作，我们可以直接将新元素放到那个已存在的窗口，然后扩展该窗口的大小，看起来就像和新元素的窗口合并了一样。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p><a href="https://issues.apache.org/jira/browse/FLINK-3174" target="_blank" rel="noopener">FLINK-3174</a> 这个JIRA中有对 Flink 如何支持 session window 的详细说明，以及代码更新。建议可以结合该 <a href="https://github.com/apache/flink/pull/1460" target="_blank" rel="noopener">PR</a>的代码来理解本文讨论的实现原理。</p><p>为了扩展 Flink 中的窗口机制，使得能够支持窗口合并，首先 window assigner 要能合并现有的窗口，Flink 增加了一个新的抽象类 <code>MergingWindowAssigner</code> 继承自 <code>WindowAssigner</code>，这里面主要多了一个 <code>mergeWindows</code> 的方法，用来决定哪些窗口是可以合并的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MergingWindowAssigner&lt;T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span> <span class="keyword">extends</span> <span class="title">WindowAssigner&lt;T</span>, <span class="title">W&gt;</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 决定哪些窗口需要被合并。对于每组需要合并的窗口, 都会调用 callback.merge(toBeMerged, mergeResult)</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param windows 现存的窗口集合 The window candidates.</span></span><br><span class="line"><span class="comment">   * @param callback 需要被合并的窗口会回调 callback.merge 方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  public <span class="keyword">abstract</span> void mergeWindows(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; windows, <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; callback);</span><br><span class="line"></span><br><span class="line">  public interface <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用来声明合并窗口的具体动作（合并窗口底层状态、合并窗口trigger等）。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @param toBeMerged  需要被合并的窗口列表</span></span><br><span class="line"><span class="comment">     * @param mergeResult 合并后的窗口</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    void merge(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; toBeMerged, <span class="type">W</span> mergeResult);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有已经存在的 assigner 都继承自 <code>WindowAssigner</code>，只有新加入的 session window assigner 继承自 <code>MergingWindowAssigner</code>，如：<code>ProcessingTimeSessionWindows</code>和<code>EventTimeSessionWindows</code>。</p><p>另外，Trigger 也需要能支持对合并窗口后的响应，所以 Trigger 添加了一个新的接口 <code>onMerge(W window, OnMergeContext ctx)</code>，用来响应发生窗口合并之后对trigger的相关动作，比如根据合并后的窗口注册新的 event time 定时器。</p><p>OK，接下来我们看下最核心的代码，也就是对于每个进入的元素的处理，代码位于<code>WindowOperator.processElement</code>方法中，如下所示：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">public void processElement(StreamRecord<span class="variable">&lt;IN&gt;</span> element) throws Exception &#123;</span><br><span class="line">  Collection<span class="variable">&lt;W&gt;</span> elementWindows = windowAssigner.assignWindows(element.getValue(), element.getTimestamp());</span><br><span class="line">  final K key = (K) getStateBackend().getCurrentKey();</span><br><span class="line">  if (windowAssigner instanceof MergingWindowAssigner) &#123;</span><br><span class="line">    // 对于session window 的特殊处理，我们只关注该条件块内的代码</span><br><span class="line">    MergingWindowSet<span class="variable">&lt;W&gt;</span> mergingWindows = getMergingWindowSet();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line">      final Tuple1<span class="variable">&lt;TriggerResult&gt;</span> mergeTriggerResult = new Tuple1<span class="variable">&lt;&gt;</span>(TriggerResult.CONTINUE);</span><br><span class="line">      </span><br><span class="line">      // 加入新窗口, 如果没有合并发生,那么actualWindow就是新加入的窗口</span><br><span class="line">      // 如果有合并发生, 那么返回的actualWindow即为合并后的窗口,</span><br><span class="line">      // 并且会调用 MergeFunction.merge 方法, 这里方法中的内容主要是更新trigger, 合并旧窗口中的状态到新窗口中</span><br><span class="line">      W actualWindow = mergingWindows.addWindow(window, new MergingWindowSet.MergeFunction<span class="variable">&lt;W&gt;</span>() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void merge(W mergeResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedWindows, W <span class="keyword">state</span>WindowResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedStateWindows) throws Exception &#123;</span><br><span class="line">          context.key = key;</span><br><span class="line">          context.window = mergeResult;</span><br><span class="line"></span><br><span class="line">          // 这里面会根据新窗口的结束时间注册新的定时器</span><br><span class="line">          mergeTriggerResult.f0 = context.<span class="keyword">on</span>Merge(mergedWindows);</span><br><span class="line"></span><br><span class="line">          // 删除旧窗口注册的定时器</span><br><span class="line">          <span class="keyword">for</span> (W m: mergedWindows) &#123;</span><br><span class="line">            context.window = m;</span><br><span class="line">            context.clear();</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          // 合并旧窗口(mergedStateWindows)中的状态到新窗口（<span class="keyword">state</span>WindowResult）中</span><br><span class="line">          getStateBackend().mergePartitionedStates(<span class="keyword">state</span>WindowResult,</span><br><span class="line">              mergedStateWindows,</span><br><span class="line">              windowSerializer,</span><br><span class="line">              (StateDescriptor<span class="variable">&lt;? extends MergingState&lt;?,?&gt;</span>, ?&gt;) windowStateDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      // 取 actualWindow 对应的用来存状态的窗口</span><br><span class="line">      W <span class="keyword">state</span>Window = mergingWindows.getStateWindow(actualWindow);</span><br><span class="line">      // 从状态后端拿出对应的状态 </span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(<span class="keyword">state</span>Window, windowSerializer, windowStateDescriptor);</span><br><span class="line">      // 将新进入的元素数据加入到新窗口（或者说合并后的窗口）中对应的状态中</span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = actualWindow;</span><br><span class="line"></span><br><span class="line">      // 检查是否需要fire or purge </span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      TriggerResult combinedTriggerResult = TriggerResult.merge(triggerResult, mergeTriggerResult.f0);</span><br><span class="line"></span><br><span class="line">      // 根据trigger结果决定怎么处理窗口中的数据</span><br><span class="line">      processTriggerResult(combinedTriggerResult, actualWindow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 对于普通window assigner的处理， 这里我们不关注</span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(window, windowSerializer,</span><br><span class="line">          windowStateDescriptor);</span><br><span class="line"></span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = window;</span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      processTriggerResult(triggerResult, window);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实这段代码写的并不是很clean，并且不是很好理解。在第六行中有用到<code>MergingWindowSet</code>，这个类很重要所以我们先介绍它。这是一个用来跟踪窗口合并的类。比如我们有A、B、C三个窗口需要合并，合并后的窗口为D窗口。这三个窗口在底层都有对应的状态集合，为了避免代价高昂的状态替换（创建新状态是很昂贵的），我们保持其中一个窗口作为原始的状态窗口，其他几个窗口的数据合并到该状态窗口中去，比如随机选择A作为状态窗口，那么B和C窗口中的数据需要合并到A窗口中去。这样就没有新状态产生了，但是我们需要额外维护窗口与状态窗口之间的映射关系（D-&gt;A），这就是<code>MergingWindowSet</code>负责的工作。这个映射关系需要在失败重启后能够恢复，所以<code>MergingWindowSet</code>内部也是对该映射关系做了容错。状态合并的工作示意图如下所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" alt="img"></a></p><p>然后我们来解释下processElement的代码，首先根据window assigner为新进入的元素分配窗口集合。接着进入第一个条件块，取出当前的<code>MergingWindowSet</code>。对于每个分配到的窗口，我们就会将其加入到<code>MergingWindowSet</code>中（<code>addWindow</code>方法），由<code>MergingWindowSet</code>维护窗口与状态窗口之间的关系，并在需要窗口合并的时候，合并状态和trigger。然后根据映射关系，取出结果窗口对应的状态窗口，根据状态窗口取出对应的状态。将新进入的元素数据加入到该状态中。最后，根据trigger结果来对窗口数据进行处理，对于session window来说，这里都是不进行任何处理的。真正对窗口处理是由定时器超时后对完成的窗口调用<code>processTriggerResult</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文在上一篇文章《Window机制》的基础上，深入讲解了 Flink 是如何支持 session window 的，核心的原理是窗口的合并。Flink 对于 session window 的支持很大程度上受到了 Google DataFlow 的启发，所以也建议阅读下 DataFlow 的论文。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
    
    <summary type="html">
    
      本文在上一篇文章《Window机制》的基础上，深入讲解了 Flink 是如何支持 session window 的，核心的原理是窗口的合并。
    
    </summary>
    
      <category term="实时计算框架" scheme="https://gjtmaster.github.io/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/"/>
    
    
      <category term="Flink" scheme="https://gjtmaster.github.io/tags/Flink/"/>
    
      <category term="实时计算" scheme="https://gjtmaster.github.io/tags/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
