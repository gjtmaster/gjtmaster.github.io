<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink 进阶：Time 深度解析</title>
      <link href="/2019/09/17/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/09/17/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>原  作  者 | 崔星灿</p><p>原整理者 | 沙晟阳（成阳）</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/001-1024x449.png" alt="img"></p><h3 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h3><p>在不同的应用场景中时间语义是各不相同的，Flink 作为一个先进的分布式流处理引擎，它本身支持不同的时间语义。其核心是 Processing Time 和 Event Time（Row Time），这两类时间主要的不同点如下表所示：</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/002-1024x415.png" alt="img"></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time 则是根据每一条处理记录所携带的时间戳来判定。</p><p>这两种时间在 Flink 内部的处理以及还是用户的实际使用方面，难易程度都是不同的。相对而言的 Processing Time 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用 Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，使用 Event Time 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同。如果希望结果完全相同，就只能用 Event Time；如果接受结果不同，则可以用 Processing Time。Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用 Processing Time。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/003-1024x499.png" alt="img"></p><h4 id="时间的特性"><a href="#时间的特性" class="headerlink" title="时间的特性"></a>时间的特性</h4><p><strong>时间的一个重要特性是：时间只能递增，不会来回穿越。</strong> 在使用时间的时候我们要充分利用这个特性。假设我们有这么一些记录，然后我们来分别看一下 Processing Time 还有 Event Time 对于时间的处理。</p><ul><li>对于 Processing Time，因为我们是使用的是本地节点的时间（假设这个节点的时钟同步没有问题），我们每一次取到的 Processing Time 肯定都是递增的，递增就代表着有序，所以说我们相当于拿到的是一个有序的数据流。</li><li>而在用 Event Time 的时候因为时间是绑定在每一条的记录上的，由于网络延迟、程序内部逻辑、或者其他一些分布式系统的原因，数据的时间可能会存在一定程度的乱序，比如上图的例子。在 Event Time 场景下，我们把每一个记录所包含的时间称作 Record Timestamp。如果 Record Timestamp 所得到的时间序列存在乱序，我们就需要去处理这种情况。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/004.png" alt="img"></p><p>如果单条数据之间是乱序，我们就考虑对于整个序列进行更大程度的离散化。简单地讲，就是把数据按照一定的条数组成一些小批次，但这里的小批次并不是攒够多少条就要去处理，而是为了对他们进行时间上的划分。经过这种更高层次的离散化之后，我们会发现最右边方框里的时间就是一定会小于中间方框里的时间，中间框里的时间也一定会小于最左边方框里的时间。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/005.png" alt="img"></p><p>这个时候我们在整个时间序列里插入一些类似于标志位的一些特殊的处理数据，这些特殊的处理数据叫做 watermark。一个 watermark 本质上就代表了这个 watermark 所包含的 timestamp 数值，表示以后到来的数据已经再也没有小于或等于这个时间的了。</p><h3 id="Timestamp-和-Watermark-行为概览"><a href="#Timestamp-和-Watermark-行为概览" class="headerlink" title="Timestamp 和 Watermark 行为概览"></a>Timestamp 和 Watermark 行为概览</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/006-1024x442.png" alt="img"></p><p>接下来我们重点看一下 Event Time 里的 Record Timestamp（简写成 timestamp）和 watermark 的一些基本信息。绝大多数的分布式流计算引擎对于数据都是进行了 DAG 图的抽象，它有自己的数据源，有处理算子，还有一些数据汇。数据在不同的逻辑算子之间进行流动。watermark 和 timestamp 有自己的生命周期，接下来我会从 watermark 和 timestamp 的产生、他们在不同的节点之间的传播、以及在每一个节点上的处理，这三个方面来展开介绍。</p><h4 id="Timestamp-分配和-Watermark-生成"><a href="#Timestamp-分配和-Watermark-生成" class="headerlink" title="Timestamp 分配和 Watermark 生成"></a>Timestamp 分配和 Watermark 生成</h4><p>Flink 支持两种 watermark 生成方式。第一种是在 SourceFunction 中产生，相当于把整个的 timestamp 分配和 watermark 生成的逻辑放在流处理应用的源头。我们可以在 SourceFunction 里面通过这两个方法产生 watermark：</p><ul><li>通过 collectWithTimestamp 方法发送一条数据，其中第一个参数就是我们要发送的数据，第二个参数就是这个数据所对应的时间戳；也可以调用 emitWatermark 方法去产生一条 watermark，表示接下来不会再有时间戳小于等于这个数值记录。</li><li>另外，有时候我们不想在 SourceFunction 里生成 timestamp 或者 watermark，或者说使用的 SourceFunction 本身不支持，我们还可以在使用 DataStream API 的时候指定，调用的 DataStream.assignTimestampsAndWatermarks 这个方法，能够接收不同的 timestamp 和 watermark 的生成器。</li></ul><p>总体上而言生成器可以分为两类：第一类是定期生成器；第二类是根据一些在流处理数据流中遇到的一些特殊记录生成的。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/007.png" alt="img"></p><p>两者的区别主要有三个方面，首先定期生成是现实时间驱动的，这里的“定期生成”主要是指 watermark（因为 timestamp 是每一条数据都需要有的），即定期会调用生成逻辑去产生一个 watermark。而根据特殊记录生成是数据驱动的，即是否生成 watermark 不是由现实时间来决定，而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了，这个时候相当于每一次分配 Timestamp 之后都会调用用户实现的 watermark 生成方法，用户需要在生成方法中去实现 watermark 的生成逻辑。</p><p>大家要注意的是就是我们在分配 timestamp 和生成 watermark 的过程，虽然在 SourceFunction 和 DataStream 中都可以指定，但是还是建议生成的工作越靠近 DataSource 越好。这样会方便让程序逻辑里面更多的 operator 去判断某些数据是否乱序。Flink 内部提供了很好的机制去保证这些 timestamp 和 watermark 被正确地传递到下游的节点。</p><h4 id="Watermark-传播"><a href="#Watermark-传播" class="headerlink" title="Watermark 传播"></a>Watermark 传播</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/008-1024x474.png" alt="img"></p><p>具体的传播策略基本上遵循这三点。</p><ul><li>首先，watermark 会以广播的形式在算子之间进行传播。比如说上游的算子，它连接了三个下游的任务，它会把自己当前的收到的 watermark 以广播的形式传到下游。</li><li>第二，如果在程序里面收到了一个 Long.MAX_VALUE 这个数值的 watermark，就表示对应的那一条流的一个部分不会再有数据发过来了，它相当于就是一个终止的一个标志。</li><li>第三，对于单流而言，这个策略比较好理解，而对于有多个输入的算子，watermark 的计算就有讲究了，一个原则是：单输入取其大，多输入取小。</li></ul><p>举个例子，假设这边蓝色的块代表一个算子的一个任务，然后它有三个输入，分别是 W1、W2、W3，这三个输入可以理解成任何的输入，这三个输入可能是属于同一个流，也可能是属于不同的流。然后在计算 watermark 的时候，对于单个输入而言是取他们的最大值，因为我们都知道 watermark 应该遵循一个单调递增的一个原则。对于多输入，它要统计整个算子任务的 watermark 时，就会取这三个计算出来的 watermark 的最小值。即一个多个输入的任务，它的 watermark 受制于最慢的那条输入流。这一点类似于木桶效应，整个木桶中装的水会就是受制于最矮的那块板。</p><p>watermark 在传播的时候有一个特点是，它的传播是幂等的。多次收到相同的 watermark，甚至收到之前的 watermark 都不会对最后的数值产生影响，因为对于单个输入永远是取最大的，而对于整个任务永远是取一个最小的。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在它没有区分你这个输入是一条流多个 partition 还是来自于不同的逻辑上的流的 JOIN。对于同一个流的不同 partition，我们对他做这种强制的时钟同步是没有问题的，因为一开始就是把一条流拆散成不同的部分，但每一个部分之间共享相同的时钟。但是如果算子的任务是在做类似于 JOIN 操作，那么要求你两个输入的时钟强制同步其实没有什么道理的，因为完全有可能是把一条离现在时间很近的数据流和一个离当前时间很远的数据流进行 JOIN，这个时候对于快的那条流，因为它要等慢的那条流，所以说它可能就要在状态中去缓存非常多的数据，这对于整个集群来说是一个很大的性能开销。</p><h4 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h4><p>在正式介绍 watermark 的处理之前，先简单介绍 ProcessFunction，因为 watermark 在任务里的处理逻辑分为内部逻辑和外部逻辑。外部逻辑其实就是通过 ProcessFunction 来体现的，如果你需要使用 Flink 提供的时间相关的 API 的话就只能写在 ProcessFunction 里。</p><p>ProcessFunction 和时间相关的功能主要有三点：</p><ul><li>第一点就是根据你当前系统使用的时间语义不同，你可以去获取当前你正在处理这条记录的 Record Timestamp，或者当前的 Processing Time。</li><li>第二点就是它可以获取当前算子的时间，可以把它理解成当前的 watermark。</li><li>第三点就是为了在 ProcessFunction 中去实现一些相对复杂的功能，允许注册一些 timer（定时器）。比如说在 watermark 达到某一个时间点的时候就触发定时器，所有的这些回调逻辑也都是由用户来提供，涉及到如下三个方法，registerEventTimeTimer、registerProcessingTimeTimer 和 onTimer。在 onTimer 方法中就需要去实现自己的回调逻辑，当条件满足时回调逻辑就会被触发。</li></ul><p>一个简单的应用是，我们在做一些时间相关的处理的时候，可能需要缓存一部分数据，但这些数据不能一直去缓存下去，所以需要有一些过期的机制，我们可以通过 timer 去设定这么一个时间，指定某一些数据可能在将来的某一个时间点过期，从而把它从状态里删除掉。所有的这些和时间相关的逻辑在 Flink 内部都是由自己的 Time Service（时间服务）完成的。</p><h4 id="Watermark-处理"><a href="#Watermark-处理" class="headerlink" title="Watermark 处理"></a>Watermark 处理</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/009-1024x376.png" alt="img"></p><p>一个算子的实例在收到 watermark 的时候，首先要更新当前的算子时间，这样的话在 ProcessFunction 里方法查询这个算子时间的时候，就能获取到最新的时间。第二步它会遍历计时器队列，这个计时器队列就是我们刚刚说到的 timer，你可以同时注册很多 timer，Flink 会把这些 Timer 按照触发时间放到一个优先队列中。第三步 Flink 得到一个时间之后就会遍历计时器的队列，然后逐一触发用户的回调逻辑。 通过这种方式，Flink 的某一个任务就会将当前的 watermark 发送到下游的其他任务实例上，从而完成整个 watermark 的传播，从而形成一个闭环。</p><h3 id="Table-API-中的时间"><a href="#Table-API-中的时间" class="headerlink" title="Table API 中的时间"></a>Table API 中的时间</h3><p>下面我们来看一看 Table/SQL API 中的时间。为了让时间参与到 Table/SQL 这一层的运算中，我们需要提前把时间属性放到表的 schema 中，这样的话我们才能够在 SQL 语句或者 Table 的一些逻辑表达式里面去使用这些时间去完成需求。</p><h4 id="Table-中指定时间列"><a href="#Table-中指定时间列" class="headerlink" title="Table 中指定时间列"></a>Table 中指定时间列</h4><p>其实之前社区就怎么在 Table/SQL 中去使用时间这个问题做过一定的讨论，是把获取当前 Processing Time 的方法是作为一个特殊的 UDF，还是把这一个列物化到整个的 schema 里面，最终采用了后者。我们这里就分开来讲一讲 Processing Time 和 Event Time 在使用的时候怎么在 Table 中指定。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/010-1024x487.png" alt="img"></p><p>对于 Processing Time，我们知道要得到一个 Table 对象（或者注册一个 Table）有两种手段：</p><ol><li>可以从一个 DataStream 转化成一个 Table；</li><li>直接通过 TableSource 去生成这么一个 Table；</li></ol><p>对于第一种方法而言，我们只需要在你已有的这些列中（例子中 f1 和 f2 就是两个已有的列），在最后用“列名.proctime”这种写法就可以把最后的这一列注册为一个 Processing Time，以后在写查询的时候就可以去直接使用这一列。如果 Table 是通过 TableSource 生成的，就可以通过实现这一个 DefinedRowtimeAttributes 接口，然后就会自动根据你提供的逻辑去生成对应的 Processing Time。</p><p>相对而言，在使用 Event Time 时则有一个限制，因为 Event Time 不像 Processing Time 那样是随拿随用。如果你要从 DataStream 去转化得到一个 Table，必须要提前保证原始的 DataStream 里面已经存在了 Record Timestamp 和 watermark。如果你想通过 TableSource 生成的，也一定要保证你要接入的一个数据里面存在一个类型为 long 或者 timestamp 的这么一个时间字段。</p><p>具体来说，如果你要从 DataStream 去注册一个表，和 proctime 类似，你只需要加上“列名.rowtime”就可以。需要注意的是，如果你要用 Processing Time，必须保证你要新加的字段是整个 schema 中的最后一个字段，而 Event Time 的时候你其实可以去替换某一个已有的列，然后 Flink 会自动的把这一列转化成需要的 rowtime 这个类型。 如果是通过 TableSource 生成的，只需要实现 DefinedRowtimeAttributes 接口就可以了。需要说明的一点是，在 DataStream API 这一侧其实不支持同时存在多个 Event Time（rowtime），但是在 Table 这一层理论上可以同时存在多个 rowtime。因为 DefinedRowtimeAttributes 接口的返回值是一个对于 rowtime 描述的 List，即其实可以同时存在多个 rowtime 列，在将来可能会进行一些其他的改进，或者基于去做一些相应的优化。</p><h4 id="时间列和-Table-操作"><a href="#时间列和-Table-操作" class="headerlink" title="时间列和 Table 操作"></a>时间列和 Table 操作</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/011-1024x475.png" alt="img"></p><p>指定完了时间列之后，当我们要真正去查询时就会涉及到一些具体的操作。这里我列举的这些操作都是和时间列紧密相关，或者说必须在这个时间列上才能进行的。比如说“Over 窗口聚合”和“Group by 窗口聚合”这两种窗口聚合，在写 SQL 提供参数的时候只能允许你在这个时间列上进行这种聚合。第三个就是时间窗口聚合，你在写条件的时候只支持对应的时间列。最后就是排序，我们知道在一个无尽的数据流上对数据做排序几乎是不可能的事情，但因为这个数据本身到来的顺序已经是按照时间属性来进行排序，所以说我们如果要对一个 DataStream 转化成 Table 进行排序的话，你只能是按照时间列进行排序，当然同时你也可以指定一些其他的列，但是时间列这个是必须的，并且必须放在第一位。</p><p>为什么说这些操作只能在时间列上进行？因为我们有的时候可以把到来的数据流就看成是一张按照时间排列好的一张表，而我们任何对于表的操作，其实都是必须在对它进行一次顺序扫描的前提下完成的。因为大家都知道数据流的特性之一就是一过性，某一条数据处理过去之后，将来其实不太好去访问它。当然因为 Flink 中内部提供了一些状态机制，我们可以在一定程度上去弱化这个特性，但是最终还是不能超越的限制状态不能太大。所有这些操作为什么只能在时间列上进行，因为这个时间列能够保证我们内部产生的状态不会无限的增长下去，这是一个最终的前提。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.7.1与Kafka0.11.0.1</title>
      <link href="/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/"/>
      <url>/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink的Checkpoint"><a href="#Flink的Checkpoint" class="headerlink" title="Flink的Checkpoint"></a>Flink的Checkpoint</h1><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><ul><li>使用StreamExecutionEnvironment.enableCheckpointing方法来设置开启checkpoint；具体可以使用enableCheckpointing(long interval)，或者enableCheckpointing(long interval, CheckpointingMode mode)；interval用于指定checkpoint的触发间隔(单位milliseconds)，而CheckpointingMode默认是CheckpointingMode.EXACTLY_ONCE，也可以指定为CheckpointingMode.AT_LEAST_ONCE</li><li>也可以通过StreamExecutionEnvironment.getCheckpointConfig().setCheckpointingMode来设置CheckpointingMode，一般对于超低延迟的应用(大概几毫秒)可以使用CheckpointingMode.AT_LEAST_ONCE，其他大部分应用使用CheckpointingMode.EXACTLY_ONCE就可以</li><li>checkpointTimeout用于指定checkpoint执行的超时时间(单位milliseconds)，超时没完成就会被abort掉</li><li>minPauseBetweenCheckpoints用于指定checkpoint coordinator上一个checkpoint完成之后最小等多久可以出发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</li><li>maxConcurrentCheckpoints用于指定运行中的checkpoint最多可以有多少个，用于包装topology不会花太多的时间在checkpoints上面；如果有设置了minPauseBetweenCheckpoints，则maxConcurrentCheckpoints这个参数就不起作用了(大于1的值不起作用)</li><li>enableExternalizedCheckpoints用于开启checkpoints的外部持久化，但是在job失败的时候不会自动清理，需要自己手工清理state；ExternalizedCheckpointCleanup用于指定当job canceled的时候externalized checkpoint该如何清理，DELETE_ON_CANCELLATION的话，在job canceled的时候会自动删除externalized state，但是如果是FAILED的状态则会保留；RETAIN_ON_CANCELLATION则在job canceled的时候会保留externalized checkpoint state</li><li>failOnCheckpointingErrors用于指定在checkpoint发生异常的时候，是否应该fail该task，默认为true，如果设置为false，则task会拒绝checkpoint然后继续运行</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.env.getCheckpointConfig().setFailOnCheckpointingErrors(true);</code></pre><h1 id="FlinkKafkaConsumer011"><a href="#FlinkKafkaConsumer011" class="headerlink" title="FlinkKafkaConsumer011"></a>FlinkKafkaConsumer011</h1><h2 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h2><ul><li>setStartFromGroupOffsets()【默认消费策略】<br>默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</li><li>setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息</li><li>setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息</li><li>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)</li></ul><ul><li>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</li><li>为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次</li><li>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:<br>(1) Checkpoint关闭时： 可以通过下面两个参数配置<br>enable.auto.commit<br>auto.commit.interval.ms<br>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置<br>setCommitOffsetsOnCheckpoints(boolean)<br>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略</li></ul><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; public class StreamingKafkaSource {    public static void main(String[] args) throws Exception {        //获取Flink的运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //checkpoint配置        env.enableCheckpointing(5000);        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);        env.getCheckpointConfig().setCheckpointTimeout(60000);        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);        //设置statebackend        //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://hadoop100:9000/flink/checkpoints&quot;,true));        String topic = &quot;kafkaConsumer&quot;;        Properties prop = new Properties();        prop.setProperty(&quot;bootstrap.servers&quot;,&quot;SparkMaster:9092&quot;);        prop.setProperty(&quot;group.id&quot;,&quot;kafkaConsumerGroup&quot;);        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;&gt;(topic, new SimpleStringSchema(), prop);        myConsumer.setStartFromGroupOffsets();//默认消费策略        DataStreamSource&lt;String&gt; text = env.addSource(myConsumer);        text.print().setParallelism(1);        env.execute(&quot;StreamingFromCollection&quot;);    }}</code></pre><h1 id="FlinkKafkaProducer011"><a href="#FlinkKafkaProducer011" class="headerlink" title="FlinkKafkaProducer011"></a>FlinkKafkaProducer011</h1><h2 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h2><ul><li>Kafka Producer的容错-Kafka 0.9 and 0.10</li><li>如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数:<br>setLogFailuresOnly(false)<br>setFlushOnCheckpoint(true)</li><li>注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是0】</li><li>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义<br>Semantic.NONE<br>Semantic.AT_LEAST_ONCE【默认】<br>Semantic.EXACTLY_ONCE</li></ul><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><pre><code>public class StreamingKafkaSink {    public static void main(String[] args) throws Exception {    //获取Flink的运行环境    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    //checkpoint配置    env.enableCheckpointing(5000);    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);    env.getCheckpointConfig().setCheckpointTimeout(60000);    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);    env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);    //设置statebackend    //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://SparkMaster:9000/flink/checkpoints&quot;,true));    DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;SparkMaster&quot;, 9001, &quot;\n&quot;);    String brokerList = &quot;SparkMaster:9092&quot;;    String topic = &quot;kafkaProducer&quot;;    Properties prop = new Properties();    prop.setProperty(&quot;bootstrap.servers&quot;,brokerList);    //第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间    //设置事务超时时间    //prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);    //第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。    //FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());    //使用EXACTLY_ONCE语义的kafkaProducer    FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(topic, new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);    text.addSink(myProducer);    env.execute(&quot;StreamingFromCollection&quot;);  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink On Yarn HA</title>
      <link href="/2018/11/10/Flink%20on%20Yarn%20HA/"/>
      <url>/2018/11/10/Flink%20on%20Yarn%20HA/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 关闭防火墙</span><br><span class="line"><span class="meta">#</span> 配好主机映射</span><br><span class="line"><span class="meta">#</span> 配置免密登录</span><br><span class="line"><span class="meta">#</span> 准备好安装包 hadoop-2.8.5.tar.gz、flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz</span><br><span class="line"><span class="meta">#</span> 创建flink用户，后续操作均在flink用户下操作</span><br><span class="line"><span class="meta">#</span> 将Hadoop安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 将flink安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 节点配置如下：</span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">192.168.23.51</td><td align="center">flink01</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、NameNode、DFSZKFailoverController、DataNode</td></tr><tr><td align="center">192.168.23.52</td><td align="center">flink02</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、 NameNode、DFSZKFailoverController、DataNode、ResourceManager</td></tr><tr><td align="center">192.168.23.53</td><td align="center">flink03</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、ResourceManager、DataNode</td></tr></tbody></table><h2 id="Hadoop-HA配置"><a href="#Hadoop-HA配置" class="headerlink" title="Hadoop HA配置"></a>Hadoop HA配置</h2><h3 id="进入hadoop配置目录"><a href="#进入hadoop配置目录" class="headerlink" title="进入hadoop配置目录"></a>进入hadoop配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 进入hadoop配置目录</span><br><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改Java环境配置"><a href="#修改Java环境配置" class="headerlink" title="修改Java环境配置"></a>修改Java环境配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 修改hadoop-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置yarn-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置mapred-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/name1,/data/apps/hadoop-2.8.5/tmp/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/data1,/data/apps/hadoop-2.8.5/tmp/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://flink01:8485;flink02:8485;flink03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行</span></span><br><span class="line"><span class="comment">sshfence:当Active出问题后，standby切换成Active，此时，原Active又没有停止服务，这种情况下会被强制杀死进程。</span></span><br><span class="line"><span class="comment">shell(/bin/true)：NN Active和它的ZKFC一起挂了，没有人通知ZK，ZK长期没有接到通知，standby要切换，此时，standby调一个shell（脚本内容），这个脚本返回true则切换成功。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/flink/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rmcluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群的地址--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--启用自动恢复--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置capacity-scheduler-xml"><a href="#配置capacity-scheduler-xml" class="headerlink" title="配置capacity-scheduler.xml"></a>配置capacity-scheduler.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>集群中可用于运行application master的资源比例上限.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="启动Zookeeper集群"><a href="#启动Zookeeper集群" class="headerlink" title="启动Zookeeper集群"></a>启动Zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="初始化Hadoop环境"><a href="#初始化Hadoop环境" class="headerlink" title="初始化Hadoop环境"></a>初始化Hadoop环境</h3><p>启动journalnode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>格式化zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行完成后，会在zookeeper 上创建一个目录，查看是否创建成功：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入zookeeper家目录，执行bin/zkCli.sh客户端连接ZK。在ZK客户端的shell命令行查看：ls /</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 出现hadoop-ha即表示成功。</span></span><br></pre></td></tr></table></figure><p>启动主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>备用NN 同步主NN信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>关闭已启动的所有journalnode和主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="启动hadoop集群"><a href="#启动hadoop集群" class="headerlink" title="启动hadoop集群"></a>启动hadoop集群</h3><p>启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令（建议先启动所有journalnode以防出现namenode连接journalnode超时）</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个namenode的状态</span></span><br><span class="line">bin/hdfs haadmin -getServiceState nn1     #查看nn1状态</span><br><span class="line">bin/hdfs haadmin -getServiceState nn2     #查看nn2状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动切换namenode状态（此处禁用，有需要再执行）</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1##切换成active</span><br><span class="line">bin/hdfs haadmin -transitionToStandby nn1##切换成standby</span><br></pre></td></tr></table></figure><p>启动Yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在flink03执行以下命令</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个Resourcemanager的状态</span></span><br><span class="line">bin/yarn rmadmin -getServiceState rm1      ##查看rm1的状态</span><br><span class="line">bin/yarn rmadmin -getServiceState rm2      ##查看rm2的状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当flink02的ResourceManager是Active状态的时候，访问flink03的ResourceManager会自动跳转到flink02的web页面</span></span><br></pre></td></tr></table></figure><h2 id="Flink-HA配置"><a href="#Flink-HA配置" class="headerlink" title="Flink HA配置"></a>Flink HA配置</h2><h3 id="进入flink配置目录"><a href="#进入flink配置目录" class="headerlink" title="进入flink配置目录"></a>进入flink配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html" target="_blank" rel="noopener">点此查看flink配置说明</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The config parameter defining the network address to connect to <span class="keyword">for</span> communication with the job manager. This value is only interpreted <span class="keyword">in</span> setups <span class="built_in">where</span> a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used <span class="keyword">in</span> many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</span></span><br><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the JobManager.</span></span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the TaskManagers, <span class="built_in">which</span> are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager<span class="string">'s YARN container, minus a certain tolerance value.</span></span></span><br><span class="line">taskmanager.heap.size: 2048m</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of parallel operator or user <span class="keyword">function</span> instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a <span class="keyword">function</span> or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or <span class="keyword">function</span> instances. This value is typically proportional to the number of physical CPU cores that the TaskManager<span class="string">'s machine has (e.g., equal to the number of cores, or half the number of cores).</span></span></span><br><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line"><span class="meta">#</span><span class="bash"> Default parallelism <span class="keyword">for</span> <span class="built_in">jobs</span>.</span></span><br><span class="line">parallelism.default: 2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defines high-availability mode used <span class="keyword">for</span> the cluster execution. To <span class="built_in">enable</span> high-availability, <span class="built_in">set</span> this mode to <span class="string">"ZOOKEEPER"</span> or specify FQN of factory class.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> high-availability mode (required): The high-availability mode has to be <span class="built_in">set</span> <span class="keyword">in</span> conf/flink-conf.yaml to zookeeper <span class="keyword">in</span> order to <span class="built_in">enable</span> high availability mode. Alternatively this option can be <span class="built_in">set</span> to FQN of factory class Flink should use to create HighAvailabilityServices instance.</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash"> File system path (URI) <span class="built_in">where</span> Flink persists metadata <span class="keyword">in</span> high-availability setups.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Storage directory (required): JobManager metadata is persisted <span class="keyword">in</span> the file system storageDir and only a pointer to this state is stored <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The storageDir stores all metadata needed to recover a JobManager failure.</span></span><br><span class="line">high-availability.storageDir: hdfs://ns1/flink/recovery</span><br><span class="line"><span class="meta">#</span><span class="bash"> The ZooKeeper quorum to use, when running Flink <span class="keyword">in</span> a high-availability mode with ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper quorum (required): A ZooKeeper quorum is a replicated group of ZooKeeper servers, <span class="built_in">which</span> provide the distributed coordination service.</span></span><br><span class="line">high-availability.zookeeper.quorum: flink01:2181,flink02:2181,flink03:2181</span><br><span class="line"><span class="meta">#</span><span class="bash"> The root path under <span class="built_in">which</span> Flink stores its entries <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper root (recommended): The root ZooKeeper node, under <span class="built_in">which</span> all cluster nodes are placed.</span></span><br><span class="line">high-availability.zookeeper.path.root: /flink</span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts: The number of ApplicationMaster (+ its TaskManager containers) attempts. If this value is <span class="built_in">set</span> to 1 (default), the entire YARN session will fail when the Application master fails. Higher values specify the number of restarts of the ApplicationMaster by YARN.</span></span><br><span class="line">yarn.application-attempts: 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The state backend to be used to store and checkpoint state.</span></span><br><span class="line">state.backend: rocksdb</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory used <span class="keyword">for</span> storing the data files and meta data of checkpoints <span class="keyword">in</span> a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).</span></span><br><span class="line">state.checkpoints.dir: hdfs://ns1/flink/flink-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory <span class="keyword">for</span> savepoints. Used by the state backends that write savepoints to file systems (MemoryStateBackend, FsStateBackend, RocksDBStateBackend).</span></span><br><span class="line">state.savepoints.dir: hdfs://ns1/flink/save-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> Option whether the state backend should create incremental checkpoints, <span class="keyword">if</span> possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Some state backends may not support incremental checkpoints and ignore this option.</span></span><br><span class="line">state.backend.incremental: true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Directories <span class="keyword">for</span> temporary files, separated by<span class="string">","</span>, <span class="string">"|"</span>, or the system<span class="string">'s java.io.File.pathSeparator.</span></span></span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br></pre></td></tr></table></figure><p>切记：Flink On Yarn HA一定不要手动配置high-availability.cluster-id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper cluster-id (recommended): The cluster-id ZooKeeper node, under <span class="built_in">which</span> all required coordination data <span class="keyword">for</span> a cluster is placed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Important: You should not <span class="built_in">set</span> this value manually when running a YARN cluster, a per-job YARN session, or on another cluster manager. In those cases a cluster-id is automatically being generated based on the application id. Manually setting a cluster-id overrides this behaviour <span class="keyword">in</span> YARN. Specifying a cluster-id with the -z CLI option, <span class="keyword">in</span> turn, overrides manual configuration. If you are running multiple Flink HA clusters on bare metal, you have to manually configure separate cluster-ids <span class="keyword">for</span> each cluster.</span></span><br><span class="line">high-availability.cluster-id: /default</span><br></pre></td></tr></table></figure><h3 id="替换日志框架为logback"><a href="#替换日志框架为logback" class="headerlink" title="替换日志框架为logback"></a>替换日志框架为logback</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><p><strong>logback-yarn.xml配置示例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn-HA测试说明"><a href="#Flink-on-Yarn-HA测试说明" class="headerlink" title="Flink on Yarn HA测试说明"></a>Flink on Yarn HA测试说明</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开始一个yarn-session（命名为FlinkTestCluster）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager内存2048M</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个TaskManager内存2048M且分配4个slot（The session cluster will automatically allocate additional containers <span class="built_in">which</span> run the Task Managers when <span class="built_in">jobs</span> are submitted to the cluster.）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式启动</span></span><br><span class="line">yarn-session.sh -jm 2048 -tm 2048 -s 4 -nm FlinkTestCluster -d</span><br></pre></td></tr></table></figure><table><thead><tr><th>配置</th><th>测试方案</th><th>现象</th><th>备注</th></tr></thead><tbody><tr><td>Job本身配置了Flink的重启策略</td><td>提供bug程序，导致Job失败</td><td>重启失败的Job</td><td>保证Job HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnSessionClusterEntrypoint进程（<em>JobManager</em>和AM的共同进程）</td><td>重启JobManager和AM，该进程会迁移到其它节点（非必须）且进程号改变，全部Job重启</td><td>保证JobManager HA</td></tr><tr><td>Job本身配置了Flink的重启策略、Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnTaskExecutorRunner进程（TaskManager进程）</td><td>重启TaskManager，该进程会迁移到其它节点（非必须）且进程号改变，被Kill掉的TaskManager包含的Job重启</td><td>保证TaskManager HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts</td><td>未主动Cancel掉Flink集群中的Job，但不小心kill掉对应的yarn-session(对应Yarn队列中的一个Application)、之后在命令行重新提交yarn-session</td><td>启动新的yarn-session、之前未Cancel掉的Job自动迁移到当前yarn-session、JobManager和TaskManager自动创建</td><td>保证 YarnSessionHA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts、配置了Yarn的HA</td><td>Kill掉Resourcemanager</td><td>ResourceManager迁移到另一台节点，yarn-session重启，所有Job重启</td><td>保证Yarn HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts（也可在yarn-session提交时通过-D动态配置）、配置了HDFS的HA</td><td>Kill掉NameNode</td><td>NameNode迁移到另一台节点</td><td>保证HDFS HA</td></tr></tbody></table><h2 id="Yarn的基本思想"><a href="#Yarn的基本思想" class="headerlink" title="Yarn的基本思想"></a>Yarn的基本思想</h2><p>YARN的基本思想是将资源管理和作业调度/监视的功能分解为单独的守护进程。我们的想法是拥有一个全局ResourceManager（<em>RM</em>）和每个应用程序ApplicationMaster（<em>AM</em>）。应用程序可以是单个作业，也可以是作业的DAG。</p><p>ResourceManager和NodeManager构成了数据计算框架。ResourceManager是在系统中的所有应用程序之间仲裁资源的最终权限。NodeManager是每台机器上负责Containers的代理框架，监视其资源使用情况（CPU，内存，磁盘，网络）并将其报告给ResourceManager / Scheduler。</p><p>每个应用程序ApplicationMaster实际上是一个含具体库的框架，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</p><p><img src="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="MapReduce NextGen架构"></p><p>ResourceManager有两个主要组件：Scheduler和ApplicationsManager。</p><p>Scheduler负责根据熟悉的容量，队列等约束将资源分配给各种正在运行的应用程序。Scheduler是纯调度程序，因为它不执行应用程序状态的监视或跟踪。此外，当出现应用程序故障或硬件故障，它无法保证重新启动失败的任务。Scheduler根据应用程序的资源需求执行其调度功能; 它是基于资源<em>Container</em>的抽象概念，它包含内存，CPU，磁盘，网络等元素。</p><p>Scheduler具有可插入策略，该策略负责在各种队列，应用程序等之间对集群资源进行分区。当前的调度程序（如<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler）</a>将是插件的一些示例。</p><p>ApplicationsManager负责接受作业提交，协商第一个容器以执行特定于应用程序的ApplicationMaster，并提供在失败时重新启动ApplicationMaster容器的服务。每个应用程序ApplicationMaster负责从Scheduler协调适当的资源容器，跟踪其状态并监视进度。</p><h2 id="Flink-on-Yarn的基本思想"><a href="#Flink-on-Yarn的基本思想" class="headerlink" title="Flink on Yarn的基本思想"></a>Flink on Yarn的基本思想</h2><p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.7/fig/FlinkOnYarn.svg" alt="img"></p><p>YARN客户端需要访问Hadoop配置以连接到YARN资源管理器和HDFS。它使用以下策略确定Hadoop配置：</p><ul><li>按顺序测试是否配置<code>YARN_CONF_DIR</code>，<code>HADOOP_CONF_DIR</code>或<code>HADOOP_CONF_PATH</code>。如果设置了其中一个变量，则用于读取配置。</li><li>如果上述策略失败（在正确的YARN设置中不应该这样），则客户端使用配置的<code>HADOOP_HOME</code>环境变量。如果<code>HADOOP_HOME</code>环境变量已配置，则客户端尝试访问<code>$HADOOP_HOME/etc/hadoop</code>（Hadoop 2）或<code>$HADOOP_HOME/conf</code>（Hadoop 1）。</li></ul><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（ApplicationMaster的memory和vcores）是否可用。之后，它将包含Flink的jar包和配置信息上传到HDFS（步骤1）。</p><p>客户端的下一步是请求（步骤2）YARN容器以启动<em>ApplicationMaster</em>（步骤3）。客户端将配置信息和jar文件注册为容器的资源，在特定机器上运行的NodeManager将负责准备容器（例如下载文件的工作）。完成后，将启动<em>ApplicationMaster</em>（AM）。</p><p>该<em>JobManager</em>和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（Flink主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager），该文件也上传到HDFS。此外，<em>AM</em>容器还提供Flink的Web界面。YARN代码分配的所有端口都是<em>临时端口</em>。这允许用户并行执行多个Flink YARN会话。</p><p>之后，AM开始为Flink的TaskManagers分配容器（步骤4），这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a> </p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html#yarn-cluster-high-availability" target="_blank" rel="noopener">JobManager High Availability (HA)</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Flink on Yarn</a></p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Yarn </tag>
            
            <tag> Flink on Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink On Yarn集群部署</title>
      <link href="/2018/10/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/10/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配好主机映射</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建flink用户</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置免密登录</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 准备好相关资源：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.8.5.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> flink-1.7.1-bin-hadoop28-scala_2.11</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 节点配置如下：(建议每台NM节点预留2G内存给系统)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">hostname</th><th align="center">资源配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">flink01</td><td align="center">16G/16cores</td><td align="center">NameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink02</td><td align="center">16G/16cores</td><td align="center">ResourceManager/DataNode/NodeManager</td></tr><tr><td align="center">flink03</td><td align="center">16G/16cores</td><td align="center">SecondaryNameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink04</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink05</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink06</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr></tbody></table><h2 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h2><h3 id="将Hadoop安装包解压至flink01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至flink01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至flink01节点的/data/apps路径下"></a>将Hadoop安装包解压至flink01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录"><a href="#进入配置目录" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改hadoop-env-sh中的JAVA-HOME"><a href="#修改hadoop-env-sh中的JAVA-HOME" class="headerlink" title="修改hadoop-env.sh中的JAVA_HOME"></a>修改hadoop-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置yarn-env-sh中的JAVA-HOME"><a href="#配置yarn-env-sh中的JAVA-HOME" class="headerlink" title="配置yarn-env.sh中的JAVA_HOME"></a>配置yarn-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置mapred-env-sh中的JAVA-HOME"><a href="#配置mapred-env-sh中的JAVA-HOME" class="headerlink" title="配置mapred-env.sh中的JAVA_HOME"></a>配置mapred-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br><span class="line">fli<span class="symbol">nk04</span></span><br><span class="line">fli<span class="symbol">nk05</span></span><br><span class="line">fli<span class="symbol">nk06</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS的路径的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该属性定义了 HDFS WEB访问服务器的主机名和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 定义secondarynamenode 外部地址 访问的主机和端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager 的主机位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>7168<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>14336<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>含义</th><th>值</th><th>备注</th></tr></thead><tbody><tr><td>yarn.nodemanager.aux-services</td><td>设置yarn中的服务类</td><td>mapreduce_shuffle</td><td></td></tr><tr><td>yarn.resourcemanager.hostname</td><td>配置resourcemanager 的主机位置</td><td>flink02</td><td></td></tr><tr><td>yarn.resourcemanager.am.max-attempts</td><td>AM重启最大尝试次数</td><td>4</td><td></td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>开启物理内存限制</td><td>true</td><td>检测物理内存的使用是否超出分配值，若任务超出分配值，则将其杀掉，默认true。</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>关闭虚拟内存限制</td><td>false</td><td>检测虚拟内存的使用是否超出；若任务超出分配值，则将其杀掉，默认true。在确定内存不会泄漏的情况下可以设置此项为 False；</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>每个Container请求的最小内存</td><td>1024</td><td>单个容器/调度器可申请的最少物理内存量，默认是1024（MB）；一般每个contain都分配这个值；即：capacity memory:3072, vCores:1，如果提示物理内存溢出，提高这个值即可；</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>每个Container请求的最大内存</td><td>7168</td><td>单个容器/调度器可申请的最大物理内存量</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container请求的最小virtual CPU cores</td><td>1</td><td></td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container请求的最大virtual CPU cores</td><td>16</td><td></td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>限制 NodeManager 能够使用的最大物理内存</td><td>14336</td><td>该节点上YARN可使用的物理内存总量，【向操作系统申请的总量】默认是8192（MB）</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>限制 NodeManager 能够使用的最大virtual CPU cores</td><td>16</td><td>该节点上YARN可使用的总核心数；一般设为cat /proc/cpuinfo| grep “processor”| wc -l 的值。默认是8个</td></tr><tr><td>yarn.log-aggregation-enable</td><td>启用日志聚集功能</td><td>true</td><td></td></tr><tr><td>yarn.nodemanager.log.retain-seconds</td><td>设置HDFS上日志的保存时间,默认设置为7天</td><td>10800</td><td></td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>虚拟内存率</td><td>5</td><td>任务每使用1MB物理内存，最多可使用虚拟内存量比率，默认2.1；关闭虚拟内存限制的情况下，配置此项就无意义了</td></tr></tbody></table><h3 id="修改capacity-scheduler-xml"><a href="#修改capacity-scheduler-xml" class="headerlink" title="修改capacity-scheduler.xml"></a>修改capacity-scheduler.xml</h3><p><strong>（flink yarn session启用的jobmanager占用的资源总量受此参数限制）</strong></p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;    &lt;value&gt;0.3&lt;/value&gt;    &lt;description&gt;集群中可用于运行application master的资源比例上限.&lt;/description&gt;&lt;/property&gt;</code></pre><h3 id="快速安装Hadoop"><a href="#快速安装Hadoop" class="headerlink" title="快速安装Hadoop"></a>快速安装Hadoop</h3><p><strong>（使用此脚本安装完后需要单独修改capacity-scheduler.xml）</strong></p><p><strong>将安装脚本和安装包放在相同路径下并执行以下命令可快速完成上述配置步骤！</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认相关资源已放在当前用户的~路径下</span></span><br><span class="line">sh ~/install-hadoop.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">加入以下内容（这里提前加上了flink的环境变量）：</span><br><span class="line">export FLINK_HOME = /data/apps/flink-1.7.1</span><br><span class="line">export HADOOP_HOME=/data/apps/hadoop-2.8.5</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLINK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h3><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>在NameNode所在节点启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>在ResourceManager所在节点启动YARN</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="Flink集群"><a href="#Flink集群" class="headerlink" title="Flink集群"></a>Flink集群</h2><h3 id="将Hadoop安装包解压至kafka01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至kafka01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至kafka01节点的/data/apps路径下"></a>将Hadoop安装包解压至kafka01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录-1"><a href="#进入配置目录-1" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line">parallelism.default: 2</span><br><span class="line">taskmanager.numberOfTaskSlots: 8</span><br><span class="line">state.backend: rocksdb</span><br><span class="line">state.checkpoints.dir: hdfs://flink01:9000/flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs://flink01:9000/flink-savepoints</span><br><span class="line">state.backend.incremental: true</span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br><span class="line">yarn.application-attempts: 4</span><br></pre></td></tr></table></figure><h3 id="删除Flink原先使用的日志框架log4j相关资源"><a href="#删除Flink原先使用的日志框架log4j相关资源" class="headerlink" title="删除Flink原先使用的日志框架log4j相关资源"></a>删除Flink原先使用的日志框架log4j相关资源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br></pre></td></tr></table></figure><h3 id="更换Flink的日志框架为logback"><a href="#更换Flink的日志框架为logback" class="headerlink" title="更换Flink的日志框架为logback"></a>更换Flink的日志框架为logback</h3><p>（1）添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</p><p>（2）自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn的两种运行模式"><a href="#Flink-on-Yarn的两种运行模式" class="headerlink" title="Flink on Yarn的两种运行模式"></a>Flink on Yarn的两种运行模式</h2><h3 id="Start-a-long-running-Flink-cluster-on-YARN"><a href="#Start-a-long-running-Flink-cluster-on-YARN" class="headerlink" title="Start a long-running Flink cluster on YARN"></a>Start a long-running Flink cluster on YARN</h3><p>​    这种方式需要先启动集群，然后在提交Flink-Job（同一个Session中可以提交多个Flink-Job，可以在Flink的WebUI上submit，也可以使用Flink run命令提交）。启动集群时会向yarn申请一块空间，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成，释放了资源，那下一个作业才会正常提交.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认配置启动flink on yarn（默认启动资源如下）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> &#123;masterMemoryMB=1024, taskManagerMemoryMB=1024,numberTaskManagers=1, slotsPerTaskManager=1&#125;</span></span><br><span class="line">yarn-session.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 系统默认使用con/flink-conf.yaml里的配置，Flink on yarn将会覆盖掉几个参数：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jobmanager.rpc.address因为jobmanager的在集群的运行位置并不是事先确定的，其实就是AM的地址；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.tmp.dirs使用yarn给定的临时目录;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism.default也会被覆盖掉，如果在命令行里指定了slot数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 自定义配置可选参数如下 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Required     </span></span><br><span class="line"> -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)   </span><br><span class="line"><span class="meta">#</span><span class="bash"> Optional     </span></span><br><span class="line"> -D &lt;arg&gt;                        Dynamic properties     </span><br><span class="line"> -d,--detached                   Start detached     </span><br><span class="line"> -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)     </span><br><span class="line"> -nm,--name                      Set a custom name for the application on YARN     </span><br><span class="line"> -q,--query                      Display available YARN resources (memory, cores)     </span><br><span class="line"> -qu,--queue &lt;arg&gt;               Specify YARN queue.     </span><br><span class="line"> -s,--slots &lt;arg&gt;                Number of slots per TaskManager     </span><br><span class="line"> -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)     </span><br><span class="line"> -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例：启动15个TaskManager，1个JobManager，JobManager内存1024M，每个TaskManager内存1024M且含有8个slot，自定义该应用的名称为FlinkOnYarnSession，-d以分离式模式执行（不指定-d则以客户端模式执行）</span></span><br><span class="line">yarn-session.sh -n 15 -jm 1024 -tm 1024 -s 8 -nm FlinkOnYarnSession -d</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 客户端模式指的是在终端启动一个客户端，这种方式是不能断开终端的，断开即相当于<span class="built_in">kill</span>掉Flink集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式指的是启动Flink on Yarn后，Flink YARN客户端将仅向Yarn提交Flink，然后自行关闭。，要<span class="built_in">kill</span>掉Flink集群需要使用如下命令：</span></span><br><span class="line">yarn application -kill &lt;appId&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;appId&gt;指的是发布在Yarn上的作业ID，在Yarn集群上可以查到对应的ID</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于Flink On Yarn来说，一个JobManager占用一个Container，一个TaskManager占用一个Container</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager的数量+TaskManager的数量 = 申请的Container的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下以6台16核，16G内存的机器举例说明（每台节点预留2G内存给系统）</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcores=16 每台NodeManager节点为YARN集群分配的cpu为16核</span><br><span class="line">yarn.nodemanager.resource.memory-mb=14336 每台NodeManager节点为YARN集群分配的物理内存为14G</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores=1 每台NodeManager节点上每个Contaniner最小使用1核cpu</span><br><span class="line">yarn.scheduler.minimum-allocation-mb=1024 每台NodeManager节点上每个Contaniner最小使用1G的物理内存</span><br><span class="line"><span class="meta">#</span><span class="bash"> 若所有节点全部用于Flink作业,推荐提供的Flink集群：</span></span><br><span class="line">（总的资源为14*6=84G内存，16*6=96核）</span><br><span class="line">yarn-session.sh -n 8 -jm 4096 -tm 3584 -s 16 -nm FlinkOnYarnSession -d</span><br><span class="line">一共占用32G内存，9cores，申请了1个4G/1cores的JobManager和8个3.5G/1cores/16slots的TaskManager</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############## Recovery behavior of Flink on YARN</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Flink’s YARN client has the following configuration parameters to control how to behave <span class="keyword">in</span> <span class="keyword">case</span> of container failures. These parameters can be <span class="built_in">set</span> either from the conf/flink-conf.yaml or when starting the YARN session, using -D parameters</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.reallocate-failed : 控制 Flink是否应该重新分配失败的TaskManager容器，默认<span class="literal">true</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.maximum-failed-containers : ApplicationMaster接收container失败的最大次数，默认是TaskManager的次数（-n的值）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts : ApplicationMaster尝试次数。如果这个值为1（默认），那么当Application Master失败时，整个YARN session就会失败。更高的值是指ApplicationMaster重新启动的次数</span></span><br></pre></td></tr></table></figure><h3 id="Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）"><a href="#Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）" class="headerlink" title="Run a Flink job on YARN（Flink per-job cluster模式）"></a>Run a Flink job on YARN（Flink per-job cluster模式）</h3><p>这种方式不需要先启动集群，每提交一个Flink-Job都会在Yarn上启动一个Flink集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> TaskManager slots number配置</span></span><br><span class="line">这个参数是配置一个TaskManager有多少个并发的slot数。有两种配置方式：</span><br><span class="line">- taskmanager.numberOfTaskSlots. 在conf/flink-conf.yaml中更改，默认值为1，表示默认一个TaskManager只有1个task slot.</span><br><span class="line">- 提交作业时通过参数配置。--yarnslots 1，表示TaskManager的slot数为1.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TaskManager的个数</span></span><br><span class="line">注意： Per job模式提交作业时并不像session模式能够指定拉起多少个TaskManager，TaskManager的数量是在提交作业时根据并发度动态计算。</span><br><span class="line">首先，根据设定的operator的最大并发度计算，例如，如果作业中operator的最大并发度为10，则 Parallelism/numberOfTaskSlots为向YARN申请的TaskManager数。</span><br></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#######################示例</span><br><span class="line"># flink run -m yarn-cluster 必须指定</span><br><span class="line"># -d 分离模式启动（不指定则以客户端模式启动）</span><br><span class="line"># 启动<span class="number">1</span>个JobManager，内存占用<span class="number">1024</span>M</span><br><span class="line"># 每台TaskManager指定<span class="number">4</span>个slot、内存占用<span class="number">1024</span>M</span><br><span class="line"># 假设abc.jar所有operator中最大并发度为<span class="number">8</span>，则会启动<span class="number">8</span>/<span class="number">4</span>=<span class="number">2</span>台TaskManager</span><br><span class="line">flink run -m yarn-cluster -d --yarnslots <span class="number">4</span> -yjm <span class="number">1024</span> -ytm <span class="number">1024</span> /data/abc.jar</span><br></pre></td></tr></table></figure><h2 id="Log-Files"><a href="#Log-Files" class="headerlink" title="Log Files"></a>Log Files</h2><p>In cases where the Flink YARN session fails during the deployment itself, users have to rely on the logging capabilities of Hadoop YARN. The most useful feature for that is the <a href="http://hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="noopener">YARN log aggregation</a>. To enable it, users have to set the <code>yarn.log-aggregation-enable</code>property to <code>true</code> in the <code>yarn-site.xml</code> file. Once that is enabled, users can use the following command to retrieve all log files of a (failed) YARN session.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt;</span><br></pre></td></tr></table></figure><p>Note that it takes a few seconds after the session has finished until the logs show up.</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Yarn </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink使用Logback作为日志框架的相关配置</title>
      <link href="/2018/10/13/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/10/13/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink切换日志框架为Logback"><a href="#Flink切换日志框架为Logback" class="headerlink" title="Flink切换日志框架为Logback"></a>Flink切换日志框架为Logback</h1><h2 id="client端pom文件配置"><a href="#client端pom文件配置" class="headerlink" title="client端pom文件配置"></a>client端pom文件配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Add the two required logback dependencies --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add the log4j -&gt; sfl4j (-&gt; logback) bridge into the classpath</span></span><br><span class="line"><span class="comment">     Hadoop is logging to log4j! --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>添加logback-core、logback-classic及log4j-over-slf4j依赖，</li><li>之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；</li><li><strong>最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</strong></li></ul><h2 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h2><ul><li><p>添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)</p><p>相关jar包在logback官网上都有，<a href="https://download.csdn.net/download/qq_36643786/11190626" target="_blank" rel="noopener">嫌麻烦的可以点此链接直接下载！</a></p></li><li><p>移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)</p></li><li><p>如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</p></li></ul><h3 id="flink-daemon-sh"><a href="#flink-daemon-sh" class="headerlink" title="flink-daemon.sh"></a>flink-daemon.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-daemon.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#  Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment">#  or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment">#  distributed with this work for additional information</span></span><br><span class="line"><span class="comment">#  regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment">#  to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment">#  "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment">#  with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">#  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">#  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start/stop a Flink daemon.</span></span><br><span class="line">USAGE=<span class="string">"Usage: flink-daemon.sh (start|stop|stop-all) (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">STARTSTOP=<span class="variable">$1</span></span><br><span class="line">DAEMON=<span class="variable">$2</span></span><br><span class="line">ARGS=(<span class="string">"<span class="variable">$&#123;@:3&#125;</span>"</span>) <span class="comment"># get remaining arguments as array</span></span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"<span class="variable">$0</span>"</span>`</span><br><span class="line">bin=`<span class="built_in">cd</span> <span class="string">"<span class="variable">$bin</span>"</span>; <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">. <span class="string">"<span class="variable">$bin</span>"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$DAEMON</span> <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unknown daemon '<span class="variable">$&#123;DAEMON&#125;</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$FLINK_IDENT_STRING</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_IDENT_STRING=<span class="string">"<span class="variable">$USER</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">pid=<span class="variable">$FLINK_PID_DIR</span>/flink-<span class="variable">$FLINK_IDENT_STRING</span>-<span class="variable">$DAEMON</span>.pid</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Log files for daemons are indexed from the process ID's position in the PID</span></span><br><span class="line"><span class="comment"># file. The following lock prevents a race condition during daemon startup</span></span><br><span class="line"><span class="comment"># when multiple daemons read, index, and write to the PID file concurrently.</span></span><br><span class="line"><span class="comment"># The lock is created on the PID directory since a lock file cannot be safely</span></span><br><span class="line"><span class="comment"># removed. The daemon is started with the lock closed and the lock remains</span></span><br><span class="line"><span class="comment"># active in this script until the script exits.</span></span><br><span class="line"><span class="built_in">command</span> -v flock &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">exec</span> 200&lt;<span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line">    flock 200</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ascending ID depending on number of lines in pid file.</span></span><br><span class="line"><span class="comment"># This allows us to start multiple daemon of each type.</span></span><br><span class="line">id=$([ -f <span class="string">"<span class="variable">$pid</span>"</span> ] &amp;&amp; <span class="built_in">echo</span> $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>"</span>) || <span class="built_in">echo</span> <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">FLINK_LOG_PREFIX=<span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>/flink-<span class="variable">$&#123;FLINK_IDENT_STRING&#125;</span>-<span class="variable">$&#123;DAEMON&#125;</span>-<span class="variable">$&#123;id&#125;</span>-<span class="variable">$&#123;HOSTNAME&#125;</span>"</span></span><br><span class="line"><span class="built_in">log</span>=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.log"</span></span><br><span class="line">out=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.out"</span></span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog.file=<span class="variable">$&#123;log&#125;</span>"</span> <span class="string">"-Dlog4j.configuration=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/log4j.properties"</span> <span class="string">"-Dlogback.configurationFile=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/logback.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$(<span class="variable">$&#123;JAVA_RUN&#125;</span> -version 2&gt;&amp;1 | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only set JVM 8 arguments if we have correctly extracted the version</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$&#123;JAVA_VERSION&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_VERSION</span>"</span> -lt 18 ]; <span class="keyword">then</span></span><br><span class="line">        JVM_ARGS=<span class="string">"<span class="variable">$JVM_ARGS</span> -XX:MaxPermSize=256m"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STARTSTOP</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">    (start)</span><br><span class="line">        <span class="comment"># Rotate log files</span></span><br><span class="line">        rotateLogFilesWithPrefix <span class="string">"<span class="variable">$FLINK_LOG_DIR</span>"</span> <span class="string">"<span class="variable">$FLINK_LOG_PREFIX</span>"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print a warning if daemons are already running on host</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          active=()</span><br><span class="line">          <span class="keyword">while</span> IFS=<span class="string">''</span> <span class="built_in">read</span> -r p || [[ -n <span class="string">"<span class="variable">$p</span>"</span> ]]; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">kill</span> -0 <span class="variable">$p</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">              active+=(<span class="variable">$p</span>)</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">          <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          count=<span class="string">"<span class="variable">$&#123;#active[@]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> [ <span class="variable">$&#123;count&#125;</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"[INFO] <span class="variable">$count</span> instance(s) of <span class="variable">$DAEMON</span> are already running on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">          <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Evaluate user options for local variable expansion</span></span><br><span class="line">        FLINK_ENV_JAVA_OPTS=$(<span class="built_in">eval</span> <span class="built_in">echo</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Starting <span class="variable">$DAEMON</span> daemon on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="variable">$JAVA_RUN</span> <span class="variable">$JVM_ARGS</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span> <span class="string">"<span class="variable">$&#123;log_setting[@]&#125;</span>"</span> -classpath <span class="string">"`manglePathList "</span><span class="variable">$FLINK_TM_CLASSPATH</span>:<span class="variable">$INTERNAL_HADOOP_CLASSPATHS</span><span class="string">"`"</span> <span class="variable">$&#123;CLASS_TO_RUN&#125;</span> <span class="string">"<span class="variable">$&#123;ARGS[@]&#125;</span>"</span> &gt; <span class="string">"<span class="variable">$out</span>"</span> 200&lt;&amp;- 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line">        mypid=$!</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to pid file if successful start</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="variable">$&#123;mypid&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]] &amp;&amp; <span class="built_in">kill</span> -0 <span class="variable">$mypid</span> &gt; /dev/null 2&gt;&amp;1 ; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="variable">$mypid</span> &gt;&gt; <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Error starting <span class="variable">$DAEMON</span> daemon."</span></span><br><span class="line">            <span class="built_in">exit</span> 1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Remove last in pid file</span></span><br><span class="line">            to_stop=$(tail -n 1 <span class="string">"<span class="variable">$pid</span>"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> [ -z <span class="variable">$to_stop</span> ]; <span class="keyword">then</span></span><br><span class="line">                rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                sed \<span class="variable">$d</span> <span class="string">"<span class="variable">$pid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="comment"># all but last line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                [ $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>.tmp"</span>) -eq 0 ] &amp;&amp; rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$pid</span>.tmp"</span> || mv <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) is running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop-all)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            mv <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">read</span> to_stop; <span class="keyword">do</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Skipping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>), because it is not running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">            rm <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unexpected argument '<span class="variable">$STARTSTOP</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml</li></ul><h3 id="flink-console-sh"><a href="#flink-console-sh" class="headerlink" title="flink-console.sh"></a>flink-console.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-console.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line"># Start a Flink service <span class="keyword">as</span> a console application. Must be stopped <span class="keyword">with</span> Ctrl-C</span><br><span class="line"># or <span class="keyword">with</span> SIGTERM by kill or the controlling process.</span><br><span class="line">USAGE=<span class="string">"Usage: flink-console.sh (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">SERVICE=$<span class="number">1</span></span><br><span class="line">ARGS=(<span class="string">"$&#123;@:2&#125;"</span>) # get remaining arguments <span class="keyword">as</span> array</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> $SERVICE <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        echo <span class="string">"Unknown service '$&#123;SERVICE&#125;'. $USAGE."</span></span><br><span class="line">        exit <span class="number">1</span></span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog4j.configuration=file:$&#123;FLINK_CONF_DIR&#125;/log4j-console.properties"</span> <span class="string">"-Dlogback.configurationFile=file:$&#123;FLINK_CONF_DIR&#125;/logback-console.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$($&#123;JAVA_RUN&#125; -version <span class="number">2</span>&gt;&amp;<span class="number">1</span> | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"># Only set JVM <span class="number">8</span> arguments <span class="keyword">if</span> we have correctly extracted the version</span><br><span class="line"><span class="keyword">if</span> [[ $&#123;JAVA_VERSION&#125; =~ $&#123;IS_NUMBER&#125; ]]; then</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"$JAVA_VERSION"</span> -lt <span class="number">18</span> ]; then</span><br><span class="line">        JVM_ARGS=<span class="string">"$JVM_ARGS -XX:MaxPermSize=256m"</span></span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"Starting $SERVICE as a console application on host $HOSTNAME."</span></span><br><span class="line">exec $JAVA_RUN $JVM_ARGS $&#123;FLINK_ENV_JAVA_OPTS&#125; <span class="string">"$&#123;log_setting[@]&#125;"</span> -classpath <span class="string">"`manglePathList "</span>$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS<span class="string">"`"</span> $&#123;CLASS_TO_RUN&#125; <span class="string">"$&#123;ARGS[@]&#125;"</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml</li></ul><h3 id="yarn-session-sh"><a href="#yarn-session-sh" class="headerlink" title="yarn-session.sh"></a>yarn-session.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line"># get Flink config</span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$FLINK_IDENT_STRING"</span> = <span class="string">""</span> ]; then</span><br><span class="line">        FLINK_IDENT_STRING=<span class="string">"$USER"</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">JVM_ARGS=<span class="string">"$JVM_ARGS -Xmx512m"</span></span><br><span class="line"></span><br><span class="line">CC_CLASSPATH=`manglePathList $(constructFlinkClassPath):$INTERNAL_HADOOP_CLASSPATHS`</span><br><span class="line"></span><br><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-yarn-session-$HOSTNAME.log</span><br><span class="line">log_setting=<span class="string">"-Dlog.file="</span>$log<span class="string">" -Dlog4j.configuration=file:"</span>$FLINK_CONF_DIR<span class="string">"/log4j-yarn-session.properties -Dlogback.configurationFile=file:"</span>$FLINK_CONF_DIR<span class="string">"/logback-yarn.xml"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> FLINK_CONF_DIR</span><br><span class="line"></span><br><span class="line">$JAVA_RUN $JVM_ARGS -classpath <span class="string">"$CC_CLASSPATH"</span> $log_setting org.apache.flink.yarn.cli.FlinkYarnSessionCli -j <span class="string">"$FLINK_LIB_DIR"</span>/flink-dist*.jar <span class="string">"$@"</span></span><br></pre></td></tr></table></figure><ul><li>使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h2 id="doc"><a href="#doc" class="headerlink" title="doc"></a>doc</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/best_practices.html#using-logback-instead-of-log4j" target="_blank" rel="noopener">Using Logback instead of Log4j</a></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>client端使用logback的话，要在pom文件添加logback-core、logback-classic及log4j-over-slf4j依赖，之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</li><li>服务端使用logback的话，要在添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)；移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)；如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</li><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h1 id="Logback配置文件详解"><a href="#Logback配置文件详解" class="headerlink" title="Logback配置文件详解"></a>Logback配置文件详解</h1><p>Logback，Java 日志框架。</p><p>Logback 如何加载配置的</p><ol><li>logback 首先会查找 logback.groovy 文件</li><li>当没有找到，继续试着查找 logback-test.xml 文件</li><li>当没有找到时，继续试着查找 logback.xml 文件</li><li>如果仍然没有找到，则使用默认配置（打印到控制台）</li></ol><h2 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h2><p>configuration 是配置文件的根节点，他包含的属性：</p><ul><li>scan<br>　　当此属性设置为 true 时，配置文件如果发生改变，将会被重新加载，默认值为 true</li><li>scanPeriod<br>　　设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。但 scan 为 true 时，此属性生效，默认的时间间隔为 1 分钟</li><li>debug<br>　　当此属性设置为 true 时，将打印出 logback 内部日志信息，实时查看 logback 运行状态，默认值为 false。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="configuration-的子节点"><a href="#configuration-的子节点" class="headerlink" title="configuration 的子节点"></a>configuration 的子节点</h2><h4 id="设置上下文名称：contextName"><a href="#设置上下文名称：contextName" class="headerlink" title="设置上下文名称：contextName"></a>设置上下文名称：contextName</h4><p>每个 logger 度关联到 logger 上下文，默认上下文名称为 “default”。可以通过设置 contextName 修改上下文名称，用于区分不同应该程序的记录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>myAppName<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置变量：property"><a href="#设置变量：property" class="headerlink" title="设置变量：property"></a>设置变量：property</h4><p>用于定义键值对的变量， property 有两个属性 name 和 value，name 是键，value 是值，通过 property 定义的键值对会保存到logger 上下文的 map 集合内。定义变量后，可以使用 “${}” 来使用变量</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"APP_Name"</span> <span class="attr">value</span>=<span class="string">"myAppName"</span> /&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;APP_Name&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="获取时间戳字符串：timestamp"><a href="#获取时间戳字符串：timestamp" class="headerlink" title="获取时间戳字符串：timestamp"></a>获取时间戳字符串：timestamp</h4><p>timestamp 有两个属性，key：标识此 timestamp 的名字；datePattern：时间输出格式，遵循SimpleDateFormat 的格式</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">timestamp</span> <span class="attr">key</span>=<span class="string">"bySecond"</span> <span class="attr">datePattern</span>=<span class="string">"yyyyMMdd'T'HHmmss"</span>/&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;bySecond&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="logger"><a href="#logger" class="headerlink" title="logger"></a>logger</h2><p>logger 有两种级别，一种是 root，一种是普通的 logger，logger 是用来设置某一个包或者具体的某一个类的日志打印机级别，以及制定的 appender。<br>logger 有三个属性</p><ul><li>name：用来指定此 logger 约束的某一个包或者具体的某一个类</li><li>level：用来设置打印机别，</li><li>addtivity：是否向上级 logger 传递打印信息。默认是 true</li></ul><p>每个 logger 都有对应的父级关系，它通过包名来决定父级关系，root 是最高级的父元素。<br>下面定义了四个 logger，他们的父子关系从小到大为：<br>com.lwc.qg.test.logbackDemo → com.lwc.qg.tes → com.lwc.qg → root</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 根 logger --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    普通的 logger</span></span><br><span class="line"><span class="comment">    name：类名或包名，标志该 logger 与哪个包或哪个类绑定</span></span><br><span class="line"><span class="comment">    level：该 logger 的日志级别</span></span><br><span class="line"><span class="comment">    additivity：是否将日志信息传递给上一级</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test.logbackDemo"</span> <span class="attr">level</span>=<span class="string">"debug"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br></pre></td></tr></table></figure><p>　　从该种级别来看，如果此时在最低层的 logger 输出日志信息，以该配置作为基础，它将会向父级的所有 logger 依次传递，所以按理来说一个打印信息将会打印四次</p><p>　　从控制台上看，的确每条日志信息都被打印出了四次，但是细心从配置文件上来看，root 的日志级别配置的为 info，但是却输出<br>debug 级别的日志信息，所以从测试结果可以看出，向上传递的日志信息的日志级别将由最底层的子元素决定（最初传递信息的<br>logger），因为子元素设置的日志级别为 debug，所以也输出了 debug 级别的信息。<br>　　因此，从理论上来说，如果子元素日志级别设置高一点，那么也将会只输出高级别的日志信息。实际上也是如此，如果我们把 com.lwc.qg.test.logbackDemo 对应的 logger 日志级别设为 warn，那么将只会输出 warn 及其以上的信息</p><h2 id="root"><a href="#root" class="headerlink" title="root"></a>root</h2><p>root 也是 logger 元素，但它是根 logger。只有一个 level 属性</p><h2 id="appender"><a href="#appender" class="headerlink" title="appender"></a>appender</h2><p>appender 是负责写日志的组件，常用的组件有：</p><ul><li>ConsoleAppender</li><li>FileAppender</li><li>RollingFileAppender</li></ul><h2 id="ConsoleAppender"><a href="#ConsoleAppender" class="headerlink" title="ConsoleAppender"></a>ConsoleAppender</h2><p>控制台日志组件，该组件将日志信息输出到控制台,该组件有以下节点</p><ul><li>encoder：对日志进行格式化</li><li>target：System.out 或者 System.err，默认是 System.out</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="FileAppender"><a href="#FileAppender" class="headerlink" title="FileAppender"></a>FileAppender</h2><p>文件日志组件，该组件将日志信息输出到日志文件中，该组件有以下节点</p><ul><li>file：被写入的文件名，可以是相对路径，也可以是绝对路径。如果上级目录不存在会自动创建，没有默认值</li><li>append：如果是 true，日志被追加到文件结尾；如果是 false，清空现存文件，默认是 true。</li><li>encoder：格式化</li><li>prudent：如果是 true，日志会被安全的写入文件，即使其他的 FileAppender 也在向此文件做写入操作，效率低，默认是 false。</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>testFile.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>true<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">prudent</span>&gt;</span>true<span class="tag">&lt;/<span class="name">prudent</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="RollingFileAppender"><a href="#RollingFileAppender" class="headerlink" title="RollingFileAppender"></a>RollingFileAppender</h2><p>滚动记录文件日志组件，先将日志记录记录到指定文件，当符合某个条件时，将日志记录到其他文件，该组件有以下节点</p><ul><li>file：文件名</li><li>encoder：格式化</li><li>rollingPolicy：当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名</li><li>triggeringPolicy：告知 RollingFileAppender 合适激活滚动</li><li>prudent：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。</li></ul><p>#### </p><h3 id="rollingPolicy"><a href="#rollingPolicy" class="headerlink" title="rollingPolicy"></a>rollingPolicy</h3><p>滚动策略</p><ol><li>TimeBasedRollingPolicy：最常用的滚动策略，它根据时间来制定滚动策略，即负责滚动也负责触发滚动，包含节点：<ul><li>fileNamePattern：文件名模式</li><li>maxHistoury：控制文件的最大数量，超过数量则删除旧文件</li></ul></li><li>FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略，包含节点<ul><li>minInedx：窗口索引最小值</li><li>maxIndex：串口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12</li><li>fileNamePattern：文件名模式，必须包含%i，命名模式为 log%i.log，会产生 log1.log，log2.log 这样的文件</li></ul></li><li>triggeringPolicy：根据文件大小的滚动策略，包含节点<ul><li>maxFileSize：日志文件最大大小</li></ul></li></ol><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>logFile.%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="filter-过滤器"><a href="#filter-过滤器" class="headerlink" title="filter 过滤器"></a>filter 过滤器</h2><p>过滤器是用于日志组件中的，每经过一个过滤器都会返回一个确切的枚举值，分别是</p><ul><li>DENY：返回 DENY，日志将立即被抛弃不再经过其他过滤器</li><li>NEUTRAL：有序列表的下个过滤器接着处理日志</li><li>ACCEPT：日志会被立即处理，不再经过剩余过滤器</li></ul><h3 id="常用过滤器"><a href="#常用过滤器" class="headerlink" title="常用过滤器"></a>常用过滤器</h3><p>常用的过滤器有以下：</p><ul><li>LevelFilter<br>级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 omMatch 和 omMismatch 接受或拒绝日志。他有以下节点<br>　　level：过滤级别<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作<br>例：该组件设置一个 INFO 级别的过滤器，那么所有非 INFO 级别的日志都会被过滤掉　　</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>ThresholdFilter<br>临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器会返回 NEUTRAL；当日志级别低于临界值时，日志会被拒绝<br>例：过滤掉所有低于 INFO 级别的日志</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span> </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>EvaluatorFilter<br>求值过滤器，评估、鉴别日志是否符合指定条件，包含节点：<br>　　evaluator：鉴别器，通过子标签 expression 配置求值条件<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作</li></ul>]]></content>
      
      
      <categories>
          
          <category> 日志框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> Logback </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink SQL 深度解析</title>
      <link href="/2018/09/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2018/09/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据计算领域对SQL的应用"><a href="#大数据计算领域对SQL的应用" class="headerlink" title="大数据计算领域对SQL的应用"></a>大数据计算领域对SQL的应用</h1><h2 id="离线计算（批计算）"><a href="#离线计算（批计算）" class="headerlink" title="离线计算（批计算）"></a>离线计算（批计算）</h2><p>提及大数据计算领域不得不说MapReduce计算模型，MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法，并发于2004年发表了论文Simplified Data Processing on Large Clusters。论文发表之后Apache 开源社区参考Google MapReduce，基于Java设计开发了一个称为Hadoop的开源MapReduce并行计算框架。很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并了解MapReduce的运行原理，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛，进而也成功的将SQL应用到了大数据计算领域当中来。</p><h2 id="实时计算（流计算）"><a href="#实时计算（流计算）" class="headerlink" title="实时计算（流计算）"></a>实时计算（流计算）</h2><p>SQL不仅仅被成功的应用到了离线计算，SQL的易用性也吸引了流计算产品，目前最热的Spark，Flink也纷纷支持了SQL，尤其是Flink支持的更加彻底，集成了Calcite，完全遵循ANSI-SQL标准。Apache Flink在low-level API上面用DataSet支持批计算，用DataStream支持流计算，但在High-Level API上面利用SQL将流与批进行了统一，使得用户编写一次SQL既可以在流计算中使用，又可以在批计算中使用，为既有流计算业务，又有批计算业务的用户节省了大量开发成本。</p><h1 id="SQL高性能与简洁性"><a href="#SQL高性能与简洁性" class="headerlink" title="SQL高性能与简洁性"></a>SQL高性能与简洁性</h1><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>SQL经过传统数据库领域几十年的不断打磨，查询优化器已经能够极大的优化SQL的查询性能，Apache Flink 应用Calcite进行查询优化，复用了大量数据库查询优化规则，在性能上不断追求极致，能够让用户关心但不用担心性能问题。如下图(Alibaba 对 Apache Flink 进行架构优化后的组件栈)</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN43HQMdZty0IxMowiaBs1oaPZwyEeVpLvkLakk4V51uz6iaMbz9toslicw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>相对于DataStream而言，SQL会经过Optimization模块透明的为用户进行查询优化，用户专心编写自己的业务逻辑，不用担心性能，却能得到最优的查询性能!</p><h2 id="简洁"><a href="#简洁" class="headerlink" title="简洁"></a>简洁</h2><p>就简洁性而言，SQL与DataSet和DataStream相比具有很大的优越性，我们先用一个WordCount示例来直观的查看用户的代码量：</p><p>DataStream/DataSetAPI</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">... <span class="comment">//省略初始化代码</span></span><br><span class="line"><span class="comment">// 核心逻辑</span></span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> WordCount.Tokenizer()).keyBy(<span class="keyword">new</span> <span class="built_in">int</span>[]&#123;<span class="number">0</span>&#125;).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// flatmap 代码定义</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> class Tokenizer implements FlatMapFunction&lt;<span class="keyword">String</span>, Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> Tokenizer() &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> flatMap(<span class="keyword">String</span> value, Collector&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; out) &#123;</span><br><span class="line"><span class="keyword">String</span>[] tokens = value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>);</span><br><span class="line"><span class="keyword">String</span>[] var4 = tokens;</span><br><span class="line"><span class="built_in">int</span> var5 = tokens.length;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">int</span> var6 = <span class="number">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class="line"><span class="keyword">String</span> token = var4[var6];</span><br><span class="line"><span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">out.collect(<span class="keyword">new</span> Tuple2(token, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SQL</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...//省略初始化代码</span><br><span class="line"><span class="keyword">SELECT</span> word, <span class="built_in">COUNT</span>(word) <span class="keyword">FROM</span> tab <span class="keyword">GROUP</span> <span class="keyword">BY</span> word;</span><br></pre></td></tr></table></figure><p>我们直观的体会到相同的统计功能使用SQL的简洁性。</p><h1 id="Flink-SQL-Job的组成"><a href="#Flink-SQL-Job的组成" class="headerlink" title="Flink SQL Job的组成"></a>Flink SQL Job的组成</h1><p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这三个部分，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNmwGyZgbFaPfs2bjXzSGdh9jSTKnxrYlSbLzwMUn95uVLOuHcueGLnw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p><ul><li>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</li><li>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</li><li>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</li></ul><h1 id="Flink-SQL-核心算子"><a href="#Flink-SQL-核心算子" class="headerlink" title="Flink SQL 核心算子"></a>Flink SQL 核心算子</h1><p>目前Flink SQL支持Union，Join，Projection,Difference, Intersection以及Window等大多数传统数据库支持的操作，接下来为大家分别进行简单直观的介绍。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>为了很好的体验和理解Apache Flink SQL算子我们需要先准备一下测试环境，我们选择IDEA，以ITCase测试方式来进行体验。IDEA 安装这里不占篇幅介绍了，相信大家能轻松搞定！我们进行功能体验有两种方式，具体如下：</p><h2 id="源码方式"><a href="#源码方式" class="headerlink" title="源码方式"></a>源码方式</h2><p>对于开源爱好者可能更喜欢源代码方式理解和体验Apache Flink SQL功能，那么我们需要下载源代码并导入到IDEA中：</p><ul><li>下载源码：</li></ul><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载源代码</span></span><br><span class="line">git clone https:<span class="comment">//github.com/apache/flink.git study</span></span><br><span class="line"><span class="comment">// 进入源码目录</span></span><br><span class="line">cd study</span><br><span class="line"><span class="comment">// 拉取稳定版release-1.6</span></span><br><span class="line">git fetch origin <span class="built_in">release</span><span class="number">-1.6</span>:<span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//切换到稳定版</span></span><br><span class="line">git checkout <span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//将依赖安装到本地mvn仓库，耐心等待需要一段时间</span></span><br><span class="line">mvn clean install -DskipTests</span><br></pre></td></tr></table></figure><ul><li>导入到IDEA<br>将Flink源码导入到IDEA过程这里不再占用篇幅，导入后确保在IDEA中可以运行 <code>org.apache.flink.table.runtime.stream.sql.SqlITCase</code> 并测试全部通过，即证明体验环境已经完成，即证明体验环境已经完成。如下图所示：</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNo09iaFxmhAfdNGPSjCc6qnjDUWyZaCO8UBSkyUJy1EEcicoSv4qa8wzg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上图运行测试后显示测试通过，我们就可以继续下面的Apache Flink SQL功能体验了。</p><h2 id="依赖Flink包方式"><a href="#依赖Flink包方式" class="headerlink" title="依赖Flink包方式"></a>依赖Flink包方式</h2><p>我们还有一种更简单直接的方式，就是新建一个mvn项目，并在pom中添加如下依赖：</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">table.version</span>&gt;</span>1.6-SNAPSHOT<span class="tag">&lt;/<span class="name">table.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>完成环境准备后，我们开始准备测试数据和写一个简单的测试类。</p><h2 id="示例数据及测试类"><a href="#示例数据及测试类" class="headerlink" title="示例数据及测试类"></a>示例数据及测试类</h2><h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><ul><li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li></ul><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><ul><li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li></ul><table><thead><tr><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr></tbody></table><ul><li>Item_tab<br> 商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li></ul><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>30</td></tr><tr><td>ITEM004</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td></tr></tbody></table><ul><li>PageAccess_tab<br>页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><ul><li>PageAccessCount_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userCount</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>100</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>86</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>210</td><td>2017.11.11 10:06:00</td></tr><tr><td>BeiJing</td><td>33</td><td>2017.11.11 10:10:00</td></tr><tr><td>ShangHai</td><td>129</td><td>2017.11.11 12:10:00</td></tr></tbody></table><ul><li>PageAccessSession_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:01:00</td></tr><tr><td>ShangHai</td><td>U0012</td><td>2017-11-11 10:02:00</td></tr><tr><td>ShangHai</td><td>U0013</td><td>2017-11-11 10:03:00</td></tr><tr><td>ShangHai</td><td>U0015</td><td>2017-11-11 10:05:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U0110</td><td>2017-11-11 10:10:00</td></tr><tr><td>ShangHai</td><td>U2010</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0410</td><td>2017-11-11 12:16:00</td></tr></tbody></table><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><p>我们创建一个<code>SqlOverviewITCase.scala</code> 用于接下来介绍Flink SQL算子的功能体验。代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.<span class="type">StateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.<span class="type">MemoryStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.<span class="type">RichSinkFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">TableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.junit.rules.<span class="type">TemporaryFolder</span></span><br><span class="line"><span class="keyword">import</span> org.junit.&#123;<span class="type">Rule</span>, <span class="type">Test</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqlOverviewITCase</span> </span>&#123;</span><br><span class="line"><span class="keyword">val</span> _tempFolder = <span class="keyword">new</span> <span class="type">TemporaryFolder</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Rule</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFolder</span></span>: <span class="type">TemporaryFolder</span> = _tempFolder</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStateBackend</span></span>: <span class="type">StateBackend</span> = &#123;</span><br><span class="line"><span class="keyword">new</span> <span class="type">MemoryStateBackend</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 客户表数据</span></span><br><span class="line"><span class="keyword">val</span> customer_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">customer_data.+=((<span class="string">"c_001"</span>, <span class="string">"Kevin"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_002"</span>, <span class="string">"Sunny"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_003"</span>, <span class="string">"JinCheng"</span>, <span class="string">"from HeBei"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 订单表数据</span></span><br><span class="line"><span class="keyword">val</span> order_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">order_data.+=((<span class="string">"o_001"</span>, <span class="string">"c_002"</span>, <span class="string">"2018-11-05 10:01:01"</span>, <span class="string">"iphone"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_002"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:01:55"</span>, <span class="string">"ipad"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_003"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:03:44"</span>, <span class="string">"flink book"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 商品销售表数据</span></span><br><span class="line"><span class="keyword">val</span> item_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="number">20</span>, <span class="string">"ITEM001"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="number">50</span>, <span class="string">"ITEM002"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">30</span>, <span class="string">"ITEM003"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">60</span>, <span class="string">"ITEM004"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365780000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="number">40</span>, <span class="string">"ITEM005"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="number">20</span>, <span class="string">"ITEM006"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365960000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366020000</span>L, (<span class="number">1510366020000</span>L, <span class="number">70</span>, <span class="string">"ITEM007"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366020000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366080000</span>L, (<span class="number">1510366080000</span>L, <span class="number">20</span>, <span class="string">"ITEM008"</span>, <span class="string">"Clothes"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">151036608000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据</span></span><br><span class="line"><span class="keyword">val</span> pageAccess_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问量表数据2</span></span><br><span class="line"><span class="keyword">val</span> pageAccessCount_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="number">100</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="number">86</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="string">"BeiJing"</span>, <span class="number">210</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="number">33</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="number">129</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据3</span></span><br><span class="line"><span class="keyword">val</span> pageAccessSession_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0012"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0013"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0015"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373760000</span>L, (<span class="number">1510373760000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0410"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373760000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">procTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将order_tab, customer_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> customer = env.fromCollection(customer_data).toTable(tEnv).as(<span class="symbol">'c_id</span>, <span class="symbol">'c_name</span>, <span class="symbol">'c_desc</span>)</span><br><span class="line">    <span class="keyword">val</span> order = env.fromCollection(order_data).toTable(tEnv).as(<span class="symbol">'o_id</span>, <span class="symbol">'c_id</span>, <span class="symbol">'o_time</span>, <span class="symbol">'o_desc</span>)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"order_tab"</span>, order)</span><br><span class="line">    tEnv.registerTable(<span class="string">"customer_tab"</span>, customer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rowTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setStateBackend(getStateBackend)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将item_tab, pageAccess_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> item =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">String</span>)](item_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'onSellTime</span>, <span class="symbol">'price</span>, <span class="symbol">'itemID</span>, <span class="symbol">'itemType</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccess =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccess_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessCount =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">Int</span>)](pageAccessCount_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'accessCount</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessSession =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccessSession_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"item_tab"</span>, item)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccess_tab"</span>, pageAccess)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessCount_tab"</span>, pageAccessCount)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessSession_tab"</span>, pageAccessSession)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testSelect</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">"替换想要测试的SQL"</span></span><br><span class="line">    <span class="comment">// 非window 相关用 procTimePrint(sql)</span></span><br><span class="line">    <span class="comment">// Window 相关用 rowTimePrint(sql)</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义Sink</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RetractingSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> retractedResults: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(v: (<span class="type">Boolean</span>, <span class="type">Row</span>)) &#123;</span><br><span class="line">    retractedResults.synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> value = v._2.toString</span><br><span class="line">    <span class="keyword">if</span> (v._1) &#123;</span><br><span class="line">    retractedResults += value</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = retractedResults.indexOf(value)</span><br><span class="line">    <span class="keyword">if</span> (idx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    retractedResults.remove(idx)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Tried to retract a value that wasn't added first. "</span> +</span><br><span class="line">    <span class="string">"This is probably an incorrectly implemented test. "</span> +</span><br><span class="line">    <span class="string">"Try to set the parallelism of the sink to 1."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    retractedResults.sorted.foreach(println(_))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Water mark 生成器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventTimeSourceFunction</span>[<span class="type">T</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">dataWithTimestampList: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span></span>), <span class="title">Long</span>]]) <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(w) =&gt; ctx.emitWatermark(<span class="keyword">new</span> <span class="type">Watermark</span>(w))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h2><p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某</p><p>些列, 如下图所示:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNibZ25Mic3yIbEcG8icTWkkJiaMcTr5oq0wTkT7rdZ5EkUpXEp26ZKVTrKw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_name, <span class="keyword">CONCAT</span>(c_name, <span class="string">' come '</span>, c_desc) <span class="keyword">as</span> <span class="keyword">desc</span> <span class="keyword">FROM</span> customer_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><table><thead><tr><th>c_name</th><th>desc</th></tr></thead><tbody><tr><td>Kevin</td><td>Kevin come from JinLin</td></tr><tr><td>Sunny</td><td>Sunny come from JinLin</td></tr><tr><td>Jincheng</td><td>Jincheng come from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是携带 <code>DISTINCT</code> 关键字，示例如下：</p><p><strong>SQL 示例</strong></p><p>在订单表查询所有的客户id，消除重复客户id, 如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c_id <span class="keyword">FROM</span> order_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th></tr></thead><tbody><tr><td>c_001</td></tr><tr><td>c_002</td></tr></tbody></table><h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> 用于从数据集/流中过滤数据，与<span class="keyword">SELECT</span>一起使用，语法遵循<span class="keyword">ANSI</span>-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</span><br></pre></td></tr></table></figure><p><strong>SQL 示例</strong></p><p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id = <span class="string">'c_001'</span> <span class="keyword">OR</span> c_id = <span class="string">'c_003'</span>;</span><br></pre></td></tr></table></figure><p><strong>Result</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>AND</code>， <code>OR</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 <code>WHERE</code> 可以结合<code>IN</code>,<code>NOT IN</code>联合使用，具体如下：</p><p><strong>SQL 示例 (IN 常量)</strong></p><p>使用 <code>IN</code> 在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="string">'c_001'</span>, <span class="string">'c_003'</span>);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>SQL 示例 (IN 子查询)</strong></p><p>使用 <code>IN</code>和 子查询 在<code>customer_tab</code>查询已经下过订单的客户信息，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="keyword">SELECT</span> c_id <span class="keyword">FROM</span> order_tab);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr></tbody></table><p><strong>IN/NOT IN 与关系代数</strong></p><p>如上介绍IN是关系代数中的Intersection， NOT IN是关系代数的Difference， 如下图示意：</p><ul><li>IN(Intersection</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblp6icardxtCKeaf7RHrFbN6TVbyqyGOGuSWYY7uY3DJb5ODYsOqvv1mWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li><li>NOT IN(Difference）</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpcofHeFia7icQorYjiaGmHO9yiclrFaMCk3l6sBuQa2sm5QlrtepLOrdIMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li></ul><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpoeicHXKPbhnpAKEe8cMRzf4WHDQiagwAHRIlH6icqn107hHkiaeJh2CWDQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>SQL 示例</strong></p><p>将order_tab信息按customer_tab分组统计订单数量，简单示例如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT c_id, count(o_id) as o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY c_id;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>o_count</th></tr></thead><tbody><tr><td>c_001</td><td>2</td></tr><tr><td>c_002</td><td>1</td></tr></tbody></table><p><strong>特别说明</strong></p><p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p><p><strong>SQL 示例</strong></p><p>按时间进行分组，查询每分钟的订单数量，如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT SUBSTRING(o_time, 1, 16) AS o_time_min, count(o_id) AS o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY SUBSTRING(o_time, 1, 16)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>o_time_min</th><th>o_count</th></tr></thead><tbody><tr><td>2018-11-05 10:01</td><td>2</td></tr><tr><td>2018-11-05 10:03</td><td>1</td></tr></tbody></table><p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p><h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNpaHuv6VYq4P9Zyke2cuHCIwibTbicJpXicRJWemZsJN6Y1Nq3vKVNzpNg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p><h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNALmfI36VBMGEontFaDkRleLsSbErPHtRYvT0dBQ4ic6kwQD3AEJIhfQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p><h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p><ul><li>JOIN - INNER JOIN</li><li>LEFT JOIN - LEFT OUTER JOIN</li><li>RIGHT JOIN - RIGHT OUTER JOIN</li><li>FULL JOIN - FULL OUTER JOIN</li></ul><p>JOIN与关系代数的Join语义相同，具体如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN8qxel16siciaMAH8x3aQCcZ6q0ic8QrtZtco3D9frZFjHfZYj4q33hszg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例 (JOIN)</strong></p><p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr></tbody></table><p><strong>SQL 示例 (LEFT JOIN)</strong></p><p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNLcrI9iar3vKlgxMwRceIAMCZm2uNbhUWINhK1yRAllPdkwVJ1PcHhqQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>对应的SQL语句如下(LEFT JOIN)：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, <span class="built_in">T2</span>.ColC, ColE FROM TI LEFT <span class="keyword">JOIN </span><span class="built_in">T2</span> ON <span class="built_in">T1</span>.ColC = <span class="built_in">T2</span>.ColC <span class="comment">;</span></span><br></pre></td></tr></table></figure><ul><li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li></ul><p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr></tbody></table><p><strong>特别说明</strong></p><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p><h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p><h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p><ul><li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li><li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li></ul><h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p><p><strong>语义</strong></p><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN6yotUibfmTVgbnFd7dvC4tgfFEddh0xJ6PzC9wzLDgiaemZoCCjVNxaw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p><p><strong>语法</strong></p><p>Bounded ROWS OVER Window 语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">ROWS</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | rowCount) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>rowCount - 是定义根据当前行开始向前追溯几行元素。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> onSellTime </span><br><span class="line">        <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>50</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>60</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p><p><strong>语义</strong></p><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNTtvBlDvT0wfxJvTOL8e9CbVJg6YVxAfLMKskjXibicrCeOGgIZxAJxdw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p><p><strong>语法</strong></p><p>Bounded RANGE OVER Window的语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">RANGE</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | timeInterval) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li></ul><p><strong>SQL 示例</strong></p><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下（Bounded RANGE OVER Windo</strong>w）</p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>60</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>40</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><p><strong>特别说明</strong></p><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p><ul><li>GROUP BY - <code>SELECT d, MAX(c) FROM table GROUP BY d</code></li><li>OVER Window = <code>SELECT a, b, c, d, MAX(c) OVER(PARTITION BY d, ORDER BY ProcTime())</code><br>如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li></ul><h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p><ul><li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li><li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li><li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li></ul><p>说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p><h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><p><strong>语义</strong></p><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN9PPeHiaOUQib8BG2xs3YPxpN8EYibnRNkFxgicW1kPrNeicE8vpcUB7tspA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Tumble 滚动窗口对应的语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)], </span><br><span class="line">    [TUMBLE_END(timeCol, size)], </span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] - 决定了流是Keyed还是/Non-Keyed;</li><li>TUMBLE_START - 窗口开始时间;</li><li>TUMBLE_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>size - 表示窗口的大小，如 秒，分钟，小时，天。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region,</span><br><span class="line">    TUMBLE_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    TUMBLE_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv</span><br><span class="line"><span class="keyword">FROM</span> pageAccess_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, TUMBLE(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p><p><strong>语义</strong></p><p>Hop 滑动窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNCwyBicMTKEicSxibebwTfwvImiaA2TlN0FuM0wuG6zAibYyk5JrfBTmrwEA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Hop 滑动窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,  </span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggN(colN) </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>HOP_START - 窗口开始时间;</li><li>HOP_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>slide - 是滑动步伐的大小；</li><li>size - 是窗口的大小，如 秒，分钟，小时，天；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  HOP_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">  HOP_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">  <span class="keyword">SUM</span>(accessCount) <span class="keyword">AS</span> accessCount  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessCount_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>winStart</th><th>winEnd</th><th>accessCount</th></tr></thead><tbody><tr><td>2017-11-11 01:55:00.0</td><td>2017-11-11 02:05:00.0</td><td>186</td></tr><tr><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:10:00.0</td><td>396</td></tr><tr><td>2017-11-11 02:05:00.0</td><td>2017-11-11 02:15:00.0</td><td>243</td></tr><tr><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:20:00.0</td><td>33</td></tr><tr><td>2017-11-11 04:05:00.0</td><td>2017-11-11 04:15:00.0</td><td>129</td></tr><tr><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:20:00.0</td><td>129</td></tr></tbody></table><h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p><p>语义</p><p>Session 会话窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNfhx9EQvp4OyBYHue50QEzW3qZfxeRV5DCb8CkcneoGjadj7NqNHq9w/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Seeeion 会话窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    SESSION_START(timeCol, gap) AS winStart,  </span><br><span class="line">    SESSION_END(timeCol, gap) AS winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>SESSION_START - 窗口开始时间；</li><li>SESSION_END - 窗口结束时间；</li><li>timeCol - 是流表中表示时间字段；</li><li>gap - 是窗口数据非活跃周期的时长；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessSession_tab</code>测试数据，我们按地域统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region, </span><br><span class="line">    SESSION_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd, </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessSession_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, <span class="keyword">SESSION</span>(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:13:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:01:00.0</td><td>2017-11-11 02:08:00.0</td><td>4</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:14:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:16:00.0</td><td>2017-11-11 04:19:00.0</td><td>1</td></tr></tbody></table><h2 id="UDX"><a href="#UDX" class="headerlink" title="UDX"></a>UDX</h2><p>Apache Flink 除了提供了大部分ANSI-SQL的核心算子，也为用户提供了自己编写业务代码的机会，那就是User-Defined Function,目前支持如下三种 User-Defined Function：</p><ul><li>UDF - User-Defined Scalar Function</li><li>UDTF - User-Defined Table Function</li><li>UDAF - User-Defined Aggregate Funciton</li></ul><p>UDX都是用户自定义的函数，那么Apache Flink框架为啥将自定义的函数分成三类呢？是根据什么划分的呢？Apache Flink对自定义函数进行分类的依据是根据函数语义的不同，函数的输入和输出不同来分类的，具体如下：</p><table><thead><tr><th>UDX</th><th>INPUT</th><th>OUTPUT</th><th>INPUT:OUTPUT</th></tr></thead><tbody><tr><td>UDF</td><td>单行中的N(N&gt;=0)列</td><td>单行中的1列</td><td>1:1</td></tr><tr><td>UDTF</td><td>单行中的N(N&gt;=0)列</td><td>M(M&gt;=0)行</td><td>1:N(N&gt;=0)</td></tr><tr><td>UDAF</td><td>M(M&gt;=0)行中的每行的N(N&gt;=0)列</td><td>单行中的1列</td><td>M：1(M&gt;=0)</td></tr></tbody></table><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ul><li>定义<br>用户想自己编写一个字符串联接的UDF，我们只需要实现<code>ScalarFunction#eval()</code>方法即可，简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyConnect</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="meta">@varargs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(args: <span class="type">String</span>*): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; args.length) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args(i) == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      sb.append(args(i))</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    sb.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> <span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = MyConnect</span></span><br><span class="line"> tEnv.registerFunction(<span class="string">"myConnect"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"> <span class="keyword">val</span> sql = <span class="string">"SELECT myConnect(a, b) as str FROM tab"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><ul><li>定义<br>用户想自己编写一个字符串切分的UDTF，我们只需要实现<code>TableFunction#eval()</code>方法即可，简单实现如下：</li></ul><p>ScalarFunction#eval()`</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>))&#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(collect)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>, prefix: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>)) &#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(s =&gt; collect(prefix + s))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MySplit</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"mySplit"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT c, s FROM MyTable, LATERAL TABLE(mySplit(c)) AS T(s)"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><ul><li>定义<br>UDAF 要实现的接口比较多，我们以一个简单的CountAGG为例，做简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The initial accumulator for count aggregate function */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span> <span class="keyword">extends</span> <span class="title">JTuple1</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  f0 = <span class="number">0</span>L <span class="comment">//count</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * User-defined count aggregate function</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCount</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">JLong</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 += <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 -= <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 -= <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">JLong</span> = &#123;</span><br><span class="line">    acc.f0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">CountAccumulator</span>, its: <span class="type">JIterable</span>[<span class="type">CountAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = its.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      acc.f0 += iter.next().f0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CountAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getAccumulatorType</span></span>: <span class="type">TypeInformation</span>[<span class="type">CountAccumulator</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TupleTypeInfo</span>(classOf[<span class="type">CountAccumulator</span>], <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResultType</span></span>: <span class="type">TypeInformation</span>[<span class="type">JLong</span>] =</span><br><span class="line">    <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MyCount</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"myCount"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT myCount(c) FROM MyTable GROUP BY  a"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="Source-amp-Sink"><a href="#Source-amp-Sink" class="headerlink" title="Source&amp;Sink"></a>Source&amp;Sink</h1><p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p><h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MySourceFunction</span>[<span class="type">T</span>](<span class="title">dataWithTimestampList</span>: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span>), <span class="type">Long</span>]]) </span></span><br><span class="line"><span class="class">  extends <span class="type">SourceFunction</span>[<span class="type">T</span>] &#123;</span></span><br><span class="line"><span class="class">  override def run(<span class="title">ctx</span>: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span></span><br><span class="line"><span class="class">    dataWithTimestampList.foreach &#123;</span></span><br><span class="line"><span class="class">      case <span class="type">Left</span>(<span class="title">t</span>) =&gt; ctx.collectWithTimestamp(<span class="title">t</span>.<span class="title">_2</span>, <span class="title">t</span>.<span class="title">_1</span>)</span></span><br><span class="line"><span class="class">      case <span class="type">Right</span>(<span class="title">w</span>) =&gt; ctx.emitWatermark(<span class="title">new</span> <span class="type">Watermark(w)</span>)</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">  override def cancel(): <span class="type">Unit</span> = ???</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTableSource</span> <span class="keyword">extends</span> <span class="title">StreamTableSource</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fieldNames = <span class="type">Array</span>(<span class="string">"accessTime"</span>, <span class="string">"region"</span>, <span class="string">"userId"</span>)</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">TableSchema</span>(fieldNames, <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">SQL_TIMESTAMP</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">  <span class="keyword">val</span> rowType = <span class="keyword">new</span> <span class="type">RowTypeInfo</span>(</span><br><span class="line">    <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">LONG</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>).asInstanceOf[<span class="type">Array</span>[<span class="type">TypeInformation</span>[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 页面访问表数据 rows with timestamps and watermarks</span></span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366200000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366200000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366200000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366260000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366260000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366260000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510373400000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510373400000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510373400000</span>L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRowtimeAttributeDescriptors</span></span>: util.<span class="type">List</span>[<span class="type">RowtimeAttributeDescriptor</span>] = &#123;</span><br><span class="line">    <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">RowtimeAttributeDescriptor</span>(</span><br><span class="line">      <span class="string">"accessTime"</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExistingField</span>(<span class="string">"accessTime"</span>),</span><br><span class="line">      <span class="type">PreserveWatermarks</span>.<span class="type">INSTANCE</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDataStream</span></span>(execEnv: <span class="type">StreamExecutionEnvironment</span>): <span class="type">DataStream</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    execEnv.addSource(<span class="keyword">new</span> <span class="type">MySourceFunction</span>[<span class="type">Row</span>](data)).setParallelism(<span class="number">1</span>).returns(rowType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReturnType</span></span>: <span class="type">TypeInformation</span>[<span class="type">Row</span>] = rowType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getTableSchema</span></span>: <span class="type">TableSchema</span> = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = <span class="built_in">File</span>.createTempFile(<span class="string">"csv_sink_"</span>, <span class="string">"tem"</span>)</span><br><span class="line">    <span class="comment">// 打印sink的文件路径，方便我们查看运行结果</span></span><br><span class="line">    <span class="built_in">println</span>(<span class="string">"Sink path : "</span> + tempFile)</span><br><span class="line">    <span class="built_in">if</span> (tempFile.<span class="built_in">exists</span>()) &#123;</span><br><span class="line">      tempFile.<span class="keyword">delete</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[<span class="keyword">String</span>](<span class="string">"region"</span>, <span class="string">"winStart"</span>, <span class="string">"winEnd"</span>, <span class="string">"pv"</span>),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//方便我们查出输出数据</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sourceTableName = <span class="string">"mySource"</span></span><br><span class="line">    <span class="comment">// 创建自定义source数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSource = <span class="keyword">new</span> <span class="type">MyTableSource</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sinkTableName = <span class="string">"csvSink"</span></span><br><span class="line">    <span class="comment">// 创建CSV sink 数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册source</span></span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    <span class="comment">// 注册sink</span></span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">"SELECT  "</span> +</span><br><span class="line">      <span class="string">"  region, "</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,"</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv "</span> +</span><br><span class="line">      <span class="string">" FROM mySource "</span> +</span><br><span class="line">      <span class="string">" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region"</span></span><br><span class="line"></span><br><span class="line">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink <span class="string">path :</span> <span class="regexp">/var/</span>folders<span class="regexp">/88/</span><span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn<span class="regexp">/T/</span>csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure><p>Cat 方式查看计算结果，如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/<span class="number">88</span>/<span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:12:00.0</span>,<span class="number">2</span></span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">04:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">04:12:00.0</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure><p>表格化如上结果：</p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job收尾。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> FlinkSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用ogg实现oracle到kafka的增量数据实时同步</title>
      <link href="/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"/>
      <url>/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。</p><p>ogg即Oracle GoldenGate是Oracle的同步工具，本文讲如何配置ogg以实现Oracle数据库增量数据实时同步到kafka中，其中同步消息格式为json。</p><p>下面是我的源端和目标端的一些配置信息：</p><table><thead><tr><th align="center">-</th><th align="center">版本</th><th align="center">OGG版本</th><th align="center">ip</th><th align="center">主机名</th></tr></thead><tbody><tr><td align="center">源端</td><td align="center">OracleRelease 11.2.0.1.0</td><td align="center">Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64</td><td align="center">192.168.23.167</td><td align="center">cdh01</td></tr><tr><td align="center">目标端</td><td align="center">kafka_2.11-0.11.0.1</td><td align="center">Oracle GoldenGate for Big Data 12.3.0.1.0 on Linux x86-64</td><td align="center">192.168.23.168</td><td align="center">cdh02</td></tr></tbody></table><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>注意：源端和目标端的文件不一样，目标端需要下载Oracle GoldenGate for Big Data,源端需要下载Oracle GoldenGate for Oracle具体下载方法见最后的附录截图。</p><p>目标端在<a href="http://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html" target="_blank" rel="noopener">这里</a>查询下载，源端在<a href="https://edelivery.oracle.com/osdc/faces/SoftwareDelivery" target="_blank" rel="noopener">旧版本</a>查询下载。</p><h2 id="源端（Oracle）配置"><a href="#源端（Oracle）配置" class="headerlink" title="源端（Oracle）配置"></a>源端（Oracle）配置</h2><p>注意：源端是创建了oracle用户且安装了oracle数据库，oracle环境变量之前都配置好了</p><p>（后面只要涉及到源端均在oracle用户下操作）</p><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>先建立ogg目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /oracledata/data/ogg</span><br><span class="line">unzip Oracle GoldenGate_11.2.1.0.3.zip</span><br></pre></td></tr></table></figure><p>解压后得到一个tar包，再解压这个tar</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /oracledata/data/ogg</span><br></pre></td></tr></table></figure><h3 id="配置ogg环境变量"><a href="#配置ogg环境变量" class="headerlink" title="配置ogg环境变量"></a>配置ogg环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export OGG_HOME=/oracledata/data/ogg</span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib</span><br><span class="line">export PATH=$OGG_HOME:$PATH</span><br></pre></td></tr></table></figure><p>使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><p>如果命令成功即可进行下一步，不成功请检查前面的步骤。</p><h3 id="oracle打开归档模式"><a href="#oracle打开归档模式" class="headerlink" title="oracle打开归档模式"></a>oracle打开归档模式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以DBA身份连接数据库</span></span><br><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>执行下面的命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><p>若显示如下，则说明当前未开启归档模式</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Database <span class="built_in">log</span> <span class="built_in">mode</span>       No Archive <span class="built_in">Mode</span></span><br><span class="line">Automatic archival       Disabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online <span class="built_in">log</span> sequence     <span class="number">12</span></span><br><span class="line">Current <span class="built_in">log</span> sequence       <span class="number">14</span></span><br></pre></td></tr></table></figure><p>手动打开即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 立即关闭数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动实例并加载数据库，但不打开</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup mount</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改数据库为归档模式</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database archivelog;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用自动归档</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system archive <span class="built_in">log</span> start;</span></span><br></pre></td></tr></table></figure><p>再执行一下命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Database log mode       Archive Mode</span><br><span class="line">Automatic archival       Enabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online log sequence     12</span><br><span class="line">Next log sequence to archive   14</span><br><span class="line">Current log sequence       14</span><br></pre></td></tr></table></figure><p>可以看到为Enabled，则成功打开归档模式。</p><h3 id="Oracle打开日志相关"><a href="#Oracle打开日志相关" class="headerlink" title="Oracle打开日志相关"></a>Oracle打开日志相关</h3><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">NO</span>     <span class="literal">NO</span></span><br></pre></td></tr></table></figure><p>若为NO，则需要通过命令修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database force logging;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database add supplemental <span class="built_in">log</span> data;</span></span><br></pre></td></tr></table></figure><p>再查看一下为YES即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">YES</span>    <span class="literal">YES</span></span><br></pre></td></tr></table></figure><p>上述操作只是开启了最小补充日志，如果要抽取全部字段需要开启全列补充日志,否则值为null的字段不会在抽取日志中显示！！！</p><p>补充日志开启命令参考：<a href="https://blog.csdn.net/aaron8219/article/details/16825963" target="_blank" rel="noopener">https://blog.csdn.net/aaron8219/article/details/16825963</a></p><p><strong>注：开启全列补充日志会导致磁盘快速增长，LGWR进程繁忙，不建议使用。大家可根据自己的情况使用。</strong></p><p>查看数据库是否开启了全列补充日志</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;  </span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">NO</span></span><br></pre></td></tr></table></figure><p>若未开启可以通过以下命令开启。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; alter database add supplemental log data(all) columns;</span><br><span class="line"></span><br><span class="line">Database altered.</span><br><span class="line"></span><br><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">YES</span></span><br></pre></td></tr></table></figure><h3 id="oracle创建复制用户"><a href="#oracle创建复制用户" class="headerlink" title="oracle创建复制用户"></a>oracle创建复制用户</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="regexp">/oracledata/</span>data<span class="regexp">/tablespace/</span>dbsrv2</span><br></pre></td></tr></table></figure><p>然后执行下面sql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create tablespace oggtbs datafile <span class="string">'/oracledata/data/tablespace/dbsrv2/oggtbs01.dbf'</span> size 1000M autoextend on;</span></span><br><span class="line">控制台显示的内容：Tablespace created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">  create user ogg identified by 123456 default tablespace oggtbs;</span></span><br><span class="line">控制台显示的内容：User created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to ogg;</span></span><br><span class="line">控制台显示的内容：Grant succeeded.</span><br></pre></td></tr></table></figure><h3 id="OGG初始化"><a href="#OGG初始化" class="headerlink" title="OGG初始化"></a>OGG初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">GGSCI (cdh01) 1&gt; create subdirs</span><br></pre></td></tr></table></figure><p>控制台显示的内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Creating subdirectories under current directory /oracledata/data/ogg</span><br><span class="line"></span><br><span class="line">Parameter files                /oracledata/data/ogg/dirprm: created</span><br><span class="line">Report files                   /oracledata/data/ogg/dirrpt: created</span><br><span class="line">Checkpoint files               /oracledata/data/ogg/dirchk: created</span><br><span class="line">Process status files           /oracledata/data/ogg/dirpcs: created</span><br><span class="line">SQL script files               /oracledata/data/ogg/dirsql: created</span><br><span class="line">Database definitions files     /oracledata/data/ogg/dirdef: created</span><br><span class="line">Extract data files             /oracledata/data/ogg/dirdat: created</span><br><span class="line">Temporary files                /oracledata/data/ogg/dirtmp: created</span><br><span class="line">Stdout files                   /oracledata/data/ogg/dirout: created</span><br></pre></td></tr></table></figure><h3 id="Oracle创建测试表"><a href="#Oracle创建测试表" class="headerlink" title="Oracle创建测试表"></a>Oracle创建测试表</h3><p>创建一个用户,在该用户下新建测试表，用户名、密码、表名均为 test_ogg。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create user test_ogg identified by test_ogg default tablespace users;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> conn test_ogg/test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create table test_ogg(id int,name varchar(20),sex varchar(4),primary key(id));</span></span><br></pre></td></tr></table></figure><h2 id="目标端（kafka）配置"><a href="#目标端（kafka）配置" class="headerlink" title="目标端（kafka）配置"></a>目标端（kafka）配置</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -<span class="selector-tag">p</span> /data/apps/ogg</span><br><span class="line">unzip OGG_BigData_12.<span class="number">3.0</span>.<span class="number">1.0</span>_Release.zip</span><br><span class="line">tar xf ggs_Adapters_Linux_x64<span class="selector-class">.tar</span>  -C /data/apps/ogg</span><br></pre></td></tr></table></figure><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> /etc/<span class="keyword">profile</span></span><br></pre></td></tr></table></figure><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/opt/java</span><span class="regexp">/jdk1.8.0_211</span></span><br><span class="line"><span class="regexp">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line"></span><br><span class="line">export OGG_HOME=<span class="regexp">/data/apps</span><span class="regexp">/ogg</span></span><br><span class="line"><span class="regexp">export LD_LIBRARY_PATH=$JAVA_HOME/jre</span><span class="regexp">/lib/amd</span>64:$JAVA_HOME/jre/<span class="class"><span class="keyword">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">libjsig</span>.<span class="title">so</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>/<span class="title">libjvm</span>.<span class="title">so</span>:$<span class="title">OGG_HOME</span>/<span class="title">lib</span></span></span><br><span class="line">export PATH=$<span class="symbol">OGG_HOME:</span>$PATH</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure><p>同样测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><h3 id="初始化目录"><a href="#初始化目录" class="headerlink" title="初始化目录"></a>初始化目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create subdirs</span><br></pre></td></tr></table></figure><h2 id="OGG源端配置"><a href="#OGG源端配置" class="headerlink" title="OGG源端配置"></a>OGG源端配置</h2><p>Oracle实时传输到Hadoop集群（HDFS，<a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">Hive</a>，Kafka等）的基本原理如图：<br><img src="https://mc.qcloudimg.com/static/img/dd548277beb41f51d0e5914dccda9134/image.png" alt="img"><br>根据如上原理，配置大概分为如下步骤：源端目标端配置ogg管理器（mgr）；源端配置extract进程进行Oracle日志抓取；源端配置pump进程传输抓取内容到目标端；目标端配置replicate进程复制日志到Kafka集群。</p><h3 id="配置OGG的全局变量"><a href="#配置OGG的全局变量" class="headerlink" title="配置OGG的全局变量"></a>配置OGG的全局变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 1&gt; dblogin userid ogg password 123456</span><br><span class="line">控制台显示的内容：Successfully logged into database.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 2&gt; edit param ./globals</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">oggschema ogg</span></span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr"><a href="#配置管理器mgr" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">3</span>&gt; edit param mgr</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><p>说明：PORT即mgr的默认监听端口；</p><p>DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；</p><p>AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；</p><p>PURGEOLDEXTRACTS即TRAIL文件的定期清理</p><h3 id="添加复制表"><a href="#添加复制表" class="headerlink" title="添加复制表"></a>添加复制表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 4&gt; add trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo data enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 5&gt; info trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo log data is enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line">控制台显示的内容：Columns supplementally logged for table TEST_OGG.TEST_OGG: ID</span><br></pre></td></tr></table></figure><h3 id="配置extract进程"><a href="#配置extract进程" class="headerlink" title="配置extract进程"></a>配置extract进程</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">6</span>&gt; edit param extkafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">extract</span> extkafka</span><br><span class="line"><span class="attribute">dynamicresolution</span></span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (ORACLE_SID = <span class="string">"dbsrv2"</span>)</span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (NLS_LANG = <span class="string">"american_america.AL32UTF8"</span>)</span><br><span class="line"><span class="attribute">GETUPDATEBEFORES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSDELETES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSUPDATES</span></span><br><span class="line"><span class="attribute">userid</span> ogg,password 123456</span><br><span class="line"><span class="attribute">exttrail</span> /oracledata/data/ogg/dirdat/to</span><br><span class="line"><span class="attribute">table</span> test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>dynamicresolution动态解析；</p><p>SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；</p><p>userid ogg,password 123456即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；</p><p>table即复制表的表名，支持*通配，必须以;结尾</p><p>添加extract进程：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) <span class="number">7</span>&gt; <span class="keyword">add </span><span class="keyword">extract </span><span class="keyword">extkafka,tranlog,begin </span>now</span><br><span class="line">控制台显示的内容：<span class="keyword">EXTRACT </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure><p>(注：若报错</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ERROR: </span>Could not create checkpoint file /opt/ogg/dirchk/EXTKAFKA.cpe (error 2, No such file or directory).</span><br></pre></td></tr></table></figure><p>执行下面的命令再重新添加即可。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="keyword">subdirs</span></span><br></pre></td></tr></table></figure><p>)</p><p>添加trail文件的定义与extract进程绑定：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 8&gt; <span class="builtin-name">add</span> exttrail /oracledata/data/ogg/dirdat/<span class="keyword">to</span>,extract extkafka</span><br><span class="line">控制台显示的内容：EXTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置pump进程"><a href="#配置pump进程" class="headerlink" title="配置pump进程"></a>配置pump进程</h3><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">9</span>&gt; edit param pukafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">extract pukafka</span><br><span class="line">passthru</span><br><span class="line">dynamicresolution</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">rmthost <span class="number">192.168</span><span class="number">.23</span><span class="number">.168</span> mgrport <span class="number">7809</span></span><br><span class="line">rmttrail /data/apps/ogg/dirdat/to</span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；</p><p>dynamicresolution动态解析；</p><p>userid ogg,password ogg即OGG连接Oracle数据库的帐号密码</p><p>rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口；</p><p>rmttrail即目标端trail文件存储位置以及名称。<strong>(注意，这里很容易犯错！！！注意是目标端的路径！！！)</strong></p><p>分别将本地trail文件和目标端的trail文件绑定到extract进程：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 10&gt; <span class="builtin-name">add</span> extract pukafka,exttrailsource /oracledata/data/ogg/dirdat/<span class="keyword">to</span></span><br><span class="line">控制台显示的内容：EXTRACT added.</span><br><span class="line">GGSCI (cdh01) 11&gt; <span class="builtin-name">add</span> rmttrail /data/apps/ogg/dirdat/<span class="keyword">to</span>,extract pukafka</span><br><span class="line">控制台显示的内容：RMTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置defgen文件"><a href="#配置defgen文件" class="headerlink" title="配置defgen文件"></a>配置defgen文件</h3><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">12</span>&gt; edit param test_ogg</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defsfile <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>退出GGSCI</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">13</span>&gt; quit</span><br></pre></td></tr></table></figure><p>进行OGG主目录下执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">./defgen paramfile dirprm/test_ogg.prm</span><br></pre></td></tr></table></figure><p>输出以下内容则执行成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">        Oracle GoldenGate Table Definition Generator for Oracle</span><br><span class="line"> Version 11.2.1.0.3 14400833 OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258</span><br><span class="line">   Linux, x64, 64bit (optimized), Oracle 11g on Aug 23 2012 16:58:29</span><br><span class="line"> </span><br><span class="line">Copyright (C) 1995, 2012, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    Starting at 2018-05-23 05:03:04</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">Operating System Version:</span><br><span class="line">Linux</span><br><span class="line">Version #1 SMP Wed Apr 12 15:04:24 UTC 2017, Release 3.10.0-514.16.1.el7.x86_64</span><br><span class="line">Node: ambari.master.com</span><br><span class="line">Machine: x86_64</span><br><span class="line">                         soft limit   hard limit</span><br><span class="line">Address Space Size   :    unlimited    unlimited</span><br><span class="line">Heap Size            :    unlimited    unlimited</span><br><span class="line">File Size            :    unlimited    unlimited</span><br><span class="line">CPU Time             :    unlimited    unlimited</span><br><span class="line"></span><br><span class="line">Process id: 13126</span><br><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">**            Running with the following parameters                  **</span><br><span class="line">***********************************************************************</span><br><span class="line">defsfile /opt/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password ***</span><br><span class="line">table test_ogg.test_ogg;</span><br><span class="line">Retrieving definition for TEST_OGG.TEST_OGG</span><br><span class="line"></span><br><span class="line">Definitions generated for 1 table in /oracledata/data/ogg/dirdef/test_ogg.test_ogg</span><br></pre></td></tr></table></figure><p>将生成的/oracledata/data/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /oracledata/data/ogg/dirdef/test_ogg.test_ogg root@cdh02:/data/apps/ogg/dirdef/</span><br></pre></td></tr></table></figure><h2 id="OGG目标端配置"><a href="#OGG目标端配置" class="headerlink" title="OGG目标端配置"></a>OGG目标端配置</h2><h3 id="开启kafka服务"><a href="#开启kafka服务" class="headerlink" title="开启kafka服务"></a>开启kafka服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Zookeeper</span></span><br><span class="line">/data/apps/apache-zookeeper-3.5.5-bin/bin/zkServer.sh start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启Kafka</span></span><br><span class="line">/data/apps/kafka_2.11-0.11.0.1/bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr-1"><a href="#配置管理器mgr-1" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt;  edit param mgr</span><br><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><h3 id="配置checkpoint"><a href="#配置checkpoint" class="headerlink" title="配置checkpoint"></a>配置checkpoint</h3><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 2&gt; edit param ./GLOBALS</span><br><span class="line">CHECKPOINTTABLE test_ogg.checkpoint</span><br></pre></td></tr></table></figure><h3 id="配置replicate进程"><a href="#配置replicate进程" class="headerlink" title="配置replicate进程"></a>配置replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 3&gt; edit param rekafka</span><br><span class="line">REPLICAT rekafka</span><br><span class="line">sourcedefs /data/apps/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props</span><br><span class="line">REPORTCOUNT EVERY 1 MINUTES, RATE </span><br><span class="line">GROUPTRANSOPS 10000</span><br><span class="line">MAP test_ogg.test_ogg, TARGET test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>REPLICATE rekafka定义rep进程名称；</p><p>sourcedefs即在4.6中在源服务器上做的表映射文件；</p><p>TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props；</p><p>REPORTCOUNT即复制任务的报告生成频率；</p><p>GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；</p><p>MAP即源端与目标端的映射关系</p><h3 id="配置kafka-props"><a href="#配置kafka-props" class="headerlink" title="配置kafka.props"></a>配置kafka.props</h3><p><strong>本环节配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirprm/</span><br><span class="line">vim kafka.props</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> handler类型</span></span><br><span class="line">gg.handlerlist=kafkahandler</span><br><span class="line">gg.handler.kafkahandler.type=kafka</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kafka生产者配置文件</span></span><br><span class="line">gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的topic名称，无需手动创建</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gg.handler.kafkahandler.topicMappingTemplate=test_ogg（新版topicName属性的设置方式）</span></span><br><span class="line">gg.handler.kafkahandler.topicName=test_ogg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 传输文件的格式，支持json，xml等</span></span><br><span class="line">gg.handler.kafkahandler.format=json</span><br><span class="line">gg.handler.kafkahandler.format.insertOpKey = I  </span><br><span class="line">gg.handler.kafkahandler.format.updateOpKey = U  </span><br><span class="line">gg.handler.kafkahandler.format.deleteOpKey = D</span><br><span class="line">gg.handler.kafkahandler.format.truncateOpKey=T</span><br><span class="line">gg.handler.kafkahandler.format.includePrimaryKeys=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> OGG <span class="keyword">for</span> Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次</span></span><br><span class="line">gg.handler.kafkahandler.mode=op</span><br><span class="line"><span class="meta">#</span><span class="bash"> 类路径</span></span><br><span class="line">gg.classpath=dirprm/:/data/apps/kafka_2.11-0.11.0.1/libs/*:/data/apps/ogg/:/data/apps/ogg/lib/*</span><br></pre></td></tr></table></figure><p>紧接着创建Kafka生产者配置文件：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim custom_kafk<span class="built_in">a_producer</span>.properties</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kafkabroker的地址</span></span><br><span class="line">bootstrap.servers=cdh01:9092,cdh02:9092,cdh03:9092</span><br><span class="line">acks=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 压缩类型</span></span><br><span class="line">compression.type=gzip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重连延时</span></span><br><span class="line">reconnect.backoff.ms=1000</span><br><span class="line">value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">batch.size=102400</span><br><span class="line">linger.ms=10000</span><br></pre></td></tr></table></figure><p><strong>配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><h3 id="添加trail文件到replicate进程"><a href="#添加trail文件到replicate进程" class="headerlink" title="添加trail文件到replicate进程"></a>添加trail文件到replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt; add replicat rekafka exttrail /data/apps/ogg/dirdat/to,checkpointtable test_ogg.checkpoint</span><br><span class="line">控制台显示的内容：REPLICAT added.</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="启动所有进程"><a href="#启动所有进程" class="headerlink" title="启动所有进程"></a>启动所有进程</h3><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。<br>启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。<br>全部需要在ogg目录下执行ggsci目录进入ogg命令行。<br>源端依次是</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> extkafka</span><br><span class="line"><span class="literal">start</span> pukafka</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> rekafka</span><br></pre></td></tr></table></figure><p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功<br>源端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.master.com) 5&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">EXTRACT     RUNNING     EXTKAFKA    04:50:21      00:00:03    </span><br><span class="line">EXTRACT     RUNNING     PUKAFKA     00:00:00      00:00:03</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.slave1.com) 3&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">REPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</span><br></pre></td></tr></table></figure><h3 id="异常解决"><a href="#异常解决" class="headerlink" title="异常解决"></a>异常解决</h3><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ggser.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>或者ogg命令行,以rekafka进程为例</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh02</span>) <span class="number">2</span>&gt; view report rekafka</span><br></pre></td></tr></table></figure><h3 id="测试同步更新效果"><a href="#测试同步更新效果" class="headerlink" title="测试同步更新效果"></a>测试同步更新效果</h3><p>现在源端执行sql语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conn test_ogg/test_ogg</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_ogg <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'test'</span>,<span class="literal">null</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">update</span> test_ogg <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'zhangsan'</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">delete</span> test_ogg <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p>查看源端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-rw-rw- <span class="number">1</span> oracle oinstall <span class="number">1464</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看目标端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-r----- <span class="number">1</span> root root <span class="number">1504</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看kafka是否自动建立对应的主题</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.<span class="keyword">sh</span> --<span class="keyword">list</span> --zookeeper localhos<span class="variable">t:2181</span></span><br></pre></td></tr></table></figure><p>在列表中显示有test_ogg则表示没问题<br>通过消费者看是否有同步消息</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.44</span><span class="number">.129</span>:<span class="number">9092</span> --topic test_ogg --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"I"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:04:39.001362"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:04:44.610000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001246"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"U"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:05:44.000411"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:05:50.764000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001541"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;,<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"D"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:06:33.000312"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:06:39.845000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001670"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>before代表操作之前的数据，after代表操作后的数据，现在已经可以从kafka获取到同步的json数据了，后面可以用SparkStreaming和Storm等解析然后存到hadoop等大数据平台里</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想通配整个库的话，只需要把上面的配置所有表名改为*，如test_ogg<span class="selector-class">.test_ogg</span> 改为 test_ogg.*,但是kafka的topic不能通配，所以需要把所有表的数据放在一个topic，后面再用程序解析表名即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">若后期因业务需要导致表结构发生改变，需要重新生成源端表结构的defgen定义文件，再把定义文件通过scp放到目标端。defgen文件的作用是，记录了源端的表结构，然后我们再把这个文件放到目标端，在目标端应用SQL时就能根据defgen文件与目标端表结构，来做一定的转换。</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/purpleraintear/p/6071038.html" target="_blank" rel="noopener">基于OGG的Oracle与Hadoop集群准实时同步介绍</a></p><p><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">Fusion Middleware Integrating Oracle GoldenGate for Big Data</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据的导入导出 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
            <tag> Kafka </tag>
            
            <tag> ogg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka消费者Consumer参数设置及调优</title>
      <link href="/2018/09/04/kafka%E6%B6%88%E8%B4%B9%E8%80%85Consumer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/04/kafka%E6%B6%88%E8%B4%B9%E8%80%85Consumer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="消息的接收-gt-基于Consumer-Group"><a href="#消息的接收-gt-基于Consumer-Group" class="headerlink" title="消息的接收-&gt;基于Consumer Group"></a>消息的接收-&gt;基于Consumer Group</h2><p>Consumer Group 主要用于实现高伸缩性，高容错性的Consumer机制。因此，消息的接收是基于Consumer Group 的。组内多个Consumer实例可以同时读取Kafka消息，同一时刻一条消息只能被一个消费者消费，而且一旦某一个consumer “挂了”， Consumer Group 会立即将已经崩溃的Consumer负责的分区转交给其他Consumer来负责。从而保证 Consumer Group 能够正常工作。</p><h2 id="位移保存-gt-基于Consumer-Group"><a href="#位移保存-gt-基于Consumer-Group" class="headerlink" title="位移保存-&gt;基于Consumer Group"></a>位移保存-&gt;基于Consumer Group</h2><p>说来奇怪，位移保存是基于Consumer Group，同时引入检查点模式，定期实现offset的持久化。</p><h2 id="位移提交-gt-抛弃ZooKeeper"><a href="#位移提交-gt-抛弃ZooKeeper" class="headerlink" title="位移提交-&gt;抛弃ZooKeeper"></a>位移提交-&gt;抛弃ZooKeeper</h2><p>Consumer会定期向kafka集群汇报自己消费数据的进度，这一过程叫做位移的提交。这一过程已经抛弃Zookeeper，因为Zookeeper只是一个协调服务组件，不能作为存储组件，高并发的读取势必造成Zk的压力。</p><ul><li>新版本位移提交是在kafka内部维护了一个内部Topic(_consumer_offsets)。</li><li>在kafka内部日志目录下面，总共有50个文件夹，每一个文件夹包含日志文件和索引文件。日志文件主要是K-V结构，（group.id,topic,分区号）。</li><li>假设线上有很多的consumer和ConsumerGroup，通过对group.id做Hash求模运算，这50个文件夹就可以分散同时位移提交的压力。</li></ul><h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><h3 id="自动提交位移"><a href="#自动提交位移" class="headerlink" title="自动提交位移"></a>自动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="手动提交位移"><a href="#手动提交位移" class="headerlink" title="手动提交位移"></a>手动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">200</span>;</span><br><span class="line">List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        buffer.add(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">        insertIntoDb(buffer);</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">        buffer.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="kafka-Consumer参数设置"><a href="#kafka-Consumer参数设置" class="headerlink" title="kafka Consumer参数设置"></a>kafka Consumer参数设置</h2><ul><li>consumer.poll(1000) 重要参数</li><li>新版本的Consumer的Poll方法使用了类似于Select I/O机制，因此所有相关事件（包括reblance，消息获取等）都发生在一个事件循环之中。</li><li>1000是一个超时时间，一旦拿到足够多的数据（参数设置），consumer.poll(1000)会立即返回 ConsumerRecords&lt;String, String&gt; records。</li><li>如果没有拿到足够多的数据，会阻塞1000ms，但不会超过1000ms就会返回。</li></ul><hr><ul><li>session. timeout. ms &lt;=  coordinator检测失败的时间</li><li>默认值是10s</li><li>该参数是 Consumer Group 主动检测 (组内成员consummer)崩溃的时间间隔。若设置10min，那么Consumer Group的管理者（group coordinator）可能需要10分钟才能感受到。太漫长了是吧。</li></ul><hr><ul><li>max. poll. interval. ms &lt;= 处理逻辑最大时间</li><li>这个参数是0.10.1.0版本后新增的，可能很多地方看不到喔。这个参数需要根据实际业务处理时间进行设置，一旦Consumer处理不过来，就会被踢出Consumer Group。</li><li>注意：如果业务平均处理逻辑为1分钟，那么max. poll. interval. ms需要设置稍微大于1分钟即可，但是session. timeout. ms可以设置小一点（如10s），用于快速检测Consumer崩溃。</li></ul><hr><ul><li>auto.offset.reset</li><li>该属性指定了消费者在读取一个没有偏移量或者偏移量无效（消费者长时间失效当前的偏移量已经过时并且被删除了）的分区的情况下，应该作何处理，默认值是latest，也就是从最新记录读取数据（消费者启动之后生成的记录），另一个值是earliest，意思是在偏移量无效的情况下，消费者从起始位置开始读取数据。</li></ul><hr><ul><li>enable.auto.commit</li><li>对于精确到一次的语义，最好手动提交位移</li></ul><hr><ul><li>fetch.max.bytes</li><li>单次获取数据的最大消息数。</li></ul><hr><ul><li>max.poll.records  &lt;=  吞吐量</li><li>单次poll调用返回的最大消息数，如果处理逻辑很轻量，可以适当提高该值。</li><li>一次从kafka中poll出来的数据条数,max.poll.records条数据需要在在session.timeout.ms这个时间内处理完</li><li>默认值为500</li></ul><hr><ul><li>heartbeat. interval. ms &lt;= 居然拖家带口</li><li>heartbeat心跳主要用于沟通交流，及时返回请求响应。这个时间间隔真是越快越好。因为一旦出现reblance,那么就会将新的分配方案或者通知重新加入group的命令放进心跳响应中。</li></ul><hr><ul><li>connection. max. idle. ms &lt;= socket连接</li><li>kafka会定期的关闭空闲Socket连接。默认是9分钟。如果不在乎这些资源开销，推荐把这些参数值为-1，即不关闭这些空闲连接。</li></ul><hr><ul><li>request. timeout. ms</li><li>这个配置控制一次请求响应的最长等待时间。如果在超时时间内未得到响应，kafka要么重发这条消息，要么超过重试次数的情况下直接置为失败。</li><li>消息发送的最长等待时间.需大于session.timeout.ms这个时间</li></ul><hr><ul><li>fetch.min.bytes</li><li>server发送到消费端的最小数据，若是不满足这个数值则会等待直到满足指定大小。默认为1表示立即接收。</li></ul><hr><ul><li><p>fetch.wait.max.ms</p></li><li><p>若是不满足fetch.min.bytes时，等待消费端请求的最长等待时间</p></li></ul><hr><ul><li>0.11 新功能</li><li>空消费组延时rebalance，主要在server.properties文件配置</li><li>group.initial.rebalance.delay.ms&lt;= ，防止成员加入请求后本应立即开启的rebalance</li><li>对于用户来说，这个改进最直接的效果就是新增了一个broker配置：group.initial.rebalance.delay.ms，</li><li>默认是3秒钟。</li><li>主要作用是让coordinator推迟空消费组接收到成员加入请求后本应立即开启的rebalance。在实际使用时，假设你预估你的所有consumer组成员加入需要在10s内完成，那么你就可以设置该参数=10000。</li></ul><h2 id="线上采坑"><a href="#线上采坑" class="headerlink" title="线上采坑"></a>线上采坑</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.consumer</span><span class="selector-class">.CommitFailedException</span>:</span><br><span class="line"> Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. </span><br><span class="line">This means that the <span class="selector-tag">time</span> between subsequent calls to poll() was longer than the configured session<span class="selector-class">.timeout</span><span class="selector-class">.ms</span>, which typically implies that the poll loop is spending too much <span class="selector-tag">time</span> message processing. </span><br><span class="line">You can <span class="selector-tag">address</span> this either by increasing the session timeout or by reducing the maximum size of batches returned <span class="keyword">in</span> poll() with max<span class="selector-class">.poll</span><span class="selector-class">.records</span>. [com<span class="selector-class">.bonc</span><span class="selector-class">.framework</span><span class="selector-class">.server</span><span class="selector-class">.kafka</span><span class="selector-class">.consumer</span><span class="selector-class">.ConsumerLoop</span>]</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure><p>基于最新版本10，注意此版本session. timeout. ms 与max.poll.interval.ms进行功能分离了。</p><ul><li>可以发现频繁reblance，并伴随者重复性消费，这是一个很严重的问题，就是处理逻辑过重,max.poll. interval.ms过小导致。发生的原因就是 poll（）的循环调用时间过长，出现了处理超时。此时只用调大max.poll. interval.ms ，调小max.poll.records即可，同时要把request. timeout. ms设置大于max.poll. interval.ms</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化会继续，暂时把核心放在request. timeout. ms, max. poll. interval. ms，max.poll.records 上，避免因为处理逻辑过重，导致Consumer被频繁的踢出Consumer group。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产者Producer参数设置及调优</title>
      <link href="/2018/09/03/kafka%E7%94%9F%E4%BA%A7%E8%80%85Producer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/03/kafka%E7%94%9F%E4%BA%A7%E8%80%85Producer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="Producer核心工作流程"><a href="#Producer核心工作流程" class="headerlink" title="Producer核心工作流程"></a>Producer核心工作流程</h2><ul><li>Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。</li><li>进行序列化后，发送给Partioner，由Partioner确定目标分区后，发送到Producer程序中的一块内存缓冲区中。</li><li>Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次，统一发送给对应的broker中。</li></ul><h2 id="producer-主要参数设置"><a href="#producer-主要参数设置" class="headerlink" title="producer 主要参数设置"></a>producer 主要参数设置</h2><h3 id="producer-参数acks-设置（无数据丢失）"><a href="#producer-参数acks-设置（无数据丢失）" class="headerlink" title="producer 参数acks 设置（无数据丢失）"></a>producer 参数acks 设置（无数据丢失）</h3><p>在消息被认为是“已提交”之前，producer需要leader确认的produce请求的应答数。该参数用于控制消息的持久性，目前提供了3个取值：</p><p>acks = 0: 表示produce请求立即返回，不需要等待leader的任何确认。这种方案有最高的吞吐率，但是不保证消息是否真的发送成功。</p><p>acks = -1: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为produce请求成功。这种方案提供最高的消息持久性保证，但是理论上吞吐率也是最差的。</p><p>acks = 1: 表示leader副本必须应答此produce请求并写入消息到本地日志，之后produce请求被认为成功。如果此时leader副本应答请求之后挂掉了，消息会丢失。这是个较好的方案，提供了不错的持久性保证和吞吐。</p><p><strong>商业环境推荐：</strong></p><p>如果要较高的持久性要求以及无数据丢失的需求，设置acks = -1。其他情况下设置acks = 1</p><h3 id="producer参数-buffer-memory-设置（吞吐量）"><a href="#producer参数-buffer-memory-设置（吞吐量）" class="headerlink" title="producer参数 buffer.memory 设置（吞吐量）"></a>producer参数 buffer.memory 设置（吞吐量）</h3><p>该参数用于指定Producer端用于缓存消息的缓冲区大小，单位为字节，默认值为：33554432合计为32M。kafka采用的是异步发送的消息架构，prducer启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由一个专属线程负责从缓冲区读取消息进行真正的发送。</p><p><strong>商业环境推荐：</strong></p><ul><li>消息持续发送过程中，当缓冲区被填满后，producer立即进入阻塞状态直到空闲内存被释放出来，这段时间不能超过max.blocks.ms设置的值，一旦超过，producer则会抛出TimeoutException 异常，因为Producer是线程安全的，若一直报TimeoutException，需要考虑调高buffer.memory 了。</li><li>用户在使用多个线程共享kafka producer时，很容易把 buffer.memory 打满。</li></ul><h3 id="producer参数-compression-type-设置（lZ4）"><a href="#producer参数-compression-type-设置（lZ4）" class="headerlink" title="producer参数 compression.type 设置（lZ4）"></a>producer参数 compression.type 设置（lZ4）</h3><p>producer压缩器，目前支持none（不压缩），gzip，snappy和lz4。</p><p><strong>商业环境推荐：</strong></p><p>基于公司的大数据平台，试验过目前lz4的效果最好。当然2016年8月，FaceBook开源了Ztandard。官网测试： Ztandard压缩率为2.8，snappy为2.091，LZ4 为2.101 。</p><h3 id="producer参数-retries设置-注意消息乱序-EOS"><a href="#producer参数-retries设置-注意消息乱序-EOS" class="headerlink" title="producer参数 retries设置(注意消息乱序,EOS)"></a>producer参数 retries设置(注意消息乱序,EOS)</h3><p>producer重试的次数设置。重试时producer会重新发送之前由于瞬时原因出现失败的消息。瞬时失败的原因可能包括：元数据信息失效、副本数量不足、超时、位移越界或未知分区等。倘若设置了retries &gt; 0，那么这些情况下producer会尝试重试。</p><p><strong>商业环境推荐：</strong></p><ul><li>producer还有个参数：max.in.flight.requests.per.connection。如果设置该参数大约1，那么设置retries就有可能造成发送消息的乱序。</li><li>版本为0.11.0.1的kafka已经支持”精确到一次的语义”，因此消息的重试不会造成消息的重复发送。</li></ul><h3 id="producer参数batch-size设置-吞吐量和延时性能"><a href="#producer参数batch-size设置-吞吐量和延时性能" class="headerlink" title="producer参数batch.size设置(吞吐量和延时性能)"></a>producer参数batch.size设置(吞吐量和延时性能)</h3><p>producer都是按照batch进行发送的，因此batch大小的选择对于producer性能至关重要。producer会把发往同一分区的多条消息封装进一个batch中，当batch满了后，producer才会把消息发送出去。但是也不一定等到满了，这和另外一个参数linger.ms有关。默认值为16K，合计为16384.</p><p><strong>商业环境推荐：</strong></p><ul><li>batch 越小，producer的吞吐量越低，越大，吞吐量越大。</li></ul><h3 id="producer参数linger-ms设置-吞吐量和延时性能"><a href="#producer参数linger-ms设置-吞吐量和延时性能" class="headerlink" title="producer参数linger.ms设置(吞吐量和延时性能)"></a>producer参数linger.ms设置(吞吐量和延时性能)</h3><p>producer是按照batch进行发送的，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。</p><p><strong>商业环境推荐：</strong></p><ul><li>为了减少了网络IO，提升了整体的TPS。假设设置linger.ms=5，表示producer请求可能会延时5ms才会被发送。</li></ul><h3 id="producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能"><a href="#producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能" class="headerlink" title="producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)"></a>producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)</h3><p>producer的IO线程在单个Socket连接上能够发送未应答produce请求的最大数量。增加此值应该可以增加IO线程的吞吐量，从而整体上提升producer的性能。不过就像之前说的如果开启了重试机制，那么设置该参数大于1的话有可能造成消息的乱序。</p><p><strong>商业环境推荐：</strong></p><ul><li>默认值5是一个比较好的起始点,如果发现producer的瓶颈在IO线程，同时各个broker端负载不高，那么可以尝试适当增加该值.</li></ul><ul><li>过大增加该参数会造成producer的整体内存负担，同时还可能造成不必要的锁竞争反而会降低TPS</li></ul><h2 id="Java客户端"><a href="#Java客户端" class="headerlink" title="Java客户端"></a>Java客户端</h2><p>KafkaProducer(org.apache.kafka.clients.producer.KafkaProducer)是一个用于向kafka集群发送数据的Java客户端。该Java客户端是线程安全的，多个线程可以共享同一个producer实例，而且这通常比在多个线程中每个线程创建一个实例速度要快些。本文介绍的内容来自于kafka官方文档，详情参见<a href="http://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="noopener">KafkaProducer</a> </p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">package com.test.kafkaProducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.PartitionInfo;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class TestProducer &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, <span class="string">"192.168.137.200:9092"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="comment">//生产者发送消息 </span></span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"mytopic"</span>;</span><br><span class="line">        Producer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; procuder = <span class="keyword">new</span> KafkaProducer&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(props);</span><br><span class="line">        <span class="built_in">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">String</span> value = <span class="string">"value_"</span> + i;</span><br><span class="line">            ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; msg = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(topic, value);</span><br><span class="line">            procuder.send(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//列出topic的相关信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = <span class="keyword">new</span> ArrayList&lt;PartitionInfo&gt;() ;</span><br><span class="line">        partitions = procuder.partitionsFor(topic);</span><br><span class="line">        <span class="built_in">for</span>(PartitionInfo p:partitions)</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.<span class="built_in">println</span>(p);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.<span class="built_in">println</span>(<span class="string">"send message over."</span>);</span><br><span class="line">        procuder.<span class="built_in">close</span>(<span class="number">100</span>,TimeUnit.MILLISECONDS);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>producer包含一个用于保存待发送消息的缓冲池，缓冲池中消息是还没来得及传输到kafka集群的消息。位于底层的kafka I/O线程负责将缓冲池中的消息转换成请求发送到集群。如果在结束produce时，没有调用close()方法，那么这些资源会发生泄露。<br>用于建立消费者的相关参数说明及其默认值参见producerconfigs，此处对代码中用到的几个参数进行解释：<br>bootstrap.servers:用于初始化时建立链接到kafka集群，以host:port形式，多个以逗号分隔host1:port1,host2:port2；<br>acks:生产者需要server端在接收到消息后，进行反馈确认的尺度，主要用于消息的可靠性传输；acks=0表示生产者不需要来自server的确认；acks=1表示server端将消息保存后即可发送ack，而不必等到其他follower角色的都收到了该消息；acks=all(or acks=-1)意味着server端将等待所有的副本都被接收后才发送确认。<br>retries:生产者发送失败后，重试的次数<br>batch.size:当多条消息发送到同一个partition时，该值控制生产者批量发送消息的大小，批量发送可以减少生产者到服务端的请求数，有助于提高客户端和服务端的性能。<br>linger.ms:默认情况下缓冲区的消息会被立即发送到服务端，即使缓冲区的空间并没有被用完。可以将该值设置为大于0的值，这样发送者将等待一段时间后，再向服务端发送请求，以实现每次请求可以尽可能多的发送批量消息。<br>batch.size和linger.ms是两种实现让客户端每次请求尽可能多的发送消息的机制，它们可以并存使用，并不冲突。<br>buffer.memory:生产者缓冲区的大小，保存的是还未来得及发送到server端的消息，如果生产者的发送速度大于消息被提交到server端的速度，该缓冲区将被耗尽。<br>key.serializer,value.serializer说明了使用何种序列化方式将用户提供的key和vaule值序列化成字节。</p></blockquote><p><strong>kafka客户端的API</strong>  </p><p><strong>KafkaProducer对象实例化方法</strong>,可以使用map形式的键值对或者Properties对象来配置客户端的属性</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keySerializer:发送数据key值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> *valueSerializer:发送数据value值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>消息发送方法send()</strong></p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *record:key-value形式的待发送数据</span></span><br><span class="line"><span class="comment"> *callback:到发送的消息被borker端确认后的回调函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record</span>)</span>; <span class="comment">// Equivalent to send(record, null)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record,Callback callback</span>)</span>;</span><br></pre></td></tr></table></figure><p>send方法负责将缓冲池中的消息异步的发送到broker的指定topic中。异步发送是指，方法将消息存储到底层待发送的I/O缓存后，将立即返回，这可以实现并行无阻塞的发送更多消息。send方法的返回值是RecordMetadata类型，它含有消息将被投递的partition信息，该条消息的offset，以及时间戳。<br>因为send返回的是Future对象，因此在该对象上调用get()方法将阻塞，直到相关的发送请求完成并返回元数据信息；或者在发送时抛出异常而退出。<br>阻塞发送的方法如下：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">String</span> <span class="built_in">key</span> = <span class="string">"Key"</span>;</span><br><span class="line"><span class="keyword">String</span> value = <span class="string">"Value"</span>;</span><br><span class="line">ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(<span class="built_in">key</span>, value);</span><br><span class="line">producer.send(record).<span class="built_in">get</span>();</span><br></pre></td></tr></table></figure><p>可以充分利用回调函数和异步发送方式来确认消息发送的进度:</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(<span class="string">"the-topic"</span>, <span class="built_in">key</span>, value);</span><br><span class="line">producer.send(myRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> onCompletion(RecordMetadata metadata, Exception e) &#123;</span><br><span class="line">                        <span class="keyword">if</span>(e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            System.out.<span class="built_in">println</span>(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset());</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure><p><strong>flush</strong> </p><p>立即发送缓存数据</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure><p>调用该方法将使得缓冲区的所有消息被立即发送（即使linger.ms参数被设置为大于0），且会阻塞直到这些相关消息的发送请求完成。flush方法的前置条件是：之前发送的所有消息请求已经完成。一个请求被视为完成是指：根据acks参数配置项收到了相应的确认，或者发送中抛出异常失败了。<br>下面的例子展示了从一个topic消费后发到另一个topic，flush方法在此非常有用，它提供了一种方便的方法来确保之前发送的消息确实已经完成了：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">for</span>(ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record: consumer.poll(<span class="number">100</span>))</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"my-topic"</span>, record.key(), record.value());</span><br><span class="line">producer.<span class="built_in">flush</span>();  <span class="comment">//将缓冲区的消息立即发送</span></span><br><span class="line">consumer.commit(); <span class="comment">//消费者手动确认消费进度</span></span><br></pre></td></tr></table></figure><p><strong>partitionsFor</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取指定topic的partition元数据信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="built_in">List</span>&lt;PartitionInfo&gt; partitionsFor(<span class="built_in">String</span> topic);</span><br></pre></td></tr></table></figure><p><strong>close</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//关闭producer，方法将被阻塞，直到之前的发送请求已经完成</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;<span class="comment">//  equivalent to close(Long.MAX_VALUE, TimeUnit.MILLISECONDS)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout,TimeUnit timeUnit)</span></span>; <span class="comment">//同上，方法将等待timeout时长，以让未完成的请求完成发送</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产者和消费者吞吐量测试</title>
      <link href="/2018/09/02/kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%9E%E5%90%90%E9%87%8F%E6%B5%8B%E8%AF%95/"/>
      <url>/2018/09/02/kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%9E%E5%90%90%E9%87%8F%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="测试集群配置："><a href="#测试集群配置：" class="headerlink" title="测试集群配置："></a>测试集群配置：</h2><blockquote><p><strong>三台4核cpu/8G内存/50G硬盘的CentOS7机器</strong></p><p><strong>Kafka版本：Kafka_2.11-0.11.0.1</strong></p></blockquote><h2 id="kafka生产者吞吐量测试"><a href="#kafka生产者吞吐量测试" class="headerlink" title="kafka生产者吞吐量测试"></a>kafka生产者吞吐量测试</h2><p><strong>使用kafka-producer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic test --num-records 500000 --record-size 200 --throughput -1 --producer-props bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092 acks=1</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">500000</span> records sent ,<span class="number">180701.120347</span> records/sec (<span class="number">34.47</span> MB/sec),<span class="number">315.17</span> ms/avg latency ,<span class="number">982.00</span> ms max latency ,<span class="number">167</span>ms <span class="number">50</span>h ,<span class="number">902</span>ms <span class="number">95</span>th ,<span class="number">969</span> ms <span class="number">99</span>h, <span class="number">981</span>ms <span class="number">99.9</span>th</span><br></pre></td></tr></table></figure><blockquote><p>测试结果显示：</p><p>发送了500000条消息，kafka 的平均吞吐量是34.47 MB/sec ，即占用275Mb/s左右的带宽，平均每秒发送180701条消息。</p><p>平均延时为315 ms，最大延时为982 ms，</p><p>发送50%的消息需要167ms，发送95%的消息需要902ms，发送99%的消息需要969ms，发送99.9%的消息需要981ms。</p></blockquote><h2 id="kafka消费者吞吐量测试"><a href="#kafka消费者吞吐量测试" class="headerlink" title="kafka消费者吞吐量测试"></a>kafka消费者吞吐量测试</h2><p><strong>使用kafka-consumer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --message-size 200 --messages 500000 --topic test</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:41</span><span class="selector-pseudo">:388</span>, 2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:43</span><span class="selector-pseudo">:906</span>, 95<span class="selector-class">.3674</span>, 37<span class="selector-class">.8743</span>, 500000, 198570<span class="selector-class">.2939</span></span><br></pre></td></tr></table></figure><blockquote><p>看测试结果显示：</p><p>消费了95.3674MB消息，吞吐量为37.8743MB/s,也即303Mb/s，</p><p>消费了500000条消息，吞吐量为198570条/s</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产环境规划</title>
      <link href="/2018/09/01/kafka%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92/"/>
      <url>/2018/09/01/kafka%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h2 id="操作系统选型"><a href="#操作系统选型" class="headerlink" title="操作系统选型"></a>操作系统选型</h2><p>因为kafka服务端代码是Scala语言开发的，因此属于JVM系的大数据框架，目前部署最多的3类操作系统主要由Linux ，OS X 和Windows,但是部署在Linux数量最多，为什么呢？因为I/O模型的使用和数据网络传输效率两点。</p><ul><li>第一：Kafka新版本的Clients在设计底层网络库时采用了Java的Select模型，而在Linux实现机制是epoll,感兴趣的读者可以查询一下epoll和select的区别，明确一点就是：kafka跑在Linux上效率更高，因为epoll取消了轮询机制，换成了回调机制，当底层连接socket数较多时，可以避免CPU的时间浪费。</li><li>第二：网络传输效率上。kafka需要通过网络和磁盘进行数据传输，而大部分操作系统都是通过Java的FileChannel.transferTo方法实现，而Linux操作系统则会调用sendFile系统调用，也即零拷贝（Zero Copy 技术），避免了数据在内核地址空间和用户程序空间进行重复拷贝。</li></ul><h2 id="磁盘类型规划"><a href="#磁盘类型规划" class="headerlink" title="磁盘类型规划"></a>磁盘类型规划</h2><ul><li>机械磁盘（HDD）一般机械磁盘寻道时间是毫秒级的，若有大量随机I/O，则将会出现指数级的延迟，但是kafka是顺序读写的，因此对于机械磁盘的性能也是不弱的，所以，基于成本问题可以考虑。</li><li>固态硬盘（SSD）读写速度可观，没有成本问题可以考虑。</li><li>JBOD (Just Bunch Of Disks ) 经济实惠的方案，对数据安全级别不是非常非常高的情况下可以采用，建议用户在Broker服务器上设置多个日志路径，每个路径挂载在不同磁盘上，可以极大提升并发的日志写入速度。</li><li>RAID 磁盘阵列常见的RAID是RAID10，或者称为（RAID 1+0） 这种磁盘阵列结合了磁盘镜像和磁盘带化技术来保护数据，因为使用了磁盘镜像技术，使用率只有50%，注意，LinkedIn公司采用的就是RAID作为存储来提供服务的。那么弊端在什么地方呢？如果Kafka副本数量设置为3，那么实际上数据将存在6倍的冗余数据，利用率实在太低。因此，LinkedIn正在计划更改方案为JBOD.</li></ul><h2 id="磁盘容量规划"><a href="#磁盘容量规划" class="headerlink" title="磁盘容量规划"></a>磁盘容量规划</h2><p>假设每天大约能够产生一亿条消息，假设副本replica设置为2 （其实我们设置为3），数据留存时间为1周，平均每条上报事件消息为1K左右，那么每天产生的消息总量为：1亿 乘 2 乘  1K 除以 1000 除以 1000 =200G磁盘。预留10%的磁盘空间，为210G。一周大约为1.5T。采用压缩，平均压缩比为0.5，整体磁盘容量为0.75T。关联因素主要有：</p><ul><li>新增消息数</li><li>副本数</li><li>是否启用压缩</li><li>消息大小</li><li>消息保留时间</li></ul><h2 id="内存容量规划"><a href="#内存容量规划" class="headerlink" title="内存容量规划"></a>内存容量规划</h2><p>kafka对于内存的使用，并不过多依赖JVM 内存,而是更多的依赖操作系统的页缓存，consumer若命中页缓存，则不用消耗物理I/O操作。一般情况下，java堆内存的使用属于朝生夕灭的，很快会被GC,一般情况下，不会超过6G，对于16G内存的机器，文件系统page cache 可以达到10-14GB。</p><ul><li>怎么设计page cache，可以设置为单个日志段文件大小，若日志段为10G,那么页缓存应该至少设计为10G以上。</li><li>堆内存最好不要超过6G。</li></ul><h2 id="CPU选择规划"><a href="#CPU选择规划" class="headerlink" title="CPU选择规划"></a>CPU选择规划</h2><p>kafka不属于计算密集型系统，因此CPU核数够多就可以，而不必追求时钟频率，因此核数选择最好大于8。</p><h2 id="网络带宽决定Broker数量"><a href="#网络带宽决定Broker数量" class="headerlink" title="网络带宽决定Broker数量"></a>网络带宽决定Broker数量</h2><p>带宽主要有1Gb/s 和10 Gb/s 。我们可以称为千兆位网络和万兆位网络。举例如下：我们的系统一天每小时都要处理1Tb的数据，我们选择1Gb/b带宽，那么需要选择多少机器呢？</p><ul><li>假设网络带宽kafka专用，且分配给kafka服务器70%带宽，那么单台Borker带宽就是710Mb/s，但是万一出现突发流量问题，很容易把网卡打满，因此在降低1/3,也即240Mb/s。因为1小时处理1TTB数据，每秒需要处理292MB,1MB=8Mb，也就是2336Mb数据，那么一小时处理1TB数据至少需要2336/240=10台Broker数据。冗余设计，最终可以定为20台机器。</li></ul><h2 id="典型推荐"><a href="#典型推荐" class="headerlink" title="典型推荐"></a>典型推荐</h2><ul><li>cpu 核数 32</li><li>内存 32GB</li><li>磁盘 3TB 7200转 SAS盘三块</li><li>带宽 1Gb/s</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka0.11版本+Zookeeper-3.4.10集群部署</title>
      <link href="/2018/09/01/Kafka0.11%E7%89%88%E6%9C%AC+Zookeeper-3.4.10%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/09/01/Kafka0.11%E7%89%88%E6%9C%AC+Zookeeper-3.4.10%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.shields.io/badge/Author-Joker-green" alt></p><p><img src="https://img.shields.io/badge/Email-gaojintao999%40163.com-blue" alt></p><h2 id="集群环境："><a href="#集群环境：" class="headerlink" title="集群环境："></a>集群环境：</h2><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th></tr></thead><tbody><tr><td align="center">192.168.23.167</td><td align="center">kafka01</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.168</td><td align="center">kafka02</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.169</td><td align="center">kafka03</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr></tbody></table><p><strong>集群安装目录：/data/apps</strong></p><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><h3 id="root用户配置主机映射"><a href="#root用户配置主机映射" class="headerlink" title="root用户配置主机映射"></a>root用户配置主机映射</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.23.167  kafka01</span><br><span class="line">192.168.23.168  kafka02</span><br><span class="line">192.168.23.169  kafka03</span><br></pre></td></tr></table></figure><h3 id="在root用户下新建kafka用户（kafka-admin）"><a href="#在root用户下新建kafka用户（kafka-admin）" class="headerlink" title="在root用户下新建kafka用户（kafka/admin）"></a>在root用户下新建kafka用户（kafka/admin）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">adduser kafka -g kafka</span><br><span class="line">passwd kafka</span><br></pre></td></tr></table></figure><h3 id="在root用户下将apps目录下的用户及用户组均更改为kafka"><a href="#在root用户下将apps目录下的用户及用户组均更改为kafka" class="headerlink" title="在root用户下将apps目录下的用户及用户组均更改为kafka"></a>在root用户下将apps目录下的用户及用户组均更改为kafka</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R kafka:kafka /data/apps/</span><br></pre></td></tr></table></figure><h3 id="切换到kafka用户（之后的操作均在kafka用户下）"><a href="#切换到kafka用户（之后的操作均在kafka用户下）" class="headerlink" title="切换到kafka用户（之后的操作均在kafka用户下）"></a>切换到kafka用户（之后的操作均在kafka用户下）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su kafka</span><br></pre></td></tr></table></figure><h3 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id kafka@kafka01</span><br><span class="line">ssh-copy-id kafka@kafka02</span><br><span class="line">ssh-copy-id kafka@kafka03</span><br></pre></td></tr></table></figure><h3 id="将所有安装资源上传到kafka01节点的kafka用户的家目录下"><a href="#将所有安装资源上传到kafka01节点的kafka用户的家目录下" class="headerlink" title="将所有安装资源上传到kafka01节点的kafka用户的家目录下"></a>将所有安装资源上传到kafka01节点的kafka用户的家目录下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper-3.4.10.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka_2.11-0.11.0.1.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka-manager-1.3.3.22 (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> influxdb-1.7.5.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jmxtrans-270.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grafana-6.0.2-1.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-kafka.sh (文件内容见附录)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-zookeeper.sh (文件内容见附录)</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行zookeeper集群安装脚本"><a href="#在kafka01节点上执行zookeeper集群安装脚本" class="headerlink" title="在kafka01节点上执行zookeeper集群安装脚本"></a>在kafka01节点上执行zookeeper集群安装脚本</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sh</span> ~/install-zookeeper.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行kafka集群安装脚本"><a href="#在kafka01节点上执行kafka集群安装脚本" class="headerlink" title="在kafka01节点上执行kafka集群安装脚本"></a>在kafka01节点上执行kafka集群安装脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ~/install-kafka.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 内容如下：</span></span><br><span class="line">export ZOOKEEPER_HOME=/data/apps/zookeeper-3.4.10</span><br><span class="line">export KAFKA_HOME=/data/apps/kafka_0.11.0.1</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使环境变量生效</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动zookeeper集群"><a href="#在kafka01、02、03节点上执行命令启动zookeeper集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动zookeeper集群"></a>在kafka01、02、03节点上执行命令启动zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动kafka集群"><a href="#在kafka01、02、03节点上执行命令启动kafka集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动kafka集群"></a>在kafka01、02、03节点上执行命令启动kafka集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JMX_PORT=9999 kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure><h3 id="Zookeeper常用命令"><a href="#Zookeeper常用命令" class="headerlink" title="Zookeeper常用命令"></a>Zookeeper常用命令</h3><p>查看znode中的内容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ls</span>  /</span><br></pre></td></tr></table></figure><p>创建普通的节点 </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create</span></span><br></pre></td></tr></table></figure><p>获得节点的信息</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">get</span></span><br></pre></td></tr></table></figure><p>创建临时节点  </p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> -<span class="built_in">e</span></span><br></pre></td></tr></table></figure><p>编号节点： </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create -s</span></span><br></pre></td></tr></table></figure><p> 删除一个节点 </p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span></span><br></pre></td></tr></table></figure><p>递归删除节点</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rmr</span></span><br></pre></td></tr></table></figure><p>修改节点内容</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span></span><br></pre></td></tr></table></figure><p>监听节点  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">get</span> /test watch</span><br></pre></td></tr></table></figure><p>在其他节点进行修改  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> /<span class="built_in">test</span> 555</span><br></pre></td></tr></table></figure><p>监听节点上收到WatchedEvent state:SyncConnected type:NodeDataChanged path:/test</p><h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>关闭Kafka（关闭Kafka之前禁止关闭Zookeeper）</p><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">server</span>-<span class="keyword">stop</span>.sh</span><br></pre></td></tr></table></figure><p>创建Topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --replication-factor 3 --partitions 3 --topic test</span><br></pre></td></tr></table></figure><p>查看Topic列表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper kafka01:2181,kafka02:2181,kafka03:2181</span><br></pre></td></tr></table></figure><p>查看Topic详细信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><p>建立发布者console-producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --topic test</span><br></pre></td></tr></table></figure><p>建立订阅者console-consumer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafka01:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure><p>删除topic(需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><h2 id="使用kafka-manager管理kafka集群"><a href="#使用kafka-manager管理kafka集群" class="headerlink" title="使用kafka-manager管理kafka集群"></a>使用kafka-manager管理kafka集群</h2><p><strong>注：以下均在kafka用户下搭建，仅在kafka01节点上安装kafka-manager</strong></p><h3 id="将kafka-manager的安装包放到-data-apps目录下"><a href="#将kafka-manager的安装包放到-data-apps目录下" class="headerlink" title="将kafka-manager的安装包放到/data/apps目录下"></a>将kafka-manager的安装包放到/data/apps目录下</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv ~/kafka-manager<span class="number">-1.3</span><span class="number">.3</span><span class="number">.22</span> /data/apps</span><br></pre></td></tr></table></figure><h3 id="配置kafka-manager"><a href="#配置kafka-manager" class="headerlink" title="配置kafka-manager"></a>配置kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd conf</span><br><span class="line">vim application.conf</span><br><span class="line">修改kafka-manager.zkhosts="kafka01:2181,kafka02:2181,kafka03:2181"</span><br></pre></td></tr></table></figure><h3 id="启动kafka-manager"><a href="#启动kafka-manager" class="headerlink" title="启动kafka-manager"></a>启动kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</span><br></pre></td></tr></table></figure><h3 id="kafka-manager的WebUI"><a href="#kafka-manager的WebUI" class="headerlink" title="kafka-manager的WebUI"></a>kafka-manager的WebUI</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka01:8080</span><br></pre></td></tr></table></figure><h2 id="使用jmxtrans-influxdb-grafana监控JMX指标"><a href="#使用jmxtrans-influxdb-grafana监控JMX指标" class="headerlink" title="使用jmxtrans+influxdb+grafana监控JMX指标"></a>使用jmxtrans+influxdb+grafana监控JMX指标</h2><p><strong>注：以下均在root用户下搭建，除了jmxtrans，其他组件仅在kafka01节点上安装</strong></p><h3 id="开启Kafka-JMX端口"><a href="#开启Kafka-JMX端口" class="headerlink" title="开启Kafka JMX端口"></a>开启Kafka JMX端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd bin</span><br><span class="line">vim kafka-run-class.sh</span><br></pre></td></tr></table></figure><p><strong>第一行增加<code>JMX_PORT=9999</code>即可</strong></p><p><strong>修改好后重启kafka，查看Kafka以及JMX端口状态</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ps</span> -ef | <span class="keyword">grep</span> kafka</span><br><span class="line">netstat -anop | <span class="keyword">grep</span> <span class="number">9999</span></span><br></pre></td></tr></table></figure><h3 id="安装InfluxDB"><a href="#安装InfluxDB" class="headerlink" title="安装InfluxDB"></a>安装InfluxDB</h3><p><strong>下载InfluxDB rpm安装包</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http<span class="variable">s:</span>//<span class="keyword">dl</span>.influxdata.<span class="keyword">com</span>/influxdb/releases/influxdb-<span class="number">1.7</span>.<span class="number">5</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">influxdb-1</span><span class="selector-class">.7</span><span class="selector-class">.5</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动InfluxDB</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service <span class="literal">inf</span>luxdb <span class="literal">start</span>（systemctl <span class="literal">start</span> <span class="literal">inf</span>luxdb）</span><br></pre></td></tr></table></figure><p><strong>查看InfluxDB状态</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep influxdb</span><br><span class="line">service influxdb status（systemctl status influxdb）</span><br></pre></td></tr></table></figure><p><strong>使用InfluxDB客户端</strong></p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">influx</span></span><br></pre></td></tr></table></figure><p><strong>创建用户和数据库</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">"admin"</span> <span class="keyword">WITH</span> <span class="keyword">PASSWORD</span> <span class="string">'admin'</span> <span class="keyword">WITH</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="string">"jmxDB"</span>;</span><br></pre></td></tr></table></figure><p><strong>创建完成InfluxDB的用户和数据库暂时就够用了，其它简单操作如下，后面会用到</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建数据库</span></span><br><span class="line">create database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示所有的数据库</span></span><br><span class="line">show databases</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除数据库</span></span><br><span class="line">drop database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用数据库</span></span><br><span class="line">use db_name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示该数据库中所有的表</span></span><br><span class="line">show measurements</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建表，直接在插入数据的时候指定表名</span></span><br><span class="line">insert test,host=127.0.0.1,monitor_name=test count=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop measurement "measurement_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出</span></span><br><span class="line">quit</span><br></pre></td></tr></table></figure><h3 id="安装jmxtrans（所有kafka节点均安装）"><a href="#安装jmxtrans（所有kafka节点均安装）" class="headerlink" title="安装jmxtrans（所有kafka节点均安装）"></a>安装jmxtrans（所有kafka节点均安装）</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http:<span class="regexp">//</span>central.maven.org<span class="regexp">/maven2/</span>org<span class="regexp">/jmxtrans/</span>jmxtrans<span class="regexp">/270/</span>jmxtrans-<span class="number">270</span>.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">jmxtrans-270</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>jmxtrans相关路径</strong></p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jmxtrans安装目录：/usr/share/jmxtrans</span><br><span class="line">json文件默认目录：/var/<span class="class"><span class="keyword">lib</span>/<span class="title">jmxtrans</span>/</span></span><br><span class="line">日志路径：/var/log/jmxtrans/jmxtrans.log</span><br></pre></td></tr></table></figure><p><strong>配置json，jmxtrans的github上有一段示例配置</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"servers"</span> : [ &#123;</span><br><span class="line">    <span class="attr">"port"</span> : <span class="string">"9999"</span>,</span><br><span class="line">    <span class="attr">"host"</span> : <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="attr">"queries"</span> : [ &#123;</span><br><span class="line">      <span class="attr">"obj"</span> : <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line">      <span class="attr">"attr"</span> : [ <span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span> ],</span><br><span class="line">      <span class="attr">"resultAlias"</span>:<span class="string">"jvmMemory"</span>,</span><br><span class="line">      <span class="attr">"outputWriters"</span> : [ &#123;</span><br><span class="line">        <span class="attr">"@class"</span> : <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"http://127.0.0.1:8086/"</span>,</span><br><span class="line">        <span class="attr">"username"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"password"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"database"</span> : <span class="string">"jmxDB"</span>,</span><br><span class="line">        <span class="attr">"tags"</span>     : &#123;<span class="attr">"application"</span> : <span class="string">"kafka"</span>&#125;</span><br><span class="line">      &#125; ]</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>host：监控服务器</li><li>port：jmx端口</li><li>obj：对应jmx的ObjectName，就是我们要监控的指标</li><li>attr：对应ObjectName的属性，可以理解为我们要监控的指标的值</li><li>resultAlias：对应metric 的名称，在InfluxDB里面就是MEASUREMENTS名</li><li>tags：对应InfluxDB的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</li></ul><p><strong>附上两段配置的json示例文件（完整的均放在了三台节点的/var/lib/jmxtrans/目录下）</strong></p><p>base_10.164.204.248.json</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesRejectedPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchFollower"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MemoryUsage"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MemoryUsage"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=GarbageCollector,name=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"CollectionCount"</span>, <span class="string">"CollectionTime"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"GC"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"GC"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Threading"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"PeakThreadCount"</span>, <span class="string">"ThreadCount"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"Thread"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Thread"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaFetcherManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MaxLag"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=PartitionCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"PartitionCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"UnderReplicatedPartitions"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=LeaderCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LeaderCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=IsrShrinksPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"IsrShrinksPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>flink_sf_lx_248.json</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.log:type=Log,name=LogEndOffset,topic=FLINK_SF_LX,partition=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LogEndOffset"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>配置说明：</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">1、全局指标</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesOutPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesRejectedPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesRejectedPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒的消息写入总量</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"MessagesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchFollower的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchConsumer的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">每秒Produce的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line">内存使用的使用情况</span><br><span class="line">"obj" : "java.lang:type=Memory"</span><br><span class="line">"attr" : [ "HeapMemoryUsage", "NonHeapMemoryUsage" ]</span><br><span class="line">"resultAlias":"MemoryUsage"</span><br><span class="line">"tags" : &#123;"application" : "MemoryUsage"&#125;</span><br><span class="line"></span><br><span class="line">GC的耗时和次数</span><br><span class="line">"obj" : "java.lang:type=GarbageCollector,name=*"</span><br><span class="line">"attr" : [ "CollectionCount","CollectionTime" ]</span><br><span class="line">"resultAlias":"GC"</span><br><span class="line">"tags" : &#123;"application" : "GC"&#125;</span><br><span class="line"></span><br><span class="line">线程的使用情况</span><br><span class="line">"obj" : "java.lang:type=Threading"</span><br><span class="line">"attr" : [ "PeakThreadCount","ThreadCount" ]</span><br><span class="line">"resultAlias":"Thread"</span><br><span class="line">"tags" : &#123;"application" : "Thread"&#125;</span><br><span class="line"></span><br><span class="line">副本落后主分片的最大消息数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaFetcherManager"</span><br><span class="line">"tags" : &#123;"application" : "MaxLag"&#125;</span><br><span class="line"></span><br><span class="line">该broker上的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=PartitionCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "PartitionCount"&#125;</span><br><span class="line"></span><br><span class="line">正在做复制的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "UnderReplicatedPartitions"&#125;</span><br><span class="line"></span><br><span class="line">Leader的replica的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=LeaderCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "LeaderCount"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchConsumer耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchFollower耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">一个请求Produce耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2、topic的监控指标</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的写入流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的输出流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒写入消息的数量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=falcon_monitor_us"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us在每个分区最后的Offset</span><br><span class="line">"obj" : "kafka.log:type=Log,name=LogEndOffset,topic=falcon_monitor_us,partition=*"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "LogEndOffset"&#125;</span><br><span class="line"></span><br><span class="line">PS：</span><br><span class="line">1、参数说明</span><br><span class="line">"obj"对应jmx的ObjectName，就是我们要监控的指标</span><br><span class="line">"attr"对应ObjectName的属性，可以理解为我们要监控的指标的值</span><br><span class="line">"resultAlias"对应metric 的名称，在InfluxDb里面就是MEASUREMENTS名</span><br><span class="line">"tags" 对应InfluxDb的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</span><br><span class="line"></span><br><span class="line">2、对于全局监控，每一个监控指标对应一个MEASUREMENTS，所有的kafka节点同一个监控指标数据写同一个MEASUREMENTS ，对于topc监控的监控指标，同一个topic所有kafka节点写到同一个MEASUREMENTS，并且以topic名称命名</span><br></pre></td></tr></table></figure><p><strong>启动jmxtrans</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service jmxtrans <span class="literal">start</span>(systemctl <span class="literal">start</span> jmxtrans)</span><br></pre></td></tr></table></figure><p><strong>查看日志没有报错即为成功</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail /<span class="built_in">var</span>/<span class="keyword">log</span>/jmxtrans/jmxtrans.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><h3 id="安装Grafana"><a href="#安装Grafana" class="headerlink" title="安装Grafana"></a>安装Grafana</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://s3-us-west<span class="string">-2</span>.amazonaws.com/grafana-releases/release/grafana<span class="string">-6</span>.0.2<span class="string">-1</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包（如果缺少依赖，下载依赖）</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">grafana-6</span><span class="selector-class">.0</span><span class="selector-class">.2-1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install --downloadonly --downloaddir=./ fontconfig</span><br><span class="line"></span><br><span class="line">yum localinstall fontconfig-<span class="number">2.13</span>.<span class="number">0</span>-<span class="number">4.3</span><span class="selector-class">.el7</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br><span class="line"></span><br><span class="line">yum install --downloadonly --downloaddir=./ urw-fonts</span><br><span class="line"></span><br><span class="line">yum localinstall urw-fonts-<span class="number">2.4</span>-<span class="number">11</span><span class="selector-class">.el6</span><span class="selector-class">.noarch</span><span class="selector-class">.rpm</span> </span><br><span class="line"></span><br><span class="line">rpm -ivh grafana-<span class="number">6.0</span>.<span class="number">2</span>-<span class="number">1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动Grafana</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service grafana-server <span class="keyword">start</span>（systemctl <span class="keyword">start</span> grafana-<span class="keyword">server</span>）</span><br></pre></td></tr></table></figure><p><strong>打开浏览器</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//127.0.0.1:3000</span></span><br></pre></td></tr></table></figure><p><strong>先输入默认用户名密码admin/admin</strong></p><p><strong>点击Add data source，选择InfluxDB</strong></p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Url、Database、User、Password需要和jmxtrans采集数据配置文件里面的写一致，然后点击<span class="keyword">Save</span>&amp;<span class="keyword">Test</span>，提示成功就正常了</span><br></pre></td></tr></table></figure><p><strong>通过后点击Back返回</strong></p><p><strong>左侧 + 可以创建或引入仪表盘，创建一个dashboard，然后在这里配置每一个监控指标的图</strong></p><p><strong>主要配置项说明：</strong></p><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>DataSource</td><td>选择Grafana已配置的数据源</td></tr><tr><td>FROM-Default</td><td>默认Schema，保持不变即可</td></tr><tr><td>FROM-measurement</td><td>对应的InfluxDB的表名</td></tr><tr><td>WHERE</td><td>WHERE条件，根据自己需求选择</td></tr><tr><td>SELECT-Field</td><td>对应选的字段，可根据需求增减</td></tr><tr><td>SELECT-mean()</td><td>选择的字段对应的InfluxDB的函数</td></tr><tr><td>GroupBY-time()</td><td>根据时间分组</td></tr><tr><td>GROUPBY-fill()</td><td>当不存在数据时，以null为默认值填充</td></tr></tbody></table><p><strong>要点说明：</strong></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、对于监控指标为Count的监控项，需要通过Grafana做计算得到我们想要的监控，比如BytesInPerSec这个指标，它的监控值是一个累计值，我们想要取到每秒的流量，肯定需要计算，(本次采集的值-上次采集的值)/<span class="number">60</span> ,jmxtrans是一分钟采集一次数据，具体配置参考下面截图：</span><br><span class="line"></span><br><span class="line">因为我们是一分钟采集一次数据，所以group <span class="keyword">by</span> 和derivative选<span class="number">1</span>分钟；因为我们要每秒的流量，所以math这里除以<span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、X轴的单位选择，比如流量的单位、时间的单位、每秒消息的个数无单位等等，下面分布举一个例子介绍说明</span><br><span class="line"></span><br><span class="line">设置流量的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》data（Metric）--》bytes</span></span><br><span class="line"></span><br><span class="line">设置时间的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》time--》milliseconds(ms)</span></span><br><span class="line"></span><br><span class="line">设置按原始值展示，无单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》none--》none</span></span><br></pre></td></tr></table></figure><h2 id="附录（kafka配置说明）"><a href="#附录（kafka配置说明）" class="headerlink" title="附录（kafka配置说明）"></a>附录（kafka配置说明）</h2><h3 id="server-properties"><a href="#server-properties" class="headerlink" title="server.properties"></a>server.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.server.KafkaConfig <span class="keyword">for</span> additional details and defaults</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Server Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的ID，必须与其它节点不同</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Switch to <span class="built_in">enable</span> topic deletion or not, default value is <span class="literal">false</span></span></span><br><span class="line">delete.topic.enable=false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> java.net.InetAddress.getCanonicalHostName() <span class="keyword">if</span> not configured.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器监听的地址。如果没有配置，就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   FORMAT:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   EXAMPLE:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line">listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Hostname and port the broker will advertise to producers and consumers. If not <span class="built_in">set</span>, </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> it uses the value <span class="keyword">for</span> <span class="string">"listeners"</span> <span class="keyword">if</span> configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的主机名会通知给生产者和消费者。如果没有设置，如果配置了<span class="string">"listeners"</span>就使用<span class="string">"listeners"</span>的值。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line">advertised.listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> Maps listener names to security protocols, the default is <span class="keyword">for</span> them to be the same. See the config documentation <span class="keyword">for</span> more details</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将侦听器的名称映射到安全协议，默认情况下它们是相同的。有关详细信息，请参阅配置文档</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来接受请求或者发送响应的线程数</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> processing requests, <span class="built_in">which</span> may include disk I/O</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来处理请求的线程数，可能包括磁盘IO</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的发送缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的接收缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单个请求最大能接收的数据量</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A comma seperated list of directories under <span class="built_in">which</span> to store <span class="built_in">log</span> files</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个逗号分隔的目录列表，用来存储日志文件</span></span><br><span class="line">log.dirs=/data/apps/kafkaapp/logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default number of <span class="built_in">log</span> partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism <span class="keyword">for</span> consumption, but this will also result <span class="keyword">in</span> more files across</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the brokers.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个主题的日志分区的默认数量。更多的分区允许更大的并行操作，但是它会导致节点产生更多的文件</span></span><br><span class="line">num.partitions=6</span><br><span class="line">default.replication.factor=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads per data directory to be used <span class="keyword">for</span> <span class="built_in">log</span> recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This value is recommended to be increased <span class="keyword">for</span> installations with data <span class="built_in">dirs</span> located <span class="keyword">in</span> RAID array.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个数据目录中的线程数，用于在启动时日志恢复，并在关闭时刷新。</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Internal Topic Settings  #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The replication factor <span class="keyword">for</span> the group metadata internal topics <span class="string">"__consumer_offsets"</span> and <span class="string">"__transaction_state"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For anything other than development testing, a value greater than 1 is recommended <span class="keyword">for</span> to ensure availability such as 3.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于除了开发测试之外的其他任何东西，group元数据内部主题的复制因子“__consumer_offsets”和“__transaction_state”，建议值大于1，以确保可用性(如3)。</span></span><br><span class="line">offsets.topic.replication.factor=3</span><br><span class="line">transaction.state.log.replication.factor=3</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Flush Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Messages are immediately written to the filesystem but by default we only fsync() to sync</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the OS cache lazily. The following configurations control the flush of data to disk.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息直接被写入文件系统，但是默认情况下我们仅仅调用fsync()以延迟的同步系统缓存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There are a few important trade-offs here:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这些有一些重要的权衡</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    1. Durability: Unflushed data may be lost <span class="keyword">if</span> you are not using replication.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. 持久性:如果不使用复制，未刷新的数据可能会丢失。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 延迟:非常大的刷新间隔可能会在刷新时导致延迟，因为将会有大量数据刷新。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 吞吐量:刷新通常是最昂贵的操作，而一个小的刷新间隔可能会导致过多的搜索。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The settings below allow one to configure the flush policy to flush data after a period of time or</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> every N messages (or both). This can be <span class="keyword">done</span> globally and overridden on a per-topic basis.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的设置允许你去配置刷新策略，每隔一段时间刷新或者一次N个消息（或者两个都配置）。这可以在全局范围内完成，并在每个主题的基础上重写。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of messages to accept before forcing a flush of data to disk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.messages=10000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum amount of time a message can sit <span class="keyword">in</span> a <span class="built_in">log</span> before we force a flush</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.ms=1000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Retention Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configurations control the disposal of <span class="built_in">log</span> segments. The policy can</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> be <span class="built_in">set</span> to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下的配置控制了日志段的处理。策略可以配置为每隔一段时间删除片段或者到达一定大小之后。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> from the end of the <span class="built_in">log</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当满足这些条件时，将会删除一个片段。删除总是发生在日志的末尾。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The minimum age of a <span class="built_in">log</span> file to be eligible <span class="keyword">for</span> deletion due to age</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个日志的最小存活时间，可以被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A size-based retention policy <span class="keyword">for</span> logs. Segments are pruned from the <span class="built_in">log</span> as long as the remaining</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> segments don<span class="string">'t drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.retention.bytes=1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a <span class="built_in">log</span> segment file. When this size is reached a new <span class="built_in">log</span> segment will be created.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每一个日志段大小的最大值。当到达这个大小时，会生成一个新的片段。</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The interval at <span class="built_in">which</span> <span class="built_in">log</span> segments are checked to see <span class="keyword">if</span> they can be deleted according</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to the retention policies</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查日志段的时间间隔，看是否可以根据保留策略删除它们</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string (see zookeeper docs <span class="keyword">for</span> details).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper连接字符串（具体见Zookeeper文档）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这是一个以逗号为分割的部分，每一个都匹配一个Zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 您还可以将一个可选的chroot字符串附加到url，以指定所有kafka znode的根目录。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> root directory <span class="keyword">for</span> all kafka znodes.</span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接到Zookeeper的超时时间</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Group Coordinator Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configuration specifies the time, <span class="keyword">in</span> milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default value <span class="keyword">for</span> this is 3 seconds.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> We override this to 0 here as it makes <span class="keyword">for</span> a better out-of-the-box experience <span class="keyword">for</span> development and testing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> However, <span class="keyword">in</span> production environments the default value of 3 seconds is more suitable as this will <span class="built_in">help</span> to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br></pre></td></tr></table></figure><h3 id="producer-properties"><a href="#producer-properties" class="headerlink" title="producer.properties"></a>producer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.producer.ProducerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Producer Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> list of brokers used <span class="keyword">for</span> bootstrapping knowledge about the rest of the cluster</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> format: host1:port1,host2:port2 ...</span></span><br><span class="line">bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> specify the compression codec <span class="keyword">for</span> all data generated: none, gzip, snappy, lz4</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。</span></span><br><span class="line">compression.type=none</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> name of the partitioner class <span class="keyword">for</span> partitioning events; default partition spreads data randomly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区</span></span><br><span class="line">partitioner.class=kafka.producer.DefaultPartitioner</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum amount of time the client will <span class="built_in">wait</span> <span class="keyword">for</span> the response of a request</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功)</span></span><br><span class="line">request.timeout.ms=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> how long `KafkaProducer.send` and `KafkaProducer.partitionsFor` will block <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.block.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the producer will <span class="built_in">wait</span> <span class="keyword">for</span> up to the given delay to allow other records to be sent so that the sends can be batched together</span></span><br><span class="line"><span class="meta">#</span><span class="bash">linger.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum size of a request <span class="keyword">in</span> bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.request.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the default batch size <span class="keyword">in</span> bytes when batching multiple records sent to a partition</span></span><br><span class="line"><span class="meta">#</span><span class="bash">batch.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the total bytes of memory the producer can use to buffer records waiting to be sent to the server</span></span><br><span class="line"><span class="meta">#</span><span class="bash">buffer.memory=</span></span><br></pre></td></tr></table></figure><h3 id="consumer-properties"><a href="#consumer-properties" class="headerlink" title="consumer.properties"></a>consumer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.consumer.ConsumerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span></span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer group id</span></span><br><span class="line">group.id=test-consumer-group</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer timeout</span></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer.timeout.ms=500</span></span><br></pre></td></tr></table></figure><h3 id="install-kafka-sh"><a href="#install-kafka-sh" class="headerlink" title="install-kafka.sh"></a>install-kafka.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置KAFKA的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入kafka的安装目录,不存在脚本自动创建,最后一个/不要写,如 /data/apps"</span><br><span class="line">read kafkainstallpath</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建kafka安装的目录</span></span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">   mkdir -p $kafkainstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">  echo "创建目录$kafkainstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'kafka.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   # 当前目录没有kafka的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现kafka*.tar.gz,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   # 解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'kafka.*[gz]$') -C $kafkainstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka版本全称</span></span><br><span class="line">kafkaversion=`ls $kafkainstallpath| grep 'kafka_2.*'`</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka配置文件存储路径</span></span><br><span class="line">confpath=$kafkainstallpath/$kafkaversion/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改配置文件</span></span><br><span class="line">echo -e "请输入当前kafka节点的broker.id：唯一值 例如 0"</span><br><span class="line">read brokerid</span><br><span class="line">sed -i "s/^broker.id=0/broker.id=$&#123;brokerid&#125;/g" $confpath/server.properties</span><br><span class="line"></span><br><span class="line">echo -e "请输入当前kafka节点的hostname: 例如kafka01"</span><br><span class="line">read hostname</span><br><span class="line">sed -i "s/^#listeners=PLAINTEXT:\/\/:9092/listeners=PLAINTEXT:\/\/$hostname:9092/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e "请输入kafka消息存储目录：例如 /data/apps/kafkaapp/log"</span><br><span class="line">read kafkalogspath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建KAFKA日志存储目录</span></span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">   mkdir -p $kafkalogspath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">  echo "创建目录$kafkalogspath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='log.dirs=/tmp/kafka-logs'</span><br><span class="line">new_dir='log.dirs='$kafkalogspath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e '请输入zookeeper集群的所有节点：（严格按照示例格式） 例如kafka01:2181,kafka02:2181,kafka03:2181'</span><br><span class="line">read allhosts</span><br><span class="line">sed -i "s/^zookeeper.connect=localhost:2181/zookeeper.connect=$allhosts/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭删除topic的权限</span></span><br><span class="line">sed -i 's/^#delete.topic.enable=true/delete.topic.enable=false/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置topic的默认分区数量为6</span></span><br><span class="line">sed -i 's/^num.partitions=1/num.partitions=6/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line">sed -i 's/^#log.retention.bytes=1073741824/log.retention.bytes=1073741824/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line">sed -i 's/^#log.flush.interval.messages=10000/log.flush.interval.messages=10000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line">sed -i 's/^#log.flush.interval.ms=1000/log.flush.interval.ms=1000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置偏移量topic的复制因子为3</span></span><br><span class="line">sed -i 's/^offsets.topic.replication.factor=1/offsets.topic.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置事务topic的复制因子为3</span></span><br><span class="line">sed -i 's/^transaction.state.log.replication.factor=1/transaction.state.log.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认副本因子为3</span></span><br><span class="line">echo ""&gt;&gt;$confpath/server.properties</span><br><span class="line">echo "default.replication.factor=3" &gt;&gt;$confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"log.cleanup.policy=delete"</span> &gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka参数优化</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">'s/^log.retention.hours=16/log.retention.hours=72/g'</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> param=`cat /proc/cpuinfo | grep <span class="string">"cpu cores"</span>| uniq`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_count=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_count=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+1))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_count&#125;</span>!<span class="variable">$&#123;new_count&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_io=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_io=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+<span class="variable">$&#123;param:0-1:1&#125;</span>))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_io&#125;</span>!<span class="variable">$&#123;new_io&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#KAFKA <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$KAFKA_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"> </span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">kafkapath=$kafkainstallpath/$kafkaversion</span><br><span class="line">kafkapathtemp=$kafkainstallpath/$kafkaversion-temp</span><br><span class="line">cp -r $kafkapath $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$&#123;hostname&#125;),严格符合以下格式hostname:brokerid,空格隔开， 如kafka02:1 kafka03:2'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> kafkahostname=$&#123;array3[0]&#125;</span><br><span class="line"> kafkabrokerid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $kafkahostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $kafkahostname "rm -rf $kafkapath $kafkalogspath"</span><br><span class="line"> ssh $kafkahostname "mkdir -p $kafkapath $kafkalogspath"</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改broker.id</span></span><br><span class="line"> old_brokerid="broker.id=$brokerid"</span><br><span class="line"> new_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"><span class="meta"> #</span><span class="bash">修改listeners</span></span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$&#123;hostname&#125;:9092"</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$&#123;kafkahostname&#125;:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> scp -r $kafkapathtemp/* $&#123;user&#125;@$kafkahostname:$kafkapath/</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo '#KAFKA <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo 'export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">'echo "export PATH=\$PATH:\$KAFKA_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">再次修改回来 防止修改错误</span></span><br><span class="line"> new_brokerid="broker.id=$brokerid"</span><br><span class="line"> old_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$hostname:9092"</span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$kafkahostname:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> echo ======= $kafkahostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="install-zookeeper-sh"><a href="#install-zookeeper-sh" class="headerlink" title="install-zookeeper.sh"></a>install-zookeeper.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置zk的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入zk的安装目录,不存在脚本自动创建,最后一个/不要写 如/data/apps"</span><br><span class="line">read zkinstallpath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk安装的目录</span></span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">   mkdir -p $zkinstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">  echo "创建目录$zkinstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit:</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'zookeeper-.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   #当前目录没有zk的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现zookeeper的gz压缩包,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   #解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'zookeeper-.*[gz]$') -C $zkinstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">zkversion=`ls $zkinstallpath| grep 'zookeeper-.*'`</span><br><span class="line"></span><br><span class="line">confpath=$zkinstallpath/$zkversion/conf</span><br><span class="line"></span><br><span class="line">cp $confpath/zoo_sample.cfg  $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line">echo -e "请输入zk数据存储目录：例如 /data/apps/zookeeperapp"</span><br><span class="line">read zkdatapath</span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk数据的目录</span></span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">   mkdir -p $zkdatapath</span><br><span class="line">fi</span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">  echo "创建目录$zkdatapath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='dataDir=/tmp/zookeeper'</span><br><span class="line">new_dir='dataDir='$zkdatapath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo  "请输入所有的zk集群节点：（按照空格分割） 例如 zk01 zk02 zk03"</span><br><span class="line">read zkNodes</span><br><span class="line">array=(`echo $zkNodes | tr ' ' ' '` )</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line">echo ""&gt;&gt;$confpath/zoo.cfg</span><br><span class="line">for i in `seq 0 $(($&#123;#array[@]&#125;-1))`</span><br><span class="line">do</span><br><span class="line"> echo "server.$(($&#123;i&#125;+1))=$&#123;array[$&#123;i&#125;]&#125;:2888:3888" &gt;&gt;$confpath/zoo.cfg</span><br><span class="line">done </span><br><span class="line"></span><br><span class="line">echo  "请输入zk的myid,不能重复,唯一值 例如 1" </span><br><span class="line">read myid</span><br><span class="line">echo $myid &gt; $zkdatapath/myid</span><br><span class="line"></span><br><span class="line">binpath=$zkinstallpath/$zkversion/bin</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG_DIR=\".\"/ZOO_LOG_DIR=\"$&#123;ZOOKEEPER_PREFIX&#125;\/logs\"/g' $binpath/zkEnv.sh</span><br><span class="line"></span><br><span class="line">echo "ZOO_LOG_DIR修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG4J_PROP=\"INFO,CONSOLE\"/ZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\"/g' $binpath/zkEnv.sh</span><br><span class="line">echo "ZOO_LOG4J_PROP修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.out\"/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.log\"/g' $binpath/zkServer.sh</span><br><span class="line">echo "_ZOO_DAEMON_OUT修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/zookeeper.root.logger=INFO, CONSOLE/zookeeper.root.logger=INFO, ROLLINGFILE/g' $confpath/log4j.properties</span><br><span class="line">echo "zookeeper.root.logger修改成功"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#zookeeper <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$ZK_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"></span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">zkpath=$zkinstallpath/$zkversion</span><br><span class="line">zkpathtemp=$zkinstallpath/$zkversion-temp</span><br><span class="line">cp -r $zkpath $zkpathtemp</span><br><span class="line"></span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">currentnode=`hostname`</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$currentnode),严格符合以下格式hostname:zkID,空格隔开， 如zk02:2 zk03:3 zk04:4 zk05:5 zk06:6'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> hostname=$&#123;array3[0]&#125;</span><br><span class="line"> zkid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $hostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $hostname "mkdir -p $zkpath"</span><br><span class="line"> ssh $hostname "mkdir -p $zkdatapath"</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">修改zk的myid唯一值</span></span><br><span class="line"> ssh $hostname "echo $zkid &gt; $zkdatapath/myid"</span><br><span class="line"></span><br><span class="line"> scp -r $zkpathtemp/* $&#123;user&#125;@$hostname:$zkpath/</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo '#zk <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo 'export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">'echo "export PATH=\$PATH:\$ZK_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"></span><br><span class="line"> echo ======= $hostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $zkpathtemp</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JsonSchema全套解决方案</title>
      <link href="/2018/08/17/JsonSchema%E5%85%A8%E5%A5%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2018/08/17/JsonSchema%E5%85%A8%E5%A5%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Json-Schema说明"><a href="#Json-Schema说明" class="headerlink" title="Json Schema说明"></a>Json Schema说明</h1><ol><li>json schema 本身也是一个json串；</li><li>每个schema可以描述一个json实例，并且该json实例里每一个节点都可以用一个schema来描述，因此schema与json一样，本身也是一个层级结构，一个schema中可能嵌套着另外若干层schema；</li><li>json schema 定义的检查规则以数据格式验证为主（字段存在性、字段类型），并可以支持一些简单的数据正确性验证（例如数值范围、字符串的模式等），但不能进行复杂的逻辑校验（例如进价必须小于售价等）；</li></ol><h1 id="Json-Schema-格式"><a href="#Json-Schema-格式" class="headerlink" title="Json Schema 格式"></a>Json Schema 格式</h1><p>Json schema 本身遵循Json规范，本身就是一个Json字符串，先来看一个例子</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来看一下json schema 最外层包含以下几个字段</p><table><thead><tr><th>$schema</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>$schema</td><td>$schema 关键字状态，表示这个模式与 v4 规范草案书写一致。</td><td></td></tr><tr><td>title</td><td>标题，用来描述结构</td><td></td></tr><tr><td>description</td><td>描述</td><td></td></tr><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr></tbody></table><p>上面只是一个简单的例子，从上面可以看出Json schema 本身是一个JSON字符串，由通过key-value的形式进行标示。<br>type 和 properties 用来定义json 属性的类型。required 是对Object字段的必段性进行约束。事实上,json Schema定义了json所支持的类型，每种类型都有0-N种约束方式。下一节我们来，细致介绍一下。</p><hr><h1 id="Json-Schema-类型"><a href="#Json-Schema-类型" class="headerlink" title="Json Schema 类型"></a>Json Schema 类型</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>object类型有三个关键字:type（限定类型）,properties(定义object的各个字段),required（限定必需字段）,如下：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr><tr><td>maxProperties</td><td>最大属性个数</td><td></td></tr><tr><td>minProperties</td><td>最小属性个数</td><td></td></tr><tr><td>additionalProperties</td><td>true or false or object</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html" target="_blank" rel="noopener">参考</a></td></tr></tbody></table><p>properties 定义每个属性的名字和类型，方式如上例。</p><h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">     &#125;,</span><br><span class="line">     <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">     <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>array有三个单独的属性:items,minItems,uniqueItems:</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>items</td><td>array 每个元素的类型</td><td>.</td></tr><tr><td>minItems</td><td>约束属性，数组最小的元素个数</td><td></td></tr><tr><td>maxItems</td><td>约束属性，数组最大的元素个数</td><td></td></tr><tr><td>uniqueItems</td><td>约束属性，每个元素都不相同</td><td></td></tr><tr><td>additionalProperties</td><td>约束items的类型，不建议使用</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/array.html" target="_blank" rel="noopener">示例</a></td></tr><tr><td>Dependencies</td><td>属性依赖</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=additionalproperties" target="_blank" rel="noopener">用法</a></td></tr><tr><td>patternProperties</td><td></td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=patternproperties" target="_blank" rel="noopener">用法</a></td></tr></tbody></table><h2 id="string"><a href="#string" class="headerlink" title="string"></a>string</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"ip"</span>: &#123;</span><br><span class="line">            <span class="attr">"mail"</span>: <span class="string">"string"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"w+([-+.]w+)*@w+([-.]w+)*.w+([-.]w+)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"host"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"phoneNumber"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"((d&#123;3,4&#125;)|d&#123;3,4&#125;-)?d&#123;7,8&#125;(-d&#123;3&#125;)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"ip"</span>, <span class="string">"host"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>maxLength</td><td>定义字符串的最大长度，&gt;=0</td><td>.</td></tr><tr><td>minLength</td><td>定义字符串的最小长度，&gt;=0</td><td></td></tr><tr><td>pattern</td><td>用正则表达式约束字符串</td><td></td></tr></tbody></table><h2 id="integer"><a href="#integer" class="headerlink" title="integer"></a>integer</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr><tr><td>multipleOf</td><td>是某数的倍数，必须大于0的整数</td><td></td></tr></tbody></table><h2 id="number"><a href="#number" class="headerlink" title="number"></a>number</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>number 关键字可以描述任意长度，任意小数点的数字。number类型的约束有以下几个：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr></tbody></table><h2 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"boolean"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>true or false</p><h2 id="enum"><a href="#enum" class="headerlink" title="enum"></a>enum</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以这么做</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]                   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="null"><a href="#null" class="headerlink" title="null"></a>null</h2><h1 id="Json-Schema进阶"><a href="#Json-Schema进阶" class="headerlink" title="Json Schema进阶"></a>Json Schema进阶</h1><p>了解了上面的各个类型的定义及约定条件，就可以满足大部分情况了。但为了写出更好的json schema,我们再学习几个关键字</p><h2 id="ref"><a href="#ref" class="headerlink" title="$ref"></a>$ref</h2><p>$ref 用来引用其它schema,<br>示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product set"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">        <span class="attr">"properties"</span>: &#123;</span><br><span class="line">            <span class="attr">"id"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"name"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"price"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">                <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"tags"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">                <span class="attr">"items"</span>: &#123;</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">                <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"dimensions"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">                <span class="attr">"properties"</span>: &#123;</span><br><span class="line">                    <span class="attr">"length"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"width"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"height"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"required"</span>: [<span class="string">"length"</span>, <span class="string">"width"</span>, <span class="string">"height"</span>]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"warehouseLocation"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"Coordinates of the warehouse with the product"</span>,</span><br><span class="line">                <span class="attr">"$ref"</span>: <span class="string">"http://json-schema.org/geo"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="definitions"><a href="#definitions" class="headerlink" title="definitions"></a>definitions</h2><p>当一个schema写的很大的时候，可能需要创建内部结构体，再使用$ref进行引用，示列如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/positiveInteger"</span> &#125;,</span><br><span class="line">    <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">        <span class="attr">"positiveInteger"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="allOf"><a href="#allOf" class="headerlink" title="allOf"></a>allOf</h2><p>意思是展示全部属性，建议用requires替代</p><p>不建议使用，示例如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">    <span class="attr">"address"</span>: &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">      <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"street_address"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"city"</span>:           &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"state"</span>:          &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"required"</span>: [<span class="string">"street_address"</span>, <span class="string">"city"</span>, <span class="string">"state"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"allOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/address"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: &#123; <span class="attr">"enum"</span>: [ <span class="string">"residential"</span>, <span class="string">"business"</span> ] &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="anyOf"><a href="#anyOf" class="headerlink" title="anyOf"></a>anyOf</h2><p>意思是展示任意属性，建议用requires替代和minProperties替代，示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"anyOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="oneOf"><a href="#oneOf" class="headerlink" title="oneOf"></a>oneOf</h2><p>其中之一</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"oneOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">5</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">3</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="not"><a href="#not" class="headerlink" title="not"></a>not</h2><p>非 * 类型<br>示例</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">"not"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125; &#125;</span><br></pre></td></tr></table></figure><h1 id="Java-Json-Schema库"><a href="#Java-Json-Schema库" class="headerlink" title="Java Json Schema库"></a>Java Json Schema库</h1><p>表中给出了两种java中使用的JSON Schema库</p><table><thead><tr><th>库名称</th><th>地址</th><th>支持草案</th></tr></thead><tbody><tr><td>fge</td><td><a href="https://github.com/daveclayton/json-schema-validator" target="_blank" rel="noopener">https://github.com/daveclayton/json-schema-validator</a></td><td>draft-04 draft-03</td></tr><tr><td>everit</td><td><a href="https://github.com/everit-org/json-schema" target="_blank" rel="noopener">https://github.com/everit-org/json-schema</a></td><td>draft-04</td></tr></tbody></table><p>建议：</p><ol><li><p>如果在项目中使用了jackson json，那么使用fge是一个好的选择，因为fge就是使用的jackson json。</p></li><li><p>如果项目中使用的是org.json API，那么使用everit会更好。</p></li><li><p>如果是使用以上两个库以外的库，那么就使用everit，因为everit会比fge的性能好上两倍。</p></li></ol><h2 id="fge的使用："><a href="#fge的使用：" class="headerlink" title="fge的使用："></a>fge的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.github.fge&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;json-schema-validator&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.2.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  JsonNode schema = readJsonFile(<span class="string">"src/main/resources/Schema.json"</span>);</span><br><span class="line">  JsonNode data = readJsonFile(<span class="string">"src/main/resources/failure.json"</span>);</span><br><span class="line">  ProcessingReport report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  Assert.assertTrue(report.isSuccess());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> JsonNode <span class="title">readJsonFile</span><span class="params">(String filePath)</span> </span>&#123;</span><br><span class="line">  JsonNode instance = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">instance = <span class="keyword">new</span> JsonNodeReader().fromReader(<span class="keyword">new</span> FileReader(filePath));</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> instance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真正的调用只有一行代码，需要传入验证规则和数据。分别有validate和validateUnchecked两种方法，区别在于validateUnchecked方法不会抛出ProcessingException异常。</p><p>还可以从字符串中读取json，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  String failure = <span class="keyword">new</span> String(<span class="string">"&#123;\"foo\":1234&#125;"</span>);</span><br><span class="line">  String Schema = <span class="string">"&#123;\"type\": \"object\", \"properties\" : &#123;\"foo\" : &#123;\"type\" : \"string\"&#125;&#125;&#125;"</span>;</span><br><span class="line">  ProcessingReport report = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">JsonNode data = JsonLoader.fromString(failure);</span><br><span class="line">JsonNode schema = JsonLoader.fromString(Schema);</span><br><span class="line">report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//Assert.assertTrue(report.isSuccess());</span></span><br><span class="line">  Iterator&lt;ProcessingMessage&gt; it = report.iterator();</span><br><span class="line">  <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">System.out.println(it.next());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中ProcessingReport对象中维护了一共迭代器，如果执行失败（执行成功时没有信息），其提供了一些高级故障信息。每个错误可能包含以下属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">level: 错误级别（应该就是error）</span><br><span class="line">schema：引起故障的模式的所在位置的 URI</span><br><span class="line">instance：错误对象</span><br><span class="line">domain：验证域</span><br><span class="line">keyword：引起错误的约束key</span><br><span class="line">found：现在类型</span><br><span class="line">expected：期望类型</span><br></pre></td></tr></table></figure><p>以上代码的json信息为：</p><p>failure.json ：  </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"foo"</span> : <span class="number">1234</span>&#125;</span><br></pre></td></tr></table></figure><p>Schema.json ：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="string">"properties"</span> : &#123;</span><br><span class="line">    <span class="string">"foo"</span> : &#123;</span><br><span class="line">      <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行错误信息为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">error: <span class="function">instance <span class="title">type</span> <span class="params">(integer)</span> does not match any allowed primitive <span class="title">type</span> <span class="params">(allowed: [<span class="string">"string"</span>])</span></span></span><br><span class="line"><span class="function">level: "error"</span></span><br><span class="line"><span class="function">schema: </span>&#123;<span class="string">"loadingURI"</span>:<span class="string">"#"</span>,<span class="string">"pointer"</span>:<span class="string">"/properties/foo"</span>&#125;</span><br><span class="line">instance: &#123;<span class="string">"pointer"</span>:<span class="string">"/foo"</span>&#125;</span><br><span class="line">domain: <span class="string">"validation"</span></span><br><span class="line">keyword: <span class="string">"type"</span></span><br><span class="line">found: <span class="string">"integer"</span></span><br><span class="line">expected: [<span class="string">"string"</span>]</span><br></pre></td></tr></table></figure><h2 id="everit的使用："><a href="#everit的使用：" class="headerlink" title="everit的使用："></a>everit的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.everit.json&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;org.everit.json.schema&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  InputStream inputStream = getClass().getResourceAsStream(<span class="string">"/Schema.json"</span>);</span><br><span class="line">  JSONObject Schema = <span class="keyword">new</span> JSONObject(<span class="keyword">new</span> JSONTokener(inputStream));</span><br><span class="line">  JSONObject data = <span class="keyword">new</span> JSONObject(<span class="string">"&#123;\"foo\" : 1234&#125;"</span>);</span><br><span class="line">  Schema schema = SchemaLoader.load(Schema);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">schema.validate(data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ValidationException e) &#123;</span><br><span class="line">System.out.println(e.getMessage());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果验证失败会抛出一个ValidationException异常，然后在catch块中打印出错误信息。everit中的错误信息想比fge来说比较简单，相同的json测试文件，打印的信息如下：</p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#/foo: expected <span class="keyword">type</span>: <span class="built_in">String</span>, found: <span class="built_in">Integer</span></span><br></pre></td></tr></table></figure><p>此外everit提供了一个format关键字，可以自定义validator来校验json中一些复杂数据，比如IP地址，电话号码等。具体请参考官方文档。</p><h2 id="性能测试："><a href="#性能测试：" class="headerlink" title="性能测试："></a>性能测试：</h2><p>1、一共执行1000次，成功和失败分开执行，每种情况执行250次。然后记录下每次的执行时间，执行10次，取平均值。</p><p>fge每1000次的执行时间(ms)：1158, 1122, 1120, 1042, 1180, 1254, 1198，1126，1177，1192<br>everit每1000次的执行时间(ms)：33, 49, 54, 57, 51, 47, 48, 52, 53, 44</p><p>2、一共执行10000次，成功和失败分开执行，每种情况执行2500次。</p><table><thead><tr><th>方法/场景</th><th>每次执行时间(ms)</th></tr></thead><tbody><tr><td>fge/场景1</td><td>1.1569</td></tr><tr><td>fge/场景2</td><td>0.3407</td></tr><tr><td>everit/场景1</td><td>0.0488</td></tr><tr><td>everit/场景2</td><td>0.0206</td></tr></tbody></table><p><strong>使用对比：</strong></p><p>​    从性能上来说everit完全是碾压fge，官方说的至少两倍，实际测试过程中，差不多有20倍的差距。虽然fge使用的是jackson json，相对来说学习成本可能较低，但是使用下来发现everit的使用也并不复杂，需要注意的是包需要导入正确（org.json）。fge唯一的优势在于错误信息比较详细。还有一点区别在于，everit验证失败是抛出异常，而fge是判断返回一个boolean类型的值。</p>]]></content>
      
      
      <categories>
          
          <category> 数据存储格式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Json </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：Session Window</title>
      <link href="/2018/05/21/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9ASession%20Window/"/>
      <url>/2018/05/21/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9ASession%20Window/</url>
      
        <content type="html"><![CDATA[<p>在上一篇文章《Window机制》中，我们介绍了窗口的概念和底层实现，以及 Flink 一些内建的窗口，包括滑动窗口、翻滚窗口。本文将深入讲解一种较为特殊的窗口：会话窗口（session window）。建议您在阅读完上一篇文章的基础上再阅读本文。</p><p>当我们需要分析用户的一段交互的行为事件时，通常的想法是将用户的事件流按照“session”来分组。session 是指一段持续活跃的期间，由活跃间隙分隔开。通俗一点说，消息之间的间隔小于超时阈值（sessionGap）的，则被分配到同一个窗口，间隔大于阈值的，则被分配到不同的窗口。目前开源领域大部分的流计算引擎都有窗口的概念，但是没有对 session window 的支持，要实现 session window，需要用户自己去做完大部分事情。而当 Flink 1.1.0 版本正式发布时，Flink 将会是开源流计算领域第一个内建支持 session window 的引擎。</p><p>在 Flink 1.1.0 之前，Flink 也可以通过自定义的window assigner和trigger来实现一个基本能用的session window。<code>release-1.0</code> 版本中提供了一个实现 session window 的 example：<a href="https://github.com/apache/flink/blob/release-1.0/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/SessionWindowing.java" target="_blank" rel="noopener">SessionWindowing</a>。这个session window范例的实现原理是，基于GlobleWindow这个window assigner，将所有元素都分配到同一个窗口中，然后指定一个自定义的trigger来触发执行窗口。这个trigger的触发机制是，对于每个到达的元素都会根据其时间戳（timestamp）注册一个会话超时的定时器（timestamp+sessionTimeout），并移除上一次注册的定时器。最新一个元素到达后，如果超过 sessionTimeout 的时间还没有新元素到达，那么trigger就会触发，当前窗口就会是一个session window。处理完窗口后，窗口中的数据会清空，用来缓存下一个session window的数据。</p><p>但是这种session window的实现是非常弱的，无法应用到实际生产环境中的。因为它无法处理乱序 event time 的消息。 而在即将到来的 Flink 1.1.0 版本中，Flink 提供了对 session window 的直接支持，用户可以通过<code>SessionWindows.withGap()</code>来轻松地定义 session widnow，而且能够处理乱序消息。Flink 对 session window 的支持主要借鉴自 Google 的 DataFlow 。</p><h2 id="Session-Window-in-Flink"><a href="#Session-Window-in-Flink" class="headerlink" title="Session Window in Flink"></a>Session Window in Flink</h2><p>假设有这么个场景，用户点开手机淘宝后会进行一系列的操作（点击、浏览、搜索、购买、切换tab等），这些操作以及对应发生的时间都会发送到服务器上进行用户行为分析。那么用户的操作行为流的样例可能会长下面这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" alt="img"></a></p><p>通过上图，我们可以很直观地观察到，用户的行为是一段一段的，每一段内的行为都是连续紧凑的，段内行为的关联度要远大于段之间行为的关联度。我们把每一段用户行为称之为“session”，段之间的空档我们称之为“session gap”。所以，理所当然地，我们应该按照 session window 对用户的行为流进行切分，并计算每个session的结果。如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" alt="img"></a></p><p>为了定义上述的窗口切分规则，我们可以使用 Flink 提供的 <code>SessionWindows</code> 这个 widnow assigner API。如果你用过 <code>SlidingEventTimeWindows</code>、<code>TumlingProcessingTimeWindows</code>等，你会对这个很熟悉。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStream <span class="built_in">input</span> = …</span><br><span class="line">DataStream result = <span class="built_in">input</span></span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .window(SessionWindows.withGap(Time.seconds(&lt;seconds&gt;))</span><br><span class="line">  .apply(&lt;window <span class="function"><span class="keyword">function</span>&gt;)</span> // <span class="keyword">or</span> reduce() <span class="keyword">or</span> fold()</span><br></pre></td></tr></table></figure><p>这样，Flink 就会基于元素的时间戳，自动地将元素放到不同的session window中。如果两个元素的时间戳间隔小于 session gap，则会在同一个session中。如果两个元素之间的间隔大于session gap，且没有元素能够填补上这个gap，那么它们会被放到不同的session中。</p><h2 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h2><p>为了实现 session window，我们需要扩展 Flink 中的窗口机制，使得能够支持窗口合并。要理解其原因，我们需要先了解窗口的现状。在上一篇文章中，我们谈到了 Flink 中 WindowAssigner 负责将元素分配到哪个/哪些窗口中去，Trigger 决定了一个窗口何时能够被计算或清除。当元素被分配到窗口之后，这些窗口是固定的不会改变的，而且窗口之间不会相互作用。</p><p>对于session window来说，我们需要窗口变得更灵活。基本的思想是这样的：<code>SessionWindows</code> assigner 会为每个进入的元素分配一个窗口，该窗口以元素的时间戳作为起始点，时间戳加会话超时时间为结束点，也就是该窗口为<code>[timestamp, timestamp+sessionGap)</code>。比如我们现在到了两个元素，它们被分配到两个独立的窗口中，两个窗口目前不相交，如图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" alt="img"></a></p><p>当第三个元素进入时，分配到的窗口与现有的两个窗口发生了叠加，情况变成了这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" alt="img"></a></p><p>由于我们支持了窗口的合并，<code>WindowAssigner</code>可以合并这些窗口。它会遍历现有的窗口，并告诉系统哪些窗口需要合并成新的窗口。Flink 会将这些窗口进行合并，合并的主要内容有两部分：</p><ol><li>需要合并的窗口的底层状态的合并（也就是窗口中缓存的数据，或者对于聚合窗口来说是一个聚合值）</li><li>需要合并的窗口的Trigger的合并（比如对于EventTime来说，会删除旧窗口注册的定时器，并注册新窗口的定时器）</li></ol><p>总之，结果是三个元素现在在同一个窗口中了：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" alt="img"></a></p><p>需要注意的是，对于每一个新进入的元素，都会分配一个属于该元素的窗口，都会检查并合并现有的窗口。在触发窗口计算之前，每一次都会检查该窗口是否可以和其他窗口合并，直到trigger触发后，会将该窗口从窗口列表中移除。对于 event time 来说，窗口的触发是要等到大于窗口结束时间的 watermark 到达，当watermark没有到，窗口会一直缓存着。所以基于这种机制，可以做到对乱序消息的支持。</p><p>这里有一个优化点可以做，因为每一个新进入的元素都会创建属于该元素的窗口，然后合并。如果新元素连续不断地进来，并且新元素的窗口一直都是可以和之前的窗口重叠合并的，那么其实这里多了很多不必要的创建窗口、合并窗口的操作，我们可以直接将新元素放到那个已存在的窗口，然后扩展该窗口的大小，看起来就像和新元素的窗口合并了一样。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p><a href="https://issues.apache.org/jira/browse/FLINK-3174" target="_blank" rel="noopener">FLINK-3174</a> 这个JIRA中有对 Flink 如何支持 session window 的详细说明，以及代码更新。建议可以结合该 <a href="https://github.com/apache/flink/pull/1460" target="_blank" rel="noopener">PR</a>的代码来理解本文讨论的实现原理。</p><p>为了扩展 Flink 中的窗口机制，使得能够支持窗口合并，首先 window assigner 要能合并现有的窗口，Flink 增加了一个新的抽象类 <code>MergingWindowAssigner</code> 继承自 <code>WindowAssigner</code>，这里面主要多了一个 <code>mergeWindows</code> 的方法，用来决定哪些窗口是可以合并的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MergingWindowAssigner&lt;T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span> <span class="keyword">extends</span> <span class="title">WindowAssigner&lt;T</span>, <span class="title">W&gt;</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 决定哪些窗口需要被合并。对于每组需要合并的窗口, 都会调用 callback.merge(toBeMerged, mergeResult)</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param windows 现存的窗口集合 The window candidates.</span></span><br><span class="line"><span class="comment">   * @param callback 需要被合并的窗口会回调 callback.merge 方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  public <span class="keyword">abstract</span> void mergeWindows(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; windows, <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; callback);</span><br><span class="line"></span><br><span class="line">  public interface <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用来声明合并窗口的具体动作（合并窗口底层状态、合并窗口trigger等）。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @param toBeMerged  需要被合并的窗口列表</span></span><br><span class="line"><span class="comment">     * @param mergeResult 合并后的窗口</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    void merge(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; toBeMerged, <span class="type">W</span> mergeResult);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有已经存在的 assigner 都继承自 <code>WindowAssigner</code>，只有新加入的 session window assigner 继承自 <code>MergingWindowAssigner</code>，如：<code>ProcessingTimeSessionWindows</code>和<code>EventTimeSessionWindows</code>。</p><p>另外，Trigger 也需要能支持对合并窗口后的响应，所以 Trigger 添加了一个新的接口 <code>onMerge(W window, OnMergeContext ctx)</code>，用来响应发生窗口合并之后对trigger的相关动作，比如根据合并后的窗口注册新的 event time 定时器。</p><p>OK，接下来我们看下最核心的代码，也就是对于每个进入的元素的处理，代码位于<code>WindowOperator.processElement</code>方法中，如下所示：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">public void processElement(StreamRecord<span class="variable">&lt;IN&gt;</span> element) throws Exception &#123;</span><br><span class="line">  Collection<span class="variable">&lt;W&gt;</span> elementWindows = windowAssigner.assignWindows(element.getValue(), element.getTimestamp());</span><br><span class="line">  final K key = (K) getStateBackend().getCurrentKey();</span><br><span class="line">  if (windowAssigner instanceof MergingWindowAssigner) &#123;</span><br><span class="line">    // 对于session window 的特殊处理，我们只关注该条件块内的代码</span><br><span class="line">    MergingWindowSet<span class="variable">&lt;W&gt;</span> mergingWindows = getMergingWindowSet();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line">      final Tuple1<span class="variable">&lt;TriggerResult&gt;</span> mergeTriggerResult = new Tuple1<span class="variable">&lt;&gt;</span>(TriggerResult.CONTINUE);</span><br><span class="line">      </span><br><span class="line">      // 加入新窗口, 如果没有合并发生,那么actualWindow就是新加入的窗口</span><br><span class="line">      // 如果有合并发生, 那么返回的actualWindow即为合并后的窗口,</span><br><span class="line">      // 并且会调用 MergeFunction.merge 方法, 这里方法中的内容主要是更新trigger, 合并旧窗口中的状态到新窗口中</span><br><span class="line">      W actualWindow = mergingWindows.addWindow(window, new MergingWindowSet.MergeFunction<span class="variable">&lt;W&gt;</span>() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void merge(W mergeResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedWindows, W <span class="keyword">state</span>WindowResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedStateWindows) throws Exception &#123;</span><br><span class="line">          context.key = key;</span><br><span class="line">          context.window = mergeResult;</span><br><span class="line"></span><br><span class="line">          // 这里面会根据新窗口的结束时间注册新的定时器</span><br><span class="line">          mergeTriggerResult.f0 = context.<span class="keyword">on</span>Merge(mergedWindows);</span><br><span class="line"></span><br><span class="line">          // 删除旧窗口注册的定时器</span><br><span class="line">          <span class="keyword">for</span> (W m: mergedWindows) &#123;</span><br><span class="line">            context.window = m;</span><br><span class="line">            context.clear();</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          // 合并旧窗口(mergedStateWindows)中的状态到新窗口（<span class="keyword">state</span>WindowResult）中</span><br><span class="line">          getStateBackend().mergePartitionedStates(<span class="keyword">state</span>WindowResult,</span><br><span class="line">              mergedStateWindows,</span><br><span class="line">              windowSerializer,</span><br><span class="line">              (StateDescriptor<span class="variable">&lt;? extends MergingState&lt;?,?&gt;</span>, ?&gt;) windowStateDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      // 取 actualWindow 对应的用来存状态的窗口</span><br><span class="line">      W <span class="keyword">state</span>Window = mergingWindows.getStateWindow(actualWindow);</span><br><span class="line">      // 从状态后端拿出对应的状态 </span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(<span class="keyword">state</span>Window, windowSerializer, windowStateDescriptor);</span><br><span class="line">      // 将新进入的元素数据加入到新窗口（或者说合并后的窗口）中对应的状态中</span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = actualWindow;</span><br><span class="line"></span><br><span class="line">      // 检查是否需要fire or purge </span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      TriggerResult combinedTriggerResult = TriggerResult.merge(triggerResult, mergeTriggerResult.f0);</span><br><span class="line"></span><br><span class="line">      // 根据trigger结果决定怎么处理窗口中的数据</span><br><span class="line">      processTriggerResult(combinedTriggerResult, actualWindow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 对于普通window assigner的处理， 这里我们不关注</span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(window, windowSerializer,</span><br><span class="line">          windowStateDescriptor);</span><br><span class="line"></span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = window;</span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      processTriggerResult(triggerResult, window);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实这段代码写的并不是很clean，并且不是很好理解。在第六行中有用到<code>MergingWindowSet</code>，这个类很重要所以我们先介绍它。这是一个用来跟踪窗口合并的类。比如我们有A、B、C三个窗口需要合并，合并后的窗口为D窗口。这三个窗口在底层都有对应的状态集合，为了避免代价高昂的状态替换（创建新状态是很昂贵的），我们保持其中一个窗口作为原始的状态窗口，其他几个窗口的数据合并到该状态窗口中去，比如随机选择A作为状态窗口，那么B和C窗口中的数据需要合并到A窗口中去。这样就没有新状态产生了，但是我们需要额外维护窗口与状态窗口之间的映射关系（D-&gt;A），这就是<code>MergingWindowSet</code>负责的工作。这个映射关系需要在失败重启后能够恢复，所以<code>MergingWindowSet</code>内部也是对该映射关系做了容错。状态合并的工作示意图如下所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" alt="img"></a></p><p>然后我们来解释下processElement的代码，首先根据window assigner为新进入的元素分配窗口集合。接着进入第一个条件块，取出当前的<code>MergingWindowSet</code>。对于每个分配到的窗口，我们就会将其加入到<code>MergingWindowSet</code>中（<code>addWindow</code>方法），由<code>MergingWindowSet</code>维护窗口与状态窗口之间的关系，并在需要窗口合并的时候，合并状态和trigger。然后根据映射关系，取出结果窗口对应的状态窗口，根据状态窗口取出对应的状态。将新进入的元素数据加入到该状态中。最后，根据trigger结果来对窗口数据进行处理，对于session window来说，这里都是不进行任何处理的。真正对窗口处理是由定时器超时后对完成的窗口调用<code>processTriggerResult</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文在上一篇文章《Window机制》的基础上，深入讲解了 Flink 是如何支持 session window 的，核心的原理是窗口的合并。Flink 对于 session window 的支持很大程度上受到了 Google DataFlow 的启发，所以也建议阅读下 DataFlow 的论文。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：Window 机制</title>
      <link href="/2018/05/20/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9AWindow%20%E6%9C%BA%E5%88%B6/"/>
      <url>/2018/05/20/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9AWindow%20%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口（window）就是从 Streaming 到 Batch 的一个桥梁。Flink 提供了非常完善的窗口机制，这是我认为的 Flink 最大的亮点之一（其他的亮点包括消息乱序处理，和 checkpoint 机制）。本文我们将介绍流式处理中的窗口概念，介绍 Flink 内建的一些窗口和 Window API，最后讨论下窗口在底层是如何实现的。</p><h2 id="什么是-Window"><a href="#什么是-Window" class="headerlink" title="什么是 Window"></a>什么是 Window</h2><p>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。</p><p>窗口可以是时间驱动的（Time Window，例如：每30秒钟），也可以是数据驱动的（Count Window，例如：每一百个元素）。一种经典的窗口分类可以分成：翻滚窗口（Tumbling Window，无重叠），滚动窗口（Sliding Window，有重叠），和会话窗口（Session Window，活动间隙）。</p><p>我们举个具体的场景来形象地理解不同窗口的概念。假设，淘宝网会记录每个用户每次购买的商品个数，我们要做的是统计不同窗口中用户购买商品的总数。下图给出了几种经典的窗口切分概述图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1bwsTJVXXXXaBaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1bwsTJVXXXXaBaXXXXXXXXXXX" alt="img"></a></p><p>上图中，raw data stream 代表用户的购买行为流，圈中的数字代表该用户本次购买的商品个数，事件是按时间分布的，所以可以看出事件之间是有time gap的。Flink 提供了上图中所有的窗口类型，下面我们会逐一进行介绍。</p><h3 id="Time-Window"><a href="#Time-Window" class="headerlink" title="Time Window"></a>Time Window</h3><p>就如名字所说的，Time Window 是根据时间对数据流进行分组的。这里我们涉及到了流处理中的时间问题，时间问题和消息乱序问题是紧密关联的，这是流处理中现存的难题之一，我们将在后续的 <a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/#" target="_blank" rel="noopener">EventTime 和消息乱序处理</a>中对这部分问题进行深入探讨。这里我们只需要知道 Flink 提出了三种时间的概念，分别是event time（事件时间：事件发生时的时间），ingestion time（摄取时间：事件进入流处理系统的时间），processing time（处理时间：消息被计算处理的时间）。Flink 中窗口机制和时间类型是完全解耦的，也就是说当需要改变时间类型时不需要更改窗口逻辑相关的代码。</p><ul><li><p><strong>Tumbling Time Window</strong><br>如上图，我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnt)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by userId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  <span class="comment">// tumbling time window of 1 minute length</span></span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">1</span>))</span><br><span class="line">  <span class="comment">// compute sum over buyCnt</span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Sliding Time Window</strong><br>但是对于某些应用，它们需要的窗口是不间断的，需要平滑地进行窗口聚合。比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val slidingCn<span class="symbol">ts:</span> DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  // sliding <span class="built_in">time</span> window of <span class="number">1</span> <span class="built_in">minute</span> length <span class="built_in">and</span> <span class="number">30</span> secs trigger interval</span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>))</span><br><span class="line">  .<span class="built_in">sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Count-Window"><a href="#Count-Window" class="headerlink" title="Count Window"></a>Count Window</h3><p>Count Window 是根据元素个数对数据流进行分组的。</p><ul><li><p><strong>Tumbling Count Window</strong><br>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window），上图所示窗口大小为3个。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by sensorId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// tumbling count window of 100 elements size</span></span><br><span class="line">  .countWindow(<span class="number">100</span>)</span><br><span class="line">  <span class="comment">// compute the buyCnt sum </span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Sliding Count Window</strong><br>当然Count Window 也支持 Sliding Window，虽在上图中未描述出来，但和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和，代码示例如下。</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val slidingCnts: DataStream[(<span class="keyword">Int</span>, <span class="keyword">Int</span>)] = vehicleCnts</span><br><span class="line"><span class="meta">  .keyBy</span>(<span class="number">0</span>)</span><br><span class="line">  // sliding count window of <span class="number">100</span> elements size <span class="keyword">and</span> <span class="number">10</span> elements trigger interval</span><br><span class="line"><span class="meta">  .countWindow</span>(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">  .sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Session-Window"><a href="#Session-Window" class="headerlink" title="Session Window"></a>Session Window</h3><p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）。Session Window 的示例代码如下：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line">  </span><br><span class="line"><span class="keyword">val</span> sessionCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = vehicleCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// session window based on a 30 seconds session gap interval </span></span><br><span class="line">  .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">30</span>)))</span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>一般而言，window 是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink 的 DataStream API 提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义窗口分配逻辑。下面我们会对 Flink 窗口相关的 API 进行剖析。</p><h2 id="剖析-Window-API"><a href="#剖析-Window-API" class="headerlink" title="剖析 Window API"></a>剖析 Window API</h2><p>得益于 Flink Window API 松耦合设计，我们可以非常灵活地定义符合特定业务的窗口。Flink 中定义一个窗口主要需要以下三个组件。</p><ul><li><p><strong>Window Assigner：</strong>用来决定某个元素被分配到哪个/哪些窗口中去。</p><p>如下类图展示了目前内置实现的 Window Assigners：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1plkxJVXXXXXqXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1plkxJVXXXXXqXpXXXXXXXXXX" alt="img"></a></p></li><li><p><strong>Trigger：</strong>触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。</p><p>如下类图展示了目前内置实现的 Triggers：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15yMeJVXXXXbbXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15yMeJVXXXXbbXFXXXXXXXXXX" alt="img"></a></p></li><li><p><strong>Evictor：</strong>可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。</p><p>如下类图展示了目前内置实现的 Evictors：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1OCT6JVXXXXcjXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1OCT6JVXXXXcjXVXXXXXXXXXX" alt="img"></a></p></li></ul><p>上述三个组件的不同实现的不同组合，可以定义出非常复杂的窗口。Flink 中内置的窗口也都是基于这三个组件构成的，当然内置窗口有时候无法解决用户特殊的需求，所以 Flink 也暴露了这些窗口机制的内部接口供用户实现自定义的窗口。下面我们将基于这三者探讨窗口的实现机制。</p><h2 id="Window-的实现"><a href="#Window-的实现" class="headerlink" title="Window 的实现"></a>Window 的实现</h2><p>下图描述了 Flink 的窗口机制以及各组件之间是如何相互工作的。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1swNgKXXXXXc4XpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1swNgKXXXXXc4XpXXXXXXXXXX" alt="img"></a></p><p>首先上图中的组件都位于一个算子（window operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给 WindowAssigner。WindowAssigner 会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，<code>Window</code>本身只是一个ID标识符，其内部可能存储了一些元数据，如<code>TimeWindow</code>中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在 Key/Value State 中，key为<code>Window</code>，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了 Flink 的 State 机制（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/state.html" target="_blank" rel="noopener">state 文档</a>）。</p><p>每一个窗口都拥有一个属于自己的 Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者 fire + purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被 purge 了。在purge之前，窗口会一直占用着内存。</p><p>当Trigger fire了，窗口中的元素集合就会交给<code>Evictor</code>（如果指定了的话）。Evictor 主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有 Evictor 的话，窗口中的所有元素会一起交给函数进行计算。</p><p>计算函数收到了窗口的元素（可能经过了 Evictor 的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的<code>sum()</code>,<code>min()</code>,<code>max()</code>，还有 <code>ReduceFunction</code>，<code>FoldFunction</code>，还有<code>WindowFunction</code>。WindowFunction 是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p><p>Flink 对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了 Evictor，则不会启用对聚合窗口的优化，因为 Evictor 需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>上述的三个组件构成了 Flink 的窗口机制。为了更清楚地描述窗口机制，以及解开一些疑惑（比如 purge 和 Evictor 的区别和用途），我们将一步步地解释 Flink 内置的一些窗口（Time Window，Count Window，Session Window）是如何实现的。</p><h3 id="Count-Window-实现"><a href="#Count-Window-实现" class="headerlink" title="Count Window 实现"></a>Count Window 实现</h3><p>Count Window 是使用三组件的典范，我们可以在 <code>KeyedStream</code> 上创建 Count Window，其源码如下所示：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">tumbling</span><span class="built_in"> count</span> <span class="keyword">window</span></span><br><span class="line">public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">window</span>(GlobalWindows.create())  // create <span class="keyword">window</span> stream using GlobalWindows</span><br><span class="line">      .trigger(PurgingTrigger.<span class="keyword">of</span>(CountTrigger.<span class="keyword">of</span>(size))); // trigger <span class="literal">is</span> <span class="keyword">window</span> size</span><br><span class="line">&#125;</span><br><span class="line">// <span class="keyword">sliding</span><span class="built_in"> count</span> <span class="keyword">window</span></span><br><span class="line">public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">window</span>(GlobalWindows.create())</span><br><span class="line">    .evictor(CountEvictor.<span class="keyword">of</span>(size))  // evictor <span class="literal">is</span> <span class="keyword">window</span> size</span><br><span class="line">    .trigger(CountTrigger.<span class="keyword">of</span>(slide)); // trigger <span class="literal">is</span> slide size</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个函数是申请翻滚计数窗口，参数为窗口大小。第二个函数是申请滑动计数窗口，参数分别为窗口大小和滑动大小。它们都是基于 <code>GlobalWindows</code> 这个 WindowAssigner 来创建的窗口，该assigner会将所有元素都分配到同一个global window中，所有<code>GlobalWindows</code>的返回值一直是 <code>GlobalWindow</code> 单例。基本上自定义的窗口都会基于该assigner实现。</p><p>翻滚计数窗口并不带evictor，只注册了一个trigger。该trigger是带purge功能的 CountTrigger。也就是说每当窗口中的元素数量达到了 window-size，trigger就会返回fire+purge，窗口就会执行计算并清空窗口中的所有元素，再接着储备新的元素。从而实现了tumbling的窗口之间无重叠。</p><p>滑动计数窗口的各窗口之间是有重叠的，但我们用的 GlobalWindows assinger 从始至终只有一个窗口，不像 sliding time assigner 可以同时存在多个窗口。所以trigger结果不能带purge，也就是说计算完窗口后窗口中的数据要保留下来（供下个滑窗使用）。另外，trigger的间隔是slide-size，evictor的保留的元素个数是window-size。也就是说，每个滑动间隔就触发一次窗口计算，并保留下最新进入窗口的window-size个元素，剔除旧元素。</p><p>假设有一个滑动计数窗口，每2个元素计算一次最近4个元素的总和，那么窗口工作示意图如下所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15vcUJVXXXXcGXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15vcUJVXXXXcGXVXXXXXXXXXX" alt="img"></a></p><p>图中所示的各个窗口逻辑上是不同的窗口，但在物理上是同一个窗口。该滑动计数窗口，trigger的触发条件是元素个数达到2个（每进入2个元素就会触发一次），evictor保留的元素个数是4个，每次计算完窗口总和后会保留剩余的元素。所以第一次触发trigger是当元素5进入，第三次触发trigger是当元素2进入，并驱逐5和2，计算剩余的4个元素的总和（22）并发送出去，保留下2,4,9,7元素供下个逻辑窗口使用。</p><h3 id="Time-Window-实现"><a href="#Time-Window-实现" class="headerlink" title="Time Window 实现"></a>Time Window 实现</h3><p>同样的，我们也可以在 <code>KeyedStream</code> 上申请 Time Window，其源码如下所示：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tumbling time window</span></span><br><span class="line">public WindowedStream&lt;T, <span class="built_in">KEY</span>, TimeWindow&gt; <span class="built_in">timeWindow</span>(<span class="built_in">Time</span> size) &#123;</span><br><span class="line">  <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(TumblingProcessingTimeWindows.<span class="built_in">of</span>(size));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(TumblingEventTimeWindows.<span class="built_in">of</span>(size));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// sliding time window</span></span><br><span class="line">public WindowedStream&lt;T, <span class="built_in">KEY</span>, TimeWindow&gt; <span class="built_in">timeWindow</span>(<span class="built_in">Time</span> size, <span class="built_in">Time</span> slide) &#123;</span><br><span class="line">  <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(SlidingProcessingTimeWindows.<span class="built_in">of</span>(size, slide));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(SlidingEventTimeWindows.<span class="built_in">of</span>(size, slide));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在方法体内部会根据当前环境注册的时间类型，使用不同的WindowAssigner创建window。可以看到，EventTime和IngestTime都使用了<code>XXXEventTimeWindows</code>这个assigner，因为EventTime和IngestTime在底层的实现上只是在Source处为Record打时间戳的实现不同，在window operator中的处理逻辑是一样的。</p><p>这里我们主要分析sliding process time window，如下是相关源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SlidingProcessingTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> slide;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">SlidingProcessingTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.size = size;</span><br><span class="line">    <span class="keyword">this</span>.slide = slide;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    timestamp = System.currentTimeMillis();</span><br><span class="line">    List&lt;TimeWindow&gt; windows = <span class="keyword">new</span> ArrayList&lt;&gt;((<span class="keyword">int</span>) (size / slide));</span><br><span class="line">    <span class="comment">// 对齐时间戳</span></span><br><span class="line">    <span class="keyword">long</span> lastStart = timestamp - timestamp % slide;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> start = lastStart;</span><br><span class="line">      start &gt; timestamp - size;</span><br><span class="line">      start -= slide) &#123;</span><br><span class="line">      <span class="comment">// 当前时间戳对应了多个window</span></span><br><span class="line">      windows.add(<span class="keyword">new</span> TimeWindow(start, start + size));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> windows;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessingTimeTrigger</span> <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="comment">// 每个元素进入窗口都会调用该方法</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 注册定时器，当系统时间到达window end timestamp时会回调该trigger的onProcessingTime方法</span></span><br><span class="line">    ctx.registerProcessingTimeTimer(window.getEnd());</span><br><span class="line">    <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="comment">// 返回结果表示执行窗口计算并清空窗口</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, TimeWindow window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> TriggerResult.FIRE_AND_PURGE;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先，<code>SlidingProcessingTimeWindows</code>会对每个进入窗口的元素根据系统时间分配到<code>(size / slide)</code>个不同的窗口，并会在每个窗口上根据窗口结束时间注册一个定时器（相同窗口只会注册一份），当定时器超时时意味着该窗口完成了，这时会回调对应窗口的Trigger的<code>onProcessingTime</code>方法，返回FIRE_AND_PURGE，也就是会执行窗口计算并清空窗口。整个过程示意图如下：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1iihcKXXXXXavXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1iihcKXXXXXavXFXXXXXXXXXX" alt="img"></a></p><p>如上图所示横轴代表时间戳（为简化问题，时间戳从0开始），第一条record会被分配到[-5,5)和[0,10)两个窗口中，当系统时间到5时，就会计算[-5,5)窗口中的数据，并将结果发送出去，最后清空窗口中的数据，释放该窗口资源。</p><h3 id="Session-Window-实现"><a href="#Session-Window-实现" class="headerlink" title="Session Window 实现"></a>Session Window 实现</h3><p>Session Window 是一个需求很强烈的窗口机制，但Session也比之前的Window更复杂，所以 Flink 也是在即将到来的 1.1.0 版本中才支持了该功能。由于篇幅问题，我们将在后续的Session Window 的实现中深入探讨 Session Window 的实现。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：生成 JobGraph</title>
      <link href="/2018/05/17/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20JobGraph/"/>
      <url>/2018/05/17/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20JobGraph/</url>
      
        <content type="html"><![CDATA[<p>继前文<a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="noopener">Flink 原理与实现：架构和拓扑概览</a>中介绍了Flink的四层执行图模型，本文将主要介绍 Flink 是如何将 StreamGraph 转换成 JobGraph 的。根据用户用Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。以 WordCount 为例，转换图如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1DzYXJFXXXXXJXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1DzYXJFXXXXXJXVXXXXXXXXXX" alt="img"></a></p><p>StreamGraph 和 JobGraph 都是在 Client 端生成的，也就是说我们可以在 IDE 中通过断点调试观察 StreamGraph 和 JobGraph 的生成过程。</p><p>JobGraph 的相关数据结构主要在 <code>org.apache.flink.runtime.jobgraph</code> 包中。构造 JobGraph 的代码主要集中在 <code>StreamingJobGraphGenerator</code> 类中，入口函数是 <code>StreamingJobGraphGenerator.createJobGraph()</code>。我们首先来看下<code>StreamingJobGraphGenerator</code>的核心源码：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">StreamingJobGraphGenerator</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> StreamGraph streamGraph;</span><br><span class="line">  <span class="keyword">private</span> JobGraph jobGraph;</span><br><span class="line">  <span class="comment">// id -&gt; JobVertex</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, JobVertex&gt; jobVertices;</span><br><span class="line">  <span class="comment">// 已经构建的JobVertex的id集合</span></span><br><span class="line">  <span class="keyword">private</span> Collection&lt;Integer&gt; builtVertices;</span><br><span class="line">  <span class="comment">// 物理边集合（排除了chain内部的边）, 按创建顺序排序</span></span><br><span class="line">  <span class="keyword">private</span> List&lt;StreamEdge&gt; physicalEdgesInOrder;</span><br><span class="line">  <span class="comment">// 保存chain信息，部署时用来构建 OperatorChain，startNodeId -&gt; (currentNodeId -&gt; StreamConfig)</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, Map&lt;Integer, StreamConfig&gt;&gt; chainedConfigs;</span><br><span class="line">  <span class="comment">// 所有节点的配置信息，id -&gt; StreamConfig</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, StreamConfig&gt; vertexConfigs;</span><br><span class="line">  <span class="comment">// 保存每个节点的名字，id -&gt; chainedName</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, String&gt; chainedNames;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 构造函数，入参只有 StreamGraph</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">StreamingJobGraphGenerator</span>(<span class="params">StreamGraph streamGraph</span>)</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.streamGraph = streamGraph;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据 StreamGraph，生成 JobGraph</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> JobGraph <span class="title">createJobGraph</span>(<span class="params"></span>)</span> &#123;</span><br><span class="line">    jobGraph = <span class="keyword">new</span> JobGraph(streamGraph.getJobName());</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// streaming 模式下，调度模式是所有节点（vertices）一起启动</span></span><br><span class="line">    jobGraph.setScheduleMode(ScheduleMode.ALL);</span><br><span class="line">    <span class="comment">// 初始化成员变量</span></span><br><span class="line">    init();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 广度优先遍历 StreamGraph 并且为每个SteamNode生成hash id，</span></span><br><span class="line">    <span class="comment">// 保证如果提交的拓扑没有改变，则每次生成的hash都是一样的</span></span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes = traverseStreamGraphAndGenerateHashes();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 最重要的函数，生成JobVertex，JobEdge等，并尽可能地将多个节点chain在一起</span></span><br><span class="line">    setChaining(hashes);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 将每个JobVertex的入边集合也序列化到该JobVertex的StreamConfig中</span></span><br><span class="line">    <span class="comment">// (出边集合已经在setChaining的时候写入了)</span></span><br><span class="line">    setPhysicalEdges();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 根据group name，为每个 JobVertex 指定所属的 SlotSharingGroup </span></span><br><span class="line">    <span class="comment">// 以及针对 Iteration的头尾设置  CoLocationGroup</span></span><br><span class="line">    setSlotSharing();</span><br><span class="line">    <span class="comment">// 配置checkpoint</span></span><br><span class="line">    configureCheckpointing();</span><br><span class="line">    <span class="comment">// 配置重启策略（不重启，还是固定延迟重启）</span></span><br><span class="line">    configureRestartStrategy();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 将 StreamGraph 的 ExecutionConfig 序列化到 JobGraph 的配置中</span></span><br><span class="line">      InstantiationUtil.writeObjectToConfig(<span class="keyword">this</span>.streamGraph.getExecutionConfig(), <span class="keyword">this</span>.jobGraph.getJobConfiguration(), ExecutionConfig.CONFIG_KEY);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Config object could not be written to Job Configuration: "</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>StreamingJobGraphGenerator</code>的成员变量都是为了辅助生成最终的JobGraph。<code>createJobGraph()</code>函数的逻辑也很清晰，首先为所有节点生成一个唯一的hash id，如果节点在多次提交中没有改变（包括并发度、上下游等），那么这个id就不会改变，这主要用于故障恢复。这里我们不能用 <code>StreamNode.id</code>来代替，因为这是一个从1开始的静态计数变量，同样的Job可能会得到不一样的id，如下代码示例的两个job是完全一样的，但是source的id却不一样了。然后就是最关键的chaining处理，和生成JobVetex、JobEdge等。之后就是写入各种配置相关的信息。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 范例1：A.id=1  B.id=2</span></span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; A = ...</span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; B = ...</span><br><span class="line">A.<span class="keyword">union</span>(B).<span class="built_in">print</span>();</span><br><span class="line"><span class="comment">// 范例2：A.id=2  B.id=1</span></span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; B = ...</span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; A = ...</span><br><span class="line">A.<span class="keyword">union</span>(B).<span class="built_in">print</span>();</span><br></pre></td></tr></table></figure><p>下面具体分析下关键函数 <code>setChaining</code> 的实现：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从source开始建立 node chains</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> setChaining(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</span><br><span class="line">  <span class="built_in">for</span> (Integer sourceNodeId : streamGraph.getSourceIDs()) &#123;</span><br><span class="line">    createChain(sourceNodeId, sourceNodeId, hashes);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建node chains，返回当前节点的物理出边</span></span><br><span class="line"><span class="comment">// startNodeId != currentNodeId 时,说明currentNode是chain中的子节点</span></span><br><span class="line"><span class="keyword">private</span> List&lt;StreamEdge&gt; createChain(</span><br><span class="line">    Integer startNodeId,</span><br><span class="line">    Integer currentNodeId,</span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过渡用的出边集合, 用来生成最终的 JobEdge, 注意不包括 chain 内部的边</span></span><br><span class="line">    List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">    List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">    List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将当前节点的出边分成 chainable 和 nonChainable 两类</span></span><br><span class="line">    <span class="built_in">for</span> (StreamEdge outEdge : streamGraph.getStreamNode(currentNodeId).getOutEdges()) &#123;</span><br><span class="line">      <span class="built_in">if</span> (isChainable(outEdge)) &#123;</span><br><span class="line">        chainableOutputs.add(outEdge);</span><br><span class="line">      &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">        nonChainableOutputs.add(outEdge);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//==&gt; 递归调用</span></span><br><span class="line">    <span class="built_in">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">      transitiveOutEdges.addAll(createChain(startNodeId, chainable.getTargetId(), hashes));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">      transitiveOutEdges.add(nonChainable);</span><br><span class="line">      createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成当前节点的显示名，如："Keyed Aggregation -&gt; Sink: Unnamed"</span></span><br><span class="line">    chainedNames.<span class="built_in">put</span>(currentNodeId, createChainedName(currentNodeId, chainableOutputs));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前节点是起始节点, 则直接创建 JobVertex 并返回 StreamConfig, 否则先创建一个空的 StreamConfig</span></span><br><span class="line">    <span class="comment">// createJobVertex 函数就是根据 StreamNode 创建对应的 JobVertex, 并返回了空的 StreamConfig</span></span><br><span class="line">    StreamConfig <span class="built_in">config</span> = currentNodeId.equals(startNodeId)</span><br><span class="line">        ? createJobVertex(startNodeId, hashes)</span><br><span class="line">        : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 JobVertex 的 StreamConfig, 基本上是序列化 StreamNode 中的配置到 StreamConfig 中.</span></span><br><span class="line">    <span class="comment">// 其中包括 序列化器, StreamOperator, Checkpoint 等相关配置</span></span><br><span class="line">    setVertexConfig(currentNodeId, <span class="built_in">config</span>, chainableOutputs, nonChainableOutputs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">      <span class="comment">// 如果是chain的起始节点。（不是chain中的节点，也会被标记成 chain start）</span></span><br><span class="line">      <span class="built_in">config</span>.setChainStart();</span><br><span class="line">      <span class="comment">// 我们也会把物理出边写入配置, 部署时会用到</span></span><br><span class="line">      <span class="built_in">config</span>.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">      <span class="built_in">config</span>.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将当前节点(headOfChain)与所有出边相连</span></span><br><span class="line">      <span class="built_in">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">        <span class="comment">// 通过StreamEdge构建出JobEdge，创建IntermediateDataSet，用来将JobVertex和JobEdge相连</span></span><br><span class="line">        <span class="built_in">connect</span>(startNodeId, edge);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将chain中所有子节点的StreamConfig写入到 headOfChain 节点的 CHAINED_TASK_CONFIG 配置中</span></span><br><span class="line">      <span class="built_in">config</span>.setTransitiveChainedTaskConfigs(chainedConfigs.<span class="built_in">get</span>(startNodeId));</span><br><span class="line"></span><br><span class="line">    &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">      <span class="comment">// 如果是 chain 中的子节点</span></span><br><span class="line">      </span><br><span class="line">      Map&lt;Integer, StreamConfig&gt; chainedConfs = chainedConfigs.<span class="built_in">get</span>(startNodeId);</span><br><span class="line"></span><br><span class="line">      <span class="built_in">if</span> (chainedConfs == null) &#123;</span><br><span class="line">        chainedConfigs.<span class="built_in">put</span>(startNodeId, <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 将当前节点的StreamConfig添加到该chain的config集合中</span></span><br><span class="line">      chainedConfigs.<span class="built_in">get</span>(startNodeId).<span class="built_in">put</span>(currentNodeId, <span class="built_in">config</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回连往chain外部的出边集合</span></span><br><span class="line">    <span class="built_in">return</span> transitiveOutEdges;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">    <span class="built_in">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 JobVertex 都会对应一个可序列化的 StreamConfig, 用来发送给 JobManager 和 TaskManager。最后在 TaskManager 中起 Task 时,需要从这里面反序列化出所需要的配置信息, 其中就包括了含有用户代码的StreamOperator。</p><p><code>setChaining</code>会对source调用<code>createChain</code>方法，该方法会递归调用下游节点，从而构建出node chains。<code>createChain</code>会分析当前节点的出边，根据<a href="http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/#Operator-Chains" target="_blank" rel="noopener">Operator Chains</a>中的chainable条件，将出边分成chainalbe和noChainable两类，并分别递归调用自身方法。之后会将StreamNode中的配置信息序列化到StreamConfig中。如果当前不是chain中的子节点，则会构建 JobVertex 和 JobEdge相连。如果是chain中的子节点，则会将StreamConfig添加到该chain的config集合中。一个node chains，除了 headOfChain node会生成对应的 JobVertex，其余的nodes都是以序列化的形式写入到StreamConfig中，并保存到headOfChain的 <code>CHAINED_TASK_CONFIG</code> 配置项中。直到部署时，才会取出并生成对应的ChainOperators，具体过程请见<a href="http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/#Operator-Chains" target="_blank" rel="noopener">理解 Operator Chains</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要对 Flink 中将 StreamGraph 转变成 JobGraph 的核心源码进行了分析。思想还是很简单的，StreamNode 转成 JobVertex，StreamEdge 转成 JobEdge，JobEdge 和 JobVertex 之间创建 IntermediateDataSet 来连接。关键点在于将多个 SteamNode chain 成一个 JobVertex的过程，这部分源码比较绕，有兴趣的同学可以结合源码单步调试分析。下一章将会介绍 JobGraph 提交到 JobManager 后是如何转换成分布式化的 ExecutionGraph 的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：计算资源</title>
      <link href="/2018/05/15/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/"/>
      <url>/2018/05/15/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>本文所讨论的计算资源是指用来执行 Task 的资源，是一个逻辑概念。本文会介绍 Flink 计算资源相关的一些核心概念，如：Slot、SlotSharingGroup、CoLocationGroup、Chain等。并会着重讨论 Flink 如何对计算资源进行管理和隔离，如何将计算资源利用率最大化等等。理解 Flink 中的计算资源对于理解 Job 如何在集群中运行的有很大的帮助，也有利于我们更透彻地理解 Flink 原理，更快速地定位问题。</p><h2 id="Operator-Chains"><a href="#Operator-Chains" class="headerlink" title="Operator Chains"></a>Operator Chains</h2><p>为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</p><p>我们仍以经典的 WordCount 为例，下面这幅图，展示了Source并行度为1，FlatMap、KeyAggregation、Sink并行度均为2，最终以5个并行的线程来执行的优化过程。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB18Gv5JFXXXXcDXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB18Gv5JFXXXXcDXXXXXXXXXXXX" alt="img"></a></p><p>上图中将KeyAggregation和Sink两个operator进行了合并，因为这两个合并后并不会改变整体的拓扑结构。但是，并不是任意两个 operator 就能 chain 一起的。其条件还是很苛刻的：</p><ol><li>上下游的并行度一致</li><li>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</li><li>上下游节点都在同一个 slot group 中（下面会解释 slot group）</li><li>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</li><li>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</li><li>两个节点间数据分区方式是 forward</li><li>用户没有禁用 chain</li></ol><p>Operator chain的行为可以通过编程API中进行指定。可以通过在DataStream的operator后面（如<code>someStream.map(..)</code>)调用<code>startNewChain()</code>来指示从该operator开始一个新的chain（与前面截断，不会被chain到前面）。或者调用<code>disableChaining()</code>来指示该operator不参与chaining（不会与前后的operator chain一起）。在底层，这两个方法都是通过调整operator的 chain 策略（HEAD、NEVER）来实现的。另外，也可以通过调用<code>StreamExecutionEnvironment.disableOperatorChaining()</code>来全局禁用chaining。</p><h3 id="原理与实现"><a href="#原理与实现" class="headerlink" title="原理与实现"></a>原理与实现</h3><p>那么 Flink 是如何将多个 operators chain在一起的呢？chain在一起的operators是如何作为一个整体被执行的呢？它们之间的数据流又是如何避免了序列化/反序列化以及网络传输的呢？下图展示了operators chain的内部实现：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1cFbJJFXXXXaIXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1cFbJJFXXXXaIXVXXXXXXXXXX" alt="img"></a></p><p>如上图所示，Flink内部是通过<code>OperatorChain</code>这个类来将多个operator链在一起形成一个新的operator。<code>OperatorChain</code>形成的框框就像一个黑盒，Flink 无需知道黑盒中有多少个ChainOperator、数据在chain内部是怎么流动的，只需要将input数据交给 HeadOperator 就可以了，这就使得<code>OperatorChain</code>在行为上与普通的operator无差别，上面的OperaotrChain就可以看做是一个入度为1，出度为2的operator。所以在实现中，对外可见的只有HeadOperator，以及与外部连通的实线输出，这些输出对应了JobGraph中的JobEdge，在底层通过<code>RecordWriterOutput</code>来实现。另外，框中的虚线是operator chain内部的数据流，这个流内的数据不会经过序列化/反序列化、网络传输，而是直接将消息对象传递给下游的 ChainOperator 处理，这是性能提升的关键点，在底层是通过 <code>ChainingOutput</code> 实现的，源码如下方所示，</p><p><em>注：HeadOperator和ChainOperator并不是具体的数据结构，前者指代chain中的第一个operator，后者指代chain中其余的operator，它们实际上都是StreamOperator。</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainingOutput</span>&lt;T&gt; <span class="title">implements</span> <span class="title">Output</span>&lt;StreamRecord&lt;T&gt;&gt; &#123;</span></span><br><span class="line">  <span class="comment">// 注册的下游operator</span></span><br><span class="line">  <span class="keyword">protected</span> final OneInputStreamOperator&lt;T, ?&gt; <span class="keyword">operator</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ChainingOutput</span><span class="params">(OneInputStreamOperator&lt;T, ?&gt; <span class="keyword">operator</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.<span class="keyword">operator</span> = <span class="keyword">operator</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  @Override</span><br><span class="line">  <span class="comment">// 发送消息方法的实现，直接将消息对象传递给operator处理，不经过序列化/反序列化、网络传输</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">collect</span><span class="params">(StreamRecord&lt;T&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">operator</span>.setKeyContextElement1(record);</span><br><span class="line">      <span class="comment">// 下游operator直接处理消息对象</span></span><br><span class="line">      <span class="keyword">operator</span>.processElement(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ExceptionInChainedOperatorException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Task-Slot"><a href="#Task-Slot" class="headerlink" title="Task Slot"></a>Task Slot</h2><p>在架构概览一文中我们介绍了 TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 <em>Task Slot</em> 的概念。</p><p>Flink 中的计算资源通过 <em>Task Slot</em> 来定义。每个 task slot 代表了 TaskManager 的一个固定大小的资源子集。例如，一个拥有3个slot的 TaskManager，会将其管理的内存平均分成三份分给各个 slot。将资源 slot 化意味着来自不同job的task不会为了内存而竞争，而是每个task都拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的内存。</p><p>通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输。也能共享一些数据结构，一定程度上减少了每个task的消耗。</p><p>每一个 TaskManager 会拥有一个或多个的 task slot，每个 slot 都能跑由多个连续 task 组成的一个 pipeline，比如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例可以组成一个 pipeline。</p><p>如上文所述的 WordCount 例子，5个Task可能会在TaskManager的slots中如下图分布，2个TaskManager，每个有3个slot：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1Q4zUJFXXXXXnXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1Q4zUJFXXXXXnXVXXXXXXXXXX" alt="img"></a></p><h2 id="SlotSharingGroup-与-CoLocationGroup"><a href="#SlotSharingGroup-与-CoLocationGroup" class="headerlink" title="SlotSharingGroup 与 CoLocationGroup"></a>SlotSharingGroup 与 CoLocationGroup</h2><p>默认情况下，Flink 允许subtasks共享slot，条件是它们都来自同一个Job的不同task的subtask。结果可能一个slot持有该job的整个pipeline。允许slot共享有以下两点好处：</p><ol><li>Flink 集群所需的task slots数与job中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个task了。</li><li>更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将基线的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks。</li></ol><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1VTj4JFXXXXX8XFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1VTj4JFXXXXX8XFXXXXXXXXXX" alt="img"></a></p><p>我们将 WordCount 的并行度从之前的2个增加到6个（Source并行度仍为1），并开启slot共享（所有operator都在default共享组），将得到如上图所示的slot分布图。首先，我们不用去计算这个job会其多少个task，总之该任务最终会占用6个slots（最高并行度为6）。其次，我们可以看到密集型操作 keyAggregation/sink 被平均地分配到各个 TaskManager。</p><p><code>SlotSharingGroup</code>是Flink中用来实现slot共享的类，它尽可能地让subtasks共享一个slot。相应的，还有一个 <code>CoLocationGroup</code> 类用来强制将 subtasks 放到同一个 slot 中。<code>CoLocationGroup</code>主要用于<a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/index.html#iterations" target="_blank" rel="noopener">迭代流</a>中，用来保证迭代头与迭代尾的第i个subtask能被调度到同一个TaskManager上。这里我们不会详细讨论<code>CoLocationGroup</code>的实现细节。</p><p>怎么判断operator属于哪个 slot 共享组呢？默认情况下，所有的operator都属于默认的共享组<code>default</code>，也就是说默认情况下所有的operator都是可以共享一个slot的。而当所有input operators具有相同的slot共享组时，该operator会继承这个共享组。最后，为了防止不合理的共享，用户也能通过API来强制指定operator的共享组，比如：<code>someStream.filter(...).slotSharingGroup(&quot;group1&quot;);</code>就强制指定了filter的slot共享组为<code>group1</code>。</p><h3 id="原理与实现-1"><a href="#原理与实现-1" class="headerlink" title="原理与实现"></a>原理与实现</h3><p>那么多个tasks（或者说operators）是如何共享slot的呢？</p><p>我们先来看一下用来定义计算资源的slot的类图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1NxjUJFXXXXXoaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1NxjUJFXXXXXoaXXXXXXXXXXX" alt="img"></a></p><p>抽象类<code>Slot</code>定义了该槽位属于哪个TaskManager（<code>instance</code>）的第几个槽位（<code>slotNumber</code>），属于哪个Job（<code>jobID</code>）等信息。最简单的情况下，一个slot只持有一个task，也就是<code>SimpleSlot</code>的实现。复杂点的情况，一个slot能共享给多个task使用，也就是<code>SharedSlot</code>的实现。SharedSlot能包含其他的SharedSlot，也能包含SimpleSlot。所以一个SharedSlot能定义出一棵slots树。</p><p>接下来我们来看看 Flink 为subtask分配slot的过程。关于Flink调度，有两个非常重要的原则我们必须知道：（1）同一个operator的各个subtask是不能呆在同一个SharedSlot中的，例如<code>FlatMap[1]</code>和<code>FlatMap[2]</code>是不能在同一个SharedSlot中的。（2）Flink是按照拓扑顺序从Source一个个调度到Sink的。例如WordCount（Source并行度为1，其他并行度为2），那么调度的顺序依次是：<code>Source</code> -&gt; <code>FlatMap[1]</code> -&gt; <code>FlatMap[2]</code> -&gt; <code>KeyAgg-&gt;Sink[1]</code> -&gt; <code>KeyAgg-&gt;Sink[2]</code>。假设现在有2个TaskManager，每个只有1个slot（为简化问题），那么分配slot的过程如图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1TM_3JFXXXXb8XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1TM_3JFXXXXb8XVXXXXXXXXXX" alt="img"></a></p><p><em>注：图中 SharedSlot 与 SimpleSlot 后带的括号中的数字代表槽位号（slotNumber）</em></p><ol><li>为<code>Source</code>分配slot。首先，我们从TaskManager1中分配出一个SharedSlot。并从SharedSlot中为<code>Source</code>分配出一个SimpleSlot。如上图中的①和②。</li><li>为<code>FlatMap[1]</code>分配slot。目前已经有一个SharedSlot，则从该SharedSlot中分配出一个SimpleSlot用来部署<code>FlatMap[1]</code>。如上图中的③。</li><li>为<code>FlatMap[2]</code>分配slot。由于TaskManager1的SharedSlot中已经有同operator的<code>FlatMap[1]</code>了，我们只能分配到其他SharedSlot中去。从TaskManager2中分配出一个SharedSlot，并从该SharedSlot中为<code>FlatMap[2]</code>分配出一个SimpleSlot。如上图的④和⑤。</li><li>为<code>Key-&gt;Sink[1]</code>分配slot。目前两个SharedSlot都符合条件，从TaskManager1的SharedSlot中分配出一个SimpleSlot用来部署<code>Key-&gt;Sink[1]</code>。如上图中的⑥。</li><li>为<code>Key-&gt;Sink[2]</code>分配slot。TaskManager1的SharedSlot中已经有同operator的<code>Key-&gt;Sink[1]</code>了，则只能选择另一个SharedSlot中分配出一个SimpleSlot用来部署<code>Key-&gt;Sink[2]</code>。如上图中的⑦。</li></ol><p>最后<code>Source</code>、<code>FlatMap[1]</code>、<code>Key-&gt;Sink[1]</code>这些subtask都会部署到TaskManager1的唯一一个slot中，并启动对应的线程。<code>FlatMap[2]</code>、<code>Key-&gt;Sink[2]</code>这些subtask都会被部署到TaskManager2的唯一一个slot中，并启动对应的线程。从而实现了slot共享。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了Flink中计算资源的相关概念以及原理实现。最核心的是 Task Slot，每个slot能运行一个或多个task。为了拓扑更高效地运行，Flink提出了Chaining，尽可能地将operators chain在一起作为一个task来处理。为了资源更充分的利用，Flink又提出了SlotSharingGroup，尽可能地让多个task共享一个slot。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：生成 StreamGraph</title>
      <link href="/2018/05/12/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20StreamGraph/"/>
      <url>/2018/05/12/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20StreamGraph/</url>
      
        <content type="html"><![CDATA[<p>继上文Flink 底层原理：架构和拓扑中介绍了Flink的四层执行图模型，本文将主要介绍 Flink 是如何根据用户用Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。</p><p><em>注：本文比较偏源码分析，所有代码都是基于 flink-1.0.x 版本，建议在阅读本文前先对Stream API有个了解，详见官方文档。</em></p><p>StreamGraph 相关的代码主要在 <code>org.apache.flink.streaming.api.graph</code> 包中。构造StreamGraph的入口函数是 <code>StreamGraphGenerator.generate(env, transformations)</code>。该函数会由触发程序执行的方法<code>StreamExecutionEnvironment.execute()</code>调用到。也就是说 StreamGraph 是在 Client 端构造的，这也意味着我们可以在本地通过调试观察 StreamGraph 的构造过程。</p><h2 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h2><p><code>StreamGraphGenerator.generate</code> 的一个关键的参数是 <code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>。<code>StreamTransformation</code>代表了从一个或多个<code>DataStream</code>生成新<code>DataStream</code>的操作。<code>DataStream</code>的底层其实就是一个 <code>StreamTransformation</code>，描述了这个<code>DataStream</code>是怎么来的。</p><p>StreamTransformation的类图如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1yQmNJFXXXXXnXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1yQmNJFXXXXXnXpXXXXXXXXXX" alt="img"></a></p><p>DataStream 上常见的 transformation 有 map、flatmap、filter等（见<a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/index.html#datastream-transformations" target="_blank" rel="noopener">DataStream Transformation</a>了解更多）。这些transformation会构造出一棵 StreamTransformation 树，通过这棵树转换成 StreamGraph。比如 <code>DataStream.map</code>源码如下，其中<code>SingleOutputStreamOperator</code>为DataStream的子类：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span>(<span class="params">MapFunction&lt;T, R&gt; mapper</span>)</span> &#123;</span><br><span class="line">  <span class="comment">// 通过java reflection抽出mapper的返回值类型</span></span><br><span class="line">  TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</span><br><span class="line">      Utils.getCallLocationName(), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回一个新的DataStream，SteramMap 为 StreamOperator 的实现类</span></span><br><span class="line">  <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">transform</span>(<span class="params">String operatorName, TypeInformation&lt;R&gt; outTypeInfo, OneInputStreamOperator&lt;T, R&gt; <span class="keyword">operator</span></span>)</span> &#123;</span><br><span class="line">  <span class="comment">// read the output type of the input Transform to coax out errors about MissingTypeInfo</span></span><br><span class="line">  transformation.getOutputType();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 新的transformation会连接上当前DataStream中的transformation，从而构建成一棵树</span></span><br><span class="line">  OneInputTransformation&lt;T, R&gt; resultTransform = <span class="keyword">new</span> OneInputTransformation&lt;&gt;(</span><br><span class="line">      <span class="keyword">this</span>.transformation,</span><br><span class="line">      operatorName,</span><br><span class="line">      <span class="keyword">operator</span>,</span><br><span class="line">      outTypeInfo,</span><br><span class="line">      environment.getParallelism());</span><br><span class="line"></span><br><span class="line">  @SuppressWarnings(&#123; <span class="string">"unchecked"</span>, <span class="string">"rawtypes"</span> &#125;)</span><br><span class="line">  SingleOutputStreamOperator&lt;R&gt; returnStream = <span class="keyword">new</span> SingleOutputStreamOperator(environment, resultTransform);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 所有的transformation都会存到 env 中，调用execute时遍历该list生成StreamGraph</span></span><br><span class="line">  getExecutionEnvironment().addOperator(resultTransform);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> returnStream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上方代码可以了解到，map转换将用户自定义的函数<code>MapFunction</code>包装到<code>StreamMap</code>这个Operator中，再将<code>StreamMap</code>包装到<code>OneInputTransformation</code>，最后该transformation存到env中，当调用<code>env.execute</code>时，遍历其中的transformation集合构造出StreamGraph。其分层实现如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB12u5yJFXXXXXhaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB12u5yJFXXXXXhaXXXXXXXXXXX" alt="img"></a></p><p>另外，并不是每一个 StreamTransformation 都会转换成 runtime 层中物理操作。有一些只是逻辑概念，比如 union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1XgmOJFXXXXaYXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1XgmOJFXXXXaYXpXXXXXXXXXX" alt="img"></a></p><p>union、split/select、partition中的信息会被写入到 Source –&gt; Map 的边中。通过源码也可以发现，<code>UnionTransformation</code>,<code>SplitTransformation</code>,<code>SelectTransformation</code>,<code>PartitionTransformation</code>由于不包含具体的操作所以都没有StreamOperator成员变量，而其他StreamTransformation的子类基本上都有。</p><h2 id="StreamOperator"><a href="#StreamOperator" class="headerlink" title="StreamOperator"></a>StreamOperator</h2><p>DataStream 上的每一个 Transformation 都对应了一个 StreamOperator，StreamOperator是运行时的具体实现，会决定UDF(User-Defined Funtion)的调用方式。下图所示为 StreamOperator 的类图（点击查看大图）：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1l9aYJFXXXXbAXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1l9aYJFXXXXbAXXXXXXXXXXXX" alt="img"></a></p><p>可以发现，所有实现类都继承了<code>AbstractStreamOperator</code>。另外除了 project 操作，其他所有可以执行UDF代码的实现类都继承自<code>AbstractUdfStreamOperator</code>，该类是封装了UDF的StreamOperator。UDF就是实现了<code>Function</code>接口的类，如<code>MapFunction</code>,<code>FilterFunction</code>。</p><h2 id="生成-StreamGraph-的源码分析"><a href="#生成-StreamGraph-的源码分析" class="headerlink" title="生成 StreamGraph 的源码分析"></a>生成 StreamGraph 的源码分析</h2><p>我们通过在DataStream上做了一系列的转换（map、filter等）得到了StreamTransformation集合，然后通过<code>StreamGraphGenerator.generate</code>获得StreamGraph，该方法的源码如下：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构造 StreamGraph 入口函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> StreamGraph generate(StreamExecutionEnvironment env, <span class="keyword">List</span>&lt;StreamTransformation<span class="meta">&lt;?</span>&gt;&gt; transformations) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> StreamGraphGenerator(env).generateInternal(transformations);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自底向上（sink-&gt;source）对转换树的每个transformation进行转换。</span></span><br><span class="line"><span class="keyword">private</span> StreamGraph generateInternal(<span class="keyword">List</span>&lt;StreamTransformation<span class="meta">&lt;?</span>&gt;&gt; transformations) &#123;</span><br><span class="line">  <span class="keyword">for</span> (StreamTransformation<span class="meta">&lt;?</span>&gt; transformation: transformations) &#123;</span><br><span class="line">    transform(transformation);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> streamGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对具体的一个transformation进行转换，转换成 StreamGraph 中的 StreamNode 和 StreamEdge</span></span><br><span class="line"><span class="comment">// 返回值为该transform的id集合，通常大小为1个（除FeedbackTransformation）</span></span><br><span class="line"><span class="keyword">private</span> Collection&lt;Integer&gt; transform(StreamTransformation<span class="meta">&lt;?</span>&gt; transform) &#123;  </span><br><span class="line">  <span class="comment">// 跳过已经转换过的transformation</span></span><br><span class="line">  <span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">    <span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为了触发 MissingTypeInfo 的异常</span></span><br><span class="line">  transform.getOutputType();</span><br><span class="line"></span><br><span class="line">  Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">  <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation<span class="meta">&lt;?</span>, <span class="meta">?&gt;</span>) &#123;</span><br><span class="line">    transformedIds = transformOnInputTransform((OneInputTransformation<span class="meta">&lt;?</span>, <span class="meta">?&gt;</span>) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation<span class="meta">&lt;?</span>, ?, <span class="meta">?&gt;</span>) &#123;</span><br><span class="line">    transformedIds = transformTwoInputTransform((TwoInputTransformation<span class="meta">&lt;?</span>, ?, <span class="meta">?&gt;</span>) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSource((SourceTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSink((SinkTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformUnion((UnionTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSplit((SplitTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSelect((SelectTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformFeedback((FeedbackTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformCoFeedback((CoFeedbackTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformPartition((PartitionTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">  <span class="comment">// transforming the feedback edges</span></span><br><span class="line">  <span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">    alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    streamGraph.setTransformationId(transform.getId(), transform.getUid());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> transformedIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终都会调用 <code>transformXXX</code> 来对具体的StreamTransformation进行转换。我们可以看下<code>transformOnInputTransform(transform)</code>的实现：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">private &lt;IN, OUT&gt; Collection&lt;Integer&gt; transformOnInputTransform(OneInputTransformation&lt;IN, OUT&gt; <span class="built_in">transform</span>) &#123;</span><br><span class="line">  // 递归对该<span class="built_in">transform</span>的直接上游<span class="built_in">transform</span>进行转换，获得直接上游id集合</span><br><span class="line">  Collection&lt;Integer&gt; inputIds = <span class="built_in">transform</span>(<span class="built_in">transform</span>.getInput());</span><br><span class="line"></span><br><span class="line">  // 递归调用可能已经处理过该<span class="built_in">transform</span>了</span><br><span class="line">  <span class="keyword">if</span> (alreadyTransformed.containsKey(<span class="built_in">transform</span>)) &#123;</span><br><span class="line">    <span class="built_in">return</span> alreadyTransformed.<span class="built_in">get</span>(<span class="built_in">transform</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  String slotSharingGroup = determineSlotSharingGroup(<span class="built_in">transform</span>.getSlotSharingGroup(), inputIds);</span><br><span class="line"></span><br><span class="line">  // 添加 StreamNode</span><br><span class="line">  streamGraph.addOperator(<span class="built_in">transform</span>.getId(),</span><br><span class="line">      slotSharingGroup,</span><br><span class="line">      <span class="built_in">transform</span>.getOperator(),</span><br><span class="line">      <span class="built_in">transform</span>.getInputType(),</span><br><span class="line">      <span class="built_in">transform</span>.getOutputType(),</span><br><span class="line">      <span class="built_in">transform</span>.getName());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">transform</span>.getStateKeySelector() != null) &#123;</span><br><span class="line">    TypeSerializer&lt;?&gt; keySerializer = <span class="built_in">transform</span>.getStateKeyType().createSerializer(env.getConfig());</span><br><span class="line">    streamGraph.setOneInputStateKey(<span class="built_in">transform</span>.getId(), <span class="built_in">transform</span>.getStateKeySelector(), keySerializer);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  streamGraph.setParallelism(<span class="built_in">transform</span>.getId(), <span class="built_in">transform</span>.getParallelism());</span><br><span class="line"></span><br><span class="line">  // 添加 StreamEdge</span><br><span class="line">  <span class="keyword">for</span> (Integer inputId: inputIds) &#123;</span><br><span class="line">    streamGraph.addEdge(inputId, <span class="built_in">transform</span>.getId(), <span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> Collections.singleton(<span class="built_in">transform</span>.getId());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数首先会对该transform的上游transform进行递归转换，确保上游的都已经完成了转化。然后通过transform构造出StreamNode，最后与上游的transform进行连接，构造出StreamNode。</p><p>最后再来看下对逻辑转换（partition、union等）的处理，如下是<code>transformPartition</code>函数的源码：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T&gt; Collection&lt;<span class="built_in">Integer</span>&gt; transformPartition(PartitionTransformation&lt;T&gt; partition) &#123;</span><br><span class="line">  StreamTransformation&lt;T&gt; input = partition.getInput();</span><br><span class="line">  <span class="built_in">List</span>&lt;<span class="built_in">Integer</span>&gt; resultIds = <span class="literal">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 直接上游的id</span></span><br><span class="line">  Collection&lt;<span class="built_in">Integer</span>&gt; transformedIds = transform(input);</span><br><span class="line">  for (<span class="built_in">Integer</span> transformedId: transformedIds) &#123;</span><br><span class="line">    <span class="comment">// 生成一个新的虚拟id</span></span><br><span class="line">    int virtualId = StreamTransformation.getNewNodeId();</span><br><span class="line">    <span class="comment">// 添加一个虚拟分区节点，不会生成 StreamNode</span></span><br><span class="line">    streamGraph.addVirtualPartitionNode(transformedId, virtualId, partition.getPartitioner());</span><br><span class="line">    resultIds.add(virtualId);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> resultIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对partition的转换没有生成具体的StreamNode和StreamEdge，而是添加一个虚节点。当partition的下游transform（如map）添加edge时（调用<code>StreamGraph.addEdge</code>），会把partition信息写入到edge中。如<code>StreamGraph.addEdgeInternal</code>所示：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> addEdge(Integer upStreamVertexID, Integer downStreamVertexID, <span class="built_in">int</span> typeNumber) &#123;</span><br><span class="line">  addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, <span class="keyword">null</span>, <span class="keyword">new</span> ArrayList&lt;<span class="keyword">String</span>&gt;());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> addEdgeInternal(Integer upStreamVertexID,</span><br><span class="line">    Integer downStreamVertexID,</span><br><span class="line">    <span class="built_in">int</span> typeNumber,</span><br><span class="line">    StreamPartitioner&lt;?&gt; partitioner,</span><br><span class="line">    List&lt;<span class="keyword">String</span>&gt; outputNames) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 当上游是select时，递归调用，并传入select信息</span></span><br><span class="line">  <span class="keyword">if</span> (virtualSelectNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">    <span class="built_in">int</span> virtualId = upStreamVertexID;</span><br><span class="line">    <span class="comment">// select上游的节点id</span></span><br><span class="line">    upStreamVertexID = virtualSelectNodes.<span class="built_in">get</span>(virtualId).f0;</span><br><span class="line">    <span class="keyword">if</span> (outputNames.isEmpty()) &#123;</span><br><span class="line">      <span class="comment">// selections that happen downstream override earlier selections</span></span><br><span class="line">      outputNames = virtualSelectNodes.<span class="built_in">get</span>(virtualId).f1;</span><br><span class="line">    &#125;</span><br><span class="line">    addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames);</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 当上游是partition时，递归调用，并传入partitioner信息</span></span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (virtuaPartitionNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">    <span class="built_in">int</span> virtualId = upStreamVertexID;</span><br><span class="line">    <span class="comment">// partition上游的节点id</span></span><br><span class="line">    upStreamVertexID = virtuaPartitionNodes.<span class="built_in">get</span>(virtualId).f0;</span><br><span class="line">    <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">      partitioner = virtuaPartitionNodes.<span class="built_in">get</span>(virtualId).f1;</span><br><span class="line">    &#125;</span><br><span class="line">    addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 真正构建StreamEdge</span></span><br><span class="line">    StreamNode upstreamNode = getStreamNode(upStreamVertexID);</span><br><span class="line">    StreamNode downstreamNode = getStreamNode(downStreamVertexID);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 未指定partitioner的话，会为其选择 forward 或 rebalance 分区。</span></span><br><span class="line">    <span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">      partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;<span class="keyword">Object</span>&gt;();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">      partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;<span class="keyword">Object</span>&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 健康检查， forward 分区必须要上下游的并发度一致</span></span><br><span class="line">    <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line">      <span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Forward partitioning does not allow "</span> +</span><br><span class="line">            <span class="string">"change of parallelism. Upstream operation: "</span> + upstreamNode + <span class="string">" parallelism: "</span> + upstreamNode.getParallelism() +</span><br><span class="line">            <span class="string">", downstream operation: "</span> + downstreamNode + <span class="string">" parallelism: "</span> + downstreamNode.getParallelism() +</span><br><span class="line">            <span class="string">" You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建 StreamEdge</span></span><br><span class="line">    StreamEdge edge = <span class="keyword">new</span> StreamEdge(upstreamNode, downstreamNode, typeNumber, outputNames, partitioner);</span><br><span class="line">    <span class="comment">// 将该 StreamEdge 添加到上游的输出，下游的输入</span></span><br><span class="line">    getStreamNode(edge.getSourceId()).addOutEdge(edge);</span><br><span class="line">    getStreamNode(edge.getTargetId()).addInEdge(edge);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实例讲解"><a href="#实例讲解" class="headerlink" title="实例讲解"></a>实例讲解</h2><p>如下程序，是一个从 Source 中按行切分成单词并过滤输出的简单流程序，其中包含了逻辑转换：随机分区shuffle。我们会分析该程序是如何生成StreamGraph的。</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; <span class="built_in">text</span> = env.socketTextStream(hostName, port);</span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> LineSplitter()).shuffle().<span class="built_in">filter</span>(<span class="keyword">new</span> HelloFilter()).<span class="built_in">print</span>();</span><br></pre></td></tr></table></figure><p>首先会在env中生成一棵transformation树，用<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>保存。其结构图如下：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1w3SQJFXXXXalXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1w3SQJFXXXXalXVXXXXXXXXXX" alt="img"></a></p><p>其中符号<code>*</code>为input指针，指向上游的transformation，从而形成了一棵transformation树。然后，通过调用<code>StreamGraphGenerator.generate(env, transformations)</code>来生成StreamGraph。自底向上递归调用每一个transformation，也就是说处理顺序是Source-&gt;FlatMap-&gt;Shuffle-&gt;Filter-&gt;Sink。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1s7SpJFXXXXXjaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1s7SpJFXXXXXjaXXXXXXXXXXX" alt="img"></a></p><p>如上图所示：</p><ol><li>首先处理的Source，生成了Source的StreamNode。</li><li>然后处理的FlatMap，生成了FlatMap的StreamNode，并生成StreamEdge连接上游Source和FlatMap。由于上下游的并发度不一样（1:4），所以此处是Rebalance分区。</li><li>然后处理的Shuffle，由于是逻辑转换，并不会生成实际的节点。将partitioner信息暂存在<code>virtuaPartitionNodes</code>中。</li><li>在处理Filter时，生成了Filter的StreamNode。发现上游是shuffle，找到shuffle的上游FlatMap，创建StreamEdge与Filter相连。并把ShufflePartitioner的信息写到StreamEdge中。</li><li>最后处理Sink，创建Sink的StreamNode，并生成StreamEdge与上游Filter相连。由于上下游并发度一样（4:4），所以此处选择 Forward 分区。</li></ol><p>最后可以通过 <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">UI可视化</a> 来观察得到的 StreamGraph。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1y_1FJFXXXXapaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1y_1FJFXXXXapaXXXXXXXXXXX" alt="img"></a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了 Stream API 中 Transformation 和 Operator 的概念，以及如何根据Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。本文的源码分析涉及到较多代码，如果有兴趣建议结合完整源码进行学习。下一篇文章将介绍 StreamGraph 如何转换成 JobGraph 的，其中设计到了图优化的技巧。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：架构和拓扑</title>
      <link href="/2018/05/08/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E6%8B%93%E6%89%91/"/>
      <url>/2018/05/08/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E6%8B%93%E6%89%91/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>要了解一个系统，一般都是从架构开始。我们关心的问题是：系统部署成功后各个节点都启动了哪些服务，各个服务之间又是怎么交互和协调的。下方是 Flink 集群启动后架构图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1ObBnJFXXXXXtXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1ObBnJFXXXXXtXVXXXXXXXXXX" alt="img"></a></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><ul><li><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li><li><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</li><li><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</li></ul><p>可以看到 Flink 的任务调度是多线程模型，并且不同Job/Task混合在一个 TaskManager 进程中。虽然这种方式可以有效提高 CPU 利用率，但是个人不太喜欢这种设计，因为不仅缺乏资源隔离机制，同时也不方便调试。类似 Storm 的进程模型，一个JVM 中只跑该 Job 的 Tasks 实际应用中更为合理。</p><h2 id="Job-例子"><a href="#Job-例子" class="headerlink" title="Job 例子"></a>Job 例子</h2><blockquote><p>本文所示例子为 flink-1.0.x 版本</p></blockquote><p>我们使用 Flink 自带的 examples 包中的 <code>SocketTextStreamWordCount</code>，这是一个从 socket 流中统计单词出现次数的例子。</p><ul><li><p>首先，使用 <strong>netcat</strong> 启动本地服务器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l 9000</span></span><br></pre></td></tr></table></figure></li><li><p>然后提交 Flink 程序</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink run examples/streaming/SocketTextStreamWordCount.jar \</span><br><span class="line">  --hostname <span class="number">10.218</span><span class="number">.130</span><span class="number">.9</span> \</span><br><span class="line">  --port <span class="number">9000</span></span><br></pre></td></tr></table></figure></li></ul><p>在netcat端输入单词并监控 taskmanager 的输出可以看到单词统计的结果。</p><p><code>SocketTextStreamWordCount</code> 的具体代码如下：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// 检查输入</span></span><br><span class="line">  <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set up the execution environment</span></span><br><span class="line">  <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// get input data</span></span><br><span class="line">  DataStream&lt;<span class="keyword">String</span>&gt; <span class="built_in">text</span> =</span><br><span class="line">      env.socketTextStream(params.<span class="built_in">get</span>(<span class="string">"hostname"</span>), params.getInt(<span class="string">"port"</span>), <span class="string">'\n'</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  DataStream&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; counts =</span><br><span class="line">      <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">      <span class="built_in">text</span>.flatMap(<span class="keyword">new</span> Tokenizer())</span><br><span class="line">          <span class="comment">// group by the tuple field "0" and sum up tuple field "1"</span></span><br><span class="line">          .keyBy(<span class="number">0</span>)</span><br><span class="line">          .sum(<span class="number">1</span>);</span><br><span class="line">  counts.<span class="built_in">print</span>();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// execute program</span></span><br><span class="line">  env.execute(<span class="string">"WordCount from SocketTextStream Example"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们将最后一行代码 <code>env.execute</code> 替换成 <code>System.out.println(env.getExecutionPlan());</code> 并在本地运行该代码（并发度设为2），可以得到该拓扑的逻辑执行计划图的 JSON 串，将该 JSON 串粘贴到 <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">http://flink.apache.org/visualizer/</a> 中，能可视化该执行图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1vB1uJFXXXXbaXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1vB1uJFXXXXbaXpXXXXXXXXXX" alt="img"></a></p><p>但这并不是最终在 Flink 中运行的执行图，只是一个表示拓扑节点关系的计划图，在 Flink 中对应了 SteramGraph。另外，提交拓扑后（并发度设为2）还能在 UI 中看到另一张执行计划图，如下所示，该图对应了 Flink 中的 JobGraph。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1QKR2JFXXXXbyaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1QKR2JFXXXXbyaXXXXXXXXXXX" alt="img"></a></p><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p>看起来有点乱，怎么有这么多不一样的图。实际上，还有更多的图。Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li><strong>StreamGraph：</strong>是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li><strong>JobGraph：</strong>StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li><strong>ExecutionGraph：</strong>JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li><strong>物理执行图：</strong>JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>例如上文中的2个并发度（Source为1个并发度）的 <code>SocketTextStreamWordCount</code> 四层执行图的演变过程如下图所示（点击查看大图）：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1tA_GJFXXXXapXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1tA_GJFXXXXapXFXXXXXXXXXX" alt="img"></a></p><p>这里对一些名词进行简单的解释。</p><ul><li><p>StreamGraph：</p><p>根据用户通过 Stream API 编写的代码生成的最初的图。</p><ul><li>StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</li><li>StreamEdge：表示连接两个StreamNode的边。</li></ul></li><li><p>JobGraph：</p><p>StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。</p><ul><li>JobVertex：经过优化后符合条件的多个StreamNode可能会chain在一起生成一个JobVertex，即一个JobVertex包含一个或多个operator，JobVertex的输入是JobEdge，输出是IntermediateDataSet。</li><li>IntermediateDataSet：表示JobVertex的输出，即经过operator处理产生的数据集。producer是JobVertex，consumer是JobEdge。</li><li>JobEdge：代表了job graph中的一条数据传输通道。source 是 IntermediateDataSet，target 是 JobVertex。即数据通过JobEdge由IntermediateDataSet传递给目标JobVertex。</li></ul></li><li><p>ExecutionGraph：</p><p>JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><ul><li>ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的 ExecutionVertex。</li><li>ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition。</li><li>IntermediateResult：和JobGraph中的IntermediateDataSet一一对应。一个IntermediateResult包含多个IntermediateResultPartition，其个数等于该operator的并发度。</li><li>IntermediateResultPartition：表示ExecutionVertex的一个输出分区，producer是ExecutionVertex，consumer是若干个ExecutionEdge。</li><li>ExecutionEdge：表示ExecutionVertex的输入，source是IntermediateResultPartition，target是ExecutionVertex。source和target都只能是一个。</li><li>Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM和TM之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者。</li></ul></li><li><p>物理执行图：</p><p>JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p><ul><li>Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator。</li><li>ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。</li><li>ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定。</li><li>InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition。</li><li>InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。</li></ul></li></ul><p>那么 Flink 为什么要设计这4张图呢，其目的是什么呢？Spark 中也有多张图，数据依赖图以及物理执行的DAG。其目的都是一样的，就是解耦，每张图各司其职，每张图对应了 Job 不同的阶段，更方便做该阶段的事情。我们给出更完整的 Flink Graph 的层次图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1qmtpJVXXXXagXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1qmtpJVXXXXagXXXXXXXXXXXX" alt="img"></a></p><p>首先我们看到，JobGraph 之上除了 StreamGraph 还有 OptimizedPlan。OptimizedPlan 是由 Batch API 转换而来的。StreamGraph 是由 Stream API 转换而来的。为什么 API 不直接转换成 JobGraph？因为，Batch 和 Stream 的图结构和优化方法有很大的区别，比如 Batch 有很多执行前的预分析用来优化图的执行，而这种优化并不普适于 Stream，所以通过 OptimizedPlan 来做 Batch 的优化会更方便和清晰，也不会影响 Stream。JobGraph 的责任就是统一 Batch 和 Stream 的图，用来描述清楚一个拓扑图的结构，并且做了 chaining 的优化，chaining 是普适于 Batch 和 Stream 的，所以在这一层做掉。ExecutionGraph 的责任是方便调度和各个 tasks 状态的监控和跟踪，所以 ExecutionGraph 是并行化的 JobGraph。而“物理执行图”就是最终分布式在各个机器上运行着的tasks了。所以可以看到，这种解耦方式极大地方便了我们在各个层所做的工作，各个层之间是相互隔离的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：内存管理</title>
      <link href="/2018/05/06/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2018/05/06/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>如今，大数据领域的开源框架（Hadoop，Spark，Storm）都使用的 JVM，当然也包括 Flink。基于 JVM 的数据分析引擎都需要面对将大量数据存到内存中，这就不得不面对 JVM 存在的几个问题：</p><ol><li>Java 对象存储密度低。一个只包含 boolean 属性的对象占用了16个字节内存：对象头占了8个，boolean 属性占了1个，对齐填充占了7个。而实际上只需要一个bit（1/8字节）就够了。</li><li>Full GC 会极大地影响性能，尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC 会达到秒级甚至分钟级。</li><li>OOM 问题影响稳定性。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。</li></ol><p>所以目前，越来越多的大数据项目开始自己管理JVM内存了，像 Spark、Flink、HBase，为的就是获得像 C 一样的性能以及避免 OOM 的发生。本文将会讨论 Flink 是如何解决上面的问题的，主要内容包括内存管理、定制的序列化工具、缓存友好的数据结构和算法、堆外内存、JIT编译优化等。</p><h2 id="积极的内存管理"><a href="#积极的内存管理" class="headerlink" title="积极的内存管理"></a>积极的内存管理</h2><p>Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 <code>MemorySegment</code>，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。你可以把 MemorySegment 想象成是为 Flink 定制的 <code>java.nio.ByteBuffer</code>。它的底层可以是一个普通的 Java 字节数组（<code>byte[]</code>），也可以是一个申请在堆外的 <code>ByteBuffer</code>。每条记录都会以序列化的形式存储在一个或多个<code>MemorySegment</code>中。</p><p>Flink 中的 Worker 名叫 TaskManager，是用来运行用户代码的 JVM 进程。TaskManager 的堆内存主要被分成了三个部分：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB17qs5JpXXXXXhXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB17qs5JpXXXXXhXpXXXXXXXXXX" alt="img"></a></p><ul><li>Network Buffers: 一定数量的32KB大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过 taskmanager.network.numberOfBuffers 来配置。（可阅读上一篇文章了解更多Network Buffer的管理）</li><li><strong>Memory Manager Pool:</strong> 这是一个由 <code>MemoryManager</code> 管理的，由众多<code>MemorySegment</code>组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。</li><li><strong>Remaining (Free) Heap:</strong> 这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从GC的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。</li></ul><p><strong>注意：Memory Manager Pool 主要在Batch模式下使用。在Steaming模式下，该池子不会预分配内存，也不会向该池子请求内存块。也就是说该部分的内存都是可以给用户代码使用的。不过社区是打算在 Streaming 模式下也能将该池子利用起来。</strong></p><p>Flink 采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据，从而使序列化/反序列化带来的开销达到最小。所以 Flink 的内部实现更像 C/C++ 而非 Java。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。如果要操作多块MemorySegment就像操作一块大的连续内存一样，Flink会使用逻辑视图（<code>AbstractPagedInputView</code>）来方便操作。下图描述了 Flink 如何存储序列化后的数据到内存块中，以及在需要的时候如何将数据存储到磁盘上。</p><p>从上面我们能够得出 Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：</p><ol><li><strong>减少GC压力。</strong>显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的<code>MemoryManager</code>中，这些<code>MemorySegment</code>一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。</li><li><strong>避免了OOM。</strong>所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，<code>OutOfMemoryErrors</code>可以有效地被避免。</li><li><strong>节省内存空间。</strong>Java 对象在存储上有很多额外的消耗（如上一节所谈）。如果只存储实际数据的二进制内容，就可以避免这部分消耗。</li><li><strong>高效的二进制操作 &amp; 缓存友好的计算。</strong>二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从 L1/L2/L3 缓存获得性能的提升（下文会详细解释）。</li></ol><h2 id="为-Flink-量身定制的序列化框架"><a href="#为-Flink-量身定制的序列化框架" class="headerlink" title="为 Flink 量身定制的序列化框架"></a>为 Flink 量身定制的序列化框架</h2><p>目前 Java 生态圈提供了众多的序列化框架：Java serialization, Kryo, Apache Avro 等等。但是 Flink 实现了自己的序列化框架。因为在 Flink 中处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。</p><p>Flink支持任意的Java或是Scala类型。Flink 在数据类型上有很大的进步，不需要实现一个特定的接口（像Hadoop中的<code>org.apache.hadoop.io.Writable</code>），Flink 能够自动识别数据类型。Flink 通过 Java Reflection 框架分析基于 Java 的 Flink 程序 UDF (User Define Function)的返回类型的类型信息，通过 Scala Compiler 分析基于 Scala 的 Flink 程序 UDF 的返回类型的类型信息。类型信息由 <code>TypeInformation</code> 类表示，TypeInformation 支持以下几种类型：</p><ul><li><code>BasicTypeInfo</code>: 任意Java 基本类型（装箱的）或 String 类型。</li><li><code>BasicArrayTypeInfo</code>: 任意Java基本类型数组（装箱的）或 String 数组。</li><li><code>WritableTypeInfo</code>: 任意 Hadoop Writable 接口的实现类。</li><li><code>TupleTypeInfo</code>: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现。</li><li><code>CaseClassTypeInfo</code>: 任意的 Scala CaseClass(包括 Scala tuples)。</li><li><code>PojoTypeInfo</code>: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法。</li><li><code>GenericTypeInfo</code>: 任意无法匹配之前几种类型的类。</li></ul><p>前六种数据类型基本上可以满足绝大部分的Flink程序，针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。对于最后一种数据类型，Flink会使用Kryo进行序列化和反序列化。每个TypeInformation中，都包含了serializer，类型会自动通过serializer进行序列化，然后用Java Unsafe接口写入MemorySegments。对于可以用作key的数据类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据进行compare、hash等操作。对于 Tuple、CaseClass、POJO 等组合类型，其TypeSerializer和TypeComparator也是组合的，序列化和比较时会委托给对应的serializers和comparators。如下图展示 一个内嵌型的Tuple3&lt;Integer,Double,Person&gt; 对象的序列化过程。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1lvdbJFXXXXa9XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1lvdbJFXXXXa9XVXXXXXXXXXX" alt="img"></a></p><p>可以看出这种序列化方式存储密度是相当紧凑的。其中 int 占4字节，double 占8字节，POJO多个一个字节的header，PojoSerializer只负责将header序列化进去，并委托每个字段对应的serializer对字段进行序列化。</p><p>Flink 的类型系统可以很轻松地扩展出自定义的TypeInformation、Serializer以及Comparator，来提升数据类型在序列化和比较时的性能。</p><h2 id="Flink-如何直接操作二进制数据"><a href="#Flink-如何直接操作二进制数据" class="headerlink" title="Flink 如何直接操作二进制数据"></a>Flink 如何直接操作二进制数据</h2><p>Flink 提供了如 group、sort、join 等操作，这些操作都需要访问海量数据。这里，我们以sort为例，这是一个在 Flink 中使用非常频繁的操作。</p><p>首先，Flink 会从 MemoryManager 中申请一批 MemorySegment，我们把这批 MemorySegment 称作 sort buffer，用来存放排序的数据。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1_hhgJFXXXXc2XFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1_hhgJFXXXXc2XFXXXXXXXXXX" alt="img"></a></p><p>我们会把 sort buffer 分成两块区域。一个区域是用来存放所有对象完整的二进制数据。另一个区域用来存放指向完整二进制数据的指针以及定长的序列化后的key（key+pointer）。如果需要序列化的key是个变长类型，如String，则会取其前缀序列化。如上图所示，当一个对象要加到 sort buffer 中时，它的二进制数据会被加到第一个区域，指针（可能还有key）会被加到第二个区域。</p><p>将实际的数据和指针加定长key分开存放有两个目的。第一，交换定长块（key+pointer）更高效，不用交换真实的数据也不用移动其他key和pointer。第二，这样做是缓存友好的，因为key都是连续存储在内存中的，可以大大减少 cache miss（后面会详细解释）。</p><p>排序的关键是比大小和交换。Flink 中，会先用 key 比大小，这样就可以直接用二进制的key比较而不需要反序列化出整个对象。因为key是定长的，所以如果key相同（或者没有提供二进制key），那就必须将真实的二进制数据反序列化出来，然后再做比较。之后，只需要交换key+pointer就可以达到排序的效果，真实的数据不用移动。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1f6BnJFXXXXbnXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1f6BnJFXXXXbnXFXXXXXXXXXX" alt="img"></a></p><p>最后，访问排序后的数据，可以沿着排好序的key+pointer区域顺序访问，通过pointer找到对应的真实数据，并写到内存或外部（更多细节可以看这篇文章 <a href="http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html" target="_blank" rel="noopener">Joins in Flink</a>）。</p><h2 id="缓存友好的数据结构和算法"><a href="#缓存友好的数据结构和算法" class="headerlink" title="缓存友好的数据结构和算法"></a>缓存友好的数据结构和算法</h2><p>随着磁盘IO和网络IO越来越快，CPU逐渐成为了大数据领域的瓶颈。从 L1/L2/L3 缓存读取数据的速度比从主内存读取数据的速度快好几个量级。通过性能分析可以发现，CPU时间中的很大一部分都是浪费在等待数据从主内存过来上。如果这些数据可以从 L1/L2/L3 缓存过来，那么这些等待时间可以极大地降低，并且所有的算法会因此而受益。</p><p>在上面讨论中我们谈到的，Flink 通过定制的序列化框架将算法中需要操作的数据（如sort中的key）连续存储，而完整数据存储在其他地方。因为对于完整的数据来说，key+pointer更容易装进缓存，这大大提高了缓存命中率，从而提高了基础算法的效率。这对于上层应用是完全透明的，可以充分享受缓存友好带来的性能提升。</p><h2 id="走向堆外内存"><a href="#走向堆外内存" class="headerlink" title="走向堆外内存"></a>走向堆外内存</h2><p>Flink 基于堆内存的内存管理机制已经可以解决很多JVM现存问题了，为什么还要引入堆外内存？</p><ol><li>启动超大内存（上百GB）的JVM需要很长时间，GC停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配Remaining Heap那一块），使得 TaskManager 扩展到上百GB内存不是问题。</li><li>高效的 IO 操作。堆外内存在写磁盘或网络传输时是 zero-copy，而堆内存的话，至少需要 copy 一次。</li><li>堆外内存是进程间共享的。也就是说，即使JVM进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink暂时没有利用起这个，不过未来很可能会去做）。</li></ol><p>但是强大的东西总是会有其负面的一面，不然为何大家不都用堆外内存呢。</p><ol><li>堆内存的使用、监控、调试都要简单很多。堆外内存意味着更复杂更麻烦。</li><li>Flink 有时需要分配短生命周期的 <code>MemorySegment</code>，这个申请在堆上会更廉价。</li><li>有些操作在堆内存上会快一点点。</li></ol><p>Flink用通过<code>ByteBuffer.allocateDirect(numBytes)</code>来申请堆外内存，用 <code>sun.misc.Unsafe</code> 来操作堆外内存。</p><p>基于 Flink 优秀的设计，实现堆外内存是很方便的。Flink 将原来的 <code>MemorySegment</code> 变成了抽象类，并生成了两个子类。<code>HeapMemorySegment</code> 和 <code>HybridMemorySegment</code>。从字面意思上也很容易理解，前者是用来分配堆内存的，后者是用来分配堆外内存<strong>和堆内存</strong>的。是的，你没有看错，后者既可以分配堆外内存又可以分配堆内存。为什么要这样设计呢？</p><p>首先假设<code>HybridMemorySegment</code>只提供分配堆外内存。在上述堆外内存的不足中的第二点谈到，Flink 有时需要分配短生命周期的 buffer，这些buffer用<code>HeapMemorySegment</code>会更高效。那么当使用堆外内存时，为了也满足堆内存的需求，我们需要同时加载两个子类。这就涉及到了 JIT 编译优化的问题。因为以前 <code>MemorySegment</code> 是一个单独的 final 类，没有子类。JIT 编译时，所有要调用的方法都是确定的，所有的方法调用都可以被去虚化（de-virtualized）和内联（inlined），这可以极大地提高性能（MemroySegment的使用相当频繁）。然而如果同时加载两个子类，那么 JIT 编译器就只能在真正运行到的时候才知道是哪个子类，这样就无法提前做优化。实际测试的性能差距在 2.7 被左右。</p><p>Flink 使用了两种方案：</p><p><strong>方案1：只能有一种 MemorySegment 实现被加载</strong></p><p>代码中所有的短生命周期和长生命周期的MemorySegment都实例化其中一个子类，另一个子类根本没有实例化过（使用工厂模式来控制）。那么运行一段时间后，JIT 会意识到所有调用的方法都是确定的，然后会做优化。</p><p><strong>方案2：提供一种实现能同时处理堆内存和堆外内存</strong></p><p>这就是 <code>HybridMemorySegment</code> 了，能同时处理堆与堆外内存，这样就不需要子类了。这里 Flink 优雅地实现了一份代码能同时操作堆和堆外内存。这主要归功于 <code>sun.misc.Unsafe</code>提供的一系列方法，如getLong方法：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sun<span class="selector-class">.misc</span><span class="selector-class">.Unsafe</span><span class="selector-class">.getLong</span>(Object reference, long offset)</span><br></pre></td></tr></table></figure><ul><li>如果reference不为空，则会取该对象的地址，加上后面的offset，从相对地址处取出8字节并得到 long。这对应了堆内存的场景。</li><li>如果reference为空，则offset就是要操作的绝对地址，从该地址处取出数据。这对应了堆外内存的场景。</li></ul><p>这里我们看下 <code>MemorySegment</code> 及其子类的实现。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 堆内存引用</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] heapMemory;</span><br><span class="line">  <span class="comment">// 堆外内存地址</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">long</span> address;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//堆内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = buffer;</span><br><span class="line">    <span class="keyword">this</span>.address = BYTE_ARRAY_BASE_OFFSET;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//堆外内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">long</span> offHeapAddress, <span class="keyword">int</span> size, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">this</span>.address = offHeapAddress;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> getLong(<span class="keyword">int</span> <span class="keyword">index</span>) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> pos = address + <span class="keyword">index</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">index</span> &gt;= <span class="number">0</span> &amp;&amp; pos &lt;= addressLimit - <span class="number">8</span>) &#123;</span><br><span class="line">      <span class="comment">// 这是我们关注的地方，使用 Unsafe 来操作 on-heap &amp; off-heap</span></span><br><span class="line">      <span class="keyword">return</span> UNSAFE.getLong(heapMemory, pos);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (address &gt; addressLimit) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"segment has been freed"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// index is in fact invalid</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 指向heapMemory的额外引用，用来如数组越界的检查</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] memory;</span><br><span class="line">  <span class="comment">// 只能初始化堆内存</span></span><br><span class="line">  HeapMemorySegment(<span class="keyword">byte</span>[] memory, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(Objects.requireNonNull(memory), owner);</span><br><span class="line">    <span class="keyword">this</span>.memory = memory;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HybridMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ByteBuffer offHeapBuffer;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆外内存初始化</span></span><br><span class="line">  HybridMemorySegment(ByteBuffer buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(checkBufferAndGetAddress(buffer), buffer.capacity(), owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = buffer;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆内存初始化</span></span><br><span class="line">  HybridMemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(buffer, owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，HybridMemorySegment 中的很多方法其实都下沉到了父类去实现。包括堆内堆外内存的初始化。<code>MemorySegment</code> 中的 <code>getXXX</code>/<code>putXXX</code> 方法都是调用了 unsafe 方法，可以说<code>MemorySegment</code>已经具有了些 Hybrid 的意思了。<code>HeapMemorySegment</code>只调用了父类的<code>MemorySegment(byte[] buffer, Object owner)</code>方法，也就只能申请堆内存。另外，阅读代码你会发现，许多方法（大量的 getXXX/putXXX）都被标记成了 final，两个子类也是 final 类型，为的也是优化 JIT 编译器，会提醒 JIT 这个方法是可以被去虚化和内联的。</p><p>对于堆外内存，使用 <code>HybridMemorySegment</code> 能同时用来代表堆和堆外内存。这样只需要一个类就能代表长生命周期的堆外内存和短生命周期的堆内存。既然<code>HybridMemorySegment</code>已经这么全能，为什么还要方案1呢？因为我们需要工厂模式来保证只有一个子类被加载（为了更高的性能），而且HeapMemorySegment比heap模式的HybridMemorySegment要快。</p><p>下方是一些性能测试数据，更详细的数据请参考<a href="http://flink.apache.org/news/2015/09/16/off-heap-memory.html#appendix-detailed-micro-benchmarks" target="_blank" rel="noopener">这篇文章</a>。</p><table><thead><tr><th>Segment</th><th>Time</th></tr></thead><tbody><tr><td>HeapMemorySegment, exclusive</td><td>1,441 msecs</td></tr><tr><td>HeapMemorySegment, mixed</td><td>3,841 msecs</td></tr><tr><td>HybridMemorySegment, heap, exclusive</td><td>1,626 msecs</td></tr><tr><td>HybridMemorySegment, off-heap, exclusive</td><td>1,628 msecs</td></tr><tr><td>HybridMemorySegment, heap, mixed</td><td>3,848 msecs</td></tr><tr><td>HybridMemorySegment, off-heap, mixed</td><td>3,847 msecs</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要总结了 Flink 面对 JVM 存在的问题，而在内存管理的道路上越走越深。从自己管理内存，到序列化框架，再到堆外内存。其实纵观大数据生态圈，其实会发现各个开源项目都有同样的趋势。比如最近炒的很火热的 Spark Tungsten 项目，与 Flink 在内存管理上的思想是及其相似的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：如何处理反压问题</title>
      <link href="/2018/05/05/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%8F%8D%E5%8E%8B%E9%97%AE%E9%A2%98/"/>
      <url>/2018/05/05/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%8F%8D%E5%8E%8B%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>流处理系统需要能优雅地处理反压（backpressure）问题。反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。</p><p>目前主流的流处理系统 Storm/JStorm/Spark Streaming/Flink 都已经提供了反压机制，不过其实现各不相同。</p><p>Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。具体实现可以看这个 JIRA <a href="https://github.com/apache/storm/pull/700" target="_blank" rel="noopener">STORM-886</a>。</p><p>JStorm 认为直接停止 Spout 的发送太过暴力，存在大量问题。当下游出现阻塞时，上游停止发送，下游消除阻塞后，上游又开闸放水，过了一会儿，下游又阻塞，上游又限流，如此反复，整个数据流会一直处在一个颠簸状态。所以 JStorm 是通过逐级降速来进行反压的，效果会较 Storm 更为稳定，但算法也更复杂。另外 JStorm 没有引入 Zookeeper 而是通过 TopologyMaster 来协调拓扑进入反压状态，这降低了 Zookeeper 的负载。</p><h2 id="Flink-中的反压"><a href="#Flink-中的反压" class="headerlink" title="Flink 中的反压"></a>Flink 中的反压</h2><p>那么 Flink 是怎么处理反压的呢？答案非常简单：Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。</p><p>Flink 在运行时主要由 <strong>operators</strong> 和 <strong>streams</strong> 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。</p><p>在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池（<code>LocalBufferPool</code>）来实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(<code>Buffer</code>)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p><p>在解释 Flink 的反压原理之前，我们必须先对 Flink 中网络传输的内存管理有个了解。</p><h3 id="网络传输中的内存管理"><a href="#网络传输中的内存管理" class="headerlink" title="网络传输中的内存管理"></a>网络传输中的内存管理</h3><p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 <code>Buffer</code> 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB14fLsHVXXXXXWXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB14fLsHVXXXXXWXFXXXXXXXXXX" alt="img"></a></p><ol><li>TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</li><li>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</li><li>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</li><li>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 <code>Buffer.recycle()</code> 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</li></ol><h3 id="反压的过程"><a href="#反压的过程" class="headerlink" title="反压的过程"></a>反压的过程</h3><p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1rCIvJpXXXXcKXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1rCIvJpXXXXcKXXXXXXXXXXXX" alt="img"></a></p><ol><li>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</li><li>记录被序列化到 buffer 中。</li><li>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</li></ol><p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p><p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p><p>这里我们需要注意两个场景：</p><ul><li>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</li><li>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</li></ul><p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p><h3 id="Netty-水位值机制"><a href="#Netty-水位值机制" class="headerlink" title="Netty 水位值机制"></a>Netty 水位值机制</h3><p>下方的代码是初始化 NettyServer 时配置的水位值参数。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认高水位值为2个buffer大小, 当接收端消费速度跟不上，发送端会立即感知到</span></span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, <span class="built_in">config</span>.getMemorySegmentSize() + <span class="number">1</span>);</span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, <span class="number">2</span> * <span class="built_in">config</span>.getMemorySegmentSize());</span><br></pre></td></tr></table></figure><p>当输出缓冲中的字节数超过了高水位值, 则 Channel.isWritable() 会返回false。当输出缓存中的字节数又掉到了低水位值以下, 则 Channel.isWritable() 会重新返回true。Flink 中发送数据的核心代码在 <code>PartitionRequestQueue</code> 中，该类是 server channel pipeline 的最后一层。发送数据关键代码如下所示。</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">writeAndFlushNextMessageIfPossible</span><span class="params">(<span class="keyword">final</span> Channel channel)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (fatalError) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Buffer buffer = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// channel.isWritable() 配合 WRITE_BUFFER_LOW_WATER_MARK </span></span><br><span class="line">    <span class="comment">// 和 WRITE_BUFFER_HIGH_WATER_MARK 实现发送端的流量控制</span></span><br><span class="line">    <span class="keyword">if</span> (channel.isWritable()) &#123;</span><br><span class="line">      <span class="comment">// 注意: 一个while循环也就最多只发送一个BufferResponse, 连续发送BufferResponse是通过writeListener回调实现的</span></span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (currentPartitionQueue == <span class="keyword">null</span> &amp;&amp; (currentPartitionQueue = queue.poll()) == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        buffer = currentPartitionQueue.getNextBuffer();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (buffer == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="comment">// 跳过这部分代码</span></span><br><span class="line">          ...</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 构造一个response返回给客户端</span></span><br><span class="line">          BufferResponse resp = <span class="keyword">new</span> BufferResponse(buffer, currentPartitionQueue.getSequenceNumber(), currentPartitionQueue.getReceiverId());</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (!buffer.isBuffer() &amp;&amp;</span><br><span class="line">              EventSerializer.fromBuffer(buffer, getClass().getClassLoader()).getClass() == EndOfPartitionEvent.class) &#123;</span><br><span class="line">            <span class="comment">// 跳过这部分代码。batch 模式中 subpartition 的数据准备就绪，通知下游消费者。</span></span><br><span class="line">            ...</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 将该response发到netty channel, 当写成功后, </span></span><br><span class="line">          <span class="comment">// 通过注册的writeListener又会回调进来, 从而不断地消费 queue 中的请求</span></span><br><span class="line">          channel.writeAndFlush(resp).addListener(writeListener);</span><br><span class="line"></span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    <span class="keyword">if</span> (buffer != <span class="keyword">null</span>) &#123;</span><br><span class="line">      buffer.recycle();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(t.getMessage(), t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当水位值降下来后（channel 再次可写），会重新触发发送函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">channelWritabilityChanged</span><span class="params">(ChannelHandlerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心发送方法中如果channel不可写，则会跳过发送。当channel再次可写后，Netty 会调用该Handle的 <code>channelWritabilityChanged</code> 方法，从而重新触发发送函数。</p><h3 id="反压实验"><a href="#反压实验" class="headerlink" title="反压实验"></a>反压实验</h3><p>另外，<a href="http://data-artisans.com/how-flink-handles-backpressure/" target="_blank" rel="noopener">官方博客</a>中为了展示反压的效果，给出了一个简单的实验。下面这张图显示了：随着时间的改变，生产者（黄色线）和消费者（绿色线）每5秒的平均吞吐与最大吞吐（在单一JVM中每秒达到8百万条记录）的百分比。我们通过衡量task每5秒钟处理的记录数来衡量平均吞吐。该实验运行在单 JVM 中，不过使用了完整的 Flink 功能栈。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1geTaHVXXXXcXXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1geTaHVXXXXcXXVXXXXXXXXXX" alt="img"></a></p><p>首先，我们运行生产task到它最大生产速度的60%（我们通过Thread.sleep()来模拟降速）。消费者以同样的速度处理数据。然后，我们将消费task的速度降至其最高速度的30%。你就会看到背压问题产生了，正如我们所见，生产者的速度也自然降至其最高速度的30%。接着，停止消费task的人为降速，之后生产者和消费者task都达到了其最大的吞吐。接下来，我们再次将消费者的速度降至30%，pipeline给出了立即响应：生产者的速度也被自动降至30%。最后，我们再次停止限速，两个task也再次恢复100%的速度。总而言之，我们可以看到：生产者和消费者在 pipeline 中的处理都在跟随彼此的吞吐而进行适当的调整，这就是我们希望看到的反压的效果。</p><h2 id="反压监控"><a href="#反压监控" class="headerlink" title="反压监控"></a>反压监控</h2><p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。如果一个 Task 因为反压而降速了，那么它会卡在向 <code>LocalBufferPool</code> 申请内存块上。那么这时候，该 Task 的 stack trace 就会长下面这样：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.Object</span><span class="selector-class">.wait</span>(Native Method)</span><br><span class="line">o<span class="selector-class">.a</span><span class="selector-class">.f</span>.[...]<span class="selector-class">.LocalBufferPool</span><span class="selector-class">.requestBuffer</span>(LocalBufferPool<span class="selector-class">.java</span>:<span class="number">163</span>)</span><br><span class="line">o<span class="selector-class">.a</span><span class="selector-class">.f</span>.[...]<span class="selector-class">.LocalBufferPool</span><span class="selector-class">.requestBufferBlocking</span>(LocalBufferPool<span class="selector-class">.java</span>:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1T3cJJpXXXXXLXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1T3cJJpXXXXXLXpXXXXXXXXXX" alt="img"></a></p><p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送<code>TriggerStackTraceSample</code>消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Flink 不需要一种特殊的机制来处理反压，因为 Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink底层原理：流处理基本概念</title>
      <link href="/2018/05/03/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/05/03/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="Dataflow-Programming"><a href="#Dataflow-Programming" class="headerlink" title="Dataflow Programming"></a>Dataflow Programming</h2><p>在讨论流处理的基本概念之前，我们首先介绍一下数据流编程（dataflow programming）的基本概念与术语。</p><h3 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h3><p>数据流程序一般由数据流图表示，数据流图描述了数据如何在操作之间流动。在数据流图中，节点被称为operator，代表计算；边代表数据依赖。</p><p>Operator是dataflow 应用中的基本单元，它们从输入消费数据，在之上执行计算，并生产数据提供给下一步处理。</p><p>没有输入的operators 称为数据源（data sources），没有输出的operator称为数据接收器（data sink）。一个dataflow graph 必须有至少一个data source以及一个data sink。例如：</p><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091510356-305611451.png" alt="img"></p><p>类似上图的dataflow graph 称为逻辑的（logical）数据流图，因为它们从高层的视角展示了计算逻辑。在执行时，逻辑图会被转换为物理图（physical dataflow graph），具体的执行逻辑会在物理数据流图中给出，如下图：</p><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091535522-555450626.png" alt="img"></p><p>例如，如果我们使用分布式处理引擎，每个operator可能有多个并行的任务跑在不同的物理机器上。逻辑图表示了执行的逻辑，而物理图表示了具体的任务。</p><h3 id="数据并行与任务并行"><a href="#数据并行与任务并行" class="headerlink" title="数据并行与任务并行"></a>数据并行与任务并行</h3><p>数据并行是指：将输入数据做partition，然后使用多个同样的task并行处理数据的子集。数据并行的意义在于将数据分散到多个计算节点上。</p><p>任务并行是指：有多个不同的task任务并行处理相同的或不同的数据。任务并行的意义在于更好的使用集群中的计算资源。</p><h3 id="数据交换策略"><a href="#数据交换策略" class="headerlink" title="数据交换策略"></a>数据交换策略</h3><p>数据交换策略定义了：在physical dataflow graph中，数据条目如何分发到task 中。下面是几种常见的数据交换策略：</p><ol><li>前向（forward）策略：从一个task发送数据到另一个接受task。如果两个task均在一个机器上，则可以避免网络传输</li><li>广播（broadcast）策略：数据发送到所有并行task中。此策略涉及到数据复制及网络传输，所以较为消耗资源</li><li>key-based 策略：根据key做partition，使具有相同key 的条目可以被同一个task处理</li><li>随机（random）策略：随机均匀分布数据到task中，均衡集群计算负载</li></ol><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091552451-1460818339.png" alt="img"></p><h2 id="并行流处理"><a href="#并行流处理" class="headerlink" title="并行流处理"></a>并行流处理</h2><p>在了解以上概念后，我们接下来讨论并行流处理。首先，我们定义数据流（data stream）：数据流是一个（可能）无限的事件序列。</p><h3 id="延迟与吞吐"><a href="#延迟与吞吐" class="headerlink" title="延迟与吞吐"></a>延迟与吞吐</h3><p>对于批处理应用，我们一般关注的是一个job的整个执行时间，或是处理引擎需要多长时间读数据、计算、以及写入结果。而流处理应用是持续运行的，并且输入数据可能是无限的，所以对于整个应用的执行时间其实并没有太多关注。但是，流处理程序在处理高频率的事件输入的同时，还必须要在输入数据后尽可能快的提供结果。我们使用延迟（latency）与吞吐（throughput）来衡量这个需求。</p><h3 id="延迟"><a href="#延迟" class="headerlink" title="延迟"></a>延迟</h3><p>延迟表示的是处理一个event所需要的时间。本质上，它是从：接受到event -&gt; 到处理完此event -&gt; 并在结果中有体现，这段时间。举个例子，假设你去咖啡店买咖啡，前面有人排队，在到你点完单后，店里会做咖啡，做好后叫号，然后你来取，取完后开始喝。这里的latency指的就是从你进咖啡店开始，一直到你喝到第一口咖啡的间隔时间。</p><p>在data streaming 中，latency由时间衡量，例如毫秒。根据application的不同，你可能会关注平均延迟、最高延迟、或是百分位数延迟（percentile latency）。例如：平均延迟为10ms，表示events平均在10ms内被处理。而百分位 95 的延迟为10ms表示的是有95% 的events在10ms内被处理。平均延迟值隐藏了处理延迟的分布，可能会难以定位问题。例如：如果咖啡师在为你准备咖啡时用光了牛奶，则你不得不去等待咖啡师去拿牛奶，这里你的咖啡会有更大的延迟，但是其他大部分用户并不会受到影响。</p><p>对于大部分流应用来说（例如系统告警、欺诈检测、网络监控等），保证低延迟至关重要。低延迟在流处理中是一个重要的特性，它是实现“实时”应用的基础。当前主流的流处理器（如Flink），可以提供低至几毫秒的延迟。相对而言，传统的批处理系统的延迟可一般会达到几分钟到几小时不等。在批处理中，首先需要的是将events收集为batch，然后再处理它。所以它的延迟取决于batch中最后一个event到达的时间，以及batch 的大小。真正的流处理并不引入这种延迟，所以可以实现真正的低延迟。在真正的流模型中，events在到达流系统后可以被立即处理，此时的延迟反应的是：在此event上执行的操作时间。</p><h3 id="吞吐"><a href="#吞吐" class="headerlink" title="吞吐"></a>吞吐</h3><p>吞吐用于衡量系统的处理能力：处理率。也就是说，它可以告诉我们，系统在每个时间片内可以处理多少个events。以咖啡店为例，如果咖啡店从早上7点开到晚上7点，每天服务600个客户，则它的平均吞吐为 50个顾客/每小时。在流系统中，我们需要延迟尽可能的低，而吞吐尽可能的高。</p><p>吞吐由每个时间单位内处理的evnets衡量。这里需要注意的是：处理速率取决于events的到达速率。低吞吐并不能完全说明系统性能低。在流系统中，一般希望确保系统最高能处理events的速率。也就是说，我们主要关心的是确定吞吐的峰值（peak throughput）：在系统处于最高负载时的性能极限。为了更好地理解顶峰吞吐（peak throughput），我们考虑一个流处理应用，它一开始并不接收任何输入，所以此时并不消耗任何系统资源。当第一个event到来时，它会立即（尽量）以最小的latency 处理。例如你是咖啡馆开门的第一个顾客，店员会立即为你去做咖啡。在理想情况下，你会希望随着更多events的进入，latency 可以保持较小值不发生太大的变动。然而，一旦输入的events到达某个速率，使得系统资源被完全使用时，就不得不开始缓存（buffering）events。拿咖啡店举例，在中午的时候，人流量会特别大，达到了咖啡店的顶峰，则这时候就需要开始排队了。这时候系统即达到了它的peak throughput，而更大的event rate只会使得latency变得更糟。如果系统继续以更高的速率接收输入（超过了它可以处理的速率），缓冲区可能会爆掉，并导致数据丢失。常规的解决方案是背压（backpressure），并有不同的策略去处理。</p><h3 id="延迟-vs-吞吐"><a href="#延迟-vs-吞吐" class="headerlink" title="延迟 vs 吞吐"></a>延迟 vs 吞吐</h3><p>在这里需要明确的是，延迟与吞吐并不是两个互相独立的指标。如果事件到达数据处理管道的事件较长，便无法保证高吞吐。类似的，如果系统的性能较低，则events 会被缓存并等待，直到系统有能力处理。</p><p>再次以咖啡店为例，首先比较好理解的是，在负载低的时候，可以达到很好的一个latency。例如咖啡店里你是第一个也是唯一的一个顾客。但是在咖啡店较忙的时候，顾客就需要排队等待，此时的latency即会增加。另外一个影响延迟的因素（并继而影响到吞吐）是处理一个事件的时间。例如咖啡店为每个顾客做咖啡所消耗的时间。假设在一个圣诞节，咖啡师需要在每杯咖啡上画一个圣诞老人。也就是说，每杯咖啡制作的时间会增加，导致每个顾客在咖啡店消耗更多的时间，最终使得整体吞吐下降。</p><p>那是否可以同时达到低延迟与高吞吐？在咖啡店的例子中，你可以招聘更有经验的咖啡师，让做咖啡的效率更高。这里主要考量的地方是：减少延迟以增加吞吐。如果一个系统执行的操作更快，则它就可以在同一时间内处理更多的event。另外的方法是招聘更多的咖啡师，让同一时间有更多的客户被服务到。在流处理管道中，通过使用多个stream并行处理events，在获取更低的延时的同时，也可以在同一时间内处理更多的events。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS 7 命令行静默安装部署oracle11g数据库</title>
      <link href="/2017/12/10/CentOS%207%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2oracle11g%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/2017/12/10/CentOS%207%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2oracle11g%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h2 id="准备Oracle-11g安装包"><a href="#准备Oracle-11g安装包" class="headerlink" title="准备Oracle 11g安装包"></a>准备Oracle 11g安装包</h2><p><a href="https://www.oracle.com/technetwork/database/enterprise-edition/downloads/index.html" target="_blank" rel="noopener">官方下载地址</a></p><h2 id="检查硬件需求"><a href="#检查硬件需求" class="headerlink" title="检查硬件需求"></a>检查硬件需求</h2><ol><li>查看系统物理内存,以下输出可以看出，有8G的内存，内存最低要求256M。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep MemTotal /proc/meminfo</span><br></pre></td></tr></table></figure><ol start="2"><li>查看交换空间大小,以下输出可以看出，有5G的交换空间，交换空间的最优设置与你物理内存大小相关，详细说明请参考安装文档</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep SwapTotal /proc/meminfo</span><br></pre></td></tr></table></figure><h2 id="本来的交换空间大小为0，所以重新设置，有以下步骤："><a href="#本来的交换空间大小为0，所以重新设置，有以下步骤：" class="headerlink" title="本来的交换空间大小为0，所以重新设置，有以下步骤："></a>本来的交换空间大小为0，所以重新设置，有以下步骤：</h2><p>关闭swap：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo swapoff -a</span><br></pre></td></tr></table></figure><p>设置swap的大小：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudd dd if=/dev/zero of=/swapfile bs=1M count=5120</span><br></pre></td></tr></table></figure><p>bs指的是Block Size，就是每一块的大小。这里的例子是1M，意思就是count的数字，是以1M为单位的。<br>count是告诉程序，新的swapfile要多少个block。这里是1024，就是说，新的swap文件是5G大小。<br>注意：有些公司的权限需要重新输入密码，而我们就是这样，输入后会看见卡在那里没动，请耐心等待，机器不一样，等待时间也不一样。</p><p>把增大后的文件变为swap文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkswap /swapfile</span><br></pre></td></tr></table></figure><p>重新打开swap：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo swapon  /swapfile</span><br></pre></td></tr></table></figure><p>让swap在启动的时候，自动生效。打开/etc/fstab文件，加上以下命令。然后保存。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc</span><br><span class="line">sudo vim fstab</span><br></pre></td></tr></table></figure><p>因为我的权限不是root权限，所以输入命令前必须加sudo才可以修改资料</p><p>附Linux编辑文件命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vi打开一个文件时，进入的是阅读模式，只有输入相关命令才会进入编辑模式：</span><br><span class="line">i :在当前位置插入</span><br><span class="line">a:在当前位置后追加</span><br><span class="line">o:在当前位置的后面插入一行</span><br><span class="line">I :在行头插入</span><br><span class="line">A:在行尾追加</span><br><span class="line">O:在当前位置的前面插入一行</span><br><span class="line">'ESC'键从编辑模式转换到阅读模式</span><br><span class="line">阅读模式（或叫命令模式）下：</span><br><span class="line">:w 保存文件</span><br><span class="line">:w filename 保存成filename文件</span><br><span class="line">:q 退出</span><br><span class="line">:q! 强行退出</span><br><span class="line">:w! 强行写</span><br><span class="line">:wq 保存退出</span><br><span class="line">:x 同wq</span><br></pre></td></tr></table></figure><p>在fstab文件加入这行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/swapfile swap swap default 0 0</span><br></pre></td></tr></table></figure><p>保存退出，再次查swap大小,就发现变成5g了</p><h2 id="查当前发行版本检查并安装依赖包："><a href="#查当前发行版本检查并安装依赖包：" class="headerlink" title="查当前发行版本检查并安装依赖包："></a>查当前发行版本检查并安装依赖包：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/redhat-release</span><br></pre></td></tr></table></figure><p>发现是7.2的，需要安装包如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> binutils-2.23.52.0.1-12.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> compat-libcap1-1.10-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gcc-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gcc-c++-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-2.17-36.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-2.17-36.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-devel-2.17-36.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-devel-2.17-36.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ksh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-0.3.109-9.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-0.3.109-9.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-devel-0.3.109-9.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-devel-0.3.109-9.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libgcc-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libgcc-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-devel-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-devel-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXi-1.7.2-1.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXi-1.7.2-1.el7.x86_64 libXtst-1.2.2-1.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXtst-1.2.2-1.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> make-3.82-19.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sysstat-10.1.5-1.el7.x86_64</span></span><br></pre></td></tr></table></figure><p>我安装的版本是有安装包的，所以版本不一样</p><p>检查安装oracle11g所需要的安装包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q install binutils compat-libcap1  gcc gcc-c++ glibc glibc glibc-devel glibc-devel ksh libaio libaio libaio-devel libaio-devel libgcc libstdc++ libstdc++ libstdc++-devel libstdc++-devel libXi libXi libXtst libXtst sysstat</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libstdc++ libstdc++-devel libXi libXtst  make sysstat  unixODBC unixODBC-devel</span><br></pre></td></tr></table></figure><p>单独检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q 包名</span><br></pre></td></tr></table></figure><p>单独安装： </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y 包名</span><br></pre></td></tr></table></figure><p>注意：安装 elfutils-libelf-devel 时候，因为存在互相依存关系，需要2个同时安装（这个我也是参考别人的，我也没试过怎么 搞，所以我也直接用下面的几条安装命令，安装比较多的包，再安装剩下单独）。</p><p>多包安装：</p><p><strong>命令(强烈推荐使用yum安装)：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y binutils compat-libcap1 gcc gcc-c++ glibc glibc glibc-devel glibc-devel ksh libaio libaio libaio-devel libaio-devel libgcc libstdc++ libstdc++ libstdc++-devel libstdc++-devel libXi libXi libXtst libXtst sysstat</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libstdc++ libstdc++-devel libXi libXtst  make sysstat  unixODBC unixODBC-devel</span><br></pre></td></tr></table></figure><p>发现下图最后ksh没安装就可以使用单独安装命令，以上命令没有权限时加 sudo 或者登录root权限安装，因地而异。</p><p>发现找不到ksh这个包，报以下错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile No package ksh available.</span><br></pre></td></tr></table></figure><p>只好查百度了，找了很久有很多方法，我这边用的是下载的方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/7/os/x86_64/Packages/ksh-20120801-137.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>附上下载jar包地址的：</p><p><a href="https://altlinux.pkgs.org/sisyphus/classic-x86_64/pdksh-5.2.14-alt5.x86_64.rpm.html" target="_blank" rel="noopener">https://altlinux.pkgs.org/sisyphus/classic-x86_64/pdksh-5.2.14-alt5.x86_64.rpm.html</a><br>接着是安装这个包，进入这个有这个包的目录，wget命令默认下载的文件放在当前目录,附上Linux命令大全地址：</p><p><a href="http://man.linuxde.net/wget" target="_blank" rel="noopener">http://man.linuxde.net/wget</a><br>接着安装这个包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install ksh-20120801-137.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><h2 id="安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹："><a href="#安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹：" class="headerlink" title="安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹："></a>安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据库配置文件目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/oraInventory</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库软件包解压目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/database</span><br></pre></td></tr></table></figure><p>检查文件夹是否创建，进入data目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /oracledata/data</span><br></pre></td></tr></table></figure><h2 id="接着是创建oracle用户组和用户："><a href="#接着是创建oracle用户组和用户：" class="headerlink" title="接着是创建oracle用户组和用户："></a>接着是创建oracle用户组和用户：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysdba</span></span><br><span class="line">sudo groupadd oraclesysdba</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysoinstall </span></span><br><span class="line">sudo groupadd oraclesysoinstall</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysoper</span></span><br><span class="line">sudo groupadd oraclesysoper</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建oracle用户，并加入到oraclesysoinstall用户组</span></span><br><span class="line">sudo useradd -g oraclesysoinstall -G oraclesysdba,oraclesysoper oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置oracle用户的登陆密码，需要确认一次，注意两次密码要一样(注意：此处的密码是linux端oracle用户登录密码)</span></span><br><span class="line">sudo passwd oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看创建的用户：</span></span><br><span class="line">id oracle</span><br></pre></td></tr></table></figure><p>为啥要创建三个用户组呢？参考：<a href="http://www.oracle.com/technetwork/cn/articles/hunter-rac11gr2-iscsi-2-092412-zhs.html#13" target="_blank" rel="noopener">http://www.oracle.com/technetwork/cn/articles/hunter-rac11gr2-iscsi-2-092412-zhs.html#13</a></p><p>a.oracle 清单组（一般为oinstall):<br> OINSTALL 组的成员被视为 Oracle 软件的“所有者”，拥有对 Oracle 中央清单 (oraInventory) 的写入权限。在一个 Linux 系统上首次安装 Oracle 软件时，OUI 会创建 /etc/oraInst.loc 文件。该文件指定 Oracle 清单组的名称（默认为 oinstall）以及 Oracle 中央清单目录的路径。<br>b.数据库管理员（OSDBA，一般为 dba）:<br> OSDBA 组的成员可通过操作系统身份验证使用 SQL 以 SYSDBA 身份连接到一个 Oracle 实例。该组的成员可执行关键的数据库管理任务，如创建数据库、启动和关闭实例。该组的默认名称为dba。SYSDBA 系统权限甚至在数据库未打开时也允许访问数据库实例。对此权限的控制完全超出了数据库本身的范围。不要混淆 SYSDBA 系统权限与数据库角色 DBA。DBA 角色不包括 SYSDBA 或 SYSOPER 系统权限。<br>c.数据库操作员组（OSOPER，一般为 oper）:<br> OSOPER 组的成员可通过操作系统身份验证使用 SQL 以 SYSOPER 身份连接到一个 Oracle 实例。这个可选组的成员拥有一组有限的数据库管理权限，如管理和运行备份。该组的默认名称为oper。SYSOPER 系统权限甚至在数据库未打开时也允许访问数据库实例。对此权限的控制完全超出了数据库本身的范围。</p><h2 id="设置目录所有者为oraclesysoinstall-用户组的oracle用户"><a href="#设置目录所有者为oraclesysoinstall-用户组的oracle用户" class="headerlink" title="设置目录所有者为oraclesysoinstall 用户组的oracle用户"></a>设置目录所有者为oraclesysoinstall 用户组的oracle用户</h2><p>进入到data的目录，敲以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/oracle　</span><br><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/oraInventory</span><br><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/database</span><br></pre></td></tr></table></figure><h2 id="修改内核参数，以便支持oracle"><a href="#修改内核参数，以便支持oracle" class="headerlink" title="修改内核参数，以便支持oracle"></a>修改内核参数，以便支持oracle</h2><p>进入/etc/sysctl.conf：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc</span><br><span class="line">sudo vim sysctl.conf</span><br></pre></td></tr></table></figure><p>在最后增加上以下参数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kernel<span class="selector-class">.shmall</span> = <span class="number">2097152</span></span><br><span class="line">kernel<span class="selector-class">.shmmax</span> = <span class="number">2147483648</span></span><br><span class="line">kernel<span class="selector-class">.shmmni</span> = <span class="number">4096</span></span><br><span class="line">kernel<span class="selector-class">.sem</span> = <span class="number">250</span> <span class="number">32000</span> <span class="number">100</span> <span class="number">128</span></span><br><span class="line">fs<span class="selector-class">.file-max</span> = <span class="number">65536</span></span><br><span class="line">net<span class="selector-class">.ipv4</span><span class="selector-class">.ip_local_port_range</span> = <span class="number">1024</span> <span class="number">65000</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.rmem_default</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.rmem_max</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.wmem_default</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.wmem_max</span>=<span class="number">262144</span></span><br></pre></td></tr></table></figure><p>执行如下命令使更改的内核生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /sbin/sysctl -p</span><br></pre></td></tr></table></figure><h2 id="修改用户的限制："><a href="#修改用户的限制：" class="headerlink" title="修改用户的限制："></a>修改用户的限制：</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/security/limits.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure><p>在limits.conf文件中， 使用vim进行编辑，在最后增加上以下参数：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">oracle soft nproc <span class="number">2047</span>  </span><br><span class="line">oracle hard nproc <span class="number">16384</span>  </span><br><span class="line">oracle soft nofile <span class="number">1024</span>  </span><br><span class="line">oracle hard nofile <span class="number">65536</span>  </span><br><span class="line">oracle soft stack <span class="number">10240</span></span><br></pre></td></tr></table></figure><p>接着在文件/etc/pam.d/login中修改，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/pam.d/login</span><br></pre></td></tr></table></figure><p>在最后添加以下内容:</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session required /<span class="class"><span class="keyword">lib</span>/<span class="title">security</span>/<span class="title">pam_limits</span>.<span class="title">so</span></span></span><br><span class="line">session required pam_limits.so</span><br></pre></td></tr></table></figure><p>最后在etc/profile添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="variable">$USER</span> = “oracle” ];<span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$SHELL</span> = “/bin/ksh”];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">ulimit</span> -p 16384</span><br><span class="line"><span class="built_in">ulimit</span> -n 65536</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">ulimit</span> -u 16384 -n 65536</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>生效命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="设置环境变量："><a href="#设置环境变量：" class="headerlink" title="设置环境变量："></a>设置环境变量：</h2><p>切记，一定要切换到oracle用户(切换用户一定要是 su-)，然后执行以下命令</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~<span class="string">/.bash_profile</span></span><br></pre></td></tr></table></figure><p>增加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">export ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line">export ORACLE_SID=dbsrv2</span><br></pre></td></tr></table></figure><p>然后使之生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="接着是解压安装包"><a href="#接着是解压安装包" class="headerlink" title="接着是解压安装包"></a>接着是解压安装包</h2><p>安装zip和unzip组件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y unzip zip</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/7/os/x86_64/Packages/zip-3.0-11.el7.x86_64.rpm  //zip</span><br><span class="line">wget http://downloads.naulinux.ru/pub/SLCE/7x/x86_64/CyrEd/RPMS//unzip-6.0-15.1.el7.x86_64.rpm </span><br><span class="line">sudo yum install zip-3.0-11.el7.x86_64.rpm</span><br><span class="line">sudo yum install unzip-6.0-15.1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>unzip 压缩文件 -c 指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unzip p13390677_112040_Linux-x86-64_1of7.zip  -d  /oracledata/data/oraInventory</span><br><span class="line">unzip p13390677_112040_Linux-x86-64_2of7.zip  -d  /oracledata/data/oraInventory</span><br></pre></td></tr></table></figure><p>看到Complete就完成了，然后切换到oracle用户，cd进入解压目录，解压oracle安装包：</p><h2 id="接着是关闭防火墙和selinux"><a href="#接着是关闭防火墙和selinux" class="headerlink" title="接着是关闭防火墙和selinux"></a>接着是关闭防火墙和selinux</h2><p>关闭防火墙是为了其他客户端能够访问到oracle</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看防火墙状态</span></span><br><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure><p>接着是关闭selinux，查看selinux状态：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usr<span class="regexp">/sbin/</span>sestatus -v</span><br></pre></td></tr></table></figure><p>已经是关闭状态，如果是enforcing ，输入以下命令：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/<span class="built_in">config</span></span><br></pre></td></tr></table></figure><p>然后进行修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled  //将SELINUX=enforcing 此处修改为SELINUX=disabled</span><br></pre></td></tr></table></figure><p>需重启系统生效</p><h2 id="修改响应文件模板"><a href="#修改响应文件模板" class="headerlink" title="修改响应文件模板"></a>修改响应文件模板</h2><p>1、复制响应文件模板</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/oracle/etc</span><br><span class="line">cp /oracledata/data/oraInventory/database/response/* /home/oracle/etc</span><br></pre></td></tr></table></figure><p>2、设置响应文件权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 700 /home/oracle/etc/*.rsp</span><br></pre></td></tr></table></figure><h2 id="修改安装Oracle软件的响应文件-oracledata-data-database-etc-db-install-rsp"><a href="#修改安装Oracle软件的响应文件-oracledata-data-database-etc-db-install-rsp" class="headerlink" title="修改安装Oracle软件的响应文件/oracledata/data/database/etc/db_install.rsp"></a>修改安装Oracle软件的响应文件/oracledata/data/database/etc/db_install.rsp</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim <span class="regexp">/home/</span>oracle<span class="regexp">/etc/</span>db_install.rsp</span><br></pre></td></tr></table></figure><p>按照下面修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装类型</span></span><br><span class="line">oracle.install.option=INSTALL_DB_SWONLY</span><br><span class="line"><span class="meta">#</span><span class="bash"> 主机名称（hostname查询）</span></span><br><span class="line">ORACLE_HOSTNAME=cdh01</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装组</span></span><br><span class="line">UNIX_GROUP_NAME=oraclesysoinstall</span><br><span class="line"><span class="meta">#</span><span class="bash"> INVENTORY中央库存目录（不填就是默认值）</span></span><br><span class="line">INVENTORY_LOCATION=/home/oracle/oraInventory</span><br><span class="line"><span class="meta">#</span><span class="bash"> 选择语言</span></span><br><span class="line">SELECTED_LANGUAGES=en,zh_CN,zh_TW</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle_home</span></span><br><span class="line">ORACLE_HOME=/oracledata/data/oracle/product/11.2.0.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle_base</span></span><br><span class="line">ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle版本</span></span><br><span class="line">oracle.install.db.InstallEdition=EE</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义安装，否，使用默认组件</span></span><br><span class="line">oracle.install.db.isCustomInstall=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> dba用户组</span></span><br><span class="line">oracle.install.db.DBA_GROUP= oraclesysdba</span><br><span class="line"><span class="meta">#</span><span class="bash"> oper用户组</span></span><br><span class="line">oracle.install.db.OPER_GROUP=oraclesysoper</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据库类型</span></span><br><span class="line">oracle.install.db.config.starterdb.type=GENERAL_PURPOSE</span><br><span class="line"><span class="meta">#</span><span class="bash"> globalDBName</span></span><br><span class="line">oracle.install.db.config.starterdb.globalDBName=orcl</span><br><span class="line"><span class="meta">#</span><span class="bash"> SID</span></span><br><span class="line">oracle.install.db.config.starterdb.SID=dbsrv2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自动管理内存的内存(M)</span></span><br><span class="line">oracle.install.db.config.starterdb.memoryLimit=81920</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设定所有数据库用户使用同一个密码</span></span><br><span class="line">oracle.install.db.config.starterdb.password.ALL=123456</span><br><span class="line"><span class="meta">#</span><span class="bash">（手动写了<span class="literal">false</span>）</span></span><br><span class="line">SECURITY_UPDATES_VIA_MYORACLESUPPORT=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置安全更新（貌似是有bug，这个一定要选<span class="literal">true</span>，否则会无限提醒邮件地址有问题，终止安装。）</span></span><br><span class="line">DECLINE_SECURITY_UPDATES=true</span><br></pre></td></tr></table></figure><h2 id="开始安装："><a href="#开始安装：" class="headerlink" title="开始安装："></a>开始安装：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 重启系统，保证所有配置完成</span></span><br><span class="line">sudo reboot</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接着登录到oracle用户，进入解压后的data目录</span></span><br><span class="line">cd /oracledata/data/oraInventory/database</span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行安装</span></span><br><span class="line">./runInstaller -silent -force -ignorePrereq -responseFile /home/oracle/etc/db_install.rsp</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接下来需要等一会儿，时间略长，出现提示信息后安装成功，告知要新开会话用root用户执行脚本</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 新开会话切换用户root或者其他用户可以用sudo的，我的就是用sudo的，根据提示输入两个脚本：</span></span><br><span class="line">sudo /home/oracle/oraInventory/orainstRoot.sh</span><br><span class="line">sudo /oracledata/data/oracle/product/11.2.0.1/root.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后切回原会话按Enter即可</span></span><br></pre></td></tr></table></figure><p>接下来是增加或者修改环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">export ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库路径</span></span><br><span class="line">export ORACLE_HOME=/oracledata/data/oracle/product/11.2.0.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle启动数据库实例名</span></span><br><span class="line">export ORACLE_SID=dbsrv2</span><br><span class="line">export ROACLE_PID=ora11g</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加系统环境变量</span></span><br><span class="line">export PATH=$PATH:$ORACLE_HOME/bin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加系统环境变量</span></span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib</span><br><span class="line"><span class="meta">#</span><span class="bash"> 防止安装过程出现乱码</span></span><br><span class="line">export LANG=C</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Oracle客户端字符集，必须与Oracle安装时设置的字符集保持一致</span></span><br><span class="line">export NLS_LANG= "AL32UTF8"</span><br><span class="line">export NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'</span><br></pre></td></tr></table></figure><p>保存退出,使其生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="配置监听程序"><a href="#配置监听程序" class="headerlink" title="配置监听程序"></a>配置监听程序</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netca /silent /responsefile /home/oracle/etc/netca.rsp</span><br></pre></td></tr></table></figure><h2 id="启动监控程序"><a href="#启动监控程序" class="headerlink" title="启动监控程序"></a>启动监控程序</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsnrctl start</span><br></pre></td></tr></table></figure><p>发现监听已经启动好了</p><h2 id="静默dbca建库"><a href="#静默dbca建库" class="headerlink" title="静默dbca建库"></a>静默dbca建库</h2><p>编辑应答文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /home/oracle/etc/dbca.rsp</span><br></pre></td></tr></table></figure><p>内容如下</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">RESPONSEFILE_VERSION</span> = <span class="string">"11.2.0"</span></span><br><span class="line"><span class="attr">OPERATION_TYPE</span> = <span class="string">"createDatabase"</span></span><br><span class="line"><span class="attr">GDBNAME</span> = <span class="string">"dbsrv2"</span></span><br><span class="line"><span class="attr">SID</span> = <span class="string">"dbsrv2"</span></span><br><span class="line"><span class="attr">TEMPLATENAME</span> = <span class="string">"General_Purpose.dbc"</span></span><br></pre></td></tr></table></figure><p>建库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> dbca -silent -responseFile /home/oracle/etc/dbca.rsp （此命令无效以下命令代替）</span></span><br><span class="line">dbca -silent -createDatabase -templateName General_Purpose.dbc -gdbName dbsrv2 -sysPassword 123456 -systemPassword 123456</span><br></pre></td></tr></table></figure><p>完成后查看输出日记：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /oracledata/data/oracle/cfgtoollogs/dbca/dbsrv2/dbsrv2.log</span><br></pre></td></tr></table></figure><p> 至此完成数据库实例的创建。</p><p>查看监听状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsnrctl status</span><br></pre></td></tr></table></figure><p>输入命令连接数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>在sqlplus终端输入sql命令启动数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">startup</span><br></pre></td></tr></table></figure><p>发现已经启动实例</p><p>如何退出sqlpuls</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quit 或 exit</span><br></pre></td></tr></table></figure><p>如何删除实例（清楚自己做什么的情况下再执行此命令）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbca -silent -deleteDatabase -sourcedb dbsrv2</span><br></pre></td></tr></table></figure><h2 id="创建可以外部访问的数据库"><a href="#创建可以外部访问的数据库" class="headerlink" title="创建可以外部访问的数据库"></a>创建可以外部访问的数据库</h2><p>输入：sqlplus “/as sysdba” (此处是用dba身份登录数据库，系统的超级用户)</p><p>1、创建临时表空间：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir /oracledata/data/oracle/tablespace</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">tablespace</span> dp_test tempfile <span class="string">'/oracledata/data/oracle/tablespace/dp_test.dbf'</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">size</span> <span class="number">1024</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">autoextend</span> <span class="keyword">on</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">next</span> <span class="number">100</span>m <span class="keyword">maxsize</span> <span class="number">10240</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">extent</span> <span class="keyword">management</span> <span class="keyword">local</span>; 点击Enter</span><br></pre></td></tr></table></figure><p>说明：</p><p>1) dp_test是临时表空间的名字</p><p>2) /oracledata/data/oracle/tablespace/dp_test.dbf是在/oracledata/data/oracle/tablespace下建一个名为dp_test.dbf的表(注意：单引号为英文状态下的输入)，</p><p>3) 1024m是表空间初始大小，</p><p>4) 100m是表空间自动增长大小，</p><p>5) 10240m是表空间最大的大小。</p><p>2、创建数据表空间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">tablespace</span> dp <span class="keyword">logging</span> <span class="keyword">datafile</span> <span class="string">'/oracledata/data/oracle/tablespace/dp.dbf'</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">size</span> <span class="number">1024</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">autoextend</span> <span class="keyword">on</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">next</span> <span class="number">100</span>m <span class="keyword">maxsize</span> <span class="number">10240</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">extent</span> <span class="keyword">management</span> <span class="keyword">local</span>; 点击Enter</span><br></pre></td></tr></table></figure><p>3、创建用户并指定表空间</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create<span class="built_in"> user </span>dp identified by dp123<span class="built_in"> default </span>tablespace dp temporary tablespace dp_test;</span><br></pre></td></tr></table></figure><p>其中dp为用户名，dp123为用户密码，dp_test是临时表空间的名字。</p><p>4、给用户授予权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> dba <span class="keyword">to</span> dp;</span><br></pre></td></tr></table></figure><p>至此，oracle在centos7下的安装和配置也就完成了，别人已经可以访问你的数据库了</p><h2 id="修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8"><a href="#修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8" class="headerlink" title="修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8"></a>修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">采用的是操作系统默认字符集：WE8MSWIN1252，将字符集修改为：AL32UTF8。</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select userenv(<span class="string">'language'</span>) from dual;</span></span><br><span class="line"></span><br><span class="line">SIMPLIFIED CHINESE_CHINA.WE8MSWIN1252</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select * from nls_database_parameters <span class="built_in">where</span> parameter <span class="keyword">in</span> (<span class="string">'NLS_CHARCTERSET'</span>,<span class="string">'NLS_NCHAR_CHARACTERSET'</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select* from v<span class="variable">$nls_parameters</span> <span class="built_in">where</span> parameter=<span class="string">'NLS_CHARACTERSET'</span>;</span></span><br><span class="line"></span><br><span class="line">操作过程如下：</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter session <span class="built_in">set</span> sql_trace=<span class="literal">true</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">enable</span> restricted session;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> show parameter job_queue_processes;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">set</span> job_queue_processes=0;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">set</span> aq_tm_processes=0;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database character <span class="built_in">set</span> INTERNAL_USE AL32UTF8;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> update props$ <span class="built_in">set</span> VALUE$=<span class="string">'UTF8'</span> <span class="built_in">where</span> NAME=<span class="string">'NLS_NCHAR_CHARACTERSET'</span>;</span></span><br><span class="line"></span><br><span class="line">维护完以后需要</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">ALTER SYSTEM DISABLE RESTRICTED SESSION;</span></span><br><span class="line"></span><br><span class="line">改变字符集后，原来已有的数据不会改变，只是之后新增的数据会是新的字符集。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解JVM</title>
      <link href="/2017/11/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM/"/>
      <url>/2017/11/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM/</url>
      
        <content type="html"><![CDATA[<h2 id="Java运行时数据区："><a href="#Java运行时数据区：" class="headerlink" title="Java运行时数据区："></a>Java运行时数据区：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Java虚拟机在执行Java程序的过程中会将其管理的内存划分为若干个不同的数据区域，这些区域有各自的用途、创建和销毁的时间，有些区域随虚拟机进程的启动而存在，有些区域则是依赖用户线程的启动和结束来建立和销毁。Java虚拟机所管理的内存包括以下几个运行时数据区域，如图：</span><br></pre></td></tr></table></figure><img src="/2017/11/03/深入理解JVM/1.png"><p>1、程序计数器：指向当前线程正在执行的字节码指令。线程私有的。<br>2、虚拟机栈：虚拟机栈是Java执行方法的内存模型。每个方法被执行的时候，都会创建一个栈帧，把栈帧压人栈，当方法正常返回或者抛出未捕获的异常时，栈帧就会出栈。<br>（1）栈帧：栈帧存储方法的相关信息，包含局部变量数表、返回值、操作数栈、动态链接<br>a、局部变量表：包含了方法执行过程中的所有变量。局部变量数组所需要的空间在编译期间完成分配，在方法运行期间不会改变局部变量数组的大小。<br>b、返回值：如果有返回值的话，压入调用者栈帧中的操作数栈中，并且把PC的值指向 方法调用指令 后面的一条指令地址。<br>c、操作数栈：操作变量的内存模型。操作数栈的最大深度在编译的时候已经确定（写入方法区code属性的max_stacks项中）。操作数栈的的元素可以是任意Java类型，包括long和double，32位数据占用栈空间为1，64位数据占用2。方法刚开始执行的时候，栈是空的，当方法执行过程中，各种字节码指令往栈中存取数据。<br>d、动态链接：每个栈帧都持有在运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态链接。<br>（2）线程私有<br>3、本地方法栈：<br>（1）调用本地native的内存模型<br>（2）线程独享。<br>4、方法区：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据<br>（1）线程共享的<br>（2）运行时常量池：</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A、是方法区的一部分</span><br><span class="line">B、存放编译期生成的各种字面量和符号引用</span><br><span class="line">C、<span class="class"><span class="keyword">Class</span>文件中除了存有类的版本、字段、方法、接口等描述信息，还有一项是常量池，存有这个类的 编译期生成的各种字面量和符号引用，这部分内容将在类加载后，存放到方法区的运行时常量池中。</span></span><br></pre></td></tr></table></figure><p>5、堆（Heap）：Java对象存储的地方<br>（1）Java堆是虚拟机管理的内存中最大的一块<br>（2）Java堆是所有线程共享的区域<br>（3）在虚拟机启动时创建<br>（4）此内存区域的唯一目的就是存放对象实例，几乎所有对象实例都在这里分配内存。存放new生成的对象和数组<br>（5）Java堆是垃圾收集器管理的内存区域，因此很多时候称为“GC堆”</p><h2 id="JMM-Java内存模型："><a href="#JMM-Java内存模型：" class="headerlink" title="JMM Java内存模型："></a>JMM Java内存模型：</h2><p>1、 Java的并发采用“共享内存”模型，线程之间通过读写内存的公共状态进行通讯。多个线程之间是不能通过直接传递数据交互的，它们之间交互只能通过共享变量实现。<br>2、 主要目的是定义程序中各个变量的访问规则。<br>3、 Java内存模型规定所有变量都存储在主内存中，每个线程还有自己的工作内存。<br>（1） 线程的工作内存中保存了被该线程使用到的变量的拷贝（从主内存中拷贝过来），线程对变量的所有操作都必须在工作内存中执行，而不能直接访问主内存中的变量。<br>（2） 不同线程之间无法直接访问对方工作内存的变量，线程间变量值的传递都要通过主内存来完成。<br>（3） 主内存主要对应Java堆中实例数据部分。工作内存对应于虚拟机栈中部分区域。</p><img src="/2017/11/03/深入理解JVM/2.png"><p>4、Java线程之间的通信由内存模型JMM（Java Memory Model）控制。<br>（1）JMM决定一个线程对变量的写入何时对另一个线程可见。<br>（2）线程之间共享变量存储在主内存中<br>（3）每个线程有一个私有的本地内存，里面存储了读/写共享变量的副本。<br>（4）JMM通过控制每个线程的本地内存之间的交互，来为程序员提供内存可见性保证。<br>5、可见性、有序性：<br>（1）当一个共享变量在多个本地内存中有副本时，如果一个本地内存修改了该变量的副本，其他变量应该能够看到修改后的值，此为可见性。<br>（2）保证线程的有序执行，这个为有序性。（保证线程安全）<br>6、内存间交互操作：<br>（1）lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。<br>（2）unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。<br>（3）read（读取）：作用于主内存变量，把主内存的一个变量读取到工作内存中。<br>（4）load（载入）：作用于工作内存，把read操作读取到工作内存的变量载入到工作内存的变量副本中<br>（5）use（使用）：作用于工作内存的变量，把工作内存中的变量值传递给一个执行引擎。<br>（6）assign（赋值）：作用于工作内存的变量。把执行引擎接收到的值赋值给工作内存的变量。<br>（7）store（存储）：把工作内存的变量的值传递给主内存<br>（8）write（写入）：把store操作的值入到主内存的变量中<br>6.1、注意：<br>（1）不允许read、load、store、write操作之一单独出现<br>（2）不允许一个线程丢弃assgin操作<br>（3）不允许一个线程不经过assgin操作，就把工作内存中的值同步到主内存中<br>（4）一个新的变量只能在主内存中生成<br>（5）一个变量同一时刻只允许一条线程对其进行lock操作。但lock操作可以被同一条线程执行多次，只有执行相同次数的unlock操作，变量才会解锁<br>（6）如果对一个变量进行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或者assgin操作初始化变量的值。<br>（7）如果一个变量没有被锁定，不允许对其执行unlock操作，也不允许unlock一个被其他线程锁定的变量<br>（8）对一个变量执行unlock操作之前，需要将该变量同步回主内存中</p><h2 id="堆的内存划分："><a href="#堆的内存划分：" class="headerlink" title="堆的内存划分："></a>堆的内存划分：</h2><img src="/2017/11/03/深入理解JVM/3.png"><p>Java堆的内存划分如图所示，分别为年轻代、Old Memory（老年代）、Perm（永久代）。其中在Jdk1.8中，永久代被移除，使用MetaSpace代替。<br>1、新生代：<br>（1）使用复制清除算法（Copinng算法），原因是年轻代每次GC都要回收大部分对象。新生代里面分成一份较大的Eden空间和两份较小的Survivor空间。每次只使用Eden和其中一块Survivor空间，然后垃圾回收的时候，把存活对象放到未使用的Survivor（划分出from、to）空间中，清空Eden和刚才使用过的Survivor空间。<br>（2）分为Eden、Survivor From、Survivor To，比例默认为8：1：1<br>（3）内存不足时发生Minor GC<br>2、老年代：<br>（1）采用标记-整理算法（mark-compact），原因是老年代每次GC只会回收少部分对象。<br>3、Perm：用来存储类的元数据，也就是方法区。<br>（1）Perm的废除：在jdk1.8中，Perm被替换成MetaSpace，MetaSpace存放在本地内存中。原因是永久代进场内存不够用，或者发生内存泄漏。<br>（2）MetaSpace（元空间）：元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。<br>4、堆内存的划分在JVM里面的示意图：</p><img src="/2017/11/03/深入理解JVM/4.png"><h2 id="GC垃圾回收："><a href="#GC垃圾回收：" class="headerlink" title="GC垃圾回收："></a>GC垃圾回收：</h2><p>一、 判断对象是否要回收的方法：可达性分析法<br>1、 可达性分析法：通过一系列“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的。不可达对象不一定会成为可回收对象。进入DEAD状态的线程还可以恢复，GC不会回收它的内存。（把一些对象当做root对象，JVM认为root对象是不可回收的，并且root对象引用的对象也是不可回收的）<br>2、 以下对象会被认为是root对象：<br>（1） 虚拟机栈（栈帧中本地变量表）中引用的对象<br>（2） 方法区中静态属性引用的对象<br>（3） 方法区中常量引用的对象<br>（4） 本地方法栈中Native方法引用的对象<br>3、 对象被判定可被回收，需要经历两个阶段：<br>（1） 第一个阶段是可达性分析，分析该对象是否可达<br>（2） 第二个阶段是当对象没有重写finalize()方法或者finalize()方法已经被调用过，虚拟机认为该对象不可以被救活，因此回收该对象。（finalize()方法在垃圾回收中的作用是，给该对象一次救活的机会）<br>4、 方法区中的垃圾回收：<br>（1） 常量池中一些常量、符号引用没有被引用，则会被清理出常量池<br>（2） 无用的类：被判定为无用的类，会被清理出方法区。判定方法如下：<br>A、 该类的所有实例被回收<br>B、 加载该类的ClassLoader被回收<br>C、 该类的Class对象没有被引用<br>5、 finalize():<br>（1） GC垃圾回收要回收一个对象的时候，调用该对象的finalize()方法。然后在下一次垃圾回收的时候，才去回收这个对象的内存。<br>（2） 可以在该方法里面，指定一些对象在释放前必须执行的操作。</p><p>二、 发现虚拟机频繁full GC时应该怎么办：<br>（full GC指的是清理整个堆空间，包括年轻代和永久代）<br>（1） 首先用命令查看触发GC的原因是什么 jstat –gccause 进程id<br>（2） 如果是System.gc()，则看下代码哪里调用了这个方法<br>（3） 如果是heap inspection(内存检查)，可能是哪里执行jmap –histo[:live]命令<br>（4） 如果是GC locker，可能是程序依赖的JNI库的原因</p><p>三、常见的垃圾回收算法：<br>1、Mark-Sweep（标记-清除算法）：<br>（1）思想：标记清除算法分为两个阶段，标记阶段和清除阶段。标记阶段任务是标记出所有需要回收的对象，清除阶段就是清除被标记对象的空间。<br>（2）优缺点：实现简单，容易产生内存碎片<br>2、Copying（复制清除算法）：<br>（1）思想：将可用内存划分为大小相等的两块，每次只使用其中的一块。当进行垃圾回收的时候了，把其中存活对象全部复制到另外一块中，然后把已使用的内存空间一次清空掉。<br>（2）优缺点：不容易产生内存碎片；可用内存空间少；存活对象多的话，效率低下。<br>3、Mark-Compact（标记-整理算法）：<br>（1）思想：先标记存活对象，然后把存活对象向一边移动，然后清理掉端边界以外的内存。<br>（2）优缺点：不容易产生内存碎片；内存利用率高；存活对象多并且分散的时候，移动次数多，效率低下</p><p>4、分代收集算法：（目前大部分JVM的垃圾收集器所采用的算法）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">思想：把堆分成新生代和老年代。（永久代指的是方法区）</span><br></pre></td></tr></table></figure><p>（1） 因为新生代每次垃圾回收都要回收大部分对象，所以新生代采用Copying算法。新生代里面分成一份较大的Eden空间和两份较小的Survivor空间。每次只使用Eden和其中一块Survivor空间，然后垃圾回收的时候，把存活对象放到未使用的Survivor（划分出from、to）空间中，清空Eden和刚才使用过的Survivor空间。<br>（2） 由于老年代每次只回收少量的对象，因此采用mark-compact算法。<br>（3） 在堆区外有一个永久代。对永久代的回收主要是无效的类和常量<br>5、GC使用时对程序的影响？<br>垃圾回收会影响程序的性能，Java虚拟机必须要追踪运行程序中的有用对象，然后释放没用对象，这个过程消耗处理器时间<br>6、几种不同的垃圾回收类型：<br>（1）Minor GC：从年轻代（包括Eden、Survivor区）回收内存。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A、当<span class="keyword">JVM无法为一个新的对象分配内存的时候，越容易触发Minor </span>GC。所以分配率越高，内存越来越少，越频繁执行Minor GC</span><br><span class="line"><span class="keyword">B、执行Minor </span>GC操作的时候，不会影响到永久代（Tenured）。从永久代到年轻代的引用，被当成GC Roots，从年轻代到老年代的引用在标记阶段直接被忽略掉。</span><br></pre></td></tr></table></figure><p>（2）Major GC：清理整个老年代，当eden区内存不足时触发。<br>（3）Full GC：清理整个堆空间，包括年轻代和老年代。当老年代内存不足时触发</p><h2 id="HotSpot-虚拟机详解："><a href="#HotSpot-虚拟机详解：" class="headerlink" title="HotSpot 虚拟机详解："></a>HotSpot 虚拟机详解：</h2><p>1、 Java对象创建过程：<br>（1）虚拟机遇到一条new指令时，首先检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已经加载、连接和初始化。如果没有，就执行该类的加载过程。<br>（2）为该对象分配内存。<br>A、假设Java堆是规整的，所有用过的内存放在一边，空闲的内存放在另外一边，中间放着一个指针作为分界点的指示器。那分配内存只是把指针向空闲空间那边挪动与对象大小相等的距离，这种分配称为“指针碰撞”<br>B、假设Java堆不是规整的，用过的内存和空闲的内存相互交错，那就没办法进行“指针碰撞”。虚拟机通过维护一个列表，记录哪些内存块是可用的，在分配的时候找出一块足够大的空间分配给对象实例，并更新表上的记录。这种分配方式称为“空闲列表“。<br>C、使用哪种分配方式由Java堆是否规整决定。Java堆是否规整由所采用的垃圾收集器是否带有压缩整理功能决定。<br>D、分配对象保证线程安全的做法：虚拟机使用CAS失败重试的方式保证更新操作的原子性。（实际上还有另外一种方案：每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲，TLAB。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才进行同步锁定。虚拟机是否使用TLAB，由-XX:+/-UseTLAB参数决定）<br>（3）虚拟机为分配的内存空间初始化为零值（默认值）<br>（4）虚拟机对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到对象的元数据信息、对象的Hash码、对象的GC分代年龄等信息。这些信息存放在对象的对象头中。<br>（5） 执行<init>方法，把对象按照程序员的意愿进行初始化。<br>2、 对象的定位访问的方式（通过引用如何去定位到堆上的具体对象的位置）：<br>（1）句柄：使用句柄的方式，Java堆中将会划分出一块内存作为作为句柄池，引用中存储的就是对象的句柄的地址。而句柄中包含了对象实例数据和对象类型数据的地址。</init></p><img src="/2017/11/03/深入理解JVM/5.png"><p>（2）直接指针：使用直接指针的方式，引用中存储的就是对象的地址。Java堆对象的布局必须必须考虑如何去访问对象类型数据。</p><img src="/2017/11/03/深入理解JVM/6.png"><p>（3）两种方式各有优点：<br>A、使用句柄访问的好处是引用中存放的是稳定的句柄地址，当对象被移动（比如说垃圾回收时移动对象），只会改变句柄中实例数据指针，而引用本身不会被修改。<br>B、使用直接指针，节省了一次指针定位的时间开销。<br>3、HotSpot的GC算法实现：<br>（1）HotSpot怎么快速找到GC Root？<br>HotSpot使用一组称为OopMap的数据结构。在类加载完成的时候，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在栈和寄存器中哪些位置是引用。这样子，在GC扫描的时候，就可以直接知道哪些是可达对象了。<br>（2）安全点：<br>A、HotSpot只在特定的位置生成OopMap，这些位置称为安全点。<br>B、程序执行过程中并非所有地方都可以停下来开始GC，只有在到达安全点是才可以暂停。<br>C、安全点的选定基本上以“是否具有让程序长时间执行“的特征选定的。比如说方法调用、循环跳转、异常跳转等。具有这些功能的指令才会产生Safepoint。<br>（3）中断方式：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">A</span>、抢占式中断：在GC发生时，首先把所有线程中断，如果发现有线程不在安全点上，就恢复线程，让它跑到安全点上。</span><br><span class="line">B、主动式中断：GC需要中断线程时，不直接对线程操作，仅仅设置一个标志，各个线程执行时主动去轮询这个标志，当发现中断标记为真就自己中断挂起。轮询标记的地方和安全点是重合的。</span><br></pre></td></tr></table></figure><p>（5）安全区域：一段代码片段中，对象的引用关系不会发生变化，在这个区域中任何地方开始GC都是安全的。在线程进入安全区域时，它首先标志自己已经进入安全区域，在这段时间里，当JVM发起GC时，就不用管进入安全区域的线程了。在线程将要离开安全区域时，它检查系统是否完成了GC过程，如果完成了，它就继续前行。否则，它就必须等待直到收到可以离开安全区域的信号。<br>4、 GC时为什么要停顿所有Java线程？<br>因为GC先进行可达性分析。可达性分析是判断GC Root对象到其他对象是否可达，假如分析过程中对象的引用关系在不断变化，分析结果的准确性就无法得到保证。<br>5、 CMS收集器：<br>（1）一种以获取最短回收停顿时间为目标的收集器。<br>（2）一般用于互联网站或者B/S系统的服务端<br>（3）基于标记-清除算法的实现，不过更为复杂，整个过程为4个步骤：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、初始标记：标记GC <span class="keyword">Root</span>能直接引用的对象</span><br><span class="line">B、并发标记：利用多线程对每个GC <span class="keyword">Root</span>对象进行tracing搜索，在堆中查找其下所有能关联到的对象。</span><br><span class="line"><span class="keyword">C</span>、重新标记：为了修正并发标记期间，用户程序继续运作而导致标志产生变动的那一部分对象的标记记录。</span><br><span class="line"><span class="keyword">D</span>、并发清除：利用多个线程对标记的对象进行清除</span><br></pre></td></tr></table></figure><p>（4）由于耗时最长的并发标记和并发清除操作都是用户线程一起工作，所以总体来说，CMS的内存回收工作是和用户线程一起并发执行的。<br>（5）缺点：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、对CPU资源占用比较多。可能因为占用一部分CPU资源导致应用程序响应变慢。</span><br><span class="line">B、CMS无法处理浮动垃圾。在并发清除阶段，用户程序继续运行，可能产生新的内存垃圾，这一部分垃圾出现在标记过程之后，因此，CMS无法清除。这部分垃圾称为“浮动垃圾“</span><br><span class="line">C、需要预留一部分内存，在垃圾回收时，给用户程序使用。</span><br><span class="line">D、基于标记-清除算法，容易产生大量内存碎片，导致<span class="literal">full</span> GC（<span class="literal">full</span> GC进行内存碎片的整理）</span><br></pre></td></tr></table></figure><p>6、 对象头部分的内存布局：HotSpot的对象头分为两部分，第一部分用于存储对象自身的运行时数据，比如哈希码、GC分代年龄等。另外一部分用于指向方法区对象类型数据的指针。<br>7、 偏向锁：偏向锁偏向于第一个获取它的线程，如果在接下来的执行过程，没有其他线程获取该锁，则持有偏向锁的线程永远不需要同步。（当一个线程获取偏向锁，它每次进入这个锁相关的同步块，虚拟机不在进行任何同步操作。当有另外一个线程尝试获取这个锁时，偏向模式宣告结束）</p><h2 id="JVM优化："><a href="#JVM优化：" class="headerlink" title="JVM优化："></a>JVM优化：</h2><p>1、一般来说，当survivor区不够大或者占用量达到50%，就会把一些对象放到老年区。通过设置合理的eden区，survivor区及使用率，可以将年轻对象保存在年轻代，从而避免full GC，使用<code>-Xmn</code>设置年轻代的大小</p><p>2、对于占用内存比较多的大对象，一般会选择在老年代分配内存。如果在年轻代给大对象分配内存，年轻代内存不够了，就要在eden区移动大量对象到老年代，然后这些移动的对象可能很快消亡，因此导致full GC。通过设置参数：<code>-XX:PetenureSizeThreshold=1000000</code>，单位为B，标明对象大小超过1M时，在老年代(tenured)分配内存空间。</p><p>3、一般情况下，年轻对象放在eden区，当第一次GC后，如果对象还存活，放到survivor区，此后，每GC一次，年龄增加1，当对象的年龄达到阈值，就被放到tenured老年区。这个阈值可以同构<code>-XX:MaxTenuringThreshold</code>设置。如果想让对象留在年轻代，可以设置比较大的阈值。</p><p>4、设置最小堆和最大堆：<code>-Xmx</code>和<code>-Xms</code>稳定的堆大小堆垃圾回收是有利的，获得一个稳定的堆大小的方法是设置-Xms和-Xmx的值一样，即最大堆和最小堆一样，如果这样子设置，系统在运行时堆大小理论上是恒定的，稳定的堆空间可以减少GC次数，因此，很多服务端都会将这两个参数设置为一样的数值。稳定的堆大小虽然减少GC次数，但是增加每次GC的时间，因为每次GC要把堆的大小维持在一个区间内。</p><p>5、一个不稳定的堆并非毫无用处。在系统不需要使用大内存的时候，压缩堆空间，使得GC每次应对一个较小的堆空间，加快单次GC次数。基于这种考虑，JVM提供两个参数，用于压缩和扩展堆空间。<br>（1）<code>-XX:MinHeapFreeRatio</code> 参数用于设置堆空间的最小空闲比率。默认值是40，当堆空间的空闲内存比率小于40，JVM便会扩展堆空间<br>（2）<code>-XX:MaxHeapFreeRatio</code> 参数用于设置堆空间的最大空闲比率。默认值是70， 当堆空间的空闲内存比率大于70，JVM便会压缩堆空间。<br>（3）当-Xmx和-Xmx相等时，上面两个参数无效</p><p>6、通过增大吞吐量提高系统性能，可以通过设置并行垃圾回收收集器。<br>（1）<code>-XX:+UseParallelGC</code>:年轻代使用并行垃圾回收收集器。这是一个关注吞吐量的收集器，可以尽可能的减少垃圾回收时间。<br>（2）<code>-XX:+UseParallelOldGC</code>:设置老年代使用并行垃圾回收收集器。</p><p>7、尝试使用大的内存分页：使用大的内存分页增加CPU的内存寻址能力，从而系统的性能。<code>-XX:+LargePageSizeInBytes</code>设置内存页的大小</p><p>8、使用非占用的垃圾收集器。<code>-XX:+UseConcMarkSweepGC</code>老年代使用CMS收集器降低停顿。</p><p>9、<code>-XXSurvivorRatio=3</code>，表示年轻代中的分配比率：survivor:eden = 2:3</p><p>10、JVM性能调优的工具：<br>（1）jps（Java Process Status）：输出JVM中运行的进程状态信息(现在一般使用jconsole)<br>（2）jstack：查看java进程内线程的堆栈信息。<br>（3）jmap：用于生成堆转存快照<br>（4）jhat：用于分析jmap生成的堆转存快照（一般不推荐使用，而是使用Ecplise Memory Analyzer）<br>（3）jstat是JVM统计监测工具。可以用来显示垃圾回收信息、类加载信息、新生代统计信息等。<br>（4）VisualVM：故障处理工具</p><h2 id="类加载机制："><a href="#类加载机制：" class="headerlink" title="类加载机制："></a>类加载机制：</h2><p>一、 概念：类加载器把class文件中的二进制数据读入到内存中，存放在方法区，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类加载的步骤如下：<br>1、加载：查找并加载类的二进制数据（把class文件里面的信息加载到内存里面）<br>2、连接：把内存中类的二进制数据合并到虚拟机的运行时环境中<br>（1）验证：确保被加载的类的正确性。包括：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、类文件的结构检查：检查是否满足<span class="keyword">Java类文件的固定格式</span></span><br><span class="line"><span class="keyword">B、语义检查：确保类本身符合Java的语法规范</span></span><br><span class="line"><span class="keyword">C、字节码验证：确保字节码流可以被Java虚拟机安全的执行。字节码流是操作码组成的序列。每一个操作码后面都会跟着一个或者多个操作数。字节码检查这个步骤会检查每一个操作码是否合法。</span></span><br><span class="line"><span class="keyword">D、二进制兼容性验证：确保相互引用的类之间是协调一致的。</span></span><br></pre></td></tr></table></figure><p>（2）准备：为类的静态变量分配内存，并将其初始化为默认值<br>（3）解析：把类中的符号引用转化为直接引用（比如说方法的符号引用，是有方法名和相关描述符组成，在解析阶段，JVM把符号引用替换成一个指针，这个指针就是直接引用，它指向该类的该方法在方法区中的内存位置）<br>3、初始化：为类的静态变量赋予正确的初始值。当静态变量的等号右边的值是一个常量表达式时，不会调用static代码块进行初始化。只有等号右边的值是一个运行时运算出来的值，才会调用static初始化。</p><p>二、双亲委派模型：<br>1、当一个类加载器收到类加载请求的时候，它首先不会自己去加载这个类的信息，而是把该<br>请求转发给父类加载器，依次向上。所以所有的类加载请求都会被传递到父类加载器中，只有当父类加载器中无法加载到所需的类，子类加载器才会自己尝试去加载该类。当当前类加载器和所有父类加载器都无法加载该类时，抛出ClassNotFindException异常。<br>2、意义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">提高系统的安全性。用户自定义的类加载器不可能加载应该由父加载器加载的可靠类。（比如用户定义了一个恶意代码，自定义的类加载器首先让系统加载器去加载，系统加载器检查该代码不符合规范，于是就不继续加载了）</span><br></pre></td></tr></table></figure><p>3、定义类加载器：如果某个类加载器能够加载一个类，那么这个类加载器就叫做定义类加载器<br>4、初始类加载器：定义类加载器及其所有子加载器都称作初始类加载器。<br>5、运行时包：<br>（1）由同一个类加载器加载并且拥有相同包名的类组成运行时包<br>（2）只有属于同一个运行时包的类，才能访问包可见（default）的类和类成员。作用是 限制用户自定义的类冒充核心类库的类去访问核心类库的包可见成员。<br>6、加载两份相同的class对象的情况：A和B不属于父子类加载器关系，并且各自都加载了同一个类。</p><p>三、特点：<br>1、全盘负责：当一个类加载器加载一个类时，该类所依赖的其他类也会被这个类加载器加载到内存中。<br>2、缓存机制：所有的Class对象都会被缓存，当程序需要使用某个Class时，类加载器先从缓存中查找，找不到，才从class文件中读取数据，转化成Class对象，存入缓存中。</p><p>三、 类加载器：<br>两种类型的类加载器：<br>1、 JVM自带的类加载器（3种）：<br>（1）根类加载器（Bootstrap）：<br>a、C++编写的，程序员无法在程序中获取该类<br>b、负责加载虚拟机的核心库，比如java.lang.Object<br>c、没有继承ClassLoader类<br>（2）扩展类加载器（Extension）：<br>a、Java编写的，从指定目录中加载类库<br>b、父加载器是根类加载器<br>c、是ClassLoader的子类<br>d、如果用户把创建的jar文件放到指定目录中，也会被扩展加载器加载。<br>（3）系统加载器（System）或者应用加载器(App)：<br>a、Java编写的<br>b、父加载器是扩展类加载器<br>c、从环境变量或者class.path中加载类<br>d、是用户自定义类加载的默认父加载器<br>e、是ClassLoader的子类</p><p>2、用户自定义的类加载器：<br>（1）Java.lang.ClassLoader类的子类<br>（2）用户可以定制类的加载方式<br>（3）父类加载器是系统加载器<br>（4）编写步骤：<br>A、继承ClassLoader<br>B、重写findClass方法。从特定位置加载class文件，得到字节数组，然后利用defineClass把字节数组转化为Class对象<br>（5）为什么要自定义类加载器？<br>A、可以从指定位置加载class文件，比如说从数据库、云端加载class文件<br>B、加密：Java代码可以被轻易的反编译，因此，如果需要对代码进行加密，那么加密以后的代码，就不能使用Java自带的ClassLoader来加载这个类了，需要自定义ClassLoader，对这个类进行解密，然后加载。</p><p>问题：Java程序对类的执行有几种方式：<br>1、 主动使用（6种情况）：<br>JVM必须在每个类“首次 主动使用”的时候，才会初始化这些类。<br>（1） 创建类的实例<br>（2） 读写某个类或者接口的静态变量<br>（3） 调用类的静态方法<br>（4） 同过反射的API（Class.forName()）获取类<br>（5） 初始化一个类的子类<br>（6） JVM启动的时候，被标明启动类的类（包含Main方法的类）<br>只有当程序使用的静态变量或者静态方法确实在该类中定义时，该可以认为是对该类或者接口的主动使用。<br>2、 被动使用：除了主动使用的6种情况，其他情况都是被动使用，都不会导致类的初始化。<br>3、 JVM规范允许类加载器在预料某个类将要被使用的时候，就预先加载它。如果该class文件缺失或者存在错误，则在程序“首次 主动使用”的时候，才报告这个错误。（Linkage Error错误）。如果这个类一直没有被程序“主动使用”，就不会报错。</p><p>类加载机制与接口：<br>1、 当Java虚拟机初始化一个类时，不会初始化该类实现的接口。<br>2、 在初始化一个接口时，不会初始化这个接口父接口。<br>3、 只有当程序首次使用该接口的静态变量时，才导致该接口的初始化。</p><p>ClassLoader：<br>1、 调用Classloader的loadClass方法去加载一个类，不是主动使用，因此不会进行类的初始化。</p><p>类的卸载：<br>1、 有JVM自带的三种类加载器（根、扩展、系统）加载的类始终不会卸载。因为JVM始终引用这些类加载器，这些类加载器使用引用他们所加载的类，因此这些Class类对象始终是可到达的。<br>2、 由用户自定义类加载器加载的类，是可以被卸载的。</p><p>补充：</p><ul><li>JDK和JRK</li></ul><p>（1）JDK ： Java Development Kit，开发的时候用到的类包。<br>（2）JRE ： Java Runtime Environment，Java运行的基础，包含运行时需要的所有类库。</p><ul><li>图解java文件转化成机器码</li></ul><img src="/2017/11/03/深入理解JVM/7.png"><p>JVM虚拟机先将java文件编译成class文件（字节码文件），然后再将class文件转换成所有操作系统都能运行的机器指令。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 内存回收 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM垃圾回收底层原理</title>
      <link href="/2017/11/03/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"/>
      <url>/2017/11/03/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h2><h3 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h3><p>最基础的收集算法是“标记-清除”(Mark-Sweep)算法，分两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。</p><p>不足：一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能导致以后在程序运行过程需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一个的垃圾收集动作。</p><h3 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h3><p>为了解决效率问题，一种称为复制(Copying)的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完了，就将还存活着的对象复制到另外一块上，然后再把已经使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。代价是内存缩小为原来的一半。</p><p>商业虚拟机用这个回收算法来回收新生代。IBM研究表明98%的对象是“朝生夕死“，不需要按照1-1的比例来划分内存空间，而是将内存分为一块较大的”Eden“空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活的对象一次性复制到另外一个Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。Hotspot虚拟机默认Eden和Survivor的比例是8-1.即每次可用整个新生代的90%, 只有一个survivor，即1/10被”浪费“。当然，98%的对象回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够时，需要依赖其他内存(老年代)进行分配担保(Handle Promotion).</p><p>如果另外一块survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。</p><h3 id="eden-survivor复制过程概述"><a href="#eden-survivor复制过程概述" class="headerlink" title="eden survivor复制过程概述"></a>eden survivor复制过程概述</h3><p>Eden Space字面意思是伊甸园，对象被创建的时候首先放到这个区域，进行垃圾回收后，不能被回收的对象被放入到空的survivor区域。</p><p>Survivor Space幸存者区，用于保存在eden space内存区域中经过垃圾回收后没有被回收的对象。Survivor有两个，分别为To Survivor、 From Survivor，这个两个区域的空间大小是一样的。执行垃圾回收的时候Eden区域不能被回收的对象被放入到空的survivor（也就是To Survivor，同时Eden区域的内存会在垃圾回收的过程中全部释放），另一个survivor（即From Survivor）里不能被回收的对象也会被放入这个survivor（即To Survivor），然后To Survivor 和 From Survivor的标记会互换，始终保证一个survivor是空的。</p><p>为啥需要两个survivor？因为需要一个完整的空间来复制过来。当满的时候晋升。每次都往标记为to的里面放，然后互换，这时from已经被清空，可以当作to了。</p><h3 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h3><p>复制收集算法在对象成活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以，老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出一种”标记-整理“Mark-Compact算法，标记过程仍然和标记-清除一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理端边界以外的内存.</p><h3 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h3><p>当前商业虚拟机的垃圾收集都采用”分代收集“(Generational Collection)算法，这种算法根据对象存活周期的不同将内存划分为几块。一般把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代，每次垃圾收集时都发现大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率较高，没有额外的空间对它进行分配担保，就必须使用”标记-清理“和”标记-整理“算法来进行回收。</p><h2 id="HotSpot算法实现"><a href="#HotSpot算法实现" class="headerlink" title="HotSpot算法实现"></a>HotSpot算法实现</h2><p>在Java语言中，可作为GC Roots的对象包括下面几种：</p><ul><li>虚拟机栈(栈帧中的本地变量表)中引用的对象</li><li>方法去中类静态属性引用的对象</li><li>方法区中常量引用的对象</li><li>本地方法栈中JNI(即一般说的Native方法)引用的对象</li></ul><p>从可达性分析中从GC Roots节点找引用链这个操作为例，可作为GC Roots的节点主要在全局性的引用(例如常量或类静态属性)与执行上下文(例如栈帧中的本地变量表)中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查里面的引用，必然消耗很多时间。</p><p>可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行–这里”一致性“的意思是指整个分析期间整个执行系统看起来就像被冻结在某个时间点，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致GC进行时必须停顿所有Java执行线程(Sun公司将这件事情称为”Stop The World“)的一个重要原因，即使是在号称(几乎)不会发生停顿的CMS收集器中，枚举根节点时也必须停顿的。</p><p>安全点，Safepoint</p><h2 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h2><h3 id="Serial-收集器"><a href="#Serial-收集器" class="headerlink" title="Serial 收集器"></a>Serial 收集器</h3><p>标记-复制。</p><p>单线程，一个CPU或一条收集线程去完成垃圾收集工作，收集时必须暂停其他所有的工作线程，直到它结束。</p><p>虽然如此，它依然是虚拟机运行在Client模式下的默认<strong>新生代</strong>收集器。简单而高效。</p><h3 id="ParNew-收集器"><a href="#ParNew-收集器" class="headerlink" title="ParNew 收集器"></a>ParNew 收集器</h3><p>ParNew是Serial收集器的多线程版本。Server模式下默认<strong>新生代</strong>收集器，除了Serial收集器之外，只有它能与CMS收集器配合工作。</p><h3 id="并行-Parallel"><a href="#并行-Parallel" class="headerlink" title="并行 Parallel"></a>并行 Parallel</h3><p>指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</p><h3 id="并发-Concurrent"><a href="#并发-Concurrent" class="headerlink" title="并发 Concurrent"></a>并发 Concurrent</h3><p>指用户线程与垃圾收集线程同时执行(但不一定是并行的，可能会交替执行)，用户程序再继续运行，而垃圾收集程序运行于另一个CPU上。</p><h3 id="Parallel-Scavenge-收集器"><a href="#Parallel-Scavenge-收集器" class="headerlink" title="Parallel Scavenge 收集器"></a>Parallel Scavenge 收集器</h3><p>Parallel Scavenge 收集器是一个<strong>新生代</strong>收集器，它也是使用复制算法的收集器。看上去来ParNew一样，有什么特别？</p><p>Parallel Scavenge 收集器的特点是它的关注点与其他收集器不同，CMS等收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间。而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。所谓吞吐量就是CPU用于运行用户代码的时间和CPU总小号时间的比值，即吞吐量 = 运行用户代码时间 / (运行用户代码时间+垃圾收集时间)，虚拟机总共运行了100min，其中垃圾收集花费了1min，那吞吐量就是99%.</p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效地利用CPU时间，主要适合在后台运算而不需要太多交互的任务。</p><p>Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 <code>-XX:MaxGCPauseMillis</code>以及直接设置吞吐量大小的<code>-XX:GCTimeRatio</code>。</p><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p>Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器。给Client模式下的虚拟机使用。</p><p>新生代采用复制算法，暂停所有用户线程；</p><p>老年代采用标记-整理算法，暂停所有用户线程；</p><h3 id="Parallel-Old-收集器"><a href="#Parallel-Old-收集器" class="headerlink" title="Parallel Old 收集器"></a>Parallel Old 收集器</h3><p>这里注意，Parallel Scavage 收集器架构中本身有PS MarkSweep收集器来收集老年代，并非直接使用了Serial Old,但二者接近。本人win10 64位系统，jdk1.8.0_102，测试默认垃圾收集器为：<strong>PS MarkSweep *<em>和 *</em>PS Scavenge</strong>。 也就是说Java8的默认并不是G1。</p><p>这是”吞吐量优先“，注重吞吐量以及CPU资源敏感的场合都可以优先考虑Parallel Scavenge和Parallel Old(PS Mark Sweep)。Java8 默认就是这个。</p><h3 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h3><p>CMS(Concurrent Mark Sweep) 收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类尤其重视服务的响应速度，希望系统停顿时间最短。CMS收集器就非常符合这类应用的需求。</p><p>CMS基于 <code>标记-清除</code>算法实现。整个过程分为4个步骤：</p><ol><li>初始标记(CMS initial mark) -stop the world</li><li>并发标记(CMS concurrent mark)</li><li>重新标记(CMS remark) -stop the world</li><li>并发清除(CMS concurrent sweep)</li></ol><p>初始标记，重新标记这两个步骤仍然需要Stop The World, 初始标记仅仅标记以下GC Roots能直接关联的对象，速度很快。</p><p>并发标记就是进行GC Roots Tracing的过程；</p><p>而重新标记阶段则是为了修正并发标记期间因为用户程序继续运作而导致标记产生变动的那一部分对象的标记记录。这个阶段停顿比初始标记稍微长，但远比并发标记的时间短。</p><p>整个过程耗时最长的并发标记和并发清除过程，收集器都可以与用户线程一起工作。总体上来说，CMS收集器的内存回收过程与用户线程一起并发执行的。</p><p>CMS特点：并发收集，低停顿。</p><p><strong>缺点</strong></p><p>1.CMS收集器对CPU资源非常敏感。默认启动的回收线程数是(CPU+3)/4. 当CPU 4个以上时，并发回收垃圾收集线程不少于25%的CPU资源。</p><p>2.CMS收集器无法处理浮动垃圾(Floating Garbage), 可能出现”Concurrent Mode Failure“失败而导致另一次Full GC的产生。由于CMS并发清理时，用户线程还在运行，伴随产生新垃圾，而这一部分出现在标记之后，只能下次GC时再清理。这一部分垃圾就称为”浮动垃圾“。</p><p>由于CMS运行时还需要给用户空间继续运行，则不能等老年代几乎被填满再进行收集，需要预留一部分空间提供并发收集时，用户程序运行。JDK1.6中，CMS启动阈值为92%. 若预留内存不够用户使用，则出现一次<code>Concurent Mode Failure</code>失败。这时虚拟机启动后备预案，临时启用Serial Old收集老年代，这样停顿时间很长。</p><p>3.CMS基于”标记-清除“算法实现的，则会产生大量空间碎片，空间碎片过多时，没有连续空间分配给大对象，不得不提前触发一次FUll GC。当然可以开启-XX:+UseCMSCompactAtFullCollection(默认开)，在CMS顶不住要FullGC时开启内存碎片合并整理过程。内存整理过程是无法并发的，空间碎片问题没了，但停顿时间变长。</p><p><strong>面试题：CMS一共会有几次STW</strong></p><p>首先，回答两次，初始标记和重新标记需要。</p><p>然后，CMS并发的代价是预留空间给用户，预留不足的时候触发FUllGC，这时Serail Old会STW.</p><p>然后，CMS是标记-清除算法，导致空间碎片，则没有连续空间分配大对象时，FUllGC, 而FUllGC会开始碎片整理， STW.</p><p>即2次或多次。</p><h2 id="CMS什么时候FUll-GC"><a href="#CMS什么时候FUll-GC" class="headerlink" title="CMS什么时候FUll GC"></a>CMS什么时候FUll GC</h2><p>除直接调用System.gc外，触发Full GC执行的情况有如下四种。</p><h3 id="1-旧生代空间不足"><a href="#1-旧生代空间不足" class="headerlink" title="1. 旧生代空间不足"></a>1. 旧生代空间不足</h3><p>旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出如下错误： java.lang.OutOfMemoryError: Java heap space 为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。</p><h3 id="2-Permanet-Generation空间满"><a href="#2-Permanet-Generation空间满" class="headerlink" title="2. Permanet Generation空间满"></a>2. Permanet Generation空间满</h3><p>PermanetGeneration中存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Permanet Generation可能会被占满，在未配置为采用CMS GC的情况下会执行Full GC。如果经过Full GC仍然回收不了，那么JVM会抛出如下错误信息： java.lang.OutOfMemoryError: PermGen space 为避免Perm Gen占满造成Full GC现象，可采用的方法为增大Perm Gen空间或转为使用CMS GC。</p><h3 id="3-CMS-GC时出现promotion-failed和concurrent-mode-failure"><a href="#3-CMS-GC时出现promotion-failed和concurrent-mode-failure" class="headerlink" title="3. CMS GC时出现promotion failed和concurrent mode failure"></a>3. CMS GC时出现promotion failed和concurrent mode failure</h3><p>对于采用CMS进行旧生代GC的程序而言，尤其要注意GC日志中是否有promotion failed和concurrent mode failure两种状况，当这两种状况出现时可能会触发Full GC。 promotionfailed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。 应对措施为：增大survivorspace、旧生代空间或调低触发并发GC的比率，但在JDK 5.0+、6.0+的版本中有可能会由于JDK的bug29导致CMS在remark完毕后很久才触发sweeping动作。对于这种状况，可通过设置-XX:CMSMaxAbortablePrecleanTime=5（单位为ms）来避免。</p><h3 id="4-统计得到的Minor-GC晋升到旧生代的平均大小大于旧生代的剩余空间"><a href="#4-统计得到的Minor-GC晋升到旧生代的平均大小大于旧生代的剩余空间" class="headerlink" title="4. 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间"></a>4. 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间</h3><p>这是一个较为复杂的触发情况，Hotspot为了避免由于新生代对象晋升到旧生代导致旧生代空间不足的现象，在进行Minor GC时，做了一个判断，如果之前统计所得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间，那么就直接触发Full GC。 例如程序第一次触发MinorGC后，有6MB的对象晋升到旧生代，那么当下一次Minor GC发生时，首先检查旧生代的剩余空间是否大于6MB，如果小于6MB，则执行Full GC。 当新生代采用PSGC时，方式稍有不同，PS GC是在Minor GC后也会检查，例如上面的例子中第一次Minor GC后，PS GC会检查此时旧生代的剩余空间是否大于6MB，如小于，则触发对旧生代的回收。 除了以上4种状况外，对于使用RMI来进行RPC或管理的Sun JDK应用而言，默认情况下会一小时执行一次Full GC。可通过在启动时通过- java-Dsun.rmi.dgc.client.gcInterval=3600000来设置Full GC执行的间隔时间或通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。</p><h2 id="G1"><a href="#G1" class="headerlink" title="G1"></a>G1</h2><h3 id="什么是垃圾回收"><a href="#什么是垃圾回收" class="headerlink" title="什么是垃圾回收"></a>什么是垃圾回收</h3><p>首先，在了解G1之前，我们需要清楚的知道，垃圾回收是什么？简单的说垃圾回收就是回收内存中不再使用的对象。</p><p>垃圾回收的基本步骤</p><p>回收的步骤有2步：</p><p>1.查找内存中不再使用的对象</p><p>2.释放这些对象占用的内存</p><h4 id="1-查找内存中不再使用的对象"><a href="#1-查找内存中不再使用的对象" class="headerlink" title="1,查找内存中不再使用的对象"></a>1,查找内存中不再使用的对象</h4><p>那么问题来了，如何判断哪些对象不再被使用呢？我们也有2个方法：</p><p><strong>1.引用计数法</strong> 引用计数法就是如果一个对象没有被任何引用指向，则可视之为垃圾。这种方法的缺点就是不能检测到环的存在。</p><p><strong>2.根搜索算法</strong></p><p>根搜索算法的基本思路就是通过一系列名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。</p><p>现在我们已经知道如何找出垃圾对象了，如何把这些对象清理掉呢？</p><h4 id="2-释放这些对象占用的内存"><a href="#2-释放这些对象占用的内存" class="headerlink" title="2. 释放这些对象占用的内存"></a>2. 释放这些对象占用的内存</h4><p>常见的方式有复制或者直接清理，但是直接清理会存在内存碎片，于是就会产生了清理再压缩的方式。</p><p>总得来说就产生了三种类型的回收算法。</p><p>1.标记-复制</p><p>2.标记-清理</p><p>3.标记-整理</p><p>基于分代的假设</p><p>由于对象的存活时间有长有短，所以对于存活时间长的对象，减少被gc的次数可以避免不必要的开销。这样我们就把内存分成新生代和老年代，新生代存放刚创建的和存活时间比较短的对象，老年代存放存活时间比较长的对象。这样每次仅仅清理年轻代，老年代仅在必要时时再做清理可以极大的提高GC效率，节省GC时间。</p><h3 id="Java垃圾收集器的历史"><a href="#Java垃圾收集器的历史" class="headerlink" title="Java垃圾收集器的历史"></a>Java垃圾收集器的历史</h3><p>第一阶段，Serial（串行）收集器</p><p>在jdk1.3.1之前，java虚拟机仅仅能使用Serial收集器。 Serial收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。</p><p>PS：开启Serial收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseSerialGC</span></span><br></pre></td></tr></table></figure><p>第二阶段，Parallel（并行）收集器</p><p>Parallel收集器也称吞吐量收集器，相比Serial收集器，Parallel最主要的优势在于使用多线程去完成垃圾清理工作，这样可以充分利用多核的特性，大幅降低gc时间。</p><p>PS:开启Parallel收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParallelGC</span> <span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParallelOldGC</span></span><br></pre></td></tr></table></figure><p>第三阶段，CMS（并发）收集器</p><p>CMS收集器在Minor GC时会暂停所有的应用线程，并以多线程的方式进行垃圾回收。在Full GC时不再暂停应用线程，而是使用若干个后台线程定期的对老年代空间进行扫描，及时回收其中不再使用的对象。</p><p>PS:开启CMS收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParNewGC</span> <span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseConcMarkSweepGC</span></span><br></pre></td></tr></table></figure><p>第四阶段，G1（并发）收集器</p><p>G1收集器（或者垃圾优先收集器）的设计初衷是为了尽量缩短处理超大堆（大于4GB）时产生的停顿。相对于CMS的优势而言是内存碎片的产生率大大降低。</p><p>PS:开启G1收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseG1GC</span></span><br></pre></td></tr></table></figure><h3 id="了解G1"><a href="#了解G1" class="headerlink" title="了解G1"></a>了解G1</h3><p>G1的第一篇paper（附录1）发表于2004年，在2012年才在jdk1.7u4中可用。oracle官方计划在jdk9中将G1变成默认的垃圾收集器，以替代CMS。为何oracle要极力推荐G1呢，G1有哪些优点</p><blockquote><p> <strong>首先，G1的设计原则就是简单可行的性能调优</strong> </p></blockquote><p>开发人员仅仅需要声明以下参数即可：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-<span class="string">XX:</span>+UseG1GC -Xmx32g -<span class="string">XX:</span>MaxGCPauseMillis=<span class="number">200</span></span><br></pre></td></tr></table></figure><p>其中-XX:+UseG1GC为开启G1垃圾收集器，-Xmx32g 设计堆内存的最大内存为32G，-XX:MaxGCPauseMillis=200设置GC的最大暂停时间为200ms。如果我们需要调优，在内存大小一定的情况下，我们只需要修改最大暂停时间即可。</p><blockquote><p> <strong>其次，G1将新生代，老年代的物理空间划分取消了。</strong> </p></blockquote><p>这样我们再也不用单独的空间对每个代进行设置了，不用担心每个代内存是否足够。</p><img src="/2017/11/03/JVM垃圾回收底层原理/1.png"><p>取而代之的是，G1算法将堆划分为若干个区域（Region），它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。</p><img src="/2017/11/03/JVM垃圾回收底层原理/2.png"><p>在G1中，还有一种特殊的区域，叫Humongous区域。 如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。这些巨型对象，默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。</p><blockquote><p> PS：在java 8中，持久代也移动到了普通的堆内存空间中，改为元空间。 </p></blockquote><h3 id="对象分配策略"><a href="#对象分配策略" class="headerlink" title="对象分配策略"></a>对象分配策略</h3><p>说起大对象的分配，我们不得不谈谈对象的分配策略。它分为3个阶段：</p><p>1.TLAB(Thread Local Allocation Buffer)线程本地分配缓冲区 2.Eden区中分配 3.Humongous区分配</p><p>TLAB为线程本地分配缓冲区，它的目的为了使对象尽可能快的分配出来。如果对象在一个共享的空间中分配，我们需要采用一些同步机制来管理这些空间内的空闲空间指针。在Eden空间中，每一个线程都有一个固定的分区用于分配对象，即一个TLAB。分配对象时，线程之间不再需要进行任何的同步。</p><p>对TLAB空间中无法分配的对象，JVM会尝试在Eden空间中进行分配。如果Eden空间无法容纳该对象，就只能在老年代中进行分配空间。</p><p>最后，G1提供了两种GC模式，Young GC和Mixed GC，两种都是Stop The World(STW)的。下面我们将分别介绍一下这2种模式。</p><h3 id="G1-Young-GC"><a href="#G1-Young-GC" class="headerlink" title="G1 Young GC"></a>G1 Young GC</h3><p>Young GC主要是对Eden区进行GC，它在Eden空间耗尽时会被触发。在这种情况下，Eden空间的数据移动到Survivor空间中，如果Survivor空间不够，Eden空间的部分数据会直接晋升到年老代空间。Survivor区的数据移动到新的Survivor区中，也有部分数据晋升到老年代空间中。最终Eden空间的数据为空，GC停止工作，应用线程继续执行。</p><img src="/2017/11/03/JVM垃圾回收底层原理/3.png"><img src="/2017/11/03/JVM垃圾回收底层原理/4.png"><p>这时，我们需要考虑一个问题，如果仅仅GC 新生代对象，我们如何找到所有的根对象呢？ 老年代的所有对象都是根么？那这样扫描下来会耗费大量的时间。于是，G1引进了RSet的概念。它的全称是Remembered Set，作用是跟踪指向某个heap区内的对象引用。</p><img src="/2017/11/03/JVM垃圾回收底层原理/5.png"><p>在CMS中，也有RSet的概念，在老年代中有一块区域用来记录指向新生代的引用。这是一种point-out，在进行Young GC时，扫描根时，仅仅需要扫描这一块区域，而不需要扫描整个老年代。</p><p>但在G1中，并没有使用point-out，这是由于一个分区太小，分区数量太多，如果是用point-out的话，会造成大量的扫描浪费，有些根本不需要GC的分区引用也扫描了。于是G1中使用point-in来解决。point-in的意思是哪些分区引用了当前分区中的对象。这样，仅仅将这些对象当做根来扫描就避免了无效的扫描。由于新生代有多个，那么我们需要在新生代之间记录引用吗？这是不必要的，原因在于每次GC时，所有新生代都会被扫描，所以只需要记录老年代到新生代之间的引用即可。</p><p>需要注意的是，如果引用的对象很多，赋值器需要对每个引用做处理，赋值器开销会很大，为了解决赋值器开销这个问题，在G1 中又引入了另外一个概念，卡表（Card Table）。一个Card Table将一个分区在逻辑上划分为固定大小的连续区域，每个区域称之为卡。卡通常较小，介于128到512字节之间。Card Table通常为字节数组，由Card的索引（即数组下标）来标识每个分区的空间地址。默认情况下，每个卡都未被引用。当一个地址空间被引用时，这个地址空间对应的数组索引的值被标记为”0″，即标记为脏被引用，此外RSet也将这个数组下标记录下来。一般情况下，这个RSet其实是一个Hash Table，Key是别的Region的起始地址，Value是一个集合，里面的元素是Card Table的Index。</p><p><strong>Young GC 阶段</strong>：</p><p><strong>阶段1：根扫描</strong></p><p>静态和本地对象被扫描</p><p><strong>阶段2：更新RS</strong></p><p>处理dirty card队列更新RS</p><p><strong>阶段3：处理RS</strong></p><p>检测从年轻代指向年老代的对象</p><p><strong>阶段4：对象拷贝</strong></p><p>拷贝存活的对象到survivor/old区域</p><p><strong>阶段5：处理引用队列</strong></p><p>软引用，弱引用，虚引用处理</p><h3 id="G1-Mix-GC"><a href="#G1-Mix-GC" class="headerlink" title="G1 Mix GC"></a>G1 Mix GC</h3><p>Mix GC不仅进行正常的新生代垃圾收集，同时也回收部分后台扫描线程标记的老年代分区。</p><p>它的GC步骤分2步：</p><p>1.全局并发标记（global concurrent marking） 2.拷贝存活对象（evacuation）</p><p>在进行Mix GC之前，会先进行global concurrent marking（全局并发标记）。 global concurrent marking的执行过程是怎样的呢？</p><p>在G1 GC中，它主要是为Mixed GC提供标记服务的，并不是一次GC过程的一个必须环节。global concurrent marking的执行过程分为五个步骤：</p><p><strong>初始标记（initial mark，STW）</strong></p><p>在此阶段，G1 GC 对根进行标记。该阶段与常规的 (STW) 年轻代垃圾回收密切相关。</p><p><strong>根区域扫描（root region scan</strong></p><p>G1 GC 在初始标记的存活区扫描对老年代的引用，并标记被引用的对象。该阶段与应用程序（非 STW）同时运行，并且只有完成该阶段后，才能开始下一次 STW 年轻代垃圾回收。</p><p><strong>并发标记（Concurrent Marking）</strong></p><p>G1 GC 在整个堆中查找可访问的（存活的）对象。该阶段与应用程序同时运行，可以被 STW 年轻代垃圾回收中断</p><p><strong>最终标记（Remark，STW）</strong></p><p>该阶段是 STW 回收，帮助完成标记周期。G1 GC 清空 SATB 缓冲区，跟踪未被访问的存活对象，并执行引用处理。</p><p><strong>清除垃圾（Cleanup，STW）</strong></p><p>在这个最后阶段，G1 GC 执行统计和 RSet 净化的 STW 操作。在统计期间，G1 GC 会识别完全空闲的区域和可供进行混合垃圾回收的区域。清理阶段在将空白区域重置并返回到空闲列表时为部分并发。</p><h3 id="三色标记算法"><a href="#三色标记算法" class="headerlink" title="三色标记算法"></a>三色标记算法</h3><p>提到并发标记，我们不得不了解并发标记的三色标记算法。它是描述追踪式回收器的一种有用的方法，利用它可以推演回收器的正确性。 首先，我们将对象分成三种类型的。</p><p><strong>黑色</strong>:根对象，或者该对象与它的子对象都被扫描</p><p><strong>灰色</strong>:对象本身被扫描,但还没扫描完该对象中的子对象</p><p><strong>白色</strong>:未被扫描对象，扫描完成所有对象之后，最终为白色的为不可达对象，即垃圾对象</p><p>当GC开始扫描对象时，按照如下图步骤进行对象的扫描：</p><p>根对象被置为黑色，子对象被置为灰色。</p><img src="/2017/11/03/JVM垃圾回收底层原理/6.png"><p>继续由灰色遍历,将已扫描了子对象的对象置为黑色。</p><img src="/2017/11/03/JVM垃圾回收底层原理/7.png"><p>遍历了所有可达的对象后，所有可达的对象都变成了黑色。不可达的对象即为白色，需要被清理。</p><img src="/2017/11/03/JVM垃圾回收底层原理/8.png"><p>这看起来很美好，但是如果在标记过程中，应用程序也在运行，那么对象的指针就有可能改变。这样的话，我们就会遇到一个问题：对象丢失问题</p><p>我们看下面一种情况，当垃圾收集器扫描到下面情况时:</p><img src="/2017/11/03/JVM垃圾回收底层原理/9.png"><p>这时候应用程序执行了以下操作：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">A.c</span>=C</span><br><span class="line"><span class="attr">B.c</span>=null</span><br></pre></td></tr></table></figure><p>这样，对象的状态图变成如下情形：</p><img src="/2017/11/03/JVM垃圾回收底层原理/10.png"><p>这时候垃圾收集器再标记扫描的时候就会下图成这样：</p><img src="/2017/11/03/JVM垃圾回收底层原理/11.png"><p>很显然，此时C是白色，被认为是垃圾需要清理掉，显然这是不合理的。那么我们如何保证应用程序在运行的时候，GC标记的对象不丢失呢？有如下2中可行的方式：</p><p>1.在插入的时候记录对象 2.在删除的时候记录对象</p><p>刚好这对应CMS和G1的2种不同实现方式：</p><p>在CMS采用的是增量更新（Incremental update），只要在写屏障（write barrier）里发现要有一个白对象的引用被赋值到一个黑对象 的字段里，那就把这个白对象变成灰色的。即插入的时候记录下来。</p><p>在G1中，使用的是STAB（snapshot-at-the-beginning）的方式，删除的时候记录所有的对象，它有3个步骤：</p><p>1.在开始标记的时候生成一个快照图标记存活对象</p><p>2.在并发标记的时候所有被改变的对象入队（在write barrier里把所有旧的引用所指向的对象都变成非白的）</p><p>3.可能存在游离的垃圾，将在下次被收集</p><p>这样，G1到现在可以知道哪些老的分区可回收垃圾最多。 当全局并发标记完成后，在某个时刻，就开始了Mix GC。这些垃圾回收被称作“混合式”是因为他们不仅仅进行正常的新生代垃圾收集，同时也回收部分后台扫描线程标记的分区。混合式垃圾收集如下图：</p><img src="/2017/11/03/JVM垃圾回收底层原理/12.png"><p>混合式GC也是采用的复制的清理策略，当GC完成后，会重新释放空间。 </p><img src="/2017/11/03/JVM垃圾回收底层原理/13.png"><h3 id="调优实践"><a href="#调优实践" class="headerlink" title="调优实践"></a>调优实践</h3><p><strong>MaxGCPauseMillis</strong>调优</p><p>前面介绍过使用GC的最基本的参数：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-<span class="string">XX:</span>+UseG1GC -Xmx32g -<span class="string">XX:</span>MaxGCPauseMillis=<span class="number">200</span></span><br></pre></td></tr></table></figure><p>前面2个参数都好理解，后面这个MaxGCPauseMillis参数该怎么配置呢？这个参数从字面的意思上看，就是允许的GC最大的暂停时间。G1尽量确保每次GC暂停的时间都在设置的MaxGCPauseMillis范围内。 那G1是如何做到最大暂停时间的呢？这涉及到另一个概念，CSet(collection set)。它的意思是在一次垃圾收集器中被收集的区域集合。</p><p>Young GC：选定所有新生代里的region。通过控制新生代的region个数来控制young GC的开销。</p><p>Mixed GC：选定所有新生代里的region，外加根据global concurrent marking统计得出收集收益高的若干老年代region。在用户指定的开销目标范围内尽可能选择收益高的老年代region。</p><p>在理解了这些后，我们再设置最大暂停时间就好办了。 首先，我们能容忍的最大暂停时间是有一个限度的，我们需要在这个限度范围内设置。但是应该设置的值是多少呢？我们需要在吞吐量跟MaxGCPauseMillis之间做一个平衡。如果MaxGCPauseMillis设置的过小，那么GC就会频繁，吞吐量就会下降。如果MaxGCPauseMillis设置的过大，应用程序暂停时间就会变长。G1的默认暂停时间是200毫秒，我们可以从这里入手，调整合适的时间。</p><p><strong>其他调优参数</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">G1HeapRegionSize</span>=n</span><br></pre></td></tr></table></figure><p>设置的 G1 区域的大小。值是 2 的幂，范围是 1 MB 到 32 MB 之间。目标是根据最小的 Java 堆大小划分出约 2048 个区域。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">ParallelGCThreads</span>=n</span><br></pre></td></tr></table></figure><p>设置 STW 工作线程数的值。将 n 的值设置为逻辑处理器的数量。n 的值与逻辑处理器的数量相同，最多为 8。</p><p>如果逻辑处理器不止八个，则将 n 的值设置为逻辑处理器数的 5/8 左右。这适用于大多数情况，除非是较大的 SPARC 系统，其中 n 的值可以是逻辑处理器数的 5/16 左右。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">ConcGCThreads</span>=n</span><br></pre></td></tr></table></figure><p>设置并行标记的线程数。将 n 设置为并行垃圾回收线程数 (ParallelGCThreads) 的 1/4 左右。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">InitiatingHeapOccupancyPercent</span>=45</span><br></pre></td></tr></table></figure><p>设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。</p><p>避免使用以下参数：</p><p>避免使用 -Xmn 选项或 -XX:NewRatio 等其他相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标。</p><h3 id="触发Full-GC"><a href="#触发Full-GC" class="headerlink" title="触发Full GC"></a>触发Full GC</h3><p>在某些情况下，G1触发了Full GC，这时G1会退化使用Serial收集器来完成垃圾的清理工作，它仅仅使用单线程来完成GC工作，GC暂停时间将达到秒级别的。整个应用处于假死状态，不能处理任何请求，我们的程序当然不希望看到这些。那么发生Full GC的情况有哪些呢？</p><h4 id="并发模式失败"><a href="#并发模式失败" class="headerlink" title="并发模式失败"></a>并发模式失败</h4><p>G1启动标记周期，但在Mix GC之前，老年代就被填满，这时候G1会放弃标记周期。这种情形下，需要增加堆大小，或者调整周期（例如增加线程数-XX:ConcGCThreads等）。</p><h4 id="晋升失败或者疏散失败"><a href="#晋升失败或者疏散失败" class="headerlink" title="晋升失败或者疏散失败"></a>晋升失败或者疏散失败</h4><p>G1在进行GC的时候没有足够的内存供存活对象或晋升对象使用，由此触发了Full GC。可以在日志中看到(to-space exhausted)或者（to-space overflow）。解决这种问题的方式是：</p><p>a. 增加 <code>-XX:G1ReservePercent</code> 选项的值（并相应增加总的堆大小），为“目标空间”增加预留内存量。</p><p>b. 通过减少<code>-XX:InitiatingHeapOccupancyPercent</code> 提前启动标记周期。</p><p>c. 也可以通过增加 <code>-XX:ConcGCThreads</code> 选项的值来增加并行标记线程的数目。</p><h4 id="巨型对象分配失败"><a href="#巨型对象分配失败" class="headerlink" title="巨型对象分配失败"></a>巨型对象分配失败</h4><p>当巨型对象找不到合适的空间进行分配时，就会启动Full GC，来释放空间。这种情况下，应该避免分配大量的巨型对象，增加内存或者增大-XX:G1HeapRegionSize，使巨型对象不再是巨型对象。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 内存回收 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
