<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink1.7.1与Kafka0.11.0.1</title>
      <link href="/2019/02/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/"/>
      <url>/2019/02/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="1-Flink的Checkpoint"><a href="#1-Flink的Checkpoint" class="headerlink" title="1 Flink的Checkpoint"></a>1 Flink的Checkpoint</h1><h2 id="1-1-API"><a href="#1-1-API" class="headerlink" title="1.1 API"></a>1.1 API</h2><ul><li>使用StreamExecutionEnvironment.enableCheckpointing方法来设置开启checkpoint；具体可以使用enableCheckpointing(long interval)，或者enableCheckpointing(long interval, CheckpointingMode mode)；interval用于指定checkpoint的触发间隔(单位milliseconds)，而CheckpointingMode默认是CheckpointingMode.EXACTLY_ONCE，也可以指定为CheckpointingMode.AT_LEAST_ONCE</li><li>也可以通过StreamExecutionEnvironment.getCheckpointConfig().setCheckpointingMode来设置CheckpointingMode，一般对于超低延迟的应用(大概几毫秒)可以使用CheckpointingMode.AT_LEAST_ONCE，其他大部分应用使用CheckpointingMode.EXACTLY_ONCE就可以</li><li>checkpointTimeout用于指定checkpoint执行的超时时间(单位milliseconds)，超时没完成就会被abort掉</li><li>minPauseBetweenCheckpoints用于指定checkpoint coordinator上一个checkpoint完成之后最小等多久可以出发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</li><li>maxConcurrentCheckpoints用于指定运行中的checkpoint最多可以有多少个，用于包装topology不会花太多的时间在checkpoints上面；如果有设置了minPauseBetweenCheckpoints，则maxConcurrentCheckpoints这个参数就不起作用了(大于1的值不起作用)</li><li>enableExternalizedCheckpoints用于开启checkpoints的外部持久化，但是在job失败的时候不会自动清理，需要自己手工清理state；ExternalizedCheckpointCleanup用于指定当job canceled的时候externalized checkpoint该如何清理，DELETE_ON_CANCELLATION的话，在job canceled的时候会自动删除externalized state，但是如果是FAILED的状态则会保留；RETAIN_ON_CANCELLATION则在job canceled的时候会保留externalized checkpoint state</li><li>failOnCheckpointingErrors用于指定在checkpoint发生异常的时候，是否应该fail该task，默认为true，如果设置为false，则task会拒绝checkpoint然后继续运行</li></ul><h2 id="1-2-实例"><a href="#1-2-实例" class="headerlink" title="1.2 实例"></a>1.2 实例</h2><pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.env.getCheckpointConfig().setFailOnCheckpointingErrors(true);</code></pre><h1 id="2-FlinkKafkaConsumer011"><a href="#2-FlinkKafkaConsumer011" class="headerlink" title="2 FlinkKafkaConsumer011"></a>2 FlinkKafkaConsumer011</h1><h2 id="2-1-API"><a href="#2-1-API" class="headerlink" title="2.1 API"></a>2.1 API</h2><ul><li>setStartFromGroupOffsets()【默认消费策略】<br>默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</li><li>setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息</li><li>setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息</li><li>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)</li></ul><ul><li>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</li><li>为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次</li><li>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:<br>(1) Checkpoint关闭时： 可以通过下面两个参数配置<br>enable.auto.commit<br>auto.commit.interval.ms<br>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置<br>setCommitOffsetsOnCheckpoints(boolean)<br>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略</li></ul><h2 id="2-2-实例"><a href="#2-2-实例" class="headerlink" title="2.2 实例"></a>2.2 实例</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; public class StreamingKafkaSource {    public static void main(String[] args) throws Exception {        //获取Flink的运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //checkpoint配置        env.enableCheckpointing(5000);        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);        env.getCheckpointConfig().setCheckpointTimeout(60000);        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);        //设置statebackend        //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://hadoop100:9000/flink/checkpoints&quot;,true));        String topic = &quot;kafkaConsumer&quot;;        Properties prop = new Properties();        prop.setProperty(&quot;bootstrap.servers&quot;,&quot;SparkMaster:9092&quot;);        prop.setProperty(&quot;group.id&quot;,&quot;kafkaConsumerGroup&quot;);        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;&gt;(topic, new SimpleStringSchema(), prop);        myConsumer.setStartFromGroupOffsets();//默认消费策略        DataStreamSource&lt;String&gt; text = env.addSource(myConsumer);        text.print().setParallelism(1);        env.execute(&quot;StreamingFromCollection&quot;);    }}</code></pre><h1 id="3-FlinkKafkaProducer011"><a href="#3-FlinkKafkaProducer011" class="headerlink" title="3 FlinkKafkaProducer011"></a>3 FlinkKafkaProducer011</h1><h2 id="3-1-API"><a href="#3-1-API" class="headerlink" title="3.1 API"></a>3.1 API</h2><ul><li>Kafka Producer的容错-Kafka 0.9 and 0.10</li><li>如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数:<br>setLogFailuresOnly(false)<br>setFlushOnCheckpoint(true)</li><li>注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是0】</li><li>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义<br>Semantic.NONE<br>Semantic.AT_LEAST_ONCE【默认】<br>Semantic.EXACTLY_ONCE</li></ul><h2 id="3-2-实例"><a href="#3-2-实例" class="headerlink" title="3.2 实例"></a>3.2 实例</h2><pre><code>public class StreamingKafkaSink {    public static void main(String[] args) throws Exception {    //获取Flink的运行环境    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    //checkpoint配置    env.enableCheckpointing(5000);    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);    env.getCheckpointConfig().setCheckpointTimeout(60000);    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);    env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);    //设置statebackend    //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://SparkMaster:9000/flink/checkpoints&quot;,true));    DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;SparkMaster&quot;, 9001, &quot;\n&quot;);    String brokerList = &quot;SparkMaster:9092&quot;;    String topic = &quot;kafkaProducer&quot;;    Properties prop = new Properties();    prop.setProperty(&quot;bootstrap.servers&quot;,brokerList);    //第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间    //设置事务超时时间    //prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);    //第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。    //FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());    //使用EXACTLY_ONCE语义的kafkaProducer    FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(topic, new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);    text.addSink(myProducer);    env.execute(&quot;StreamingFromCollection&quot;);  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Kafka </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
