<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink 进阶：Time 深度解析</title>
      <link href="/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ATime%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>原  作  者 | 崔星灿</p><p>原整理者 | 沙晟阳（成阳）</p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 的 API 大体上可以划分为三个层次：处于最底层的 ProcessFunction、中间一层的 DataStream API 和最上层的 SQL/Table API，这三层中的每一层都非常依赖于时间属性。时间属性是流处理中最重要的一个方面，是流处理系统的基石之一，贯穿这三层 API。在 DataStream API 这一层中因为封装方面的原因，我们能够接触到时间的地方不是很多，所以我们将重点放在底层的 ProcessFunction 和最上层的 SQL/Table API。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/001-1024x449.png" alt="img"></p><h3 id="Flink-时间语义"><a href="#Flink-时间语义" class="headerlink" title="Flink 时间语义"></a>Flink 时间语义</h3><p>在不同的应用场景中时间语义是各不相同的，Flink 作为一个先进的分布式流处理引擎，它本身支持不同的时间语义。其核心是 Processing Time 和 Event Time（Row Time），这两类时间主要的不同点如下表所示：</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/002-1024x415.png" alt="img"></p><p>Processing Time 是来模拟我们真实世界的时间，其实就算是处理数据的节点本地时间也不一定就是完完全全的我们真实世界的时间，所以说它是用来模拟真实世界的时间。而 Event Time 是数据世界的时间，就是我们要处理的数据流世界里面的时间。关于他们的获取方式，Process Time 是通过直接去调用本地机器的时间，而 Event Time 则是根据每一条处理记录所携带的时间戳来判定。</p><p>这两种时间在 Flink 内部的处理以及还是用户的实际使用方面，难易程度都是不同的。相对而言的 Processing Time 处理起来更加的简单，而 Event Time 要更麻烦一些。而在使用 Processing Time 的时候，我们得到的处理结果（或者说流处理应用的内部状态）是不确定的。而因为在 Flink 内部对 Event Time 做了各种保障，使用 Event Time 的情况下，无论重放数据多少次，都能得到一个相对确定可重现的结果。</p><p>因此在判断应该使用 Processing Time 还是 Event Time 的时候，可以遵循一个原则：当你的应用遇到某些问题要从上一个 checkpoint 或者 savepoint 进行重放，是不是希望结果完全相同。如果希望结果完全相同，就只能用 Event Time；如果接受结果不同，则可以用 Processing Time。Processing Time 的一个常见的用途是，我们要根据现实时间来统计整个系统的吞吐，比如要计算现实时间一个小时处理了多少条数据，这种情况只能使用 Processing Time。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/003-1024x499.png" alt="img"></p><h4 id="时间的特性"><a href="#时间的特性" class="headerlink" title="时间的特性"></a>时间的特性</h4><p><strong>时间的一个重要特性是：时间只能递增，不会来回穿越。</strong> 在使用时间的时候我们要充分利用这个特性。假设我们有这么一些记录，然后我们来分别看一下 Processing Time 还有 Event Time 对于时间的处理。</p><ul><li>对于 Processing Time，因为我们是使用的是本地节点的时间（假设这个节点的时钟同步没有问题），我们每一次取到的 Processing Time 肯定都是递增的，递增就代表着有序，所以说我们相当于拿到的是一个有序的数据流。</li><li>而在用 Event Time 的时候因为时间是绑定在每一条的记录上的，由于网络延迟、程序内部逻辑、或者其他一些分布式系统的原因，数据的时间可能会存在一定程度的乱序，比如上图的例子。在 Event Time 场景下，我们把每一个记录所包含的时间称作 Record Timestamp。如果 Record Timestamp 所得到的时间序列存在乱序，我们就需要去处理这种情况。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/004.png" alt="img"></p><p>如果单条数据之间是乱序，我们就考虑对于整个序列进行更大程度的离散化。简单地讲，就是把数据按照一定的条数组成一些小批次，但这里的小批次并不是攒够多少条就要去处理，而是为了对他们进行时间上的划分。经过这种更高层次的离散化之后，我们会发现最右边方框里的时间就是一定会小于中间方框里的时间，中间框里的时间也一定会小于最左边方框里的时间。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/005.png" alt="img"></p><p>这个时候我们在整个时间序列里插入一些类似于标志位的一些特殊的处理数据，这些特殊的处理数据叫做 watermark。一个 watermark 本质上就代表了这个 watermark 所包含的 timestamp 数值，表示以后到来的数据已经再也没有小于或等于这个时间的了。</p><h3 id="Timestamp-和-Watermark-行为概览"><a href="#Timestamp-和-Watermark-行为概览" class="headerlink" title="Timestamp 和 Watermark 行为概览"></a>Timestamp 和 Watermark 行为概览</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/006-1024x442.png" alt="img"></p><p>接下来我们重点看一下 Event Time 里的 Record Timestamp（简写成 timestamp）和 watermark 的一些基本信息。绝大多数的分布式流计算引擎对于数据都是进行了 DAG 图的抽象，它有自己的数据源，有处理算子，还有一些数据汇。数据在不同的逻辑算子之间进行流动。watermark 和 timestamp 有自己的生命周期，接下来我会从 watermark 和 timestamp 的产生、他们在不同的节点之间的传播、以及在每一个节点上的处理，这三个方面来展开介绍。</p><h4 id="Timestamp-分配和-Watermark-生成"><a href="#Timestamp-分配和-Watermark-生成" class="headerlink" title="Timestamp 分配和 Watermark 生成"></a>Timestamp 分配和 Watermark 生成</h4><p>Flink 支持两种 watermark 生成方式。第一种是在 SourceFunction 中产生，相当于把整个的 timestamp 分配和 watermark 生成的逻辑放在流处理应用的源头。我们可以在 SourceFunction 里面通过这两个方法产生 watermark：</p><ul><li>通过 collectWithTimestamp 方法发送一条数据，其中第一个参数就是我们要发送的数据，第二个参数就是这个数据所对应的时间戳；也可以调用 emitWatermark 方法去产生一条 watermark，表示接下来不会再有时间戳小于等于这个数值记录。</li><li>另外，有时候我们不想在 SourceFunction 里生成 timestamp 或者 watermark，或者说使用的 SourceFunction 本身不支持，我们还可以在使用 DataStream API 的时候指定，调用的 DataStream.assignTimestampsAndWatermarks 这个方法，能够接收不同的 timestamp 和 watermark 的生成器。</li></ul><p>总体上而言生成器可以分为两类：第一类是定期生成器；第二类是根据一些在流处理数据流中遇到的一些特殊记录生成的。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/007.png" alt="img"></p><p>两者的区别主要有三个方面，首先定期生成是现实时间驱动的，这里的“定期生成”主要是指 watermark（因为 timestamp 是每一条数据都需要有的），即定期会调用生成逻辑去产生一个 watermark。而根据特殊记录生成是数据驱动的，即是否生成 watermark 不是由现实时间来决定，而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了，这个时候相当于每一次分配 Timestamp 之后都会调用用户实现的 watermark 生成方法，用户需要在生成方法中去实现 watermark 的生成逻辑。</p><p>大家要注意的是就是我们在分配 timestamp 和生成 watermark 的过程，虽然在 SourceFunction 和 DataStream 中都可以指定，但是还是建议生成的工作越靠近 DataSource 越好。这样会方便让程序逻辑里面更多的 operator 去判断某些数据是否乱序。Flink 内部提供了很好的机制去保证这些 timestamp 和 watermark 被正确地传递到下游的节点。</p><h4 id="Watermark-传播"><a href="#Watermark-传播" class="headerlink" title="Watermark 传播"></a>Watermark 传播</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/008-1024x474.png" alt="img"></p><p>具体的传播策略基本上遵循这三点。</p><ul><li>首先，watermark 会以广播的形式在算子之间进行传播。比如说上游的算子，它连接了三个下游的任务，它会把自己当前的收到的 watermark 以广播的形式传到下游。</li><li>第二，如果在程序里面收到了一个 Long.MAX_VALUE 这个数值的 watermark，就表示对应的那一条流的一个部分不会再有数据发过来了，它相当于就是一个终止的一个标志。</li><li>第三，对于单流而言，这个策略比较好理解，而对于有多个输入的算子，watermark 的计算就有讲究了，一个原则是：单输入取其大，多输入取小。</li></ul><p>举个例子，假设这边蓝色的块代表一个算子的一个任务，然后它有三个输入，分别是 W1、W2、W3，这三个输入可以理解成任何的输入，这三个输入可能是属于同一个流，也可能是属于不同的流。然后在计算 watermark 的时候，对于单个输入而言是取他们的最大值，因为我们都知道 watermark 应该遵循一个单调递增的一个原则。对于多输入，它要统计整个算子任务的 watermark 时，就会取这三个计算出来的 watermark 的最小值。即一个多个输入的任务，它的 watermark 受制于最慢的那条输入流。这一点类似于木桶效应，整个木桶中装的水会就是受制于最矮的那块板。</p><p>watermark 在传播的时候有一个特点是，它的传播是幂等的。多次收到相同的 watermark，甚至收到之前的 watermark 都不会对最后的数值产生影响，因为对于单个输入永远是取最大的，而对于整个任务永远是取一个最小的。</p><p>同时我们可以注意到这种设计其实有一个局限，具体体现在它没有区分你这个输入是一条流多个 partition 还是来自于不同的逻辑上的流的 JOIN。对于同一个流的不同 partition，我们对他做这种强制的时钟同步是没有问题的，因为一开始就是把一条流拆散成不同的部分，但每一个部分之间共享相同的时钟。但是如果算子的任务是在做类似于 JOIN 操作，那么要求你两个输入的时钟强制同步其实没有什么道理的，因为完全有可能是把一条离现在时间很近的数据流和一个离当前时间很远的数据流进行 JOIN，这个时候对于快的那条流，因为它要等慢的那条流，所以说它可能就要在状态中去缓存非常多的数据，这对于整个集群来说是一个很大的性能开销。</p><h4 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h4><p>在正式介绍 watermark 的处理之前，先简单介绍 ProcessFunction，因为 watermark 在任务里的处理逻辑分为内部逻辑和外部逻辑。外部逻辑其实就是通过 ProcessFunction 来体现的，如果你需要使用 Flink 提供的时间相关的 API 的话就只能写在 ProcessFunction 里。</p><p>ProcessFunction 和时间相关的功能主要有三点：</p><ul><li>第一点就是根据你当前系统使用的时间语义不同，你可以去获取当前你正在处理这条记录的 Record Timestamp，或者当前的 Processing Time。</li><li>第二点就是它可以获取当前算子的时间，可以把它理解成当前的 watermark。</li><li>第三点就是为了在 ProcessFunction 中去实现一些相对复杂的功能，允许注册一些 timer（定时器）。比如说在 watermark 达到某一个时间点的时候就触发定时器，所有的这些回调逻辑也都是由用户来提供，涉及到如下三个方法，registerEventTimeTimer、registerProcessingTimeTimer 和 onTimer。在 onTimer 方法中就需要去实现自己的回调逻辑，当条件满足时回调逻辑就会被触发。</li></ul><p>一个简单的应用是，我们在做一些时间相关的处理的时候，可能需要缓存一部分数据，但这些数据不能一直去缓存下去，所以需要有一些过期的机制，我们可以通过 timer 去设定这么一个时间，指定某一些数据可能在将来的某一个时间点过期，从而把它从状态里删除掉。所有的这些和时间相关的逻辑在 Flink 内部都是由自己的 Time Service（时间服务）完成的。</p><h4 id="Watermark-处理"><a href="#Watermark-处理" class="headerlink" title="Watermark 处理"></a>Watermark 处理</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/009-1024x376.png" alt="img"></p><p>一个算子的实例在收到 watermark 的时候，首先要更新当前的算子时间，这样的话在 ProcessFunction 里方法查询这个算子时间的时候，就能获取到最新的时间。第二步它会遍历计时器队列，这个计时器队列就是我们刚刚说到的 timer，你可以同时注册很多 timer，Flink 会把这些 Timer 按照触发时间放到一个优先队列中。第三步 Flink 得到一个时间之后就会遍历计时器的队列，然后逐一触发用户的回调逻辑。 通过这种方式，Flink 的某一个任务就会将当前的 watermark 发送到下游的其他任务实例上，从而完成整个 watermark 的传播，从而形成一个闭环。</p><h3 id="Table-API-中的时间"><a href="#Table-API-中的时间" class="headerlink" title="Table API 中的时间"></a>Table API 中的时间</h3><p>下面我们来看一看 Table/SQL API 中的时间。为了让时间参与到 Table/SQL 这一层的运算中，我们需要提前把时间属性放到表的 schema 中，这样的话我们才能够在 SQL 语句或者 Table 的一些逻辑表达式里面去使用这些时间去完成需求。</p><h4 id="Table-中指定时间列"><a href="#Table-中指定时间列" class="headerlink" title="Table 中指定时间列"></a>Table 中指定时间列</h4><p>其实之前社区就怎么在 Table/SQL 中去使用时间这个问题做过一定的讨论，是把获取当前 Processing Time 的方法是作为一个特殊的 UDF，还是把这一个列物化到整个的 schema 里面，最终采用了后者。我们这里就分开来讲一讲 Processing Time 和 Event Time 在使用的时候怎么在 Table 中指定。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/010-1024x487.png" alt="img"></p><p>对于 Processing Time，我们知道要得到一个 Table 对象（或者注册一个 Table）有两种手段：</p><ol><li>可以从一个 DataStream 转化成一个 Table；</li><li>直接通过 TableSource 去生成这么一个 Table；</li></ol><p>对于第一种方法而言，我们只需要在你已有的这些列中（例子中 f1 和 f2 就是两个已有的列），在最后用“列名.proctime”这种写法就可以把最后的这一列注册为一个 Processing Time，以后在写查询的时候就可以去直接使用这一列。如果 Table 是通过 TableSource 生成的，就可以通过实现这一个 DefinedRowtimeAttributes 接口，然后就会自动根据你提供的逻辑去生成对应的 Processing Time。</p><p>相对而言，在使用 Event Time 时则有一个限制，因为 Event Time 不像 Processing Time 那样是随拿随用。如果你要从 DataStream 去转化得到一个 Table，必须要提前保证原始的 DataStream 里面已经存在了 Record Timestamp 和 watermark。如果你想通过 TableSource 生成的，也一定要保证你要接入的一个数据里面存在一个类型为 long 或者 timestamp 的这么一个时间字段。</p><p>具体来说，如果你要从 DataStream 去注册一个表，和 proctime 类似，你只需要加上“列名.rowtime”就可以。需要注意的是，如果你要用 Processing Time，必须保证你要新加的字段是整个 schema 中的最后一个字段，而 Event Time 的时候你其实可以去替换某一个已有的列，然后 Flink 会自动的把这一列转化成需要的 rowtime 这个类型。 如果是通过 TableSource 生成的，只需要实现 DefinedRowtimeAttributes 接口就可以了。需要说明的一点是，在 DataStream API 这一侧其实不支持同时存在多个 Event Time（rowtime），但是在 Table 这一层理论上可以同时存在多个 rowtime。因为 DefinedRowtimeAttributes 接口的返回值是一个对于 rowtime 描述的 List，即其实可以同时存在多个 rowtime 列，在将来可能会进行一些其他的改进，或者基于去做一些相应的优化。</p><h4 id="时间列和-Table-操作"><a href="#时间列和-Table-操作" class="headerlink" title="时间列和 Table 操作"></a>时间列和 Table 操作</h4><p><img src="https://ververica.cn/wp-content/uploads/2019/09/011-1024x475.png" alt="img"></p><p>指定完了时间列之后，当我们要真正去查询时就会涉及到一些具体的操作。这里我列举的这些操作都是和时间列紧密相关，或者说必须在这个时间列上才能进行的。比如说“Over 窗口聚合”和“Group by 窗口聚合”这两种窗口聚合，在写 SQL 提供参数的时候只能允许你在这个时间列上进行这种聚合。第三个就是时间窗口聚合，你在写条件的时候只支持对应的时间列。最后就是排序，我们知道在一个无尽的数据流上对数据做排序几乎是不可能的事情，但因为这个数据本身到来的顺序已经是按照时间属性来进行排序，所以说我们如果要对一个 DataStream 转化成 Table 进行排序的话，你只能是按照时间列进行排序，当然同时你也可以指定一些其他的列，但是时间列这个是必须的，并且必须放在第一位。</p><p>为什么说这些操作只能在时间列上进行？因为我们有的时候可以把到来的数据流就看成是一张按照时间排列好的一张表，而我们任何对于表的操作，其实都是必须在对它进行一次顺序扫描的前提下完成的。因为大家都知道数据流的特性之一就是一过性，某一条数据处理过去之后，将来其实不太好去访问它。当然因为 Flink 中内部提供了一些状态机制，我们可以在一定程度上去弱化这个特性，但是最终还是不能超越的限制状态不能太大。所有这些操作为什么只能在时间列上进行，因为这个时间列能够保证我们内部产生的状态不会无限的增长下去，这是一个最终的前提。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 进阶：Flink Connector详解</title>
      <link href="/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9AFlink%20Connector%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/08/19/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9AFlink%20Connector%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p><strong>原作者 | 董亭亭</strong></p><p><strong>快手 实时计算引擎团队负责人</strong></p><p>董亭亭，快手大数据架构实时计算引擎团队负责人。目前负责Flink引擎在快手内的研发、应用以及周边子系统建设。2013年毕业于大连理工大学，曾就职于奇虎360、58集团。主要研究领域包括：分布式计算、调度系统、分布式存储等系统。</p><p>本文主要分享 Flink connector 相关内容，分为以下三个部分的内容：第一部分会首先介绍一下 Flink Connector 有哪些。第二部分会重点介绍在生产环境中经常使用的 kafka connector 的基本的原理以及使用方法。第三部分答疑，对社区反馈的问题进行答疑。</p><h3 id="Flink-Streaming-Connector"><a href="#Flink-Streaming-Connector" class="headerlink" title="Flink Streaming Connector"></a>Flink Streaming Connector</h3><p>Flink 是新一代流批统一的计算引擎，它需要从不同的第三方存储引擎中把数据读过来，进行处理，然后再写出到另外的存储引擎中。Connector 的作用就相当于一个连接器，连接 Flink 计算引擎跟外界存储系统。Flink 里有以下几种方式，当然也不限于这几种方式可以跟外界进行数据交换：</p><ul><li>第一种 Flink 里面预定义了一些 source 和 sink。</li><li>第二种 Flink 内部也提供了一些 Boundled connectors。</li><li>第三种可以使用第三方 Apache Bahir 项目中提供的连接器。</li><li>第四种是通过异步 IO 方式。</li></ul><p>下面分别简单介绍一下这四种数据读写的方式。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/1-1024x576.jpg" alt="img"></p><h4 id="1-预定义的-source-和-sink"><a href="#1-预定义的-source-和-sink" class="headerlink" title="1. 预定义的 source 和 sink"></a>1. 预定义的 source 和 sink</h4><p>Flink 里预定义了一部分 source 和 sink。在这里分了几类。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/2-1024x576.jpg" alt="img"></p><h5 id="基于文件的-source-和-sink"><a href="#基于文件的-source-和-sink" class="headerlink" title="基于文件的 source 和 sink"></a><strong>基于文件的 source 和 sink</strong></h5><p>****如果要从文本文件中读取数据，可以直接使用：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">env</span><span class="selector-class">.readTextFile</span>(<span class="selector-tag">path</span>)</span><br></pre></td></tr></table></figure><p>就可以以文本的形式读取该文件中的内容。当然也可以使用：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">env</span><span class="selector-class">.readFile</span>(<span class="selector-tag">fileInputFormat</span>, <span class="selector-tag">path</span>)</span><br></pre></td></tr></table></figure><p>根据指定的 fileInputFormat 格式读取文件中的内容。</p><p>如果数据在 Flink 内进行了一系列的计算，想把结果写出到文件里，也可以直接使用内部预定义的一些 sink，比如将结果已文本或 csv 格式写出到文件中，可以使用 DataStream 的 writeAsText(path) 和 writeAsCsv(path)。</p><h5 id="基于-Socket-的-Source-和-Sink"><a href="#基于-Socket-的-Source-和-Sink" class="headerlink" title="基于 Socket 的 Source 和 Sink"></a><strong>基于 Socket 的 Source 和 Sink</strong></h5><p>提供 Socket 的 host name 及 port，可以直接用 StreamExecutionEnvironment 预定的接口 socketTextStream 创建基于 Socket 的 source，从该 socket 中以文本的形式读取数据。当然如果想把结果写出到另外一个 Socket，也可以直接调用 DataStream writeToSocket。</p><h5 id="基于内存-Collections、Iterators-的-Source"><a href="#基于内存-Collections、Iterators-的-Source" class="headerlink" title="基于内存 Collections、Iterators 的 Source"></a><strong>基于内存 Collections、Iterators 的 Source</strong></h5><p>可以直接基于内存中的集合或者迭代器，调用 StreamExecutionEnvironment fromCollection、fromElements 构建相应的 source。结果数据也可以直接 print、printToError 的方式写出到标准输出或标准错误。</p><p>详细也可以参考 Flink 源码中提供的一些相对应的 Examples 来查看异常预定义 source 和 sink 的使用方法，例如 WordCount、SocketWindowWordCount。</p><h4 id="2-Bundled-Connectors"><a href="#2-Bundled-Connectors" class="headerlink" title="2. Bundled Connectors"></a><strong>2. Bundled Connectors</strong></h4><p>Flink 里已经提供了一些绑定的 Connector，例如 kafka source 和 sink，Es sink等。读写 kafka、es、rabbitMQ 时可以直接使用相应 connector 的 api 即可。第二部分会详细介绍生产环境中最常用的 kafka connector。</p><p>虽然该部分是 Flink 项目源代码里的一部分，但是真正意义上不算作 Flink 引擎相关逻辑，并且该部分没有打包在二进制的发布包里面。所以在提交 Job 时候需要注意， job 代码 jar 包中一定要将相应的 connetor 相关类打包进去，否则在提交作业时就会失败，提示找不到相应的类，或初始化某些类异常。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/3-1024x576.jpg" alt="img"></p><h4 id="3-Apache-Bahir-中的连接器"><a href="#3-Apache-Bahir-中的连接器" class="headerlink" title="3. Apache Bahir 中的连接器"></a><strong>3. Apache Bahir 中的连接器</strong></h4><p>Apache Bahir 最初是从 Apache Spark 中独立出来项目提供，以提供不限于 Spark 相关的扩展/插件、连接器和其他可插入组件的实现。通过提供多样化的流连接器（streaming connectors）和 SQL 数据源扩展分析平台的覆盖面。如有需要写到 flume、redis 的需求的话，可以使用该项目提供的 connector。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/4-1024x576.jpg" alt="img"></p><h4 id="4-Async-I-O"><a href="#4-Async-I-O" class="headerlink" title="4. Async I/O"></a><strong>4. Async I/O</strong></h4><p>流计算中经常需要与外部存储系统交互，比如需要关联 MySQL 中的某个表。一般来说，如果用同步 I/O 的方式，会造成系统中出现大的等待时间，影响吞吐和延迟。为了解决这个问题，异步 I/O 可以并发处理多个请求，提高吞吐，减少延迟。</p><p>Async 的原理可参考官方文档：</p><p><strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.3/dev/stream/asyncio.html</a></strong></p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/5-1024x576.jpg" alt="img"></p><h3 id="Flink-Kafka-Connector"><a href="#Flink-Kafka-Connector" class="headerlink" title="Flink Kafka Connector"></a>Flink Kafka Connector</h3><p>本章重点介绍生产环境中最常用到的 Flink kafka connector。使用 Flink 的同学，一定会很熟悉 kafka，它是一个分布式的、分区的、多副本的、 支持高吞吐的、发布订阅消息系统。生产环境环境中也经常会跟 kafka 进行一些数据的交换，比如利用 kafka consumer 读取数据，然后进行一系列的处理之后，再将结果写出到 kafka 中。这里会主要分两个部分进行介绍，一是 Flink kafka Consumer，一个是 Flink kafka Producer。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/6-1024x576.jpg" alt="img"></p><p>首先看一个例子来串联下 Flink kafka connector。代码逻辑里主要是从 kafka 里读数据，然后做简单的处理，再写回到 kafka 中。</p><p>分别用红框框出如何构造一个 Source sink Function。Flink 提供了现成的构造FlinkKafkaConsumer、Producer 的接口，可以直接使用。这里需要注意，因为 kafka 有多个版本，多个版本之间的接口协议会不同。Flink 针对不同版本的 kafka 有相应的版本的 Consumer 和 Producer。例如：针对 08、09、10、11 版本，Flink 对应的 consumer 分别是 FlinkKafkaConsumer 08、09、010、011，producer 也是。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/7-1024x576.jpg" alt="img"></p><h4 id="1-Flink-kafka-Consumer"><a href="#1-Flink-kafka-Consumer" class="headerlink" title="1. Flink kafka Consumer"></a><strong>1. Flink kafka Consumer</strong></h4><h5 id="反序列化数据"><a href="#反序列化数据" class="headerlink" title="反序列化数据"></a><strong>反序列化数据</strong></h5><p>因为 kafka 中数据都是以二进制 byte 形式存储的。读到 Flink 系统中之后，需要将二进制数据转化为具体的 java、scala 对象。具体需要实现一个 schema 类，定义如何序列化和反序列数据。反序列化时需要实现 DeserializationSchema 接口，并重写 deserialize(byte[] message) 函数，如果是反序列化 kafka 中 kv 的数据时，需要实现 KeyedDeserializationSchema 接口，并重写 deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) 函数。</p><p>另外 Flink 中也提供了一些常用的序列化反序列化的 schema 类。例如，SimpleStringSchema，按字符串方式进行序列化、反序列化。TypeInformationSerializationSchema，它可根据 Flink 的 TypeInformation 信息来推断出需要选择的 schema。JsonDeserializationSchema 使用 jackson 反序列化 json 格式消息，并返回 ObjectNode，可以使用 .get(“property”) 方法来访问相应字段。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/8-1024x576.jpg" alt="img"></p><h5 id="消费起始位置设置"><a href="#消费起始位置设置" class="headerlink" title="消费起始位置设置"></a><strong>消费起始位置设置</strong></h5><p>如何设置作业从 kafka 消费数据最开始的起始位置，这一部分 Flink 也提供了非常好的封装。在构造好的 FlinkKafkaConsumer 类后面调用如下相应函数，设置合适的起始位置。</p><ul><li><strong>setStartFromGroupOffsets</strong>，也是默认的策略，从 group offset 位置读取数据，group offset 指的是 kafka broker 端记录的某个 group 的最后一次的消费位置。但是 kafka broker 端没有该 group 信息，会根据 kafka 的参数”auto.offset.reset”的设置来决定从哪个位置开始消费。</li><li><strong>setStartFromEarliest</strong>，从 kafka 最早的位置开始读取。</li><li><strong>setStartFromLatest</strong>，从 kafka 最新的位置开始读取。</li><li><strong>setStartFromTimestamp(long)</strong>，从时间戳大于或等于指定时间戳的位置开始读取。Kafka 时戳，是指 kafka 为每条消息增加另一个时戳。该时戳可以表示消息在 proudcer 端生成时的时间、或进入到 kafka broker 时的时间。</li><li><strong>setStartFromSpecificOffsets</strong>，从指定分区的 offset 位置开始读取，如指定的 offsets 中不存某个分区，该分区从 group offset 位置开始读取。此时需要用户给定一个具体的分区、offset 的集合。</li></ul><p>一些具体的使用方法可以参考下图。需要注意的是，因为 Flink 框架有容错机制，如果作业故障，如果作业开启 checkpoint，会从上一次 checkpoint 状态开始恢复。或者在停止作业的时候主动做 savepoint，启动作业时从 savepoint 开始恢复。这两种情况下恢复作业时，作业消费起始位置是从之前保存的状态中恢复，与上面提到跟 kafka 这些单独的配置无关。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/9-1024x576.jpg" alt="img"></p><h5 id="topic-和-partition-动态发现"><a href="#topic-和-partition-动态发现" class="headerlink" title="topic 和 partition 动态发现"></a><strong>topic 和 partition 动态发现</strong></h5><p>实际的生产环境中可能有这样一些需求，比如场景一，有一个 Flink 作业需要将五份数据聚合到一起，五份数据对应五个 kafka topic，随着业务增长，新增一类数据，同时新增了一个 kafka topic，如何在不重启作业的情况下作业自动感知新的 topic。场景二，作业从一个固定的 kafka topic 读数据，开始该 topic 有 10 个 partition，但随着业务的增长数据量变大，需要对 kafka partition 个数进行扩容，由 10 个扩容到 20。该情况下如何在不重启作业情况下动态感知新扩容的 partition？</p><p>针对上面的两种场景，首先需要在构建 FlinkKafkaConsumer 时的 properties 中设置 flink.partition-discovery.interval-millis 参数为非负值，表示开启动态发现的开关，以及设置的时间间隔。此时 FlinkKafkaConsumer 内部会启动一个单独的线程定期去 kafka 获取最新的 meta 信息。针对场景一，还需在构建 FlinkKafkaConsumer 时，topic 的描述可以传一个正则表达式描述的 pattern。每次获取最新 kafka meta 时获取正则匹配的最新 topic 列表。针对场景二，设置前面的动态发现参数，在定期获取 kafka 最新 meta 信息时会匹配新的 partition。为了保证数据的正确性，新发现的 partition 从最早的位置开始读取。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/10-1024x576.jpg" alt="img"></p><h5 id="commit-offset-方式"><a href="#commit-offset-方式" class="headerlink" title="commit offset 方式"></a><strong>commit offset 方式</strong></h5><p>Flink kafka consumer commit offset 方式需要区分是否开启了 checkpoint。</p><p>如果 checkpoint 关闭，commit offset 要依赖于 kafka 客户端的 auto commit。需设置 enable.auto.commit，auto.commit.interval.ms 参数到 consumer properties，就会按固定的时间间隔定期 auto commit offset 到 kafka。</p><p>如果开启 checkpoint，这个时候作业消费的 offset 是 Flink 在 state 中自己管理和容错。此时提交 offset 到 kafka，一般都是作为外部进度的监控，想实时知道作业消费的位置和 lag 情况。此时需要 setCommitOffsetsOnCheckpoints 为 true 来设置当 checkpoint 成功时提交 offset 到 kafka。此时 commit offset 的间隔就取决于 checkpoint 的间隔，所以此时从 kafka 一侧看到的 lag 可能并非完全实时，如果 checkpoint 间隔比较长 lag 曲线可能会是一个锯齿状。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/11-1024x576.jpg" alt="img"></p><h5 id="Timestamp-Extraction-Watermark-生成"><a href="#Timestamp-Extraction-Watermark-生成" class="headerlink" title="Timestamp Extraction/Watermark 生成"></a><strong>Timestamp Extraction/Watermark 生成</strong></h5><p>我们知道当 Flink 作业内使用 EventTime 属性时，需要指定从消息中提取时戳和生成水位的函数。FlinkKakfaConsumer 构造的 source 后直接调用 assignTimestampsAndWatermarks 函数设置水位生成器的好处是此时是每个 partition 一个 watermark assigner，如下图。source 生成的时戳为多个 partition 时戳对齐后的最小时戳。此时在一个 source 读取多个 partition，并且 partition 之间数据时戳有一定差距的情况下，因为在 source 端 watermark 在 partition 级别有对齐，不会导致数据读取较慢 partition 数据丢失。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/12-1024x576.jpg" alt="img"></p><h4 id="2-Flink-kafka-Producer"><a href="#2-Flink-kafka-Producer" class="headerlink" title="2. Flink kafka Producer"></a><strong>2. Flink kafka Producer</strong></h4><h5 id="Producer-分区"><a href="#Producer-分区" class="headerlink" title="Producer 分区"></a><strong>Producer 分区</strong></h5><p>使用 FlinkKafkaProducer 往 kafka 中写数据时，如果不单独设置 partition 策略，会默认使用 FlinkFixedPartitioner，该 partitioner 分区的方式是 task 所在的并发 id 对 topic 总 partition 数取余：parallelInstanceId % partitions.length。</p><ul><li>此时如果 sink 为 4，paritition 为 1，则 4 个 task 往同一个 partition 中写数据。但当 sink task &lt; partition 个数时会有部分 partition 没有数据写入，例如 sink task 为2，partition 总数为 4，则后面两个 partition 将没有数据写入。</li><li>如果构建 FlinkKafkaProducer 时，partition 设置为 null，此时会使用 kafka producer 默认分区方式，非 key 写入的情况下，使用 round-robin 的方式进行分区，每个 task 都会轮循的写下游的所有 partition。该方式下游的 partition 数据会比较均衡，但是缺点是 partition 个数过多的情况下需要维持过多的网络连接，即每个 task 都会维持跟所有 partition 所在 broker 的连接。</li></ul><p><img src="https://ververica.cn/wp-content/uploads/2019/09/13-1024x576.jpg" alt="img"></p><h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a><strong>容错</strong></h4><p>Flink kafka 09、010 版本下，通过 setLogFailuresOnly 为 false，setFlushOnCheckpoint 为 true，能达到 at-least-once 语义。setLogFailuresOnly，默认为 false，是控制写 kafka 失败时，是否只打印失败的 log 不抛异常让作业停止。setFlushOnCheckpoint，默认为 true，是控制是否在 checkpoint 时 fluse 数据到 kafka，保证数据已经写到 kafka。否则数据有可能还缓存在 kafka 客户端的 buffer 中，并没有真正写出到 kafka，此时作业挂掉数据即丢失，不能做到至少一次的语义。</p><p>Flink kafka 011 版本下，通过两阶段提交的 sink 结合 kafka 事务的功能，可以保证端到端精准一次。详细原理可以参考：<a href="https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka?__hstc=212828427.9e09d869892d3ee97973eb4d3a811b9f.1569201644494.1569389273878.1569459347925.10&__hssc=212828427.3.1569459347925&__hsfp=3591275194" target="_blank" rel="noopener">https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka</a></p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/14-1024x576.jpg" alt="img"></p><h3 id="一些疑问与解答"><a href="#一些疑问与解答" class="headerlink" title="一些疑问与解答"></a><strong>一些疑问与解答</strong></h3><h4 id="Q：在-Flink-consumer-的并行度的设置：是对应-topic-的-partitions-个数吗？要是有多个主题数据源，并行度是设置成总体的-partitions-数吗？"><a href="#Q：在-Flink-consumer-的并行度的设置：是对应-topic-的-partitions-个数吗？要是有多个主题数据源，并行度是设置成总体的-partitions-数吗？" class="headerlink" title="Q：在 Flink consumer 的并行度的设置：是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？"></a>Q：在 Flink consumer 的并行度的设置：是对应 topic 的 partitions 个数吗？要是有多个主题数据源，并行度是设置成总体的 partitions 数吗？</h4><p><strong>A：</strong>这个并不是绝对的，跟 topic 的数据量也有关，如果数据量不大，也可以设置小于 partitions 个数的并发数。但不要设置并发数大于 partitions 总数，因为这种情况下某些并发因为分配不到 partition 导致没有数据处理。</p><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="Q：如果-partitioner-传-null-的时候是-round-robin-发到每一个-partition？如果有-key-的时候行为是-kafka-那种按照-key-分布到具体分区的行为吗？"><a href="#Q：如果-partitioner-传-null-的时候是-round-robin-发到每一个-partition？如果有-key-的时候行为是-kafka-那种按照-key-分布到具体分区的行为吗？" class="headerlink" title="Q：如果 partitioner 传 null 的时候是 round-robin 发到每一个 partition？如果有  key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？"></a>Q：如果 partitioner 传 null 的时候是 round-robin 发到每一个 partition？如果有  key 的时候行为是 kafka 那种按照 key 分布到具体分区的行为吗？</h4><p><strong>A：</strong>如果在构造 FlinkKafkaProducer 时，如果没有设置单独的 partitioner，则默认使用 FlinkFixedPartitioner，此时无论是带 key 的数据，还是不带 key。如果主动设置 partitioner 为 null 时，不带 key 的数据会 round-robin 的方式写出，带 key 的数据会根据 key，相同 key 数据分区的相同的 partition，如果 key 为 null，再轮询写。不带 key 的数据会轮询写各 partition。</p><h4 id="Q：如果-checkpoint-时间过长，offset-未提交到-kafka，此时节点宕机了，重启之后的重复消费如何保证呢？"><a href="#Q：如果-checkpoint-时间过长，offset-未提交到-kafka，此时节点宕机了，重启之后的重复消费如何保证呢？" class="headerlink" title="Q：如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？"></a>Q：如果 checkpoint 时间过长，offset 未提交到 kafka，此时节点宕机了，重启之后的重复消费如何保证呢？</h4><p><strong>A：</strong>首先开启 checkpoint 时 offset 是 Flink 通过状态 state 管理和恢复的，并不是从 kafka 的 offset 位置恢复。在 checkpoint 机制下，作业从最近一次 checkpoint 恢复，本身是会回放部分历史数据，导致部分数据重复消费，Flink 引擎仅保证计算状态的精准一次，要想做到端到端精准一次需要依赖一些幂等的存储系统或者事务操作。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 进阶：增量 Checkpoint 详解</title>
      <link href="/2019/08/16/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9A%E5%A2%9E%E9%87%8F%20Checkpoint%20%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/08/16/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9A%E5%A2%9E%E9%87%8F%20Checkpoint%20%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>原作者 | Stefan Ricther &amp; Chris Ward<br>原翻译 | 邱从贤（山智）</p><p>via | <a href="https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html" target="_blank" rel="noopener">https://flink.apache.org/features/2018/01/30/incremental-checkpointing.html</a></p><p>Apache Flink 是一个有状态的流计算框架，状态是作业算子中已经处理过的内存状态，供后续处理时使用。状态在流计算很多复杂场景中非常重要，比如：</p><ul><li>保存所有历史记录，用来寻找某种记录模式</li><li>保存最近一分钟的所有记录，用于对每分钟的记录进行聚合统计</li><li>保存当前的模型参数，用于进行模型训练</li></ul><p>有状态的流计算框架必须有很好的容错性，才能在生产环境中发挥用处。这里的容错性是指，不管是发生硬件故障，还是程序异常，最终的结果不丢也不重。</p><p><strong>Flink 的容错性从一开始就是一个非常强大的特性，在遇到故障时，能够保证不丢不重，且对正常逻辑处理的性能影响很小。</strong></p><p>这里面的核心就是 checkpoint 机制，Flink 使用 checkpoint 机制来进行状态保证，在 Flink 中 checkpoint 是一个定时触发的全局异步快照，并持久化到持久存储系统上（通常是分布式文件系统）。发生故障后，Flink 选择从最近的一个快照进行恢复。有用户的作业状态达到 GB 甚至 TB 级别，对这么大的作业状态做一次 checkpoint 会非常耗时，耗资源，因此我们在 Flink 1.3 中引入了增量 checkpoint 机制。</p><p>在增量 checkpoint 之前，Flink 的每个 checkpoint 都包含作业的所有状态。我们在观察到状态在 checkpoint 之间的变化并没有那么大之后，支持了增量 checkpoint。增量 checkpoint 仅包含上次 checkpoint 和本次 checkpoint 之间状态的差异（也就是“增量”）。</p><p>对于状态非常大的作业，增量 checkpoint 对性能的提升非常明显。<strong>有生产用户反馈对于 TB 级别的作业，使用增量 checkpoint 后能将 checkpoint 的整体时间从 3 分钟降到 30 秒。</strong>这些时间节省主要归功于不需要在每次 checkpoint 都将所有状态写到持久化存储系统。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>当前，仅能够在 RocksDB StateBackend 上使用增量 checkpoint 机制，Flink 依赖 RocksDB 内部的备份机制来生成 checkpoint 文件。Flink 会自动清理掉之前的 checkpoint 文件, 因此增量 checkpoint 的历史记录不会无限增长。</p><p>为了在作业中开启增量 checkpoint，建议详细阅读 Apache Flink 的 checkpoint 文档，简单的说，你可以像之前一样开启 checkpoint，然后将构造函数的第二个参数设置为 true 来启用增量 checkpoint。</p><h3 id="Java-示例"><a href="#Java-示例" class="headerlink" title="Java 示例"></a>Java 示例</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> <span class="type">RocksDBStateBackend</span>(filebackend, <span class="literal">true</span>));</span><br></pre></td></tr></table></figure><h3 id="Scala-示例"><a href="#Scala-示例" class="headerlink" title="Scala 示例"></a>Scala 示例</h3><figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> env = StreamExecutionEnvironment.getExecutionEnvironment()</span><br><span class="line">env.setStateBackend(<span class="keyword">new</span> RocksDBStateBackend(filebackend, <span class="keyword">true</span>))</span><br></pre></td></tr></table></figure><p>Flink 默认保留一个成功的 checkpoint，如果你需要保留多个的话，可以通过下面的配置进行设置：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">state</span>.checkpoints.num-retained</span><br></pre></td></tr></table></figure><h3 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h3><p>Flink 的增量 checkpoint 以 RocksDB 的 checkpoint 为基础。RocksDB 是一个 LSM 结构的 KV 数据库，把所有的修改保存在内存的可变缓存中（称为 memtable），所有对 memtable 中 key 的修改，会覆盖之前的 value，当前 memtable 满了之后，RocksDB 会将所有数据以有序的写到磁盘。当 RocksDB 将 memtable 写到磁盘后，整个文件就不再可变，称为有序字符串表（sstable）。</p><p>RocksDB 的后台压缩线程会将 sstable 进行合并，就重复的键进行合并，合并后的 sstable 包含所有的键值对，RocksDB 会删除合并前的 sstable。</p><p>在这个基础上，Flink 会记录上次 checkpoint 之后所有新生成和删除的 sstable，另外因为 sstable 是不可变的，Flink 用 sstable 来记录状态的变化。为此，<strong>Flink 调用 RocksDB 的 flush，强制将 memtable 的数据全部写到 sstable，并硬链到一个临时目录中。这个步骤是在同步阶段完成，其他剩下的部分都在异步阶段完成，不会阻塞正常的数据处理。</strong></p><p>Flink 将所有新生成的 sstable 备份到持久化存储（比如 HDFS，S3），并在新的 checkpoint 中引用。Flink 并不备份前一个 checkpoint 中已经存在的 sstable，而是引用他们。Flink 还能够保证所有的 checkpoint 都不会引用已经删除的文件，因为 RocksDB 中文件删除是由压缩完成的，压缩后会将原来的内容合并写成一个新的 sstable。因此，Flink 增量 checkpoint 能够切断 checkpoint 历史。</p><p>为了追踪 checkpoint 间的差距，备份合并后的 sstable 是一个相对冗余的操作。但是 Flink 会增量的处理，增加的开销通常很小，并且可以保持一个更短的 checkpoint 历史，恢复时从更少的 checkpoint 进行读取文件，因此我们认为这是值得的。</p><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p><img src="https://ververica.cn/wp-content/uploads/2019/09/Checkpoint-.jpg" alt="img"></p><p>上图以一个有状态的算子为例，checkpoint 最多保留 2 个，上图从左到右分别记录每次 checkpoint 时本地的 RocksDB 状态文件，引用的持久化存储上的文件，以及当前 checkpoint 完成后文件的引用计数情况。</p><ul><li><strong>Checkpoint 1 的时候</strong>，本地 RocksDB 包含两个 sstable 文件，该 checkpoint 会把这两个文件备份到持久化存储，当 checkpoint 完成后，对这两个文件的引用计数进行加 1，引用计数使用键值对的方式保存，其中键由算子的当前并发以及文件名所组成。我们同时会维护一个引用计数中键到对应文件的隐射关系。</li><li><strong>Checkpoint 2 的时候</strong>，RocksDB 生成两个新的 sstable 文件，并且两个旧的文件还存在。Flink 会把两个新的文件进行备份，然后引用两个旧的文件，当 checkpoint 完成时，Flink 对这 4 个文件都进行引用计数 +1 操作。</li><li><strong>Checkpoint 3 的时候</strong>，RocksDB 将 sstable-(1)，sstable-(2) 以及 sstable-(3) 合并成 sstable-(1,2,3)，并且删除了三个旧文件，新生成的文件包含了三个删除文件的所有键值对。sstable-(4) 还继续存在，生成一个新的 sstable-(5) 文件。Flink 会将 sstable-(1,2,3) 和 sstable-(5) 备份到持久化存储，然后增加 sstable-4 的引用计数。由于保存的 checkpoint 数达到上限（2 个），因此会删除 checkpoint 1，然后对 checkpoint 1 中引用的所有文件（sstable-(1) 和 sstable-(2)）的引用计数进行 -1 操作。</li><li><strong>Checkpoint 4 的时候</strong>，RocksDB 将 sstable-(4)，sstable-(5) 以及新生成的 sstable-(6) 合并成一个新的 sstable-(4,5,6)。Flink 将 sstable-(4,5,6) 备份到持久化存储，并对 sstabe-(1,2,3) 和 sstable-(4,5,6) 进行引用计数 +1 操作，然后删除 checkpoint 2，并对 checkpoint 引用的文件进行引用计数 -1 操作。这个时候 sstable-(1)，sstable-(2) 以及 sstable-(3) 的引用计数变为 0，Flink 会从持久化存储删除这三个文件。</li></ul><h3 id="竞争问题以及并发-checkpoint"><a href="#竞争问题以及并发-checkpoint" class="headerlink" title="竞争问题以及并发 checkpoint"></a>竞争问题以及并发 checkpoint</h3><p>Flink 支持并发 checkpoint，有时晚触发的 checkpoint 会先完成，因此增量 checkpoint 需要选择一个正确的基准。Flink 仅会引用成功的 checkpoint 文件，从而防止引用一些被删除的文件。</p><h3 id="从-checkpoint-恢复以及性能"><a href="#从-checkpoint-恢复以及性能" class="headerlink" title="从 checkpoint 恢复以及性能"></a>从 checkpoint 恢复以及性能</h3><p>开启增量 checkpoint 之后，不需要再进行其他额外的配置。如果 Job 异常，Flink 的 JobMaster 会通知所有 task 从上一个成功的 checkpoint 进行恢复，不管是全量 checkpoint 还是增量 checkpoint。每个 TaskManager 会从持久化存储下载他们需要的状态文件。</p><p>尽管增量 checkpoint 能减少大状态下的 checkpoint 时间，但是天下没有免费的午餐，我们需要在其他方面进行舍弃。增量 checkpoint 可以减少 checkpoint 的总时间，但是也可能导致恢复的时候需要更长的时间<strong>。</strong>如果集群的故障频繁，Flink 的 TaskManager 需要从多个 checkpoint 中下载需要的状态文件（这些文件中包含一些已经被删除的状态），作业恢复的整体时间可能比不使用增量 checkpoint 更长。</p><p>另外在增量 checkpoint 情况下，我们不能删除旧 checkpoint 生成的文件，因为新的 checkpoint 会继续引用它们，这可能导致需要更多的存储空间，并且恢复的时候可能消耗更多的带宽。</p><p>关于控制便捷性与性能之间平衡的策略可以参考此文档：</p><p><strong><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/ops/state/large_state_tuning.html</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 进阶：Runtime 核心机制剖析</title>
      <link href="/2019/08/15/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ARuntime%20%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/"/>
      <url>/2019/08/15/Flink%20%E8%BF%9B%E9%98%B6%EF%BC%9ARuntime%20%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>原作者：高赟（云骞）</p><h3 id="1-综述"><a href="#1-综述" class="headerlink" title="1. 综述"></a>1. 综述</h3><p>本文主要介绍 Flink Runtime 的作业执行的核心机制。本文将首先介绍 Flink Runtime 的整体架构以及 Job 的基本执行流程，然后介绍在这个过程，Flink 是怎么进行资源管理、作业调度以及错误恢复的。最后，本文还将简要介绍 Flink Runtime 层当前正在进行的一些工作。</p><h3 id="2-Flink-Runtime-整体架构"><a href="#2-Flink-Runtime-整体架构" class="headerlink" title="2. Flink Runtime 整体架构"></a>2. Flink Runtime 整体架构</h3><p>Flink 的整体架构如图 1 所示。Flink 是可以运行在多种不同的环境中的，例如，它可以通过单进程多线程的方式直接运行，从而提供调试的能力。它也可以运行在 Yarn 或者 K8S 这种资源管理系统上面，也可以在各种云环境中执行。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-1.png" alt="img"></p><p>​                            图1. Flink 的整体架构，其中 Runtime 层对不同的执行环境提供了一套统一的分布式执行引擎。</p><p>针对不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。Flink 在 Runtime 层之上提供了 DataStream 和 DataSet 两套 API，分别用来编写流作业与批作业，以及一组更高级的 API 来简化特定作业的编写。本文主要介绍 Flink Runtime 层的整体架构。</p><p>Flink Runtime 层的主要架构如图 2 所示，它展示了一个 Flink 集群的基本结构。Flink Runtime 层的整个架构主要是在 FLIP-6 中实现的，整体来说，它采用了标准 master-slave 的结构，其中左侧白色圈中的部分即是 master，它负责管理整个集群中的资源和作业；而右侧的两个 TaskExecutor 则是 Slave，负责提供具体的资源并实际执行作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-2.png" alt="img">                         </p><p>​                                     图2. Flink 集群的基本结构。Flink Runtime 层采用了标准的 master-slave 架构。</p><p>其中，Master 部分又包含了三个组件，即 Dispatcher、ResourceManager 和 JobManager。其中，Dispatcher 负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的 JobManager 组件。ResourceManager 负责资源的管理，在整个 Flink 集群中只有一个 ResourceManager。JobManager 负责管理作业的执行，在一个 Flink 集群中可能有多个作业同时执行，每个作业都有自己的 JobManager 组件。这三个组件都包含在 AppMaster 进程中。</p><p>基于上述结构，当用户提交作业的时候，提交脚本会首先启动一个 Client进程负责作业的编译与提交。它首先将用户编写的代码编译为一个 JobGraph，在这个过程，它还会进行一些检查或优化等工作，例如判断哪些 Operator 可以 Chain 到同一个 Task 中。然后，Client 将产生的 JobGraph 提交到集群中执行。此时有两种情况，一种是类似于 Standalone 这种 Session 模式，AM 会预先启动，此时 Client 直接与 Dispatcher 建立连接并提交作业即可。另一种是 Per-Job 模式，AM 不会预先启动，此时 Client 将首先向资源管理系统 （如Yarn、K8S）申请资源来启动 AM，然后再向 AM 中的 Dispatcher 提交作业。</p><p>当作业到 Dispatcher 后，Dispatcher 会首先启动一个 JobManager 组件，然后 JobManager 会向 ResourceManager 申请资源来启动作业中具体的任务。这时根据 Session 和 Per-Job 模式的区别， TaskExecutor 可能已经启动或者尚未启动。如果是前者，此时 ResourceManager 中已有记录了 TaskExecutor 注册的资源，可以直接选取空闲资源进行分配。否则，ResourceManager 也需要首先向外部资源管理系统申请资源来启动 TaskExecutor，然后等待 TaskExecutor 注册相应资源后再继续选择空闲资源进程分配。目前 Flink 中 TaskExecutor 的资源是通过 Slot 来描述的，一个 Slot 一般可以执行一个具体的 Task，但在一些情况下也可以执行多个相关联的 Task，这部分内容将在下文进行详述。ResourceManager 选择到空闲的 Slot 之后，就会通知相应的 TM “将该 Slot 分配分 JobManager XX ”，然后 TaskExecutor 进行相应的记录后，会向 JobManager 进行注册。JobManager 收到 TaskExecutor 注册上来的 Slot 后，就可以实际提交 Task 了。</p><p>TaskExecutor 收到 JobManager 提交的 Task 之后，会启动一个新的线程来执行该 Task。Task 启动后就会开始进行预先指定的计算，并通过数据 Shuffle 模块互相交换数据。</p><p>以上就是 Flink Runtime 层执行作业的基本流程。可以看出，Flink 支持两种不同的模式，即 Per-job 模式与 Session 模式。如图 3 所示，Per-job 模式下整个 Flink 集群只执行单个作业，即每个作业会独享 Dispatcher 和 ResourceManager 组件。此外，Per-job 模式下 AppMaster 和 TaskExecutor 都是按需申请的。因此，Per-job 模式更适合运行执行时间较长的大作业，这些作业对稳定性要求较高，并且对申请资源的时间不敏感。与之对应，在 Session 模式下，Flink 预先启动 AppMaster 以及一组 TaskExecutor，然后在整个集群的生命周期中会执行多个作业。可以看出，Session 模式更适合规模小，执行时间短的作业。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-3.png" alt="img">  </p><p>​                                                                   图3. Flink Runtime 支持两种作业执行的模式。</p><h3 id="3-资源管理与作业调度"><a href="#3-资源管理与作业调度" class="headerlink" title="3. 资源管理与作业调度"></a>3. 资源管理与作业调度</h3><p>本节对 Flink 中资源管理与作业调度的功能进行更深入的说明。实际上，作业调度可以看做是对资源和任务进行匹配的过程。如上节所述，在 Flink 中，资源是通过 Slot 来表示的，每个 Slot 可以用来执行不同的 Task。而在另一端，任务即 Job 中实际的 Task，它包含了待执行的用户逻辑。调度的主要目的就是为了给 Task 找到匹配的 Slot。逻辑上来说，每个 Slot 都应该有一个向量来描述它所能提供的各种资源的量，每个 Task 也需要相应的说明它所需要的各种资源的量。但是实际上在 1.9 之前，Flink 是不支持细粒度的资源描述的，而是统一的认为每个 Slot 提供的资源和 Task 需要的资源都是相同的。从 1.9 开始，Flink 开始增加对细粒度的资源匹配的支持的实现，但这部分功能目前仍在完善中。</p><p>作业调度的基础是首先提供对资源的管理，因此我们首先来看下 Flink 中资源管理的实现。如上文所述，Flink 中的资源是由 TaskExecutor 上的 Slot 来表示的。如图 4 所示，在 ResourceManager 中，有一个子组件叫做 SlotManager，它维护了当前集群中所有 TaskExecutor 上的 Slot 的信息与状态，如该 Slot 在哪个 TaskExecutor 中，该 Slot 当前是否空闲等。当 JobManger 来为特定 Task 申请资源的时候，根据当前是 Per-job 还是 Session 模式，ResourceManager 可能会去申请资源来启动新的 TaskExecutor。当 TaskExecutor 启动之后，它会通过服务发现找到当前活跃的 ResourceManager 并进行注册。在注册信息中，会包含该 TaskExecutor中所有 Slot 的信息。 ResourceManager 收到注册信息后，其中的 SlotManager 就会记录下相应的 Slot 信息。当 JobManager 为某个 Task 来申请资源时， SlotManager 就会从当前空闲的 Slot 中按一定规则选择一个空闲的 Slot 进行分配。当分配完成后，如第 2 节所述，RM 会首先向 TaskManager 发送 RPC 要求将选定的 Slot 分配给特定的 JobManager。TaskManager 如果还没有执行过该 JobManager 的 Task 的话，它需要首先向相应的 JobManager 建立连接，然后发送提供 Slot 的 RPC 请求。在 JobManager 中，所有 Task 的请求会缓存到 SlotPool 中。当有 Slot 被提供之后，SlotPool 会从缓存的请求中选择相应的请求并结束相应的请求过程。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-4.png" alt="img"></p><p>​                                                                   图4. Flink 中资源管理功能各模块交互关系。</p><p>当 Task 结束之后，无论是正常结束还是异常结束，都会通知 JobManager 相应的结束状态，然后在 TaskManager 端将 Slot 标记为已占用但未执行任务的状态。JobManager 会首先将相应的 Slot 缓存到 SlotPool 中，但不会立即释放。这种方式避免了如果将 Slot 直接还给 ResourceManager，在任务异常结束之后需要重启时，需要立刻重新申请 Slot 的问题。通过延时释放，Failover 的 Task 可以尽快调度回原来的 TaskManager，从而加快 Failover 的速度。当 SlotPool 中缓存的 Slot 超过指定的时间仍未使用时，SlotPool 就会发起释放该 Slot 的过程。与申请 Slot 的过程对应，SlotPool 会首先通知 TaskManager 来释放该 Slot，然后 TaskExecutor 通知 ResourceManager 该 Slot 已经被释放，从而最终完成释放的逻辑。</p><p>除了正常的通信逻辑外，在 ResourceManager 和 TaskExecutor 之间还存在定时的心跳消息来同步 Slot 的状态。在分布式系统中，消息的丢失、错乱不可避免，这些问题会在分布式系统的组件中引入不一致状态，如果没有定时消息，那么组件无法从这些不一致状态中恢复。此外，当组件之间长时间未收到对方的心跳时，就会认为对应的组件已经失效，并进入到 Failover 的流程。</p><p>在 Slot 管理基础上，Flink 可以将 Task 调度到相应的 Slot 当中。如上文所述，Flink 尚未完全引入细粒度的资源匹配，默认情况下，每个 Slot 可以分配给一个 Task。但是，这种方式在某些情况下会导致资源利用率不高。如图 5 所示，假如 A、B、C 依次执行计算逻辑，那么给 A、B、C 分配分配单独的 Slot 就会导致资源利用率不高。为了解决这一问题，Flink 提供了 Share Slot 的机制。如图 5 所示，基于 Share Slot，每个 Slot 中可以部署来自不同 JobVertex 的多个任务，但是不能部署来自同一个 JobVertex 的 Task。如图5所示，每个 Slot 中最多可以部署同一个 A、B 或 C 的 Task，但是可以同时部署 A、B 和 C 的各一个 Task。当单个 Task 占用资源较少时，Share Slot 可以提高资源利用率。 此外，Share Slot 也提供了一种简单的保持负载均衡的方式。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-5.png" alt="img"></p><p>​                                                                       图5.Flink Share Slot 示例。<br>使用 Share Slot 可以在每个 Slot 中部署来自不同 JobVertex 的多个 Task。</p><p>基于上述 Slot 管理和分配的逻辑，JobManager 负责维护作业中 Task执行的状态。如上文所述，Client 端会向 JobManager 提交一个 JobGraph，它代表了作业的逻辑结构。JobManager 会根据 JobGraph 按并发展开，从而得到 JobManager 中关键的 ExecutionGraph。ExecutionGraph 的结构如图 5 所示，与 JobGraph 相比，ExecutionGraph 中对于每个 Task 与中间结果等均创建了对应的对象，从而可以维护这些实体的信息与状态。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-6.png" alt="img"></p><p>​                                                               图6.Flink 中的 JobGraph 与 ExecutionGraph。<br>ExecutionGraph 是 JobGraph 按并发展开所形成的，它是 JobMaster 中的核心数据结构。</p><p>在一个 Flink Job 中是包含多个 Task 的，因此另一个关键的问题是在 Flink 中按什么顺序来调度 Task。如图 7 所示，目前 Flink 提供了两种基本的调度逻辑，即 Eager 调度与 Lazy From Source。Eager 调度如其名子所示，它会在作业启动时申请资源将所有的 Task 调度起来。这种调度算法主要用来调度可能没有终止的流作业。与之对应，Lazy From Source 则是从 Source 开始，按拓扑顺序来进行调度。简单来说，Lazy From Source 会先调度没有上游任务的 Source 任务，当这些任务执行完成时，它会将输出数据缓存到内存或者写入到磁盘中。然后，对于后续的任务，当它的前驱任务全部执行完成后，Flink 就会将这些任务调度起来。这些任务会从读取上游缓存的输出数据进行自己的计算。这一过程继续进行直到所有的任务完成计算。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-7.png" alt="img"></p><p>​                                                                        图7. Flink 中两种基本的调度策略。<br>其中 Eager 调度适用于流作业，而Lazy From Source 适用于批作业。</p><h3 id="4-错误恢复"><a href="#4-错误恢复" class="headerlink" title="4. 错误恢复"></a>4. 错误恢复</h3><p>在 Flink 作业的执行过程中，除正常执行的流程外，还有可能由于环境等原因导致各种类型的错误。整体上来说，错误可能分为两大类：Task 执行出现错误或 Flink 集群的 Master 出现错误。由于错误不可避免，为了提高可用性，Flink 需要提供自动错误恢复机制来进行重试。</p><p>对于第一类 Task 执行错误，Flink 提供了多种不同的错误恢复策略。如图 8 所示，第一种策略是 Restart-all，即直接重启所有的 Task。对于 Flink 的流任务，由于 Flink 提供了 Checkpoint 机制，因此当任务重启后可以直接从上次的 Checkpoint 开始继续执行。因此这种方式更适合于流作业。第二类错误恢复策略是 Restart-individual，它只适用于 Task 之间没有数据传输的情况。这种情况下，我们可以直接重启出错的任务。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-8.png" alt="img"></p><p>​                                                                           图8.Restart-all 错误恢复策略示例。<br>​                                                                            该策略会直接重启所有的 Task。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-9.png" alt="img"></p><p>​                                                                              图9.Restart-individual 错误恢复策略示例。<br>​                                     该策略只适用于 Task之间不需要数据传输的作业，对于这种作业可以只重启出现错误的 Task。</p><p>由于 Flink 的批作业没有 Checkpoint 机制，因此对于需要数据传输的作业，直接重启所有 Task 会导致作业从头计算，从而导致一定的性能问题。为了增强对 Batch 作业，Flink 在1.9中引入了一种新的Region-Based的Failover策略。在一个 Flink 的 Batch 作业中 Task 之间存在两种数据传输方式，一种是 Pipeline 类型的方式，这种方式上下游 Task 之间直接通过网络传输数据，因此需要上下游同时运行；另外一种是 Blocking 类型的试，如上节所述，这种方式下，上游的 Task 会首先将数据进行缓存，因此上下游的 Task 可以单独执行。基于这两种类型的传输，Flink 将 ExecutionGraph 中使用 Pipeline 方式传输数据的 Task 的子图叫做 Region，从而将整个 ExecutionGraph 划分为多个子图。可以看出，Region 内的 Task 必须同时重启，而不同 Region 的 Task 由于在 Region 边界存在 Blocking 的边，因此，可以单独重启下游 Region 中的 Task。</p><p>基于这一思路,如果某个 Region 中的某个 Task 执行出现错误，可以分两种情况进行考虑。如图 8 所示，如果是由于 Task 本身的问题发生错误，那么可以只重启该 Task 所属的 Region 中的 Task，这些 Task 重启之后，可以直接拉取上游 Region 缓存的输出结果继续进行计算。</p><p>另一方面，如图如果错误是由于读取上游结果出现问题，如网络连接中断、缓存上游输出数据的 TaskExecutor 异常退出等，那么还需要重启上游 Region 来重新产生相应的数据。在这种情况下，如果上游 Region 输出的数据分发方式不是确定性的（如 KeyBy、Broadcast 是确定性的分发方式，而 Rebalance、Random 则不是，因为每次执行会产生不同的分发结果），为保证结果正确性，还需要同时重启上游 Region 所有的下游 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-10.png" alt="img"></p><p>​                                                                   图10.Region-based 错误恢复策略示例一。<br>​                                             如果是由于下游任务本身导致的错误，可以只重启下游对应的 Region。</p><p><img src="https://ververica.cn/wp-content/uploads/2019/09/%E5%9B%BE%E7%89%87-11.png" alt="img"></p><p>​                                                                       图11.Region-based 错误恢复策略示例二。<br>​                                                     如果是由于上游失败导致的错误，那么需要同时重启上游的 Region 和下游的 Region。实际上，如果下游的输出使用了非确定的数据分割方式，为了保持数据一致性，还需要同时重启所有上游 Region 的下游 Region。</p><p>除了 Task 本身执行的异常外，另一类异常是 Flink 集群的 Master 进行发生异常。目前 Flink 支持启动多个 Master 作为备份，这些 Master 可以通过 ZK 来进行选主，从而保证某一时刻只有一个 Master 在运行。当前活路的 Master 发生异常时,某个备份的 Master 可以接管协调的工作。为了保证 Master 可以准确维护作业的状态，Flink 目前采用了一种最简单的实现方式，即直接重启整个作业。实际上，由于作业本身可能仍在正常运行，因此这种方式存在一定的改进空间。</p><h3 id="5-未来展望"><a href="#5-未来展望" class="headerlink" title="5. 未来展望"></a>5. 未来展望</h3><p>Flink目前仍然在Runtime部分进行不断的迭代和更新。目前来看，Flink未来可能会在以下几个方式继续进行优化和扩展：</p><ul><li><strong>更完善的资源管理</strong>：从 1.9 开始 Flink 开始了对细粒度资源匹配的支持。基于细粒度的资源匹配，用户可以为 TaskExecutor 和 Task 设置实际提供和使用的 CPU、内存等资源的数量，Flink 可以按照资源的使用情况进行调度。这一机制允许用户更大范围的控制作业的调度，从而为进一步提高资源利用率提供了基础。</li><li><strong>统一的 Stream 与 Batch</strong>：Flink 目前为流和批分别提供了 DataStream 和 DataSet 两套接口，在一些场景下会导致重复实现逻辑的问题。未来 Flink 会将流和批的接口都统一到 DataStream 之上。</li><li><strong>更灵活的调度策略</strong>：Flink 从 1.9 开始引入调度插件的支持，从而允许用户来扩展实现自己的调度逻辑。未来 Flink 也会提供更高性能的调度策略的实现。</li><li><strong>Master Failover 的优化</strong>：如上节所述，目前 Flink 在 Master Failover 时需要重启整个作业，而实际上重启作业并不是必须的逻辑。Flink 未来会对 Master failover 进行进一步的优化来避免不必要的作业重启。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink SQL 深度解析</title>
      <link href="/2018/11/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2018/11/18/FlinkSQL%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据计算领域对SQL的应用"><a href="#大数据计算领域对SQL的应用" class="headerlink" title="大数据计算领域对SQL的应用"></a>大数据计算领域对SQL的应用</h1><h2 id="离线计算（批计算）"><a href="#离线计算（批计算）" class="headerlink" title="离线计算（批计算）"></a>离线计算（批计算）</h2><p>提及大数据计算领域不得不说MapReduce计算模型，MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法，并发于2004年发表了论文Simplified Data Processing on Large Clusters。论文发表之后Apache 开源社区参考Google MapReduce，基于Java设计开发了一个称为Hadoop的开源MapReduce并行计算框架。很快得到了全球学术界和工业界的普遍关注，并得到推广和普及应用。但利用Hadoop进行MapReduce的开发，需要开发人员精通Java语言，并了解MapReduce的运行原理，这样在一定程度上提高了MapReduce的开发门槛，所以在开源社区又不断涌现了一些为了简化MapReduce开发的开源框架，其中Hive就是典型的代表。HSQL可以让用户以类SQL的方式描述MapReduce计算，比如原本需要几十行，甚至上百行才能完成的wordCount，用户一条SQL语句就能完成了，这样极大的降低了MapReduce的开发门槛，进而也成功的将SQL应用到了大数据计算领域当中来。</p><h2 id="实时计算（流计算）"><a href="#实时计算（流计算）" class="headerlink" title="实时计算（流计算）"></a>实时计算（流计算）</h2><p>SQL不仅仅被成功的应用到了离线计算，SQL的易用性也吸引了流计算产品，目前最热的Spark，Flink也纷纷支持了SQL，尤其是Flink支持的更加彻底，集成了Calcite，完全遵循ANSI-SQL标准。Apache Flink在low-level API上面用DataSet支持批计算，用DataStream支持流计算，但在High-Level API上面利用SQL将流与批进行了统一，使得用户编写一次SQL既可以在流计算中使用，又可以在批计算中使用，为既有流计算业务，又有批计算业务的用户节省了大量开发成本。</p><h1 id="SQL高性能与简洁性"><a href="#SQL高性能与简洁性" class="headerlink" title="SQL高性能与简洁性"></a>SQL高性能与简洁性</h1><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>SQL经过传统数据库领域几十年的不断打磨，查询优化器已经能够极大的优化SQL的查询性能，Apache Flink 应用Calcite进行查询优化，复用了大量数据库查询优化规则，在性能上不断追求极致，能够让用户关心但不用担心性能问题。如下图(Alibaba 对 Apache Flink 进行架构优化后的组件栈)</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN43HQMdZty0IxMowiaBs1oaPZwyEeVpLvkLakk4V51uz6iaMbz9toslicw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>相对于DataStream而言，SQL会经过Optimization模块透明的为用户进行查询优化，用户专心编写自己的业务逻辑，不用担心性能，却能得到最优的查询性能!</p><h2 id="简洁"><a href="#简洁" class="headerlink" title="简洁"></a>简洁</h2><p>就简洁性而言，SQL与DataSet和DataStream相比具有很大的优越性，我们先用一个WordCount示例来直观的查看用户的代码量：</p><p>DataStream/DataSetAPI</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">... <span class="comment">//省略初始化代码</span></span><br><span class="line"><span class="comment">// 核心逻辑</span></span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> WordCount.Tokenizer()).keyBy(<span class="keyword">new</span> <span class="built_in">int</span>[]&#123;<span class="number">0</span>&#125;).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// flatmap 代码定义</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> class Tokenizer implements FlatMapFunction&lt;<span class="keyword">String</span>, Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> Tokenizer() &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> flatMap(<span class="keyword">String</span> value, Collector&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; out) &#123;</span><br><span class="line"><span class="keyword">String</span>[] tokens = value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>);</span><br><span class="line"><span class="keyword">String</span>[] var4 = tokens;</span><br><span class="line"><span class="built_in">int</span> var5 = tokens.length;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">int</span> var6 = <span class="number">0</span>; var6 &lt; var5; ++var6) &#123;</span><br><span class="line"><span class="keyword">String</span> token = var4[var6];</span><br><span class="line"><span class="keyword">if</span> (token.length() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">out.collect(<span class="keyword">new</span> Tuple2(token, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SQL</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">...//省略初始化代码</span><br><span class="line"><span class="keyword">SELECT</span> word, <span class="built_in">COUNT</span>(word) <span class="keyword">FROM</span> tab <span class="keyword">GROUP</span> <span class="keyword">BY</span> word;</span><br></pre></td></tr></table></figure><p>我们直观的体会到相同的统计功能使用SQL的简洁性。</p><h1 id="Flink-SQL-Job的组成"><a href="#Flink-SQL-Job的组成" class="headerlink" title="Flink SQL Job的组成"></a>Flink SQL Job的组成</h1><p>我们做任何数据计算都离不开读取原始数据，计算逻辑和写入计算结果数据三部分，当然基于Apache Flink SQL编写的计算Job也离不开这三个部分，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNmwGyZgbFaPfs2bjXzSGdh9jSTKnxrYlSbLzwMUn95uVLOuHcueGLnw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上所示，一个完整的Apache Flink SQL Job 由如下三部分：</p><ul><li>Source Operator - Soruce operator是对外部数据源的抽象, 目前Apache Flink内置了很多常用的数据源实现，比如上图提到的Kafka。</li><li>Query Operators - 查询算子主要完成如图的Query Logic，目前支持了Union，Join，Projection,Difference, Intersection以及window等大多数传统数据库支持的操作。</li><li>Sink Operator - Sink operator 是对外结果表的抽象，目前Apache Flink也内置了很多常用的结果表的抽象，比如上图提到的Kafka。</li></ul><h1 id="Flink-SQL-核心算子"><a href="#Flink-SQL-核心算子" class="headerlink" title="Flink SQL 核心算子"></a>Flink SQL 核心算子</h1><p>目前Flink SQL支持Union，Join，Projection,Difference, Intersection以及Window等大多数传统数据库支持的操作，接下来为大家分别进行简单直观的介绍。</p><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>为了很好的体验和理解Apache Flink SQL算子我们需要先准备一下测试环境，我们选择IDEA，以ITCase测试方式来进行体验。IDEA 安装这里不占篇幅介绍了，相信大家能轻松搞定！我们进行功能体验有两种方式，具体如下：</p><h2 id="源码方式"><a href="#源码方式" class="headerlink" title="源码方式"></a>源码方式</h2><p>对于开源爱好者可能更喜欢源代码方式理解和体验Apache Flink SQL功能，那么我们需要下载源代码并导入到IDEA中：</p><ul><li>下载源码：</li></ul><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下载源代码</span></span><br><span class="line">git clone https:<span class="comment">//github.com/apache/flink.git study</span></span><br><span class="line"><span class="comment">// 进入源码目录</span></span><br><span class="line">cd study</span><br><span class="line"><span class="comment">// 拉取稳定版release-1.6</span></span><br><span class="line">git fetch origin <span class="built_in">release</span><span class="number">-1.6</span>:<span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//切换到稳定版</span></span><br><span class="line">git checkout <span class="built_in">release</span><span class="number">-1.6</span></span><br><span class="line"><span class="comment">//将依赖安装到本地mvn仓库，耐心等待需要一段时间</span></span><br><span class="line">mvn clean install -DskipTests</span><br></pre></td></tr></table></figure><ul><li>导入到IDEA<br>将Flink源码导入到IDEA过程这里不再占用篇幅，导入后确保在IDEA中可以运行 <code>org.apache.flink.table.runtime.stream.sql.SqlITCase</code> 并测试全部通过，即证明体验环境已经完成，即证明体验环境已经完成。如下图所示：</li></ul><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNo09iaFxmhAfdNGPSjCc6qnjDUWyZaCO8UBSkyUJy1EEcicoSv4qa8wzg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>如上图运行测试后显示测试通过，我们就可以继续下面的Apache Flink SQL功能体验了。</p><h2 id="依赖Flink包方式"><a href="#依赖Flink包方式" class="headerlink" title="依赖Flink包方式"></a>依赖Flink包方式</h2><p>我们还有一种更简单直接的方式，就是新建一个mvn项目，并在pom中添加如下依赖：</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">table.version</span>&gt;</span>1.6-SNAPSHOT<span class="tag">&lt;/<span class="name">table.version</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;table.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>JUnit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>完成环境准备后，我们开始准备测试数据和写一个简单的测试类。</p><h2 id="示例数据及测试类"><a href="#示例数据及测试类" class="headerlink" title="示例数据及测试类"></a>示例数据及测试类</h2><h3 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h3><ul><li>customer_tab 表 - 客户表保存客户id，客户姓名和客户描述信息。字段及测试数据如下：</li></ul><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><ul><li>order_tab 表 - 订单表保存客户购买的订单信息，包括订单id，订单时间和订单描述信息。 字段节测试数据如下：</li></ul><table><thead><tr><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr></tbody></table><ul><li>Item_tab<br> 商品表, 携带商品id，商品类型，出售时间，价格等信息，具体如下：</li></ul><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>30</td></tr><tr><td>ITEM004</td><td>Electronic</td><td><strong>*2017-11-11 10:03:00*</strong></td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td></tr></tbody></table><ul><li>PageAccess_tab<br>页面访问表，包含用户ID，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><ul><li>PageAccessCount_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userCount</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>100</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>86</td><td>2017.11.11 10:01:00</td></tr><tr><td>BeiJing</td><td>210</td><td>2017.11.11 10:06:00</td></tr><tr><td>BeiJing</td><td>33</td><td>2017.11.11 10:10:00</td></tr><tr><td>ShangHai</td><td>129</td><td>2017.11.11 12:10:00</td></tr></tbody></table><ul><li>PageAccessSession_tab<br>页面访问表，访问量，访问时间，用户所在地域信息，具体数据如下：</li></ul><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:01:00</td></tr><tr><td>ShangHai</td><td>U0012</td><td>2017-11-11 10:02:00</td></tr><tr><td>ShangHai</td><td>U0013</td><td>2017-11-11 10:03:00</td></tr><tr><td>ShangHai</td><td>U0015</td><td>2017-11-11 10:05:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U0110</td><td>2017-11-11 10:10:00</td></tr><tr><td>ShangHai</td><td>U2010</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0410</td><td>2017-11-11 12:16:00</td></tr></tbody></table><h3 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h3><p>我们创建一个<code>SqlOverviewITCase.scala</code> 用于接下来介绍Flink SQL算子的功能体验。代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.<span class="type">StateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.memory.<span class="type">MemoryStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.<span class="type">RichSinkFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span>.<span class="type">SourceContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.watermark.<span class="type">Watermark</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">TableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.junit.rules.<span class="type">TemporaryFolder</span></span><br><span class="line"><span class="keyword">import</span> org.junit.&#123;<span class="type">Rule</span>, <span class="type">Test</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqlOverviewITCase</span> </span>&#123;</span><br><span class="line"><span class="keyword">val</span> _tempFolder = <span class="keyword">new</span> <span class="type">TemporaryFolder</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Rule</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFolder</span></span>: <span class="type">TemporaryFolder</span> = _tempFolder</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStateBackend</span></span>: <span class="type">StateBackend</span> = &#123;</span><br><span class="line"><span class="keyword">new</span> <span class="type">MemoryStateBackend</span>()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 客户表数据</span></span><br><span class="line"><span class="keyword">val</span> customer_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">customer_data.+=((<span class="string">"c_001"</span>, <span class="string">"Kevin"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_002"</span>, <span class="string">"Sunny"</span>, <span class="string">"from JinLin"</span>))</span><br><span class="line">customer_data.+=((<span class="string">"c_003"</span>, <span class="string">"JinCheng"</span>, <span class="string">"from HeBei"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 订单表数据</span></span><br><span class="line"><span class="keyword">val</span> order_data = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">order_data.+=((<span class="string">"o_001"</span>, <span class="string">"c_002"</span>, <span class="string">"2018-11-05 10:01:01"</span>, <span class="string">"iphone"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_002"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:01:55"</span>, <span class="string">"ipad"</span>))</span><br><span class="line">order_data.+=((<span class="string">"o_003"</span>, <span class="string">"c_001"</span>, <span class="string">"2018-11-05 10:03:44"</span>, <span class="string">"flink book"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 商品销售表数据</span></span><br><span class="line"><span class="keyword">val</span> item_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="number">20</span>, <span class="string">"ITEM001"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="number">50</span>, <span class="string">"ITEM002"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">30</span>, <span class="string">"ITEM003"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365780000</span>L, (<span class="number">1510365780000</span>L, <span class="number">60</span>, <span class="string">"ITEM004"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365780000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="number">40</span>, <span class="string">"ITEM005"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="number">20</span>, <span class="string">"ITEM006"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365960000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366020000</span>L, (<span class="number">1510366020000</span>L, <span class="number">70</span>, <span class="string">"ITEM007"</span>, <span class="string">"Electronic"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366020000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366080000</span>L, (<span class="number">1510366080000</span>L, <span class="number">20</span>, <span class="string">"ITEM008"</span>, <span class="string">"Clothes"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">151036608000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据</span></span><br><span class="line"><span class="keyword">val</span> pageAccess_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问量表数据2</span></span><br><span class="line"><span class="keyword">val</span> pageAccessCount_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="number">100</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"BeiJing"</span>, <span class="number">86</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365960000</span>L, (<span class="number">1510365960000</span>L, <span class="string">"BeiJing"</span>, <span class="number">210</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="number">33</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373400000</span>L, (<span class="number">1510373400000</span>L, <span class="string">"ShangHai"</span>, <span class="number">129</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373400000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 页面访问表数据3</span></span><br><span class="line"><span class="keyword">val</span> pageAccessSession_data = <span class="type">Seq</span>(</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365660000</span>L, (<span class="number">1510365660000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365660000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0012"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365720000</span>L, (<span class="number">1510365720000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0013"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365720000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510365900000</span>L, (<span class="number">1510365900000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0015"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510365900000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366200000</span>L, (<span class="number">1510366200000</span>L, <span class="string">"BeiJing"</span>, <span class="string">"U2010"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366200000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510366260000</span>L, (<span class="number">1510366260000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510366260000</span>L)),</span><br><span class="line"><span class="type">Left</span>((<span class="number">1510373760000</span>L, (<span class="number">1510373760000</span>L, <span class="string">"ShangHai"</span>, <span class="string">"U0410"</span>))),</span><br><span class="line"><span class="type">Right</span>((<span class="number">1510373760000</span>L)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">procTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将order_tab, customer_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> customer = env.fromCollection(customer_data).toTable(tEnv).as(<span class="symbol">'c_id</span>, <span class="symbol">'c_name</span>, <span class="symbol">'c_desc</span>)</span><br><span class="line">    <span class="keyword">val</span> order = env.fromCollection(order_data).toTable(tEnv).as(<span class="symbol">'o_id</span>, <span class="symbol">'c_id</span>, <span class="symbol">'o_time</span>, <span class="symbol">'o_desc</span>)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"order_tab"</span>, order)</span><br><span class="line">    tEnv.registerTable(<span class="string">"customer_tab"</span>, customer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rowTimePrint</span></span>(sql: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setStateBackend(getStateBackend)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将item_tab, pageAccess_tab 注册到catalog</span></span><br><span class="line">    <span class="keyword">val</span> item =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">Int</span>, <span class="type">String</span>, <span class="type">String</span>)](item_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'onSellTime</span>, <span class="symbol">'price</span>, <span class="symbol">'itemID</span>, <span class="symbol">'itemType</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccess =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccess_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessCount =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">Int</span>)](pageAccessCount_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'accessCount</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pageAccessSession =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">EventTimeSourceFunction</span>[(<span class="type">Long</span>, <span class="type">String</span>, <span class="type">String</span>)](pageAccessSession_data))</span><br><span class="line">    .toTable(tEnv, <span class="symbol">'accessTime</span>, <span class="symbol">'region</span>, <span class="symbol">'userId</span>, <span class="symbol">'rowtime</span>.rowtime)</span><br><span class="line"></span><br><span class="line">    tEnv.registerTable(<span class="string">"item_tab"</span>, item)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccess_tab"</span>, pageAccess)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessCount_tab"</span>, pageAccessCount)</span><br><span class="line">    tEnv.registerTable(<span class="string">"pageAccessSession_tab"</span>, pageAccessSession)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(sql).toRetractStream[<span class="type">Row</span>]</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">RetractingSink</span></span><br><span class="line">    result.addSink(sink)</span><br><span class="line">    env.execute()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testSelect</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">"替换想要测试的SQL"</span></span><br><span class="line">    <span class="comment">// 非window 相关用 procTimePrint(sql)</span></span><br><span class="line">    <span class="comment">// Window 相关用 rowTimePrint(sql)</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义Sink</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RetractingSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>[(<span class="type">Boolean</span>, <span class="type">Row</span>)] </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> retractedResults: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">String</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(v: (<span class="type">Boolean</span>, <span class="type">Row</span>)) &#123;</span><br><span class="line">    retractedResults.synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> value = v._2.toString</span><br><span class="line">    <span class="keyword">if</span> (v._1) &#123;</span><br><span class="line">    retractedResults += value</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = retractedResults.indexOf(value)</span><br><span class="line">    <span class="keyword">if</span> (idx &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    retractedResults.remove(idx)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Tried to retract a value that wasn't added first. "</span> +</span><br><span class="line">    <span class="string">"This is probably an incorrectly implemented test. "</span> +</span><br><span class="line">    <span class="string">"Try to set the parallelism of the sink to 1."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    retractedResults.sorted.foreach(println(_))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Water mark 生成器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventTimeSourceFunction</span>[<span class="type">T</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">dataWithTimestampList: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span></span>), <span class="title">Long</span>]]) <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    dataWithTimestampList.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Left</span>(t) =&gt; ctx.collectWithTimestamp(t._2, t._1)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Right</span>(w) =&gt; ctx.emitWatermark(<span class="keyword">new</span> <span class="type">Watermark</span>(w))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h2><p>SELECT 用于从数据集/流中选择数据，语法遵循ANSI-SQL标准，语义是关系代数中的投影(Projection),对关系进行垂直分割，消去某</p><p>些列, 如下图所示:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNibZ25Mic3yIbEcG8icTWkkJiaMcTr5oq0wTkT7rdZ5EkUpXEp26ZKVTrKw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>从<code>customer_tab</code>选择用户姓名，并用内置的CONCAT函数拼接客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_name, <span class="keyword">CONCAT</span>(c_name, <span class="string">' come '</span>, c_desc) <span class="keyword">as</span> <span class="keyword">desc</span> <span class="keyword">FROM</span> customer_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下：</strong></p><table><thead><tr><th>c_name</th><th>desc</th></tr></thead><tbody><tr><td>Kevin</td><td>Kevin come from JinLin</td></tr><tr><td>Sunny</td><td>Sunny come from JinLin</td></tr><tr><td>Jincheng</td><td>Jincheng come from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>大家看到在 <code>SELECT</code> 不仅可以使用普通的字段选择，还可以使用<code>ScalarFunction</code>,当然也包括<code>User-Defined Function</code>，同时还可以进行字段的<code>alias</code>设置。其实<code>SELECT</code>可以结合聚合，在GROUPBY部分会进行介绍,一个比较特殊的使用场景是携带 <code>DISTINCT</code> 关键字，示例如下：</p><p><strong>SQL 示例</strong></p><p>在订单表查询所有的客户id，消除重复客户id, 如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> c_id <span class="keyword">FROM</span> order_tab;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th></tr></thead><tbody><tr><td>c_001</td></tr><tr><td>c_002</td></tr></tbody></table><h2 id="WHERE"><a href="#WHERE" class="headerlink" title="WHERE"></a>WHERE</h2><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WHERE</span> 用于从数据集/流中过滤数据，与<span class="keyword">SELECT</span>一起使用，语法遵循<span class="keyword">ANSI</span>-SQL标准，语义是关系代数的Selection，根据某些条件对关系做水平分割，即选择符合条件的记录，如下所示：</span><br></pre></td></tr></table></figure><p><strong>SQL 示例</strong></p><p>在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id = <span class="string">'c_001'</span> <span class="keyword">OR</span> c_id = <span class="string">'c_003'</span>;</span><br></pre></td></tr></table></figure><p><strong>Result</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>我们发现<code>WHERE</code>是对满足一定条件的数据进行过滤，<code>WHERE</code>支持=, &lt;, &gt;, &lt;&gt;, &gt;=, &lt;=以及<code>AND</code>， <code>OR</code>等表达式的组合，最终满足过滤条件的数据会被选择出来。并且 <code>WHERE</code> 可以结合<code>IN</code>,<code>NOT IN</code>联合使用，具体如下：</p><p><strong>SQL 示例 (IN 常量)</strong></p><p>使用 <code>IN</code> 在<code>customer_tab</code>查询客户id为<code>c_001</code>和<code>c_003</code>的客户信息，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="string">'c_001'</span>, <span class="string">'c_003'</span>);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>SQL 示例 (IN 子查询)</strong></p><p>使用 <code>IN</code>和 子查询 在<code>customer_tab</code>查询已经下过订单的客户信息，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc <span class="keyword">FROM</span> customer_tab <span class="keyword">WHERE</span> c_id <span class="keyword">IN</span> (<span class="keyword">SELECT</span> c_id <span class="keyword">FROM</span> order_tab);</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr></tbody></table><p><strong>IN/NOT IN 与关系代数</strong></p><p>如上介绍IN是关系代数中的Intersection， NOT IN是关系代数的Difference， 如下图示意：</p><ul><li>IN(Intersection</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblp6icardxtCKeaf7RHrFbN6TVbyqyGOGuSWYY7uY3DJb5ODYsOqvv1mWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li><li>NOT IN(Difference）</li><li><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpcofHeFia7icQorYjiaGmHO9yiclrFaMCk3l6sBuQa2sm5QlrtepLOrdIMA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></li></ul><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY 是对数据进行分组的操作，比如我需要分别计算一下一个学生表里面女生和男生的人数分别是多少，如下</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NZSOY21trjlOtCxGBVOiblpoeicHXKPbhnpAKEe8cMRzf4WHDQiagwAHRIlH6icqn107hHkiaeJh2CWDQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><p><strong>SQL 示例</strong></p><p>将order_tab信息按customer_tab分组统计订单数量，简单示例如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT c_id, count(o_id) as o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY c_id;</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>o_count</th></tr></thead><tbody><tr><td>c_001</td><td>2</td></tr><tr><td>c_002</td><td>1</td></tr></tbody></table><p><strong>特别说明</strong></p><p>在实际的业务场景中，GROUP BY除了按业务字段进行分组外，很多时候用户也可以用时间来进行分组(相当于划分窗口)，比如统计每分钟的订单数量：</p><p><strong>SQL 示例</strong></p><p>按时间进行分组，查询每分钟的订单数量，如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT SUBSTRING(o_time, 1, 16) AS o_time_min, count(o_id) AS o_count <span class="keyword">FROM</span> order_tab<span class="built_in"> GROUP </span>BY SUBSTRING(o_time, 1, 16)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>o_time_min</th><th>o_count</th></tr></thead><tbody><tr><td>2018-11-05 10:01</td><td>2</td></tr><tr><td>2018-11-05 10:03</td><td>1</td></tr></tbody></table><p>说明：如果我们时间字段是timestamp类型，建议使用内置的 <code>DATE_FORMAT</code> 函数。</p><h2 id="UNION-ALL"><a href="#UNION-ALL" class="headerlink" title="UNION ALL"></a>UNION ALL</h2><p>UNION ALL 将两个表合并起来，要求两个表的字段完全一致，包括字段类型、字段顺序,语义对应关系代数的Union，只是关系代数是Set集合操作，会有去重复操作，UNION ALL 不进行去重，如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNpaHuv6VYq4P9Zyke2cuHCIwibTbicJpXicRJWemZsJN6Y1Nq3vKVNzpNg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION ALL 对结果数据不进行去重，如果想对结果数据进行去重，传统数据库需要进行UNION操作。</p><h2 id="UNION"><a href="#UNION" class="headerlink" title="UNION"></a>UNION</h2><p>UNION 将两个流给合并起来，要求两个流的字段完全一致，包括字段类型、字段顺序，并其UNION 不同于UNION ALL，UNION会对结果数据去重,与关系代数的Union语义一致，如下：<br><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNALmfI36VBMGEontFaDkRleLsSbErPHtRYvT0dBQ4ic6kwQD3AEJIhfQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例</strong></p><p>我们简单的将<code>customer_tab</code>查询2次，将查询结果合并起来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> c_id, c_name, c_desc  <span class="keyword">FROM</span> customer_tab</span><br></pre></td></tr></table></figure><p>我们发现完全一样的表数据进行 <code>UNION</code>之后，数据是被去重的，<code>UNION</code>之后的数据并没有增加。</p><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td></tr></tbody></table><p><strong>特别说明</strong></p><p>UNION 对结果数据进行去重，在实际的实现过程需要对数据进行排序操作，所以非必要去重情况请使用UNION ALL操作。</p><h2 id="JOIN"><a href="#JOIN" class="headerlink" title="JOIN"></a>JOIN</h2><p>JOIN 用于把来自两个表的行联合起来形成一个宽表，Apache Flink支持的JOIN类型：</p><ul><li>JOIN - INNER JOIN</li><li>LEFT JOIN - LEFT OUTER JOIN</li><li>RIGHT JOIN - RIGHT OUTER JOIN</li><li>FULL JOIN - FULL OUTER JOIN</li></ul><p>JOIN与关系代数的Join语义相同，具体如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN8qxel16siciaMAH8x3aQCcZ6q0ic8QrtZtco3D9frZFjHfZYj4q33hszg/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>SQL 示例 (JOIN)</strong></p><p><code>INNER JOIN</code>只选择满足<code>ON</code>条件的记录，我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将有订单的客户和订单信息选择出来，如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr></tbody></table><p><strong>SQL 示例 (LEFT JOIN)</strong></p><p><code>LEFT JOIN</code>与<code>INNER JOIN</code>的区别是当右表没有与左边相JOIN的数据时候，右边对应的字段补<code>NULL</code>输出，语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNLcrI9iar3vKlgxMwRceIAMCZm2uNbhUWINhK1yRAllPdkwVJ1PcHhqQ/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>对应的SQL语句如下(LEFT JOIN)：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT ColA, ColB, <span class="built_in">T2</span>.ColC, ColE FROM TI LEFT <span class="keyword">JOIN </span><span class="built_in">T2</span> ON <span class="built_in">T1</span>.ColC = <span class="built_in">T2</span>.ColC <span class="comment">;</span></span><br></pre></td></tr></table></figure><ul><li>细心的读者可能发现上面T2.ColC是添加了前缀T2了，这里需要说明一下，当两张表有字段名字一样的时候，我需要指定是从那个表里面投影的。</li></ul><p>我们查询<code>customer_tab</code> 和 <code>order_tab</code>表，将客户和订单信息选择出来如下：</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> customer_tab <span class="keyword">AS</span> c <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> order_tab <span class="keyword">AS</span> o <span class="keyword">ON</span> o.c_id = c.c_id</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>c_id</th><th>c_name</th><th>c_desc</th><th>o_id</th><th>c_id</th><th>o_time</th><th>o_desc</th></tr></thead><tbody><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_002</td><td>c_001</td><td>2018-11-05 10:01:55</td><td>ipad</td></tr><tr><td>c_001</td><td>Kevin</td><td>from JinLin</td><td>o_003</td><td>c_001</td><td>2018-11-05 10:03:44</td><td>flink book</td></tr><tr><td>c_002</td><td>Sunny</td><td>from JinLin</td><td>o_oo1</td><td>c_002</td><td>2018-11-05 10:01:01</td><td>iphone</td></tr><tr><td>c_003</td><td>JinCheng</td><td>from HeBei</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr></tbody></table><p><strong>特别说明</strong></p><p><code>RIGHT JOIN</code> 相当于 <code>LEFT JOIN</code> 左右两个表交互一下位置。<code>FULL JOIN</code>相当于 <code>RIGHT JOIN</code> 和 <code>LEFT JOIN</code> 之后进行<code>UNION ALL</code>操作。</p><h2 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h2><p>在Apache Flink中有2种类型的Window，一种是OverWindow，即传统数据库的标准开窗，每一个元素都对应一个窗口。一种是GroupWindow，目前在SQL中GroupWindow都是基于时间进行窗口划分的。</p><h3 id="Over-Window"><a href="#Over-Window" class="headerlink" title="Over Window"></a>Over Window</h3><p>Apache Flink中对OVER Window的定义遵循标准SQL的定义语法。<br>按ROWS和RANGE分类是传统数据库的标准分类方法，在Apache Flink中还可以根据时间类型(ProcTime/EventTime)和窗口的有限和无限(Bounded/UnBounded)进行分类，共计8种类型。为了避免大家对过细分类造成困扰，我们按照确定当前行的不同方式将OVER Window分成两大类进行介绍，如下:</p><ul><li>ROWS OVER Window - 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</li><li>RANGE OVER Window - 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</li></ul><h4 id="Bounded-ROWS-OVER-Window"><a href="#Bounded-ROWS-OVER-Window" class="headerlink" title="Bounded ROWS OVER Window"></a>Bounded ROWS OVER Window</h4><p>Bounded ROWS OVER Window 每一行元素都视为新的计算行，即，每一行都是一个新的窗口。</p><p><strong>语义</strong></p><p>我们以3个元素(2 PRECEDING)的窗口为例，如下图:</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN6yotUibfmTVgbnFd7dvC4tgfFEddh0xJ6PzC9wzLDgiaemZoCCjVNxaw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>上图所示窗口 user 1 的 w5和w6， user 2的 窗口 w2 和 w3，虽然有元素都是同一时刻到达，但是他们仍然是在不同的窗口，这一点有别于RANGE OVER Window。</p><p><strong>语法</strong></p><p>Bounded ROWS OVER Window 语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">ROWS</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | rowCount) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>rowCount - 是定义根据当前行开始向前追溯几行元素。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>item_tab</code>测试数据，我们统计同类商品中当前和当前商品之前2个商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> onSellTime </span><br><span class="line">        <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span> <span class="number">2</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>50</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>60</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><h4 id="Bounded-RANGE-OVER-Window"><a href="#Bounded-RANGE-OVER-Window" class="headerlink" title="Bounded RANGE OVER Window"></a>Bounded RANGE OVER Window</h4><p>Bounded RANGE OVER Window 具有相同时间值的所有元素行视为同一计算行，即，具有相同时间值的所有行都是同一个窗口。</p><p><strong>语义</strong></p><p>我们以3秒中数据(INTERVAL ‘2’ SECOND)的窗口为例，如下图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNTtvBlDvT0wfxJvTOL8e9CbVJg6YVxAfLMKskjXibicrCeOGgIZxAJxdw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p>注意: 上图所示窗口 user 1 的 w6， user 2的 窗口 w3，元素都是同一时刻到达,他们是在同一个窗口，这一点有别于ROWS OVER Window。</p><p><strong>语法</strong></p><p>Bounded RANGE OVER Window的语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    agg1(col1) <span class="keyword">OVER</span>(</span><br><span class="line">     [<span class="keyword">PARTITION</span> <span class="keyword">BY</span> (value_expression1,..., value_expressionN)] </span><br><span class="line">     <span class="keyword">ORDER</span> <span class="keyword">BY</span> timeCol</span><br><span class="line">     <span class="keyword">RANGE</span> </span><br><span class="line">     <span class="keyword">BETWEEN</span> (<span class="keyword">UNBOUNDED</span> | timeInterval) <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> colName, </span><br><span class="line">... </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br></pre></td></tr></table></figure><ul><li>value_expression - 进行分区的字表达式；</li><li>timeCol - 用于元素排序的时间字段；</li><li>timeInterval - 是定义根据当前行开始向前追溯指定时间的元素行；</li></ul><p><strong>SQL 示例</strong></p><p>我们统计同类商品中当前和当前商品之前2分钟商品中的最高价格。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    itemID,</span><br><span class="line">    itemType, </span><br><span class="line">    onSellTime, </span><br><span class="line">    price,  </span><br><span class="line">    <span class="keyword">MAX</span>(price) <span class="keyword">OVER</span> (</span><br><span class="line">        <span class="keyword">PARTITION</span> <span class="keyword">BY</span> itemType </span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> rowtime </span><br><span class="line">        <span class="keyword">RANGE</span> <span class="keyword">BETWEEN</span> <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span> <span class="keyword">preceding</span> <span class="keyword">AND</span> <span class="keyword">CURRENT</span> <span class="keyword">ROW</span>) <span class="keyword">AS</span> maxPrice</span><br><span class="line">  <span class="keyword">FROM</span> item_tab</span><br></pre></td></tr></table></figure><p><strong>结果如下（Bounded RANGE OVER Windo</strong>w）</p><table><thead><tr><th>itemID</th><th>itemType</th><th>onSellTime</th><th>price</th><th>maxPrice</th></tr></thead><tbody><tr><td>ITEM001</td><td>Electronic</td><td>2017-11-11 10:01:00</td><td>20</td><td>20</td></tr><tr><td>ITEM002</td><td>Electronic</td><td>2017-11-11 10:02:00</td><td>50</td><td>50</td></tr><tr><td>ITEM003</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>30</td><td>60</td></tr><tr><td>ITEM004</td><td>Electronic</td><td>2017-11-11 10:03:00</td><td>60</td><td>60</td></tr><tr><td>ITEM005</td><td>Electronic</td><td>2017-11-11 10:05:00</td><td>40</td><td>60</td></tr><tr><td>ITEM006</td><td>Electronic</td><td>2017-11-11 10:06:00</td><td>20</td><td>40</td></tr><tr><td>ITEM007</td><td>Electronic</td><td>2017-11-11 10:07:00</td><td>70</td><td>70</td></tr><tr><td>ITEM008</td><td>Clothes</td><td>2017-11-11 10:08:00</td><td>20</td><td>20</td></tr></tbody></table><p><strong>特别说明</strong></p><p>OverWindow最重要是要理解每一行数据都确定一个窗口，同时目前在Apache Flink中只支持按时间字段排序。并且OverWindow开窗与GroupBy方式数据分组最大的不同在于，GroupBy数据分组统计时候，在<code>SELECT</code>中除了GROUP BY的key，不能直接选择其他非key的字段，但是OverWindow没有这个限制，<code>SELECT</code>可以选择任何字段。比如一张表table(a,b,c,d)4个字段，如果按d分组求c的最大值，两种写完如下:</p><ul><li>GROUP BY - <code>SELECT d, MAX(c) FROM table GROUP BY d</code></li><li>OVER Window = <code>SELECT a, b, c, d, MAX(c) OVER(PARTITION BY d, ORDER BY ProcTime())</code><br>如上 OVER Window 虽然PARTITION BY d,但SELECT 中仍然可以选择 a,b,c字段。但在GROUPBY中，SELECT 只能选择 d 字段。</li></ul><h3 id="Group-Window"><a href="#Group-Window" class="headerlink" title="Group Window"></a>Group Window</h3><p>根据窗口数据划分的不同，目前Apache Flink有如下3种Bounded Winodw:</p><ul><li>Tumble - 滚动窗口，窗口数据有固定的大小，窗口数据无叠加；</li><li>Hop - 滑动窗口，窗口数据有固定大小，并且有固定的窗口重建频率，窗口数据有叠加；</li><li>Session - 会话窗口，窗口数据没有固定的大小，根据窗口数据活跃程度划分窗口，窗口数据无叠加。</li></ul><p>说明： Aapche Flink 还支持UnBounded的 Group Window，也就是全局Window，流上所有数据都在一个窗口里面，语义非常简单，这里不做详细介绍了。</p><h4 id="Tumble"><a href="#Tumble" class="headerlink" title="Tumble"></a>Tumble</h4><p><strong>语义</strong></p><p>Tumble 滚动窗口有固定size，窗口数据不重叠,具体语义如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvN9PPeHiaOUQib8BG2xs3YPxpN8EYibnRNkFxgicW1kPrNeicE8vpcUB7tspA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Tumble 滚动窗口对应的语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk],</span><br><span class="line">    [TUMBLE_START(timeCol, size)], </span><br><span class="line">    [TUMBLE_END(timeCol, size)], </span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], TUMBLE(timeCol, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] - 决定了流是Keyed还是/Non-Keyed;</li><li>TUMBLE_START - 窗口开始时间;</li><li>TUMBLE_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>size - 表示窗口的大小，如 秒，分钟，小时，天。</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccess_tab</code>测试数据，我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV)。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region,</span><br><span class="line">    TUMBLE_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    TUMBLE_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv</span><br><span class="line"><span class="keyword">FROM</span> pageAccess_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, TUMBLE(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'2'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><h4 id="Hop"><a href="#Hop" class="headerlink" title="Hop"></a>Hop</h4><p>Hop 滑动窗口和滚动窗口类似，窗口有固定的size，与滚动窗口不同的是滑动窗口可以通过slide参数控制滑动窗口的新建频率。因此当slide值小于窗口size的值的时候多个滑动窗口会重叠。</p><p><strong>语义</strong></p><p>Hop 滑动窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNCwyBicMTKEicSxibebwTfwvImiaA2TlN0FuM0wuG6zAibYyk5JrfBTmrwEA/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Hop 滑动窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    [HOP_START(timeCol, slide, size)] ,  </span><br><span class="line">    [HOP_END(timeCol, slide, size)],</span><br><span class="line">    agg1(col1), </span><br><span class="line">    <span class="built_in">..</span>. </span><br><span class="line">    aggN(colN) </span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], HOP(timeCol, slide, size)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>HOP_START - 窗口开始时间;</li><li>HOP_END - 窗口结束时间;</li><li>timeCol - 是流表中表示时间字段；</li><li>slide - 是滑动步伐的大小；</li><li>size - 是窗口的大小，如 秒，分钟，小时，天；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessCount_tab</code>测试数据，我们需要每5分钟统计近10分钟的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">  HOP_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">  HOP_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd,  </span><br><span class="line">  <span class="keyword">SUM</span>(accessCount) <span class="keyword">AS</span> accessCount  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessCount_tab </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> HOP(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'5'</span> <span class="keyword">MINUTE</span>, <span class="built_in">INTERVAL</span> <span class="string">'10'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>winStart</th><th>winEnd</th><th>accessCount</th></tr></thead><tbody><tr><td>2017-11-11 01:55:00.0</td><td>2017-11-11 02:05:00.0</td><td>186</td></tr><tr><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:10:00.0</td><td>396</td></tr><tr><td>2017-11-11 02:05:00.0</td><td>2017-11-11 02:15:00.0</td><td>243</td></tr><tr><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:20:00.0</td><td>33</td></tr><tr><td>2017-11-11 04:05:00.0</td><td>2017-11-11 04:15:00.0</td><td>129</td></tr><tr><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:20:00.0</td><td>129</td></tr></tbody></table><h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Seeeion 会话窗口 是没有固定大小的窗口，通过session的活跃度分组元素。不同于滚动窗口和滑动窗口，会话窗口不重叠,也没有固定的起止时间。一个会话窗口在一段时间内没有接收到元素时，即当出现非活跃间隙时关闭。一个会话窗口 分配器通过配置session gap来指定非活跃周期的时长.</p><p>语义</p><p>Session 会话窗口语义如下所示：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/UdK9ByfMT2NJ7vfubd8agGwvHzRBngvNfhx9EQvp4OyBYHue50QEzW3qZfxeRV5DCb8CkcneoGjadj7NqNHq9w/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="image"></p><p><strong>语法</strong></p><p>Seeeion 会话窗口对应语法如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT </span><br><span class="line">    [gk], </span><br><span class="line">    SESSION_START(timeCol, gap) AS winStart,  </span><br><span class="line">    SESSION_END(timeCol, gap) AS winEnd,</span><br><span class="line">    agg1(col1),</span><br><span class="line">     <span class="built_in">..</span>. </span><br><span class="line">    aggn(colN)</span><br><span class="line"><span class="keyword">FROM</span> Tab1</span><br><span class="line">GROUP BY [gk], SESSION(timeCol, gap)</span><br></pre></td></tr></table></figure><ul><li>[gk] 决定了流是Keyed还是/Non-Keyed;</li><li>SESSION_START - 窗口开始时间；</li><li>SESSION_END - 窗口结束时间；</li><li>timeCol - 是流表中表示时间字段；</li><li>gap - 是窗口数据非活跃周期的时长；</li></ul><p><strong>SQL 示例</strong></p><p>利用<code>pageAccessSession_tab</code>测试数据，我们按地域统计连续的两个访问用户之间的访问时间间隔不超过3分钟的的页面访问量(PV).</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  </span><br><span class="line">    region, </span><br><span class="line">    SESSION_START(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winStart,  </span><br><span class="line">    SESSION_END(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> winEnd, </span><br><span class="line">    <span class="keyword">COUNT</span>(region) <span class="keyword">AS</span> pv  </span><br><span class="line"><span class="keyword">FROM</span> pageAccessSession_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> region, <span class="keyword">SESSION</span>(rowtime, <span class="built_in">INTERVAL</span> <span class="string">'3'</span> <span class="keyword">MINUTE</span>)</span><br></pre></td></tr></table></figure><p><strong>结果如下</strong></p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:13:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:01:00.0</td><td>2017-11-11 02:08:00.0</td><td>4</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:14:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:16:00.0</td><td>2017-11-11 04:19:00.0</td><td>1</td></tr></tbody></table><h2 id="UDX"><a href="#UDX" class="headerlink" title="UDX"></a>UDX</h2><p>Apache Flink 除了提供了大部分ANSI-SQL的核心算子，也为用户提供了自己编写业务代码的机会，那就是User-Defined Function,目前支持如下三种 User-Defined Function：</p><ul><li>UDF - User-Defined Scalar Function</li><li>UDTF - User-Defined Table Function</li><li>UDAF - User-Defined Aggregate Funciton</li></ul><p>UDX都是用户自定义的函数，那么Apache Flink框架为啥将自定义的函数分成三类呢？是根据什么划分的呢？Apache Flink对自定义函数进行分类的依据是根据函数语义的不同，函数的输入和输出不同来分类的，具体如下：</p><table><thead><tr><th>UDX</th><th>INPUT</th><th>OUTPUT</th><th>INPUT:OUTPUT</th></tr></thead><tbody><tr><td>UDF</td><td>单行中的N(N&gt;=0)列</td><td>单行中的1列</td><td>1:1</td></tr><tr><td>UDTF</td><td>单行中的N(N&gt;=0)列</td><td>M(M&gt;=0)行</td><td>1:N(N&gt;=0)</td></tr><tr><td>UDAF</td><td>M(M&gt;=0)行中的每行的N(N&gt;=0)列</td><td>单行中的1列</td><td>M：1(M&gt;=0)</td></tr></tbody></table><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><ul><li>定义<br>用户想自己编写一个字符串联接的UDF，我们只需要实现<code>ScalarFunction#eval()</code>方法即可，简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyConnect</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line">  <span class="meta">@varargs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(args: <span class="type">String</span>*): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sb = <span class="keyword">new</span> <span class="type">StringBuilder</span></span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; args.length) &#123;</span><br><span class="line">      <span class="keyword">if</span> (args(i) == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line">      &#125;</span><br><span class="line">      sb.append(args(i))</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    sb.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> <span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = MyConnect</span></span><br><span class="line"> tEnv.registerFunction(<span class="string">"myConnect"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"> <span class="keyword">val</span> sql = <span class="string">"SELECT myConnect(a, b) as str FROM tab"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDTF"><a href="#UDTF" class="headerlink" title="UDTF"></a>UDTF</h3><ul><li>定义<br>用户想自己编写一个字符串切分的UDTF，我们只需要实现<code>TableFunction#eval()</code>方法即可，简单实现如下：</li></ul><p>ScalarFunction#eval()`</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>))&#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(collect)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>, prefix: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str.contains(<span class="string">"#"</span>)) &#123;</span><br><span class="line">      str.split(<span class="string">"#"</span>).foreach(s =&gt; collect(prefix + s))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MySplit</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"mySplit"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT c, s FROM MyTable, LATERAL TABLE(mySplit(c)) AS T(s)"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><ul><li>定义<br>UDAF 要实现的接口比较多，我们以一个简单的CountAGG为例，做简单实现如下：</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The initial accumulator for count aggregate function */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CountAccumulator</span> <span class="keyword">extends</span> <span class="title">JTuple1</span>[<span class="type">Long</span>] </span>&#123;</span><br><span class="line">  f0 = <span class="number">0</span>L <span class="comment">//count</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * User-defined count aggregate function</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCount</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[<span class="type">JLong</span>, <span class="type">CountAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 += <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// process argument is optimized by Calcite.</span></span><br><span class="line">  <span class="comment">// For instance count(42) or count(*) will be optimized to count().</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 -= <span class="number">1</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 += <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">CountAccumulator</span>, value: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value != <span class="literal">null</span>) &#123;</span><br><span class="line">      acc.f0 -= <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">JLong</span> = &#123;</span><br><span class="line">    acc.f0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">CountAccumulator</span>, its: <span class="type">JIterable</span>[<span class="type">CountAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = its.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      acc.f0 += iter.next().f0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">CountAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CountAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">CountAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.f0 = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getAccumulatorType</span></span>: <span class="type">TypeInformation</span>[<span class="type">CountAccumulator</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">TupleTypeInfo</span>(classOf[<span class="type">CountAccumulator</span>], <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getResultType</span></span>: <span class="type">TypeInformation</span>[<span class="type">JLong</span>] =</span><br><span class="line">    <span class="type">BasicTypeInfo</span>.<span class="type">LONG_TYPE_INFO</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用</li></ul><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> <span class="function"><span class="keyword">fun</span> = new <span class="title">MyCount</span><span class="params">()</span></span></span><br><span class="line">tEnv.registerFunction(<span class="string">"myCount"</span>, <span class="function"><span class="keyword">fun</span>)</span></span><br><span class="line"><span class="keyword">val</span> sql = <span class="string">"SELECT myCount(c) FROM MyTable GROUP BY  a"</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="Source-amp-Sink"><a href="#Source-amp-Sink" class="headerlink" title="Source&amp;Sink"></a>Source&amp;Sink</h1><p>上面我们介绍了Apache Flink SQL核心算子的语法及语义，这部分将选取Bounded EventTime Tumble Window为例为大家编写一个完整的包括Source和Sink定义的Apache Flink SQL Job。假设有一张淘宝页面访问表(PageAccess_tab)，有地域，用户ID和访问时间。我们需要按不同地域统计每2分钟的淘宝首页的访问量(PV). 具体数据如下：</p><table><thead><tr><th>region</th><th>userId</th><th>accessTime</th></tr></thead><tbody><tr><td>ShangHai</td><td>U0010</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U1001</td><td>2017-11-11 10:01:00</td></tr><tr><td>BeiJing</td><td>U2032</td><td>2017-11-11 10:10:00</td></tr><tr><td>BeiJing</td><td>U1100</td><td>2017-11-11 10:11:00</td></tr><tr><td>ShangHai</td><td>U0011</td><td>2017-11-11 12:10:00</td></tr></tbody></table><h2 id="Source-定义"><a href="#Source-定义" class="headerlink" title="Source 定义"></a>Source 定义</h2><p>自定义Apache Flink Stream Source需要实现<code>StreamTableSource</code>, <code>StreamTableSource</code>中通过<code>StreamExecutionEnvironment</code> 的<code>addSource</code>方法获取<code>DataStream</code>, 所以我们需要自定义一个 <code>SourceFunction</code>, 并且要支持产生WaterMark，也就是要实现<code>DefinedRowtimeAttributes</code>接口。</p><h3 id="Source-Function定义"><a href="#Source-Function定义" class="headerlink" title="Source Function定义"></a>Source Function定义</h3><p>支持接收携带EventTime的数据集合，Either的数据结构，Right表示WaterMark和Left表示数据:</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MySourceFunction</span>[<span class="type">T</span>](<span class="title">dataWithTimestampList</span>: <span class="type">Seq</span>[<span class="type">Either</span>[(<span class="type">Long</span>, <span class="type">T</span>), <span class="type">Long</span>]]) </span></span><br><span class="line"><span class="class">  extends <span class="type">SourceFunction</span>[<span class="type">T</span>] &#123;</span></span><br><span class="line"><span class="class">  override def run(<span class="title">ctx</span>: <span class="type">SourceContext</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span></span><br><span class="line"><span class="class">    dataWithTimestampList.foreach &#123;</span></span><br><span class="line"><span class="class">      case <span class="type">Left</span>(<span class="title">t</span>) =&gt; ctx.collectWithTimestamp(<span class="title">t</span>.<span class="title">_2</span>, <span class="title">t</span>.<span class="title">_1</span>)</span></span><br><span class="line"><span class="class">      case <span class="type">Right</span>(<span class="title">w</span>) =&gt; ctx.emitWatermark(<span class="title">new</span> <span class="type">Watermark(w)</span>)</span></span><br><span class="line"><span class="class">    &#125;</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">  override def cancel(): <span class="type">Unit</span> = ???</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="定义-StreamTableSource"><a href="#定义-StreamTableSource" class="headerlink" title="定义 StreamTableSource"></a>定义 StreamTableSource</h3><p>我们自定义的Source要携带我们测试的数据，以及对应的WaterMark数据，具体如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTableSource</span> <span class="keyword">extends</span> <span class="title">StreamTableSource</span>[<span class="type">Row</span>] <span class="keyword">with</span> <span class="title">DefinedRowtimeAttributes</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> fieldNames = <span class="type">Array</span>(<span class="string">"accessTime"</span>, <span class="string">"region"</span>, <span class="string">"userId"</span>)</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">TableSchema</span>(fieldNames, <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">SQL_TIMESTAMP</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">  <span class="keyword">val</span> rowType = <span class="keyword">new</span> <span class="type">RowTypeInfo</span>(</span><br><span class="line">    <span class="type">Array</span>(<span class="type">Types</span>.<span class="type">LONG</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>).asInstanceOf[<span class="type">Array</span>[<span class="type">TypeInformation</span>[_]]],</span><br><span class="line">    fieldNames)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 页面访问表数据 rows with timestamps and watermarks</span></span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0010"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510365660000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510365660000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1001"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510365660000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366200000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366200000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U2032"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366200000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510366260000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510366260000</span>L), <span class="string">"BeiJing"</span>, <span class="string">"U1100"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510366260000</span>L),</span><br><span class="line">    <span class="type">Left</span>(<span class="number">1510373400000</span>L, <span class="type">Row</span>.of(<span class="keyword">new</span> <span class="type">JLong</span>(<span class="number">1510373400000</span>L), <span class="string">"ShangHai"</span>, <span class="string">"U0011"</span>)),</span><br><span class="line">    <span class="type">Right</span>(<span class="number">1510373400000</span>L)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRowtimeAttributeDescriptors</span></span>: util.<span class="type">List</span>[<span class="type">RowtimeAttributeDescriptor</span>] = &#123;</span><br><span class="line">    <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">RowtimeAttributeDescriptor</span>(</span><br><span class="line">      <span class="string">"accessTime"</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExistingField</span>(<span class="string">"accessTime"</span>),</span><br><span class="line">      <span class="type">PreserveWatermarks</span>.<span class="type">INSTANCE</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDataStream</span></span>(execEnv: <span class="type">StreamExecutionEnvironment</span>): <span class="type">DataStream</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    execEnv.addSource(<span class="keyword">new</span> <span class="type">MySourceFunction</span>[<span class="type">Row</span>](data)).setParallelism(<span class="number">1</span>).returns(rowType)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReturnType</span></span>: <span class="type">TypeInformation</span>[<span class="type">Row</span>] = rowType</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getTableSchema</span></span>: <span class="type">TableSchema</span> = schema</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Sink-定义"><a href="#Sink-定义" class="headerlink" title="Sink 定义"></a>Sink 定义</h2><p>我们简单的将计算结果写入到Apache Flink内置支持的CSVSink中，定义Sink如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def getCsvTableSink: TableSink[Row] = &#123;</span><br><span class="line">    val tempFile = <span class="built_in">File</span>.createTempFile(<span class="string">"csv_sink_"</span>, <span class="string">"tem"</span>)</span><br><span class="line">    <span class="comment">// 打印sink的文件路径，方便我们查看运行结果</span></span><br><span class="line">    <span class="built_in">println</span>(<span class="string">"Sink path : "</span> + tempFile)</span><br><span class="line">    <span class="built_in">if</span> (tempFile.<span class="built_in">exists</span>()) &#123;</span><br><span class="line">      tempFile.<span class="keyword">delete</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> CsvTableSink(tempFile.getAbsolutePath).configure(</span><br><span class="line">      Array[<span class="keyword">String</span>](<span class="string">"region"</span>, <span class="string">"winStart"</span>, <span class="string">"winEnd"</span>, <span class="string">"pv"</span>),</span><br><span class="line">      Array[TypeInformation[_]](Types.STRING, Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="构建主程序"><a href="#构建主程序" class="headerlink" title="构建主程序"></a>构建主程序</h2><p>主程序包括执行环境的定义，Source/Sink的注册以及统计查SQL的执行，具体如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Streaming 环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置EventTime</span></span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//方便我们查出输出数据</span></span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sourceTableName = <span class="string">"mySource"</span></span><br><span class="line">    <span class="comment">// 创建自定义source数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSource = <span class="keyword">new</span> <span class="type">MyTableSource</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sinkTableName = <span class="string">"csvSink"</span></span><br><span class="line">    <span class="comment">// 创建CSV sink 数据结构</span></span><br><span class="line">    <span class="keyword">val</span> tableSink = getCsvTableSink</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 注册source</span></span><br><span class="line">    tEnv.registerTableSource(sourceTableName, tableSource)</span><br><span class="line">    <span class="comment">// 注册sink</span></span><br><span class="line">    tEnv.registerTableSink(sinkTableName, tableSink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">      <span class="string">"SELECT  "</span> +</span><br><span class="line">      <span class="string">"  region, "</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_START(accessTime, INTERVAL '2' MINUTE) AS winStart,"</span> +</span><br><span class="line">      <span class="string">"  TUMBLE_END(accessTime, INTERVAL '2' MINUTE) AS winEnd, COUNT(region) AS pv "</span> +</span><br><span class="line">      <span class="string">" FROM mySource "</span> +</span><br><span class="line">      <span class="string">" GROUP BY TUMBLE(accessTime, INTERVAL '2' MINUTE), region"</span></span><br><span class="line"></span><br><span class="line">    tEnv.sqlQuery(sql).insertInto(sinkTableName);</span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="执行并查看运行结果"><a href="#执行并查看运行结果" class="headerlink" title="执行并查看运行结果"></a>执行并查看运行结果</h2><p>执行主程序后我们会在控制台得到Sink的文件路径，如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sink <span class="string">path :</span> <span class="regexp">/var/</span>folders<span class="regexp">/88/</span><span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn<span class="regexp">/T/</span>csv_sink_8025014910735142911tem</span><br></pre></td></tr></table></figure><p>Cat 方式查看计算结果，如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jinchengsunjcdeMacBook-Pro:FlinkTableApiDemo jincheng.sunjc$ cat /var/folders/<span class="number">88</span>/<span class="number">8</span>n406qmx2z73qvrzc_rbtv_r0000gn/T/csv_sink_8025014910735142911tem</span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:00:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:02:00.0</span>,<span class="number">1</span></span><br><span class="line">BeiJing,<span class="number">2017-11-11</span> <span class="number">02:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">02:12:00.0</span>,<span class="number">2</span></span><br><span class="line">ShangHai,<span class="number">2017-11-11</span> <span class="number">04:10:00.0</span>,<span class="number">2017-11-11</span> <span class="number">04:12:00.0</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure><p>表格化如上结果：</p><table><thead><tr><th>region</th><th>winStart</th><th>winEnd</th><th>pv</th></tr></thead><tbody><tr><td>BeiJing</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>BeiJing</td><td>2017-11-11 02:10:00.0</td><td>2017-11-11 02:12:00.0</td><td>2</td></tr><tr><td>ShangHai</td><td>2017-11-11 02:00:00.0</td><td>2017-11-11 02:02:00.0</td><td>1</td></tr><tr><td>ShangHai</td><td>2017-11-11 04:10:00.0</td><td>2017-11-11 04:12:00.0</td><td>1</td></tr></tbody></table><p>上面这个端到端的完整示例也可以应用到本篇前面介绍的其他算子示例中，只是大家根据Source和Sink的Schema不同来进行相应的构建即可！</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇概要的介绍了Apache Flink SQL 大部分核心功能，并附带了具体的测试数据和测试程序，最后以一个End-to-End的示例展示了如何编写Apache Flink SQL的Job收尾。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> FlinkSQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink On Yarn HA</title>
      <link href="/2018/11/16/Flink%20on%20Yarn%20HA/"/>
      <url>/2018/11/16/Flink%20on%20Yarn%20HA/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 关闭防火墙</span><br><span class="line"><span class="meta">#</span> 配好主机映射</span><br><span class="line"><span class="meta">#</span> 配置免密登录</span><br><span class="line"><span class="meta">#</span> 准备好安装包 hadoop-2.8.5.tar.gz、flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz</span><br><span class="line"><span class="meta">#</span> 创建flink用户，后续操作均在flink用户下操作</span><br><span class="line"><span class="meta">#</span> 将Hadoop安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 将flink安装包解压至flink01节点的/data/apps路径下</span><br><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br><span class="line"><span class="meta">#</span> 节点配置如下：</span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">192.168.23.51</td><td align="center">flink01</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、NameNode、DFSZKFailoverController、DataNode</td></tr><tr><td align="center">192.168.23.52</td><td align="center">flink02</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、 NameNode、DFSZKFailoverController、DataNode、ResourceManager</td></tr><tr><td align="center">192.168.23.53</td><td align="center">flink03</td><td align="center">4核cpu/8G内存/50G硬盘</td><td align="center">QuorumPeerMain、JournalNode、NodeManager、ResourceManager、DataNode</td></tr></tbody></table><h2 id="Hadoop-HA配置"><a href="#Hadoop-HA配置" class="headerlink" title="Hadoop HA配置"></a>Hadoop HA配置</h2><h3 id="进入hadoop配置目录"><a href="#进入hadoop配置目录" class="headerlink" title="进入hadoop配置目录"></a>进入hadoop配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 进入hadoop配置目录</span><br><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改Java环境配置"><a href="#修改Java环境配置" class="headerlink" title="修改Java环境配置"></a>修改Java环境配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 修改hadoop-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置yarn-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br><span class="line"><span class="meta">#</span> 配置mapred-env.sh中的JAVA_HOME</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_181</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hdfs的nameservice为ns1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/name1,/data/apps/hadoop-2.8.5/tmp/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/dfs/data1,/data/apps/hadoop-2.8.5/tmp/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn1的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的RPC通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- nn2的http通信地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://flink01:8485;flink02:8485;flink03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp/journal<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行</span></span><br><span class="line"><span class="comment">sshfence:当Active出问题后，standby切换成Active，此时，原Active又没有停止服务，这种情况下会被强制杀死进程。</span></span><br><span class="line"><span class="comment">shell(/bin/true)：NN Active和它的ZKFC一起挂了，没有人通知ZK，ZK长期没有接到通知，standby要切换，此时，standby调一个shell（脚本内容），这个脚本返回true则切换成功。</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">sshfence</span><br><span class="line">shell(/bin/true)</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/flink/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rmcluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群的地址--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:2181,flink02:2181,flink03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--启用自动恢复--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定resourcemanager的状态信息存储在zookeeper集群--&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置capacity-scheduler-xml"><a href="#配置capacity-scheduler-xml" class="headerlink" title="配置capacity-scheduler.xml"></a>配置capacity-scheduler.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>集群中可用于运行application master的资源比例上限.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="启动Zookeeper集群"><a href="#启动Zookeeper集群" class="headerlink" title="启动Zookeeper集群"></a>启动Zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">bin/zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="初始化Hadoop环境"><a href="#初始化Hadoop环境" class="headerlink" title="初始化Hadoop环境"></a>初始化Hadoop环境</h3><p>启动journalnode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01、flink02、flink03执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><p>格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs namenode -format</span><br></pre></td></tr></table></figure><p>格式化zk</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">bin/hdfs zkfc -formatZK</span><br><span class="line"><span class="meta">#</span><span class="bash"> 执行完成后，会在zookeeper 上创建一个目录，查看是否创建成功：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 进入zookeeper家目录，执行bin/zkCli.sh客户端连接ZK。在ZK客户端的shell命令行查看：ls /</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 出现hadoop-ha即表示成功。</span></span><br></pre></td></tr></table></figure><p>启动主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/hadoop-daemon.sh start namenode</span><br></pre></td></tr></table></figure><p>备用NN 同步主NN信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">bin/hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>关闭已启动的所有journalnode和主namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令</span></span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="启动hadoop集群"><a href="#启动hadoop集群" class="headerlink" title="启动hadoop集群"></a>启动hadoop集群</h3><p>启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink01执行以下命令（建议先启动所有journalnode以防出现namenode连接journalnode超时）</span></span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个namenode的状态</span></span><br><span class="line">bin/hdfs haadmin -getServiceState nn1     #查看nn1状态</span><br><span class="line">bin/hdfs haadmin -getServiceState nn2     #查看nn2状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 手动切换namenode状态（此处禁用，有需要再执行）</span></span><br><span class="line">bin/hdfs haadmin -transitionToActive nn1##切换成active</span><br><span class="line">bin/hdfs haadmin -transitionToStandby nn1##切换成standby</span><br></pre></td></tr></table></figure><p>启动Yarn</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在flink02执行以下命令</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在flink03执行以下命令</span></span><br><span class="line">sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看两个Resourcemanager的状态</span></span><br><span class="line">bin/yarn rmadmin -getServiceState rm1      ##查看rm1的状态</span><br><span class="line">bin/yarn rmadmin -getServiceState rm2      ##查看rm2的状态</span><br><span class="line"><span class="meta">#</span><span class="bash"> 当flink02的ResourceManager是Active状态的时候，访问flink03的ResourceManager会自动跳转到flink02的web页面</span></span><br></pre></td></tr></table></figure><h2 id="Flink-HA配置"><a href="#Flink-HA配置" class="headerlink" title="Flink HA配置"></a>Flink HA配置</h2><h3 id="进入flink配置目录"><a href="#进入flink配置目录" class="headerlink" title="进入flink配置目录"></a>进入flink配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html" target="_blank" rel="noopener">点此查看flink配置说明</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The config parameter defining the network address to connect to <span class="keyword">for</span> communication with the job manager. This value is only interpreted <span class="keyword">in</span> setups <span class="built_in">where</span> a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used <span class="keyword">in</span> many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.</span></span><br><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the JobManager.</span></span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line"><span class="meta">#</span><span class="bash"> JVM heap size <span class="keyword">for</span> the TaskManagers, <span class="built_in">which</span> are the parallel workers of the system. On YARN setups, this value is automatically configured to the size of the TaskManager<span class="string">'s YARN container, minus a certain tolerance value.</span></span></span><br><span class="line">taskmanager.heap.size: 2048m</span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of parallel operator or user <span class="keyword">function</span> instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a <span class="keyword">function</span> or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or <span class="keyword">function</span> instances. This value is typically proportional to the number of physical CPU cores that the TaskManager<span class="string">'s machine has (e.g., equal to the number of cores, or half the number of cores).</span></span></span><br><span class="line">taskmanager.numberOfTaskSlots: 4</span><br><span class="line"><span class="meta">#</span><span class="bash"> Default parallelism <span class="keyword">for</span> <span class="built_in">jobs</span>.</span></span><br><span class="line">parallelism.default: 2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Defines high-availability mode used <span class="keyword">for</span> the cluster execution. To <span class="built_in">enable</span> high-availability, <span class="built_in">set</span> this mode to <span class="string">"ZOOKEEPER"</span> or specify FQN of factory class.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> high-availability mode (required): The high-availability mode has to be <span class="built_in">set</span> <span class="keyword">in</span> conf/flink-conf.yaml to zookeeper <span class="keyword">in</span> order to <span class="built_in">enable</span> high availability mode. Alternatively this option can be <span class="built_in">set</span> to FQN of factory class Flink should use to create HighAvailabilityServices instance.</span></span><br><span class="line">high-availability: zookeeper</span><br><span class="line"><span class="meta">#</span><span class="bash"> File system path (URI) <span class="built_in">where</span> Flink persists metadata <span class="keyword">in</span> high-availability setups.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Storage directory (required): JobManager metadata is persisted <span class="keyword">in</span> the file system storageDir and only a pointer to this state is stored <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The storageDir stores all metadata needed to recover a JobManager failure.</span></span><br><span class="line">high-availability.storageDir: hdfs://ns1/flink/recovery</span><br><span class="line"><span class="meta">#</span><span class="bash"> The ZooKeeper quorum to use, when running Flink <span class="keyword">in</span> a high-availability mode with ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper quorum (required): A ZooKeeper quorum is a replicated group of ZooKeeper servers, <span class="built_in">which</span> provide the distributed coordination service.</span></span><br><span class="line">high-availability.zookeeper.quorum: flink01:2181,flink02:2181,flink03:2181</span><br><span class="line"><span class="meta">#</span><span class="bash"> The root path under <span class="built_in">which</span> Flink stores its entries <span class="keyword">in</span> ZooKeeper.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper root (recommended): The root ZooKeeper node, under <span class="built_in">which</span> all cluster nodes are placed.</span></span><br><span class="line">high-availability.zookeeper.path.root: /flink</span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts: The number of ApplicationMaster (+ its TaskManager containers) attempts. If this value is <span class="built_in">set</span> to 1 (default), the entire YARN session will fail when the Application master fails. Higher values specify the number of restarts of the ApplicationMaster by YARN.</span></span><br><span class="line">yarn.application-attempts: 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The state backend to be used to store and checkpoint state.</span></span><br><span class="line">state.backend: rocksdb</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory used <span class="keyword">for</span> storing the data files and meta data of checkpoints <span class="keyword">in</span> a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).</span></span><br><span class="line">state.checkpoints.dir: hdfs://ns1/flink/flink-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> The default directory <span class="keyword">for</span> savepoints. Used by the state backends that write savepoints to file systems (MemoryStateBackend, FsStateBackend, RocksDBStateBackend).</span></span><br><span class="line">state.savepoints.dir: hdfs://ns1/flink/save-checkpoints</span><br><span class="line"><span class="meta">#</span><span class="bash"> Option whether the state backend should create incremental checkpoints, <span class="keyword">if</span> possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Some state backends may not support incremental checkpoints and ignore this option.</span></span><br><span class="line">state.backend.incremental: true</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Directories <span class="keyword">for</span> temporary files, separated by<span class="string">","</span>, <span class="string">"|"</span>, or the system<span class="string">'s java.io.File.pathSeparator.</span></span></span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br></pre></td></tr></table></figure><p>切记：Flink On Yarn HA一定不要手动配置high-availability.cluster-id</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ZooKeeper cluster-id (recommended): The cluster-id ZooKeeper node, under <span class="built_in">which</span> all required coordination data <span class="keyword">for</span> a cluster is placed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be <span class="built_in">set</span> <span class="keyword">for</span> standalone clusters but is automatically inferred <span class="keyword">in</span> YARN and Mesos.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Important: You should not <span class="built_in">set</span> this value manually when running a YARN cluster, a per-job YARN session, or on another cluster manager. In those cases a cluster-id is automatically being generated based on the application id. Manually setting a cluster-id overrides this behaviour <span class="keyword">in</span> YARN. Specifying a cluster-id with the -z CLI option, <span class="keyword">in</span> turn, overrides manual configuration. If you are running multiple Flink HA clusters on bare metal, you have to manually configure separate cluster-ids <span class="keyword">for</span> each cluster.</span></span><br><span class="line">high-availability.cluster-id: /default</span><br></pre></td></tr></table></figure><h3 id="替换日志框架为logback"><a href="#替换日志框架为logback" class="headerlink" title="替换日志框架为logback"></a>替换日志框架为logback</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><p><strong>logback-yarn.xml配置示例</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>10MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn-HA测试说明"><a href="#Flink-on-Yarn-HA测试说明" class="headerlink" title="Flink on Yarn HA测试说明"></a>Flink on Yarn HA测试说明</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开始一个yarn-session（命名为FlinkTestCluster）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager内存2048M</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个TaskManager内存2048M且分配4个slot（The session cluster will automatically allocate additional containers <span class="built_in">which</span> run the Task Managers when <span class="built_in">jobs</span> are submitted to the cluster.）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式启动</span></span><br><span class="line">yarn-session.sh -jm 2048 -tm 2048 -s 4 -nm FlinkTestCluster -d</span><br></pre></td></tr></table></figure><table><thead><tr><th>配置</th><th>测试方案</th><th>现象</th><th>备注</th></tr></thead><tbody><tr><td>Job本身配置了Flink的重启策略</td><td>提供bug程序，导致Job失败</td><td>重启失败的Job</td><td>保证Job HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnSessionClusterEntrypoint进程（<em>JobManager</em>和AM的共同进程）</td><td>重启JobManager和AM，该进程会迁移到其它节点（非必须）且进程号改变，全部Job重启</td><td>保证JobManager HA</td></tr><tr><td>Job本身配置了Flink的重启策略、Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了yarn.application-attempts</td><td>kill掉YarnTaskExecutorRunner进程（TaskManager进程）</td><td>重启TaskManager，该进程会迁移到其它节点（非必须）且进程号改变，被Kill掉的TaskManager包含的Job重启</td><td>保证TaskManager HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts</td><td>未主动Cancel掉Flink集群中的Job，但不小心kill掉对应的yarn-session(对应Yarn队列中的一个Application)、之后在命令行重新提交yarn-session</td><td>启动新的yarn-session、之前未Cancel掉的Job自动迁移到当前yarn-session、JobManager和TaskManager自动创建</td><td>保证 YarnSessionHA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts、配置了Yarn的HA</td><td>Kill掉Resourcemanager</td><td>ResourceManager迁移到另一台节点，yarn-session重启，所有Job重启</td><td>保证Yarn HA</td></tr><tr><td>Yarn的yarn-site.xml配置了yarn.resourcemanager.am.max-attempts、Flink的flink-conf.yaml配置了high-availability.zookeeper.quorum、high-availability.storageDir、high-availability.zookeeper.path.root、yarn.application-attempts（也可在yarn-session提交时通过-D动态配置）、配置了HDFS的HA</td><td>Kill掉NameNode</td><td>NameNode迁移到另一台节点</td><td>保证HDFS HA</td></tr></tbody></table><h2 id="Yarn的基本思想"><a href="#Yarn的基本思想" class="headerlink" title="Yarn的基本思想"></a>Yarn的基本思想</h2><p>YARN的基本思想是将资源管理和作业调度/监视的功能分解为单独的守护进程。我们的想法是拥有一个全局ResourceManager（<em>RM</em>）和每个应用程序ApplicationMaster（<em>AM</em>）。应用程序可以是单个作业，也可以是作业的DAG。</p><p>ResourceManager和NodeManager构成了数据计算框架。ResourceManager是在系统中的所有应用程序之间仲裁资源的最终权限。NodeManager是每台机器上负责Containers的代理框架，监视其资源使用情况（CPU，内存，磁盘，网络）并将其报告给ResourceManager / Scheduler。</p><p>每个应用程序ApplicationMaster实际上是一个含具体库的框架，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</p><p><img src="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif" alt="MapReduce NextGen架构"></p><p>ResourceManager有两个主要组件：Scheduler和ApplicationsManager。</p><p>Scheduler负责根据熟悉的容量，队列等约束将资源分配给各种正在运行的应用程序。Scheduler是纯调度程序，因为它不执行应用程序状态的监视或跟踪。此外，当出现应用程序故障或硬件故障，它无法保证重新启动失败的任务。Scheduler根据应用程序的资源需求执行其调度功能; 它是基于资源<em>Container</em>的抽象概念，它包含内存，CPU，磁盘，网络等元素。</p><p>Scheduler具有可插入策略，该策略负责在各种队列，应用程序等之间对集群资源进行分区。当前的调度程序（如<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html" target="_blank" rel="noopener">CapacityScheduler</a>和<a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/FairScheduler.html" target="_blank" rel="noopener">FairScheduler）</a>将是插件的一些示例。</p><p>ApplicationsManager负责接受作业提交，协商第一个容器以执行特定于应用程序的ApplicationMaster，并提供在失败时重新启动ApplicationMaster容器的服务。每个应用程序ApplicationMaster负责从Scheduler协调适当的资源容器，跟踪其状态并监视进度。</p><h2 id="Flink-on-Yarn的基本思想"><a href="#Flink-on-Yarn的基本思想" class="headerlink" title="Flink on Yarn的基本思想"></a>Flink on Yarn的基本思想</h2><p><img src="https://ci.apache.org/projects/flink/flink-docs-release-1.7/fig/FlinkOnYarn.svg" alt="img"></p><p>YARN客户端需要访问Hadoop配置以连接到YARN资源管理器和HDFS。它使用以下策略确定Hadoop配置：</p><ul><li>按顺序测试是否配置<code>YARN_CONF_DIR</code>，<code>HADOOP_CONF_DIR</code>或<code>HADOOP_CONF_PATH</code>。如果设置了其中一个变量，则用于读取配置。</li><li>如果上述策略失败（在正确的YARN设置中不应该这样），则客户端使用配置的<code>HADOOP_HOME</code>环境变量。如果<code>HADOOP_HOME</code>环境变量已配置，则客户端尝试访问<code>$HADOOP_HOME/etc/hadoop</code>（Hadoop 2）或<code>$HADOOP_HOME/conf</code>（Hadoop 1）。</li></ul><p>启动新的Flink YARN会话时，客户端首先检查所请求的资源（ApplicationMaster的memory和vcores）是否可用。之后，它将包含Flink的jar包和配置信息上传到HDFS（步骤1）。</p><p>客户端的下一步是请求（步骤2）YARN容器以启动<em>ApplicationMaster</em>（步骤3）。客户端将配置信息和jar文件注册为容器的资源，在特定机器上运行的NodeManager将负责准备容器（例如下载文件的工作）。完成后，将启动<em>ApplicationMaster</em>（AM）。</p><p>该<em>JobManager</em>和AM在同一容器中运行。一旦它们成功启动，AM就知道JobManager（Flink主机）的地址。它正在为TaskManagers生成一个新的Flink配置文件（以便它们可以连接到JobManager），该文件也上传到HDFS。此外，<em>AM</em>容器还提供Flink的Web界面。YARN代码分配的所有端口都是<em>临时端口</em>。这允许用户并行执行多个Flink YARN会话。</p><p>之后，AM开始为Flink的TaskManagers分配容器（步骤4），这将从HDFS下载jar文件和修改后的配置。完成这些步骤后，即可建立Flink并准备接受作业。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a> </p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank" rel="noopener">ResourceManager High Availability</a></p><p><a href="https://hadoop.apache.org/docs/r2.8.5/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">Apache Hadoop YARN</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html#yarn-cluster-high-availability" target="_blank" rel="noopener">JobManager High Availability (HA)</a></p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/deployment/yarn_setup.html" target="_blank" rel="noopener">Flink on Yarn</a></p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Yarn </tag>
            
            <tag> Flink on Yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink使用Logback作为日志框架的相关配置</title>
      <link href="/2018/11/15/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/11/15/Flink%E4%BD%BF%E7%94%A8Logback%E4%BD%9C%E4%B8%BA%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E7%9A%84%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink切换日志框架为Logback"><a href="#Flink切换日志框架为Logback" class="headerlink" title="Flink切换日志框架为Logback"></a>Flink切换日志框架为Logback</h1><h2 id="client端pom文件配置"><a href="#client端pom文件配置" class="headerlink" title="client端pom文件配置"></a>client端pom文件配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Add the two required logback dependencies --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ch.qos.logback<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>logback-classic<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- Add the log4j -&gt; sfl4j (-&gt; logback) bridge into the classpath</span></span><br><span class="line"><span class="comment">     Hadoop is logging to log4j! --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.15<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>*<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>添加logback-core、logback-classic及log4j-over-slf4j依赖，</li><li>之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；</li><li><strong>最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</strong></li></ul><h2 id="服务端配置"><a href="#服务端配置" class="headerlink" title="服务端配置"></a>服务端配置</h2><ul><li><p>添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)</p><p>相关jar包在logback官网上都有，<a href="https://download.csdn.net/download/qq_36643786/11190626" target="_blank" rel="noopener">嫌麻烦的可以点此链接直接下载！</a></p></li><li><p>移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)</p></li><li><p>如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</p></li></ul><h3 id="flink-daemon-sh"><a href="#flink-daemon-sh" class="headerlink" title="flink-daemon.sh"></a>flink-daemon.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-daemon.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#  Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment">#  or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment">#  distributed with this work for additional information</span></span><br><span class="line"><span class="comment">#  regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment">#  to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment">#  "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment">#  with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#      http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">#  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">#  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Start/stop a Flink daemon.</span></span><br><span class="line">USAGE=<span class="string">"Usage: flink-daemon.sh (start|stop|stop-all) (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">STARTSTOP=<span class="variable">$1</span></span><br><span class="line">DAEMON=<span class="variable">$2</span></span><br><span class="line">ARGS=(<span class="string">"<span class="variable">$&#123;@:3&#125;</span>"</span>) <span class="comment"># get remaining arguments as array</span></span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"<span class="variable">$0</span>"</span>`</span><br><span class="line">bin=`<span class="built_in">cd</span> <span class="string">"<span class="variable">$bin</span>"</span>; <span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line">. <span class="string">"<span class="variable">$bin</span>"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$DAEMON</span> <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unknown daemon '<span class="variable">$&#123;DAEMON&#125;</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$FLINK_IDENT_STRING</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">    FLINK_IDENT_STRING=<span class="string">"<span class="variable">$USER</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">pid=<span class="variable">$FLINK_PID_DIR</span>/flink-<span class="variable">$FLINK_IDENT_STRING</span>-<span class="variable">$DAEMON</span>.pid</span><br><span class="line"></span><br><span class="line">mkdir -p <span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Log files for daemons are indexed from the process ID's position in the PID</span></span><br><span class="line"><span class="comment"># file. The following lock prevents a race condition during daemon startup</span></span><br><span class="line"><span class="comment"># when multiple daemons read, index, and write to the PID file concurrently.</span></span><br><span class="line"><span class="comment"># The lock is created on the PID directory since a lock file cannot be safely</span></span><br><span class="line"><span class="comment"># removed. The daemon is started with the lock closed and the lock remains</span></span><br><span class="line"><span class="comment"># active in this script until the script exits.</span></span><br><span class="line"><span class="built_in">command</span> -v flock &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">if</span> [[ $? -eq 0 ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">exec</span> 200&lt;<span class="string">"<span class="variable">$FLINK_PID_DIR</span>"</span></span><br><span class="line">    flock 200</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ascending ID depending on number of lines in pid file.</span></span><br><span class="line"><span class="comment"># This allows us to start multiple daemon of each type.</span></span><br><span class="line">id=$([ -f <span class="string">"<span class="variable">$pid</span>"</span> ] &amp;&amp; <span class="built_in">echo</span> $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>"</span>) || <span class="built_in">echo</span> <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">FLINK_LOG_PREFIX=<span class="string">"<span class="variable">$&#123;FLINK_LOG_DIR&#125;</span>/flink-<span class="variable">$&#123;FLINK_IDENT_STRING&#125;</span>-<span class="variable">$&#123;DAEMON&#125;</span>-<span class="variable">$&#123;id&#125;</span>-<span class="variable">$&#123;HOSTNAME&#125;</span>"</span></span><br><span class="line"><span class="built_in">log</span>=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.log"</span></span><br><span class="line">out=<span class="string">"<span class="variable">$&#123;FLINK_LOG_PREFIX&#125;</span>.out"</span></span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog.file=<span class="variable">$&#123;log&#125;</span>"</span> <span class="string">"-Dlog4j.configuration=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/log4j.properties"</span> <span class="string">"-Dlogback.configurationFile=file:<span class="variable">$&#123;FLINK_CONF_DIR&#125;</span>/logback.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$(<span class="variable">$&#123;JAVA_RUN&#125;</span> -version 2&gt;&amp;1 | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Only set JVM 8 arguments if we have correctly extracted the version</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$&#123;JAVA_VERSION&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$JAVA_VERSION</span>"</span> -lt 18 ]; <span class="keyword">then</span></span><br><span class="line">        JVM_ARGS=<span class="string">"<span class="variable">$JVM_ARGS</span> -XX:MaxPermSize=256m"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STARTSTOP</span> <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">    (start)</span><br><span class="line">        <span class="comment"># Rotate log files</span></span><br><span class="line">        rotateLogFilesWithPrefix <span class="string">"<span class="variable">$FLINK_LOG_DIR</span>"</span> <span class="string">"<span class="variable">$FLINK_LOG_PREFIX</span>"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print a warning if daemons are already running on host</span></span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          active=()</span><br><span class="line">          <span class="keyword">while</span> IFS=<span class="string">''</span> <span class="built_in">read</span> -r p || [[ -n <span class="string">"<span class="variable">$p</span>"</span> ]]; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">kill</span> -0 <span class="variable">$p</span> &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">              active+=(<span class="variable">$p</span>)</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">          <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          count=<span class="string">"<span class="variable">$&#123;#active[@]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> [ <span class="variable">$&#123;count&#125;</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"[INFO] <span class="variable">$count</span> instance(s) of <span class="variable">$DAEMON</span> are already running on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">          <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Evaluate user options for local variable expansion</span></span><br><span class="line">        FLINK_ENV_JAVA_OPTS=$(<span class="built_in">eval</span> <span class="built_in">echo</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Starting <span class="variable">$DAEMON</span> daemon on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="variable">$JAVA_RUN</span> <span class="variable">$JVM_ARGS</span> <span class="variable">$&#123;FLINK_ENV_JAVA_OPTS&#125;</span> <span class="string">"<span class="variable">$&#123;log_setting[@]&#125;</span>"</span> -classpath <span class="string">"`manglePathList "</span><span class="variable">$FLINK_TM_CLASSPATH</span>:<span class="variable">$INTERNAL_HADOOP_CLASSPATHS</span><span class="string">"`"</span> <span class="variable">$&#123;CLASS_TO_RUN&#125;</span> <span class="string">"<span class="variable">$&#123;ARGS[@]&#125;</span>"</span> &gt; <span class="string">"<span class="variable">$out</span>"</span> 200&lt;&amp;- 2&gt;&amp;1 &lt; /dev/null &amp;</span><br><span class="line"></span><br><span class="line">        mypid=$!</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add to pid file if successful start</span></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="variable">$&#123;mypid&#125;</span> =~ <span class="variable">$&#123;IS_NUMBER&#125;</span> ]] &amp;&amp; <span class="built_in">kill</span> -0 <span class="variable">$mypid</span> &gt; /dev/null 2&gt;&amp;1 ; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="variable">$mypid</span> &gt;&gt; <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Error starting <span class="variable">$DAEMON</span> daemon."</span></span><br><span class="line">            <span class="built_in">exit</span> 1</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="comment"># Remove last in pid file</span></span><br><span class="line">            to_stop=$(tail -n 1 <span class="string">"<span class="variable">$pid</span>"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> [ -z <span class="variable">$to_stop</span> ]; <span class="keyword">then</span></span><br><span class="line">                rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                sed \<span class="variable">$d</span> <span class="string">"<span class="variable">$pid</span>"</span> &gt; <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="comment"># all but last line</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># If all stopped, clean up pid file</span></span><br><span class="line">                [ $(wc -l &lt; <span class="string">"<span class="variable">$pid</span>.tmp"</span>) -eq 0 ] &amp;&amp; rm <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$pid</span>.tmp"</span> || mv <span class="string">"<span class="variable">$pid</span>.tmp"</span> <span class="string">"<span class="variable">$pid</span>"</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) is running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"No <span class="variable">$DAEMON</span> daemon to stop on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (stop-all)</span><br><span class="line">        <span class="keyword">if</span> [ -f <span class="string">"<span class="variable">$pid</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">            mv <span class="string">"<span class="variable">$pid</span>"</span> <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">read</span> to_stop; <span class="keyword">do</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$to_stop</span> &gt; /dev/null 2&gt;&amp;1; <span class="keyword">then</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Stopping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>) on host <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                    <span class="built_in">kill</span> <span class="variable">$to_stop</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="built_in">echo</span> <span class="string">"Skipping <span class="variable">$DAEMON</span> daemon (pid: <span class="variable">$to_stop</span>), because it is not running anymore on <span class="variable">$HOSTNAME</span>."</span></span><br><span class="line">                <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">done</span> &lt; <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">            rm <span class="string">"<span class="variable">$&#123;pid&#125;</span>.tmp"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"Unexpected argument '<span class="variable">$STARTSTOP</span>'. <span class="variable">$USAGE</span>."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml</li></ul><h3 id="flink-console-sh"><a href="#flink-console-sh" class="headerlink" title="flink-console.sh"></a>flink-console.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/bin/flink-console.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line"># Start a Flink service <span class="keyword">as</span> a console application. Must be stopped <span class="keyword">with</span> Ctrl-C</span><br><span class="line"># or <span class="keyword">with</span> SIGTERM by kill or the controlling process.</span><br><span class="line">USAGE=<span class="string">"Usage: flink-console.sh (taskexecutor|zookeeper|historyserver|standalonesession|standalonejob) [args]"</span></span><br><span class="line"></span><br><span class="line">SERVICE=$<span class="number">1</span></span><br><span class="line">ARGS=(<span class="string">"$&#123;@:2&#125;"</span>) # get remaining arguments <span class="keyword">as</span> array</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> $SERVICE <span class="keyword">in</span></span><br><span class="line">    (taskexecutor)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.taskexecutor.TaskManagerRunner</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (historyserver)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.webmonitor.history.HistoryServer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (zookeeper)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.zookeeper.FlinkZooKeeperQuorumPeer</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonesession)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.runtime.entrypoint.StandaloneSessionClusterEntrypoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (standalonejob)</span><br><span class="line">        CLASS_TO_RUN=org.apache.flink.container.entrypoint.StandaloneJobClusterEntryPoint</span><br><span class="line">    ;;</span><br><span class="line"></span><br><span class="line">    (*)</span><br><span class="line">        echo <span class="string">"Unknown service '$&#123;SERVICE&#125;'. $USAGE."</span></span><br><span class="line">        exit <span class="number">1</span></span><br><span class="line">    ;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line">FLINK_TM_CLASSPATH=`constructFlinkClassPath`</span><br><span class="line"></span><br><span class="line">log_setting=(<span class="string">"-Dlog4j.configuration=file:$&#123;FLINK_CONF_DIR&#125;/log4j-console.properties"</span> <span class="string">"-Dlogback.configurationFile=file:$&#123;FLINK_CONF_DIR&#125;/logback-console.xml"</span>)</span><br><span class="line"></span><br><span class="line">JAVA_VERSION=$($&#123;JAVA_RUN&#125; -version <span class="number">2</span>&gt;&amp;<span class="number">1</span> | sed <span class="string">'s/.*version "\(.*\)\.\(.*\)\..*"/\1\2/; 1q'</span>)</span><br><span class="line"></span><br><span class="line"># Only set JVM <span class="number">8</span> arguments <span class="keyword">if</span> we have correctly extracted the version</span><br><span class="line"><span class="keyword">if</span> [[ $&#123;JAVA_VERSION&#125; =~ $&#123;IS_NUMBER&#125; ]]; then</span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">"$JAVA_VERSION"</span> -lt <span class="number">18</span> ]; then</span><br><span class="line">        JVM_ARGS=<span class="string">"$JVM_ARGS -XX:MaxPermSize=256m"</span></span><br><span class="line">    fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">"Starting $SERVICE as a console application on host $HOSTNAME."</span></span><br><span class="line">exec $JAVA_RUN $JVM_ARGS $&#123;FLINK_ENV_JAVA_OPTS&#125; <span class="string">"$&#123;log_setting[@]&#125;"</span> -classpath <span class="string">"`manglePathList "</span>$FLINK_TM_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS<span class="string">"`"</span> $&#123;CLASS_TO_RUN&#125; <span class="string">"$&#123;ARGS[@]&#125;"</span></span><br></pre></td></tr></table></figure><ul><li>使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml</li></ul><h3 id="yarn-session-sh"><a href="#yarn-session-sh" class="headerlink" title="yarn-session.sh"></a>yarn-session.sh</h3><p>flink-release-1.7.1/flink-dist/src/main/flink-bin/yarn-bin/yarn-session.sh</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">################################################################################</span><br><span class="line">#  Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">#  or more contributor license agreements.  See the NOTICE file</span><br><span class="line">#  distributed <span class="keyword">with</span> this work for additional information</span><br><span class="line">#  regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">#  to you under the Apache License, Version <span class="number">2.0</span> (the</span><br><span class="line">#  <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance</span><br><span class="line">#  <span class="keyword">with</span> the License.  You may obtain a copy <span class="keyword">of</span> the License at</span><br><span class="line">#</span><br><span class="line">#      http:<span class="comment">//www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line">#</span><br><span class="line">#  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">#  distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span><br><span class="line">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">#  See the License for the specific language governing permissions and</span><br><span class="line"># limitations under the License.</span><br><span class="line">################################################################################</span><br><span class="line"></span><br><span class="line">bin=`dirname <span class="string">"$0"</span>`</span><br><span class="line">bin=`cd <span class="string">"$bin"</span>; pwd`</span><br><span class="line"></span><br><span class="line"># get Flink config</span><br><span class="line">. <span class="string">"$bin"</span>/config.sh</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$FLINK_IDENT_STRING"</span> = <span class="string">""</span> ]; then</span><br><span class="line">        FLINK_IDENT_STRING=<span class="string">"$USER"</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">JVM_ARGS=<span class="string">"$JVM_ARGS -Xmx512m"</span></span><br><span class="line"></span><br><span class="line">CC_CLASSPATH=`manglePathList $(constructFlinkClassPath):$INTERNAL_HADOOP_CLASSPATHS`</span><br><span class="line"></span><br><span class="line">log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-yarn-session-$HOSTNAME.log</span><br><span class="line">log_setting=<span class="string">"-Dlog.file="</span>$log<span class="string">" -Dlog4j.configuration=file:"</span>$FLINK_CONF_DIR<span class="string">"/log4j-yarn-session.properties -Dlogback.configurationFile=file:"</span>$FLINK_CONF_DIR<span class="string">"/logback-yarn.xml"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> FLINK_CONF_DIR</span><br><span class="line"></span><br><span class="line">$JAVA_RUN $JVM_ARGS -classpath <span class="string">"$CC_CLASSPATH"</span> $log_setting org.apache.flink.yarn.cli.FlinkYarnSessionCli -j <span class="string">"$FLINK_LIB_DIR"</span>/flink-dist*.jar <span class="string">"$@"</span></span><br></pre></td></tr></table></figure><ul><li>使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h2 id="doc"><a href="#doc" class="headerlink" title="doc"></a>doc</h2><ul><li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/best_practices.html#using-logback-instead-of-log4j" target="_blank" rel="noopener">Using Logback instead of Log4j</a></li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>client端使用logback的话，要在pom文件添加logback-core、logback-classic及log4j-over-slf4j依赖，之后对flink-java、flink-streaming-java_2.11、flink-clients_2.11等配置log4j及slf4j-log4j12的exclusions；最后通过mvn dependency:tree查看是否还有log4j12，以确认下是否都全部排除了</li><li>服务端使用logback的话，要在添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下(<code>比如/opt/flink/lib</code>)；移除flink的lib目录下(<code>比如/opt/flink/lib</code>)log4j及slf4j-log4j12的jar(<code>比如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar</code>)；如果要自定义logback的配置的话，可以覆盖flink的conf目录下的logback.xml、logback-console.xml或者logback-yarn.xml</li><li>使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</li></ul><h1 id="Logback配置文件详解"><a href="#Logback配置文件详解" class="headerlink" title="Logback配置文件详解"></a>Logback配置文件详解</h1><p>Logback，Java 日志框架。</p><p>Logback 如何加载配置的</p><ol><li>logback 首先会查找 logback.groovy 文件</li><li>当没有找到，继续试着查找 logback-test.xml 文件</li><li>当没有找到时，继续试着查找 logback.xml 文件</li><li>如果仍然没有找到，则使用默认配置（打印到控制台）</li></ol><h2 id="configuration"><a href="#configuration" class="headerlink" title="configuration"></a>configuration</h2><p>configuration 是配置文件的根节点，他包含的属性：</p><ul><li>scan<br>　　当此属性设置为 true 时，配置文件如果发生改变，将会被重新加载，默认值为 true</li><li>scanPeriod<br>　　设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。但 scan 为 true 时，此属性生效，默认的时间间隔为 1 分钟</li><li>debug<br>　　当此属性设置为 true 时，将打印出 logback 内部日志信息，实时查看 logback 运行状态，默认值为 false。</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="configuration-的子节点"><a href="#configuration-的子节点" class="headerlink" title="configuration 的子节点"></a>configuration 的子节点</h2><h4 id="设置上下文名称：contextName"><a href="#设置上下文名称：contextName" class="headerlink" title="设置上下文名称：contextName"></a>设置上下文名称：contextName</h4><p>每个 logger 度关联到 logger 上下文，默认上下文名称为 “default”。可以通过设置 contextName 修改上下文名称，用于区分不同应该程序的记录</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span><br><span class="line">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>myAppName<span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span><br><span class="line">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="设置变量：property"><a href="#设置变量：property" class="headerlink" title="设置变量：property"></a>设置变量：property</h4><p>用于定义键值对的变量， property 有两个属性 name 和 value，name 是键，value 是值，通过 property 定义的键值对会保存到logger 上下文的 map 集合内。定义变量后，可以使用 “${}” 来使用变量</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"APP_Name"</span> <span class="attr">value</span>=<span class="string">"myAppName"</span> /&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;APP_Name&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h4 id="获取时间戳字符串：timestamp"><a href="#获取时间戳字符串：timestamp" class="headerlink" title="获取时间戳字符串：timestamp"></a>获取时间戳字符串：timestamp</h4><p>timestamp 有两个属性，key：标识此 timestamp 的名字；datePattern：时间输出格式，遵循SimpleDateFormat 的格式</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">timestamp</span> <span class="attr">key</span>=<span class="string">"bySecond"</span> <span class="attr">datePattern</span>=<span class="string">"yyyyMMdd'T'HHmmss"</span>/&gt;</span>   </span></span><br><span class="line"><span class="xml">      <span class="tag">&lt;<span class="name">contextName</span>&gt;</span>$</span><span class="template-variable">&#123;bySecond&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">contextName</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">      <span class="comment">&lt;!-- 其他配置省略--&gt;</span>  </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="logger"><a href="#logger" class="headerlink" title="logger"></a>logger</h2><p>logger 有两种级别，一种是 root，一种是普通的 logger，logger 是用来设置某一个包或者具体的某一个类的日志打印机级别，以及制定的 appender。<br>logger 有三个属性</p><ul><li>name：用来指定此 logger 约束的某一个包或者具体的某一个类</li><li>level：用来设置打印机别，</li><li>addtivity：是否向上级 logger 传递打印信息。默认是 true</li></ul><p>每个 logger 都有对应的父级关系，它通过包名来决定父级关系，root 是最高级的父元素。<br>下面定义了四个 logger，他们的父子关系从小到大为：<br>com.lwc.qg.test.logbackDemo → com.lwc.qg.tes → com.lwc.qg → root</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 根 logger --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    普通的 logger</span></span><br><span class="line"><span class="comment">    name：类名或包名，标志该 logger 与哪个包或哪个类绑定</span></span><br><span class="line"><span class="comment">    level：该 logger 的日志级别</span></span><br><span class="line"><span class="comment">    additivity：是否将日志信息传递给上一级</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test.logbackDemo"</span> <span class="attr">level</span>=<span class="string">"debug"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg.test"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.lwc.qg"</span> <span class="attr">level</span>=<span class="string">"info"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br></pre></td></tr></table></figure><p>　　从该种级别来看，如果此时在最低层的 logger 输出日志信息，以该配置作为基础，它将会向父级的所有 logger 依次传递，所以按理来说一个打印信息将会打印四次</p><p>　　从控制台上看，的确每条日志信息都被打印出了四次，但是细心从配置文件上来看，root 的日志级别配置的为 info，但是却输出<br>debug 级别的日志信息，所以从测试结果可以看出，向上传递的日志信息的日志级别将由最底层的子元素决定（最初传递信息的<br>logger），因为子元素设置的日志级别为 debug，所以也输出了 debug 级别的信息。<br>　　因此，从理论上来说，如果子元素日志级别设置高一点，那么也将会只输出高级别的日志信息。实际上也是如此，如果我们把 com.lwc.qg.test.logbackDemo 对应的 logger 日志级别设为 warn，那么将只会输出 warn 及其以上的信息</p><h2 id="root"><a href="#root" class="headerlink" title="root"></a>root</h2><p>root 也是 logger 元素，但它是根 logger。只有一个 level 属性</p><h2 id="appender"><a href="#appender" class="headerlink" title="appender"></a>appender</h2><p>appender 是负责写日志的组件，常用的组件有：</p><ul><li>ConsoleAppender</li><li>FileAppender</li><li>RollingFileAppender</li></ul><h2 id="ConsoleAppender"><a href="#ConsoleAppender" class="headerlink" title="ConsoleAppender"></a>ConsoleAppender</h2><p>控制台日志组件，该组件将日志信息输出到控制台,该组件有以下节点</p><ul><li>encoder：对日志进行格式化</li><li>target：System.out 或者 System.err，默认是 System.out</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="FileAppender"><a href="#FileAppender" class="headerlink" title="FileAppender"></a>FileAppender</h2><p>文件日志组件，该组件将日志信息输出到日志文件中，该组件有以下节点</p><ul><li>file：被写入的文件名，可以是相对路径，也可以是绝对路径。如果上级目录不存在会自动创建，没有默认值</li><li>append：如果是 true，日志被追加到文件结尾；如果是 false，清空现存文件，默认是 true。</li><li>encoder：格式化</li><li>prudent：如果是 true，日志会被安全的写入文件，即使其他的 FileAppender 也在向此文件做写入操作，效率低，默认是 false。</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.FileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">file</span>&gt;</span>testFile.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">append</span>&gt;</span>true<span class="tag">&lt;/<span class="name">append</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">prudent</span>&gt;</span>true<span class="tag">&lt;/<span class="name">prudent</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="RollingFileAppender"><a href="#RollingFileAppender" class="headerlink" title="RollingFileAppender"></a>RollingFileAppender</h2><p>滚动记录文件日志组件，先将日志记录记录到指定文件，当符合某个条件时，将日志记录到其他文件，该组件有以下节点</p><ul><li>file：文件名</li><li>encoder：格式化</li><li>rollingPolicy：当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名</li><li>triggeringPolicy：告知 RollingFileAppender 合适激活滚动</li><li>prudent：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。</li></ul><p>#### </p><h3 id="rollingPolicy"><a href="#rollingPolicy" class="headerlink" title="rollingPolicy"></a>rollingPolicy</h3><p>滚动策略</p><ol><li>TimeBasedRollingPolicy：最常用的滚动策略，它根据时间来制定滚动策略，即负责滚动也负责触发滚动，包含节点：<ul><li>fileNamePattern：文件名模式</li><li>maxHistoury：控制文件的最大数量，超过数量则删除旧文件</li></ul></li><li>FixedWindowRollingPolicy：根据固定窗口算法重命名文件的滚动策略，包含节点<ul><li>minInedx：窗口索引最小值</li><li>maxIndex：串口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12</li><li>fileNamePattern：文件名模式，必须包含%i，命名模式为 log%i.log，会产生 log1.log，log2.log 这样的文件</li></ul></li><li>triggeringPolicy：根据文件大小的滚动策略，包含节点<ul><li>maxFileSize：日志文件最大大小</li></ul></li></ol><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>logFile.%d</span><span class="template-variable">&#123;yyyy-MM-dd&#125;</span><span class="xml">.log<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">maxHistory</span>&gt;</span>30<span class="tag">&lt;/<span class="name">maxHistory</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><h2 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h2><h2 id="filter-过滤器"><a href="#filter-过滤器" class="headerlink" title="filter 过滤器"></a>filter 过滤器</h2><p>过滤器是用于日志组件中的，每经过一个过滤器都会返回一个确切的枚举值，分别是</p><ul><li>DENY：返回 DENY，日志将立即被抛弃不再经过其他过滤器</li><li>NEUTRAL：有序列表的下个过滤器接着处理日志</li><li>ACCEPT：日志会被立即处理，不再经过剩余过滤器</li></ul><h3 id="常用过滤器"><a href="#常用过滤器" class="headerlink" title="常用过滤器"></a>常用过滤器</h3><p>常用的过滤器有以下：</p><ul><li>LevelFilter<br>级别过滤器，根据日志级别进行过滤。如果日志级别等于配置级别，过滤器会根据 omMatch 和 omMismatch 接受或拒绝日志。他有以下节点<br>　　level：过滤级别<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作<br>例：该组件设置一个 INFO 级别的过滤器，那么所有非 INFO 级别的日志都会被过滤掉　　</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>ThresholdFilter<br>临界值过滤器，过滤掉低于指定临界值的日志。当日志级别等于或高于临界值时，过滤器会返回 NEUTRAL；当日志级别低于临界值时，日志会被拒绝<br>例：过滤掉所有低于 INFO 级别的日志</li></ul><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span> </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">encoder</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%-4relative [%thread] %-5level %logger</span><span class="template-variable">&#123;35&#125;</span><span class="xml"> - %msg %n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span>  </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span> </span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span> </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span></span><br></pre></td></tr></table></figure><ul><li>EvaluatorFilter<br>求值过滤器，评估、鉴别日志是否符合指定条件，包含节点：<br>　　evaluator：鉴别器，通过子标签 expression 配置求值条件<br>　　onMatch：配置符合过滤条件的操作<br>　　onMismatch：配置不符合过滤条件的操作</li></ul>]]></content>
      
      
      <categories>
          
          <category> 日志框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> Logback </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink On Yarn集群部署</title>
      <link href="/2018/11/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/11/15/Flink%20On%20Yan%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 关闭防火墙</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配好主机映射</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建flink用户</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置免密登录</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 准备好相关资源：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hadoop-2.8.5.tar.gz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> flink-1.7.1-bin-hadoop28-scala_2.11</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 节点配置如下：(建议每台NM节点预留2G内存给系统)</span></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center">hostname</th><th align="center">资源配置</th><th align="center">节点名称</th></tr></thead><tbody><tr><td align="center">flink01</td><td align="center">16G/16cores</td><td align="center">NameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink02</td><td align="center">16G/16cores</td><td align="center">ResourceManager/DataNode/NodeManager</td></tr><tr><td align="center">flink03</td><td align="center">16G/16cores</td><td align="center">SecondaryNameNode/DataNode/NodeManager</td></tr><tr><td align="center">flink04</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink05</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr><tr><td align="center">flink06</td><td align="center">16G/16cores</td><td align="center">DataNode/NodeManager</td></tr></tbody></table><h2 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h2><h3 id="将Hadoop安装包解压至flink01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至flink01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至flink01节点的/data/apps路径下"></a>将Hadoop安装包解压至flink01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/hadoop-2.8.5.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录"><a href="#进入配置目录" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/hadoop-2.8.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="修改hadoop-env-sh中的JAVA-HOME"><a href="#修改hadoop-env-sh中的JAVA-HOME" class="headerlink" title="修改hadoop-env.sh中的JAVA_HOME"></a>修改hadoop-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置yarn-env-sh中的JAVA-HOME"><a href="#配置yarn-env-sh中的JAVA-HOME" class="headerlink" title="配置yarn-env.sh中的JAVA_HOME"></a>配置yarn-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置mapred-env-sh中的JAVA-HOME"><a href="#配置mapred-env-sh中的JAVA-HOME" class="headerlink" title="配置mapred-env.sh中的JAVA_HOME"></a>配置mapred-env.sh中的JAVA_HOME</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.8.0_40</span><br></pre></td></tr></table></figure><h3 id="配置slaves"><a href="#配置slaves" class="headerlink" title="配置slaves"></a>配置slaves</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves   内容如下</span><br></pre></td></tr></table></figure><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fli<span class="symbol">nk01</span></span><br><span class="line">fli<span class="symbol">nk02</span></span><br><span class="line">fli<span class="symbol">nk03</span></span><br><span class="line">fli<span class="symbol">nk04</span></span><br><span class="line">fli<span class="symbol">nk05</span></span><br><span class="line">fli<span class="symbol">nk06</span></span><br></pre></td></tr></table></figure><h3 id="配置core-site-xml"><a href="#配置core-site-xml" class="headerlink" title="配置core-site.xml"></a>配置core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS的路径的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://flink01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 修改hadoop临时保存目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/apps/hadoop-2.8.5/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置hdfs-site-xml"><a href="#配置hdfs-site-xml" class="headerlink" title="配置hdfs-site.xml"></a>配置hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置HDFS 的复制因子 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭HDFS 权限检查，在hdfs-site.xml文件中增加如下配置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该属性定义了 HDFS WEB访问服务器的主机名和端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 定义secondarynamenode 外部地址 访问的主机和端口 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink03:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置mapred-site-xml"><a href="#配置mapred-site-xml" class="headerlink" title="配置mapred-site.xml"></a>配置mapred-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置Mapreduce 框架运行名称yarn --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 单个Map task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 单个Reduce task 申请的内存大小 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- Uber模式是Hadoop2中针对小文件作业的一种优化，如果作业量足够小，可以把一个task，在一个JVM中运行完成.--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.ubertask.enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn中的服务类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A comma separated list of services where service name should only</span><br><span class="line">      contain a-zA-Z0-9_ and can not start with numbers<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 配置resourcemanager 的主机位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>flink02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- AM重启最大尝试次数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum number of application attempts. It's a global</span><br><span class="line">    setting for all application masters. Each application master can specify</span><br><span class="line">    its individual maximum number of application attempts via the API, but the</span><br><span class="line">    individual number cannot be more than the global upper bound. If it is,</span><br><span class="line">    the resourcemanager will override it. The default number is set to 2, to</span><br><span class="line">    allow at least one retry for AM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">&lt;!-- 开启物理内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether physical memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 关闭虚拟内存限制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">    containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when</span><br><span class="line">        setting memory limits for containers. Container allocations are</span><br><span class="line">        expressed in terms of physical memory, and virtual memory usage</span><br><span class="line">        is allowed to exceed this allocation by this ratio.</span><br><span class="line">        <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小内存 --&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in MBs. Memory requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>7168<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最小virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests lower than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 每个Container请求的最大virtual CPU cores --&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM,</span><br><span class="line">    in terms of virtual CPU cores. Requests higher than this will throw a</span><br><span class="line">    InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大物理内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">    hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">    when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>14336<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!-- 限制 NodeManager 能够使用的最大virtual CPU cores --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">    for containers. This is used by the RM scheduler when allocating</span><br><span class="line">    resources for containers. This is not used to limit the number of</span><br><span class="line">    CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">    yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">    automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">    In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>16<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启用日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether to enable log aggregation. Log aggregation collects</span><br><span class="line">      each container's logs and moves these logs onto a file-system, for e.g.</span><br><span class="line">      HDFS, after the application completes. Users can configure the</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir" and</span><br><span class="line">      "yarn.nodemanager.remote-app-log-dir-suffix" properties to determine</span><br><span class="line">      where these logs are moved to. Users can access the logs via the</span><br><span class="line">      Application Timeline Server.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 设置HDFS上日志的保存时间,默认设置为7天--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Time in seconds to retain user logs. Only applicable if</span><br><span class="line">    log aggregation is disabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>含义</th><th>值</th><th>备注</th></tr></thead><tbody><tr><td>yarn.nodemanager.aux-services</td><td>设置yarn中的服务类</td><td>mapreduce_shuffle</td><td></td></tr><tr><td>yarn.resourcemanager.hostname</td><td>配置resourcemanager 的主机位置</td><td>flink02</td><td></td></tr><tr><td>yarn.resourcemanager.am.max-attempts</td><td>AM重启最大尝试次数</td><td>4</td><td></td></tr><tr><td>yarn.nodemanager.pmem-check-enabled</td><td>开启物理内存限制</td><td>true</td><td>检测物理内存的使用是否超出分配值，若任务超出分配值，则将其杀掉，默认true。</td></tr><tr><td>yarn.nodemanager.vmem-check-enabled</td><td>关闭虚拟内存限制</td><td>false</td><td>检测虚拟内存的使用是否超出；若任务超出分配值，则将其杀掉，默认true。在确定内存不会泄漏的情况下可以设置此项为 False；</td></tr><tr><td>yarn.scheduler.minimum-allocation-mb</td><td>每个Container请求的最小内存</td><td>1024</td><td>单个容器/调度器可申请的最少物理内存量，默认是1024（MB）；一般每个contain都分配这个值；即：capacity memory:3072, vCores:1，如果提示物理内存溢出，提高这个值即可；</td></tr><tr><td>yarn.scheduler.maximum-allocation-mb</td><td>每个Container请求的最大内存</td><td>7168</td><td>单个容器/调度器可申请的最大物理内存量</td></tr><tr><td>yarn.scheduler.minimum-allocation-vcores</td><td>每个Container请求的最小virtual CPU cores</td><td>1</td><td></td></tr><tr><td>yarn.scheduler.maximum-allocation-vcores</td><td>每个Container请求的最大virtual CPU cores</td><td>16</td><td></td></tr><tr><td>yarn.nodemanager.resource.memory-mb</td><td>限制 NodeManager 能够使用的最大物理内存</td><td>14336</td><td>该节点上YARN可使用的物理内存总量，【向操作系统申请的总量】默认是8192（MB）</td></tr><tr><td>yarn.nodemanager.resource.cpu-vcores</td><td>限制 NodeManager 能够使用的最大virtual CPU cores</td><td>16</td><td>该节点上YARN可使用的总核心数；一般设为cat /proc/cpuinfo| grep “processor”| wc -l 的值。默认是8个</td></tr><tr><td>yarn.log-aggregation-enable</td><td>启用日志聚集功能</td><td>true</td><td></td></tr><tr><td>yarn.nodemanager.log.retain-seconds</td><td>设置HDFS上日志的保存时间,默认设置为7天</td><td>10800</td><td></td></tr><tr><td>yarn.nodemanager.vmem-pmem-ratio</td><td>虚拟内存率</td><td>5</td><td>任务每使用1MB物理内存，最多可使用虚拟内存量比率，默认2.1；关闭虚拟内存限制的情况下，配置此项就无意义了</td></tr></tbody></table><h3 id="修改capacity-scheduler-xml"><a href="#修改capacity-scheduler-xml" class="headerlink" title="修改capacity-scheduler.xml"></a>修改capacity-scheduler.xml</h3><p><strong>（flink yarn session启用的jobmanager占用的资源总量受此参数限制）</strong></p><pre><code>&lt;property&gt;    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;    &lt;value&gt;0.3&lt;/value&gt;    &lt;description&gt;集群中可用于运行application master的资源比例上限.&lt;/description&gt;&lt;/property&gt;</code></pre><h3 id="快速安装Hadoop"><a href="#快速安装Hadoop" class="headerlink" title="快速安装Hadoop"></a>快速安装Hadoop</h3><p><strong>（使用此脚本安装完后需要单独修改capacity-scheduler.xml）</strong></p><p><strong>将安装脚本和安装包放在相同路径下并执行以下命令可快速完成上述配置步骤！</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认相关资源已放在当前用户的~路径下</span></span><br><span class="line">sh ~/install-hadoop.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line">加入以下内容（这里提前加上了flink的环境变量）：</span><br><span class="line">export FLINK_HOME = /data/apps/flink-1.7.1</span><br><span class="line">export HADOOP_HOME=/data/apps/hadoop-2.8.5</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$FLINK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="启动Hadoop"><a href="#启动Hadoop" class="headerlink" title="启动Hadoop"></a>启动Hadoop</h3><p>格式化NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>在NameNode所在节点启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>在ResourceManager所在节点启动YARN</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="Flink集群"><a href="#Flink集群" class="headerlink" title="Flink集群"></a>Flink集群</h2><h3 id="将Hadoop安装包解压至kafka01节点的-data-apps路径下"><a href="#将Hadoop安装包解压至kafka01节点的-data-apps路径下" class="headerlink" title="将Hadoop安装包解压至kafka01节点的/data/apps路径下"></a>将Hadoop安装包解压至kafka01节点的/data/apps路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf ~/flink-1.7.1-bin-hadoop28-scala_2.11.tar.gz -C /data/apps</span><br></pre></td></tr></table></figure><h3 id="进入配置目录-1"><a href="#进入配置目录-1" class="headerlink" title="进入配置目录"></a>进入配置目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /data/apps/flink-1.7.1/conf</span><br></pre></td></tr></table></figure><h3 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: flink01</span><br><span class="line">jobmanager.heap.size: 1024m</span><br><span class="line">taskmanager.heap.size: 1024m</span><br><span class="line">parallelism.default: 2</span><br><span class="line">taskmanager.numberOfTaskSlots: 8</span><br><span class="line">state.backend: rocksdb</span><br><span class="line">state.checkpoints.dir: hdfs://flink01:9000/flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs://flink01:9000/flink-savepoints</span><br><span class="line">state.backend.incremental: true</span><br><span class="line">io.tmp.dirs: /data/apps/flinkapp/tmp</span><br><span class="line">yarn.application-attempts: 4</span><br></pre></td></tr></table></figure><h3 id="删除Flink原先使用的日志框架log4j相关资源"><a href="#删除Flink原先使用的日志框架log4j相关资源" class="headerlink" title="删除Flink原先使用的日志框架log4j相关资源"></a>删除Flink原先使用的日志框架log4j相关资源</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的lib目录下log4j及slf4j-log4j12的jar(如log4j-1.2.17.jar及slf4j-log4j12-1.7.15.jar)；</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 移除flink的conf目录下log4j相关的配置文件（如log4j-cli.properties、log4j-console.properties、log4j.properties、log4j-yarn-session.properties）</span></span><br></pre></td></tr></table></figure><h3 id="更换Flink的日志框架为logback"><a href="#更换Flink的日志框架为logback" class="headerlink" title="更换Flink的日志框架为logback"></a>更换Flink的日志框架为logback</h3><p>（1）添加logback-classic.jar、logback-core.jar、log4j-over-slf4j.jar到flink的lib目录下</p><p>（2）自定义logback的配置，覆盖flink的conf目录下的logback.xml、logback-console.xml、logback-yarn.xml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-daemon.sh启动的flink使用的logback配置文件是logback.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用flink-console.sh启动的flink使用的logback配置文件是logback-console.xml；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用yarn-session.sh启动的flink使用的logback配置文件是logback-yarn.xml</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">scan</span>=<span class="string">"true"</span> <span class="attr">scanPeriod</span>=<span class="string">"60 seconds"</span> <span class="attr">debug</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储目录,勿使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"/data/apps/flinkapp/logs"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度 %msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"pattern"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyyMMdd:HH:mm:ss.SSS&#125; [%thread] %-5level  %msg%n"</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台输出 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">target</span>&gt;</span>System.out<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--  &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt;</span></span><br><span class="line"><span class="comment">            &lt;level&gt;INFO&lt;/level&gt;</span></span><br><span class="line"><span class="comment">        &lt;/filter&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- INFO_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"INFO_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出INFO--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/info_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- ERROR_FILE --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"ERROR_FILE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">file</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error.log<span class="tag">&lt;/<span class="name">file</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--只输出ERROR--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.TimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/error_%d&#123;yyyy-MM-dd&#125;.log.%i.gz<span class="tag">&lt;/<span class="name">fileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">timeBasedFileNamingAndTriggeringPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP"</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">maxFileSize</span>&gt;</span>50MB<span class="tag">&lt;/<span class="name">maxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">timeBasedFileNamingAndTriggeringPolicy</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;pattern&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">charset</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">charset</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"com.haier.flink"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"INFO_FILE"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"ERROR_FILE"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Connection"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.Statement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">logger</span> <span class="attr">name</span>=<span class="string">"java.sql.PreparedStatement"</span> <span class="attr">level</span>=<span class="string">"DEBUG"</span> <span class="attr">additivity</span>=<span class="string">"false"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">logger</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--根logger--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="Flink-on-Yarn的两种运行模式"><a href="#Flink-on-Yarn的两种运行模式" class="headerlink" title="Flink on Yarn的两种运行模式"></a>Flink on Yarn的两种运行模式</h2><h3 id="Start-a-long-running-Flink-cluster-on-YARN"><a href="#Start-a-long-running-Flink-cluster-on-YARN" class="headerlink" title="Start a long-running Flink cluster on YARN"></a>Start a long-running Flink cluster on YARN</h3><p>​    这种方式需要先启动集群，然后在提交Flink-Job（同一个Session中可以提交多个Flink-Job，可以在Flink的WebUI上submit，也可以使用Flink run命令提交）。启动集群时会向yarn申请一块空间，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成，释放了资源，那下一个作业才会正常提交.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 默认配置启动flink on yarn（默认启动资源如下）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> &#123;masterMemoryMB=1024, taskManagerMemoryMB=1024,numberTaskManagers=1, slotsPerTaskManager=1&#125;</span></span><br><span class="line">yarn-session.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 系统默认使用con/flink-conf.yaml里的配置，Flink on yarn将会覆盖掉几个参数：</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jobmanager.rpc.address因为jobmanager的在集群的运行位置并不是事先确定的，其实就是AM的地址；</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> taskmanager.tmp.dirs使用yarn给定的临时目录;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism.default也会被覆盖掉，如果在命令行里指定了slot数。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############# 自定义配置可选参数如下 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Required     </span></span><br><span class="line"> -n,--container &lt;arg&gt;   Number of YARN container to allocate (=Number of Task Managers)   </span><br><span class="line"><span class="meta">#</span><span class="bash"> Optional     </span></span><br><span class="line"> -D &lt;arg&gt;                        Dynamic properties     </span><br><span class="line"> -d,--detached                   Start detached     </span><br><span class="line"> -jm,--jobManagerMemory &lt;arg&gt;    Memory for JobManager Container with optional unit (default: MB)     </span><br><span class="line"> -nm,--name                      Set a custom name for the application on YARN     </span><br><span class="line"> -q,--query                      Display available YARN resources (memory, cores)     </span><br><span class="line"> -qu,--queue &lt;arg&gt;               Specify YARN queue.     </span><br><span class="line"> -s,--slots &lt;arg&gt;                Number of slots per TaskManager     </span><br><span class="line"> -tm,--taskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with optional unit (default: MB)     </span><br><span class="line"> -z,--zookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper sub-paths for HA mode</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例：启动15个TaskManager，1个JobManager，JobManager内存1024M，每个TaskManager内存1024M且含有8个slot，自定义该应用的名称为FlinkOnYarnSession，-d以分离式模式执行（不指定-d则以客户端模式执行）</span></span><br><span class="line">yarn-session.sh -n 15 -jm 1024 -tm 1024 -s 8 -nm FlinkOnYarnSession -d</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 客户端模式指的是在终端启动一个客户端，这种方式是不能断开终端的，断开即相当于<span class="built_in">kill</span>掉Flink集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分离式模式指的是启动Flink on Yarn后，Flink YARN客户端将仅向Yarn提交Flink，然后自行关闭。，要<span class="built_in">kill</span>掉Flink集群需要使用如下命令：</span></span><br><span class="line">yarn application -kill &lt;appId&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> &lt;appId&gt;指的是发布在Yarn上的作业ID，在Yarn集群上可以查到对应的ID</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于Flink On Yarn来说，一个JobManager占用一个Container，一个TaskManager占用一个Container</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> JobManager的数量+TaskManager的数量 = 申请的Container的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下以6台16核，16G内存的机器举例说明（每台节点预留2G内存给系统）</span></span><br><span class="line">yarn.nodemanager.resource.cpu-vcores=16 每台NodeManager节点为YARN集群分配的cpu为16核</span><br><span class="line">yarn.nodemanager.resource.memory-mb=14336 每台NodeManager节点为YARN集群分配的物理内存为14G</span><br><span class="line">yarn.scheduler.minimum-allocation-vcores=1 每台NodeManager节点上每个Contaniner最小使用1核cpu</span><br><span class="line">yarn.scheduler.minimum-allocation-mb=1024 每台NodeManager节点上每个Contaniner最小使用1G的物理内存</span><br><span class="line"><span class="meta">#</span><span class="bash"> 若所有节点全部用于Flink作业,推荐提供的Flink集群：</span></span><br><span class="line">（总的资源为14*6=84G内存，16*6=96核）</span><br><span class="line">yarn-session.sh -n 8 -jm 4096 -tm 3584 -s 16 -nm FlinkOnYarnSession -d</span><br><span class="line">一共占用32G内存，9cores，申请了1个4G/1cores的JobManager和8个3.5G/1cores/16slots的TaskManager</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############## Recovery behavior of Flink on YARN</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Flink’s YARN client has the following configuration parameters to control how to behave <span class="keyword">in</span> <span class="keyword">case</span> of container failures. These parameters can be <span class="built_in">set</span> either from the conf/flink-conf.yaml or when starting the YARN session, using -D parameters</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.reallocate-failed : 控制 Flink是否应该重新分配失败的TaskManager容器，默认<span class="literal">true</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.maximum-failed-containers : ApplicationMaster接收container失败的最大次数，默认是TaskManager的次数（-n的值）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> yarn.application-attempts : ApplicationMaster尝试次数。如果这个值为1（默认），那么当Application Master失败时，整个YARN session就会失败。更高的值是指ApplicationMaster重新启动的次数</span></span><br></pre></td></tr></table></figure><h3 id="Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）"><a href="#Run-a-Flink-job-on-YARN（Flink-per-job-cluster模式）" class="headerlink" title="Run a Flink job on YARN（Flink per-job cluster模式）"></a>Run a Flink job on YARN（Flink per-job cluster模式）</h3><p>这种方式不需要先启动集群，每提交一个Flink-Job都会在Yarn上启动一个Flink集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> TaskManager slots number配置</span></span><br><span class="line">这个参数是配置一个TaskManager有多少个并发的slot数。有两种配置方式：</span><br><span class="line">- taskmanager.numberOfTaskSlots. 在conf/flink-conf.yaml中更改，默认值为1，表示默认一个TaskManager只有1个task slot.</span><br><span class="line">- 提交作业时通过参数配置。--yarnslots 1，表示TaskManager的slot数为1.</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TaskManager的个数</span></span><br><span class="line">注意： Per job模式提交作业时并不像session模式能够指定拉起多少个TaskManager，TaskManager的数量是在提交作业时根据并发度动态计算。</span><br><span class="line">首先，根据设定的operator的最大并发度计算，例如，如果作业中operator的最大并发度为10，则 Parallelism/numberOfTaskSlots为向YARN申请的TaskManager数。</span><br></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#######################示例</span><br><span class="line"># flink run -m yarn-cluster 必须指定</span><br><span class="line"># -d 分离模式启动（不指定则以客户端模式启动）</span><br><span class="line"># 启动<span class="number">1</span>个JobManager，内存占用<span class="number">1024</span>M</span><br><span class="line"># 每台TaskManager指定<span class="number">4</span>个slot、内存占用<span class="number">1024</span>M</span><br><span class="line"># 假设abc.jar所有operator中最大并发度为<span class="number">8</span>，则会启动<span class="number">8</span>/<span class="number">4</span>=<span class="number">2</span>台TaskManager</span><br><span class="line">flink run -m yarn-cluster -d --yarnslots <span class="number">4</span> -yjm <span class="number">1024</span> -ytm <span class="number">1024</span> /data/abc.jar</span><br></pre></td></tr></table></figure><h2 id="Log-Files"><a href="#Log-Files" class="headerlink" title="Log Files"></a>Log Files</h2><p>In cases where the Flink YARN session fails during the deployment itself, users have to rely on the logging capabilities of Hadoop YARN. The most useful feature for that is the <a href="http://hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/" target="_blank" rel="noopener">YARN log aggregation</a>. To enable it, users have to set the <code>yarn.log-aggregation-enable</code>property to <code>true</code> in the <code>yarn-site.xml</code> file. Once that is enabled, users can use the following command to retrieve all log files of a (failed) YARN session.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn logs -applicationId &lt;application ID&gt;</span><br></pre></td></tr></table></figure><p>Note that it takes a few seconds after the session has finished until the logs show up.</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> Yarn </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink1.7.1与Kafka0.11.0.1</title>
      <link href="/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/"/>
      <url>/2018/11/15/Flink1.7.1%E4%B8%8EKafka0.11.0.1/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Flink的Checkpoint"><a href="#Flink的Checkpoint" class="headerlink" title="Flink的Checkpoint"></a>Flink的Checkpoint</h1><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><ul><li>使用StreamExecutionEnvironment.enableCheckpointing方法来设置开启checkpoint；具体可以使用enableCheckpointing(long interval)，或者enableCheckpointing(long interval, CheckpointingMode mode)；interval用于指定checkpoint的触发间隔(单位milliseconds)，而CheckpointingMode默认是CheckpointingMode.EXACTLY_ONCE，也可以指定为CheckpointingMode.AT_LEAST_ONCE</li><li>也可以通过StreamExecutionEnvironment.getCheckpointConfig().setCheckpointingMode来设置CheckpointingMode，一般对于超低延迟的应用(大概几毫秒)可以使用CheckpointingMode.AT_LEAST_ONCE，其他大部分应用使用CheckpointingMode.EXACTLY_ONCE就可以</li><li>checkpointTimeout用于指定checkpoint执行的超时时间(单位milliseconds)，超时没完成就会被abort掉</li><li>minPauseBetweenCheckpoints用于指定checkpoint coordinator上一个checkpoint完成之后最小等多久可以出发另一个checkpoint，当指定这个参数时，maxConcurrentCheckpoints的值为1</li><li>maxConcurrentCheckpoints用于指定运行中的checkpoint最多可以有多少个，用于包装topology不会花太多的时间在checkpoints上面；如果有设置了minPauseBetweenCheckpoints，则maxConcurrentCheckpoints这个参数就不起作用了(大于1的值不起作用)</li><li>enableExternalizedCheckpoints用于开启checkpoints的外部持久化，但是在job失败的时候不会自动清理，需要自己手工清理state；ExternalizedCheckpointCleanup用于指定当job canceled的时候externalized checkpoint该如何清理，DELETE_ON_CANCELLATION的话，在job canceled的时候会自动删除externalized state，但是如果是FAILED的状态则会保留；RETAIN_ON_CANCELLATION则在job canceled的时候会保留externalized checkpoint state</li><li>failOnCheckpointingErrors用于指定在checkpoint发生异常的时候，是否应该fail该task，默认为true，如果设置为false，则task会拒绝checkpoint然后继续运行</li></ul><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// start a checkpoint every 1000 msenv.enableCheckpointing(1000);// advanced options:// set mode to exactly-once (this is the default)env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// checkpoints have to complete within one minute, or are discardedenv.getCheckpointConfig().setCheckpointTimeout(60000);// make sure 500 ms of progress happen between checkpointsenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// allow only one checkpoint to be in progress at the same timeenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// enable externalized checkpoints which are retained after job cancellationenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// This determines if a task will be failed if an error occurs in the execution of the task’s checkpoint procedure.env.getCheckpointConfig().setFailOnCheckpointingErrors(true);</code></pre><h1 id="FlinkKafkaConsumer011"><a href="#FlinkKafkaConsumer011" class="headerlink" title="FlinkKafkaConsumer011"></a>FlinkKafkaConsumer011</h1><h2 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h2><ul><li>setStartFromGroupOffsets()【默认消费策略】<br>默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据</li><li>setStartFromEarliest() 从最早的数据开始进行消费，忽略存储的offset信息</li><li>setStartFromLatest() 从最新的数据进行消费，忽略存储的offset信息</li><li>setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)</li></ul><ul><li>当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。</li><li>为了能够使用支持容错的kafka Consumer，需要开启checkpoint env.enableCheckpointing(5000); // 每5s checkpoint一次</li><li>Kafka Consumers Offset 自动提交有以下两种方法来设置，可以根据job是否开启checkpoint来区分:<br>(1) Checkpoint关闭时： 可以通过下面两个参数配置<br>enable.auto.commit<br>auto.commit.interval.ms<br>(2) Checkpoint开启时：当执行checkpoint的时候才会保存offset，这样保证了kafka的offset和checkpoint的状态偏移量保持一致。 可以通过这个参数设置<br>setCommitOffsetsOnCheckpoints(boolean)<br>这个参数默认就是true。表示在checkpoint的时候提交offset, 此时，kafka中的自动提交机制就会被忽略</li></ul><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h2><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;    &lt;version&gt;1.7.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;    &lt;version&gt;0.11.0.1&lt;/version&gt;&lt;/dependency&gt; public class StreamingKafkaSource {    public static void main(String[] args) throws Exception {        //获取Flink的运行环境        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        //checkpoint配置        env.enableCheckpointing(5000);        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);        env.getCheckpointConfig().setCheckpointTimeout(60000);        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);        //设置statebackend        //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://hadoop100:9000/flink/checkpoints&quot;,true));        String topic = &quot;kafkaConsumer&quot;;        Properties prop = new Properties();        prop.setProperty(&quot;bootstrap.servers&quot;,&quot;SparkMaster:9092&quot;);        prop.setProperty(&quot;group.id&quot;,&quot;kafkaConsumerGroup&quot;);        FlinkKafkaConsumer011&lt;String&gt; myConsumer = new FlinkKafkaConsumer011&lt;&gt;(topic, new SimpleStringSchema(), prop);        myConsumer.setStartFromGroupOffsets();//默认消费策略        DataStreamSource&lt;String&gt; text = env.addSource(myConsumer);        text.print().setParallelism(1);        env.execute(&quot;StreamingFromCollection&quot;);    }}</code></pre><h1 id="FlinkKafkaProducer011"><a href="#FlinkKafkaProducer011" class="headerlink" title="FlinkKafkaProducer011"></a>FlinkKafkaProducer011</h1><h2 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h2><ul><li>Kafka Producer的容错-Kafka 0.9 and 0.10</li><li>如果Flink开启了checkpoint，针对FlinkKafkaProducer09和FlinkKafkaProducer010 可以提供 at-least-once的语义，还需要配置下面两个参数:<br>setLogFailuresOnly(false)<br>setFlushOnCheckpoint(true)</li><li>注意：建议修改kafka 生产者的重试次数retries【这个参数的值默认是0】</li><li>Kafka Producer的容错-Kafka 0.11，如果Flink开启了checkpoint，针对FlinkKafkaProducer011 就可以提供 exactly-once的语义,但是需要选择具体的语义<br>Semantic.NONE<br>Semantic.AT_LEAST_ONCE【默认】<br>Semantic.EXACTLY_ONCE</li></ul><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h2><pre><code>public class StreamingKafkaSink {    public static void main(String[] args) throws Exception {    //获取Flink的运行环境    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();    //checkpoint配置    env.enableCheckpointing(5000);    env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);    env.getCheckpointConfig().setCheckpointTimeout(60000);    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);    env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);    //设置statebackend    //env.setStateBackend(new RocksDBStateBackend(&quot;hdfs://SparkMaster:9000/flink/checkpoints&quot;,true));    DataStreamSource&lt;String&gt; text = env.socketTextStream(&quot;SparkMaster&quot;, 9001, &quot;\n&quot;);    String brokerList = &quot;SparkMaster:9092&quot;;    String topic = &quot;kafkaProducer&quot;;    Properties prop = new Properties();    prop.setProperty(&quot;bootstrap.servers&quot;,brokerList);    //第一种解决方案，设置FlinkKafkaProducer011里面的事务超时时间    //设置事务超时时间    //prop.setProperty(&quot;transaction.timeout.ms&quot;,60000*15+&quot;&quot;);    //第二种解决方案，设置kafka的最大事务超时时间,主要是kafka的配置文件设置。    //FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(brokerList, topic, new SimpleStringSchema());    //使用EXACTLY_ONCE语义的kafkaProducer    FlinkKafkaProducer011&lt;String&gt; myProducer = new FlinkKafkaProducer011&lt;&gt;(topic, new KeyedSerializationSchemaWrapper&lt;String&gt;(new SimpleStringSchema()), prop, FlinkKafkaProducer011.Semantic.EXACTLY_ONCE);    text.addSink(myProducer);    env.execute(&quot;StreamingFromCollection&quot;);  }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka EOS 之事务性实现</title>
      <link href="/2018/09/26/Kafka%20EOS%20%E4%B9%8B%E4%BA%8B%E5%8A%A1%E6%80%A7%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/09/26/Kafka%20EOS%20%E4%B9%8B%E4%BA%8B%E5%8A%A1%E6%80%A7%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>原作者：王蒙</p><p><a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">原文链接</a></p><p>这篇文章是 Kafka Exactly-Once 实现系列的第二篇，主要讲述 Kafka 事务性的实现，这部分的实现要比幂等性的实现复杂一些，幂等性实现是事务性实现的基础，幂等性提供了单会话单 Partition Exactly-Once 语义的实现，正是因为 Idempotent Producer 不提供跨多个 Partition 和跨会话场景下的保证，因此，我们是需要一种更强的事务保证，能够原子处理多个 Partition 的写入操作，数据要么全部写入成功，要么全部失败，不期望出现中间状态。这就是 Kafka Transactions 希望解决的问题，简单来说就是能够实现 <code>atomic writes across partitions</code>，本文以 Apache Kafka 2.0.0 代码实现为例，深入分析一下 Kafka 是如何实现这一机制的。</p><p>Apache Kafka 在 Exactly-Once Semantics（EOS）上三种粒度的保证如下（来自 <a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="noopener">Exactly-once Semantics in Apache Kafka</a>）：</p><ol><li>Idempotent Producer：Exactly-once，in-order，delivery per partition；</li><li>Transactions：Atomic writes across partitions；</li><li>Exactly-Once stream processing across read-process-write tasks；</li></ol><p>第二种情况就是本文讲述的主要内容，在讲述整个事务处理流程时，也顺便分析第三种情况。</p><h2 id="Kafka-Transactions"><a href="#Kafka-Transactions" class="headerlink" title="Kafka Transactions"></a>Kafka Transactions</h2><p>Kafka 事务性最开始的出发点是为了在 Kafka Streams 中实现 Exactly-Once 语义的数据处理，这个问题提出之后，在真正的方案讨论阶段，社区又挖掘了更多的应用场景，也为了尽可能覆盖更多的应用场景，在真正的实现中，在很多地方做了相应的 tradeoffs，后面会写篇文章对比一下 RocketMQ 事务性的实现，就能明白 Kafka 事务性实现及应用场景的复杂性了。</p><p>Kafka 的事务处理，主要是允许应用可以把消费和生产的 batch 处理（涉及多个 Partition）在一个原子单元内完成，操作要么全部完成、要么全部失败。为了实现这种机制，我们需要应用能提供一个唯一 id，即使故障恢复后也不会改变，这个 id 就是 TransactionnalId（也叫 txn.id，后面会详细讲述），txn.id 可以跟内部的 PID 1:1 分配，它们不同的是 txn.id 是用户提供的，而 PID 是 Producer 内部自动生成的（并且故障恢复后这个 PID 会变化），有了 txn.id 这个机制，就可以实现多 partition、跨会话的 EOS 语义。</p><p>当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：</p><ol><li>跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；</li><li>跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；</li><li>跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。</li></ol><p>上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：</p><ol><li>对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；</li><li>一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；</li><li>Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；</li><li>Consumer 可能没有订阅这个事务涉及的全部 Partition。</li></ol><p>简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：</p><ol><li>Atomic writes across multiple partitions.</li><li>All messages in a transaction are made visible together, or none are.</li><li>Consumers must be configured to skip uncommitted messages.</li></ol><h2 id="事务性示例"><a href="#事务性示例" class="headerlink" title="事务性示例"></a>事务性示例</h2><p>Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 <code>transactional.id</code>，通过 <code>initTransactions()</code> 初始化事务状态信息，再通过 <code>beginTransaction()</code> 标识一个事务的开始，然后通过 <code>commitTransaction()</code> 或 <code>abortTransaction()</code> 对事务进行 commit 或 abort，示例如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"client.id"</span>, <span class="string">"ProducerTranscationnalExample"</span>);</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"test-transactional"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    String msg = <span class="string">"matt test"</span>;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"0"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"1"</span>, msg.toString()));</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"2"</span>, msg.toString()));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException e1) &#123;</span><br><span class="line">    e1.printStackTrace();</span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e2) &#123;</span><br><span class="line">    e2.printStackTrace();</span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure><p>事务性的 API 也同样保持了 Kafka 一直以来的简洁性，使用起来是非常方便的。</p><h2 id="事务性要解决的问题"><a href="#事务性要解决的问题" class="headerlink" title="事务性要解决的问题"></a>事务性要解决的问题</h2><p>回想一下，前面一篇文章中关于幂等性要解决的问题，事务性其实更多的是解决幂等性中没有解决的问题，比如：</p><ol><li>在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），这相当于出现了中间的状态，这并不是我们期望的结果；</li><li>Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；</li></ol><p>再来分析一下，Kafka 提供的事务性是如何解决上面两个问题的：</p><ol><li>如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；</li><li>其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，具体的实现后续会有文章详细解揭秘）。</li></ol><h2 id="事务性实现的关键"><a href="#事务性实现的关键" class="headerlink" title="事务性实现的关键"></a>事务性实现的关键</h2><p>对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。对于一个事务操作而言，其会涉及到多个 Topic-Partition 数据的写入，如果是一个 long transaction 操作，可能会涉及到非常多的数据，如何才能保证这个事务操作的原子性（要么全部完成，要么全部失败）呢？</p><ol><li>关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：<ul><li>我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；</li><li>Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；</li><li>这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 <strong>TransactionCoordinator</strong> 角色；</li></ul></li><li>有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？<ul><li>TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 <strong>事务状态信息如何恢复？</strong> 也就是事务的状态信息需要<strong>很强的容错性、一致性</strong>；</li><li>关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），那就是 <code>min.isr + ack</code> 机制；</li><li>分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 <code>__consumer_offset</code> 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，而对应事务数据（transaction log）就是 <code>__transaction_state</code> 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator 在做故障恢复也是从这个 topic 中恢复数据；</li></ul></li><li>有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：<ul><li>Kafka 在 Producer 端引入了一个 <strong>TransactionalId</strong> 来解决这个问题，这个 txn.id 是由应用来配置的；</li><li>TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方（同一个 TransactionCoordinator）进行处理；</li></ul></li><li>再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：<ul><li>这个就是<strong>事务状态转移</strong>，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态（前提是这个状态转移是有效的）。</li></ul></li></ol><blockquote><p>上面的分析都是个人见解，有问题欢迎指正~</p></blockquote><p>下面这节就讲述一下事务性实现的一些关键的实现机制（对这些细节不太感兴趣或者之前没有深入接触过 Kafka，可以直接跳过，直接去看下一节的事务流程处理，先去了解一下一个事务操作的主要流程步骤）。</p><h3 id="TransactionCoordinator"><a href="#TransactionCoordinator" class="headerlink" title="TransactionCoordinator"></a>TransactionCoordinator</h3><p>TransactionCoordinator 与 GroupCoordinator 有一些相似之处，它主要是处理来自 Transactional Producer 的一些与事务相关的请求，涉及的请求如下表所示（关于这些请求处理的详细过程会在下篇文章详细讲述，这里先有个大概的认识即可）：</p><table><thead><tr><th>请求类型</th><th>用途说明</th></tr></thead><tbody><tr><td>ApiKeys.FIND_COORDINATOR</td><td>Transaction Producer 会发送这个 FindCoordinatorRequest 请求，来查询当前事务（txn.id）对应的 TransactionCoordinator，这个与 GroupCoordinator 查询类似，是根据 txn.id 的 hash 值取模找到对应 Partition 的 leader，这个 leader 就是该事务对应的 TransactionCoordinator</td></tr><tr><td>ApiKeys.INIT_PRODUCER_ID</td><td>Producer 初始化时，会发送一个 InitProducerIdRequest 请求，来获取其分配的 PID 信息，对于幂等性的 Producer，会随机选择一台 broker 发送请求，而对于 Transaction Producer 会选择向其对应的 TransactionCoordinator 发送该请求（目的是为了根据 txn.id 对应的事务状态做一些判断）</td></tr><tr><td>ApiKeys.ADD_PARTITIONS_TO_TXN</td><td>将这个事务涉及到的 topic-partition 列表添加到事务的 meta 信息中（通过 AddPartitionsToTxnRequest 请求），事务 meta 信息需要知道当前的事务操作涉及到了哪些 Topic-Partition 的写入</td></tr><tr><td>ApiKeys.ADD_OFFSETS_TO_TXN</td><td>Transaction Producer 的这个 AddOffsetsToTxnRequest 请求是由 <code>sendOffsetsToTransaction()</code> 接口触发的，它主要是用在 consume-process-produce 的场景中，这时候 consumer 也是整个事务的一部分，只有这个事务 commit 时，offset 才会被真正 commit（主要还是用于 Failover）</td></tr><tr><td>ApiKeys.END_TXN</td><td>当提交事务时， Transaction Producer 会向 TransactionCoordinator 发送一个 EndTxnRequest 请求，来 commit 或者 abort 事务</td></tr></tbody></table><p>TransactionCoordinator 对象中还有两个关键的对象，分别是:</p><ol><li>TransactionStateManager：这个对象，从名字应该就能大概明白其作用是关于事务的状态管理，它会维护分配到这个 TransactionCoordinator 的所有事务的 meta 信息；</li><li>TransactionMarkerChannelManager：这个主要是用于向其他的 Broker 发送 Transaction Marker 数据，关于 Transaction Marker，第一次接触的人，可能会有一些困惑，什么是 Transaction Marker，Transaction Marker 是用来解决什么问题的呢？这里先留一个疑问，后面会来解密。</li></ol><p>总结一下，TransactionCoordinator 主要的功能有三个，分别是：</p><ol><li>处理事务相关的请求；</li><li>维护事务的状态信息；</li><li>向其他 Broker 发送 Transaction Marker 数据。</li></ol><h3 id="Transaction-Log（-transaction-state）"><a href="#Transaction-Log（-transaction-state）" class="headerlink" title="Transaction Log（__transaction_state）"></a>Transaction Log（__transaction_state）</h3><p>在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 topic <code>__transaction_state</code>，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 <code>__transaction_state</code> 的 partition 数取模得到，<code>__transaction_state</code> Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 <code>__transaction_state</code> Partition 2 的 leader 来处理。</p><p>对于 <code>__transaction_state</code> 这个 topic 默认是由 Server 端的 <code>transaction.state.log.replication.factor</code> 参数来配置，默认是3，如果当前 leader 故障，需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？</p><p>正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 <code>__transaction_state</code>中读取之前事务的日志信息，来恢复其状态信息，前提是要求事务日志写入做相应的不丢配置。这也是 <code>__transaction_state</code> 一个重要作用之一，用于 TransactionCoordinator 的恢复，<code>__transaction_state</code> 与 <code>__consumer_offsets</code> 一样是 compact 类型的 topic，其 scheme 如下：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Key</span> =&gt; <span class="type">Version</span> <span class="type">TransactionalId</span></span><br><span class="line">    <span class="type">Version</span> =&gt; <span class="number">0</span> (int16)</span><br><span class="line">    <span class="type">TransactionalId</span> =&gt; <span class="type">String</span></span><br><span class="line"></span><br><span class="line"><span class="type">Value</span> =&gt; <span class="type">Version</span> <span class="type">ProducerId</span> <span class="type">ProducerEpoch</span> <span class="type">TxnTimeoutDuration</span> <span class="type">TxnStatus</span> [<span class="type">TxnPartitions</span>] <span class="type">TxnEntryLastUpdateTime</span> <span class="type">TxnStartTime</span></span><br><span class="line">    <span class="type">Version</span> =&gt; <span class="number">0</span> (int16)</span><br><span class="line">    <span class="type">ProducerId</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">ProducerEpoch</span> =&gt; int16</span><br><span class="line">    <span class="type">TxnTimeoutDuration</span> =&gt; <span class="built_in">int32</span></span><br><span class="line">    <span class="type">TxnStatus</span> =&gt; int8</span><br><span class="line">    <span class="type">TxnPartitions</span> =&gt; [<span class="type">Topic</span> [<span class="type">Partition</span>]]</span><br><span class="line">        <span class="type">Topic</span> =&gt; <span class="type">String</span></span><br><span class="line">        <span class="type">Partition</span> =&gt; <span class="built_in">int32</span></span><br><span class="line">    <span class="type">TxnLastUpdateTime</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">TxnStartTime</span> =&gt; <span class="built_in">int64</span></span><br></pre></td></tr></table></figure><h3 id="Transaction-Marker"><a href="#Transaction-Marker" class="headerlink" title="Transaction Marker"></a>Transaction Marker</h3><p>终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？</p><p>Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）</p><ol><li><p>Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；</p></li><li><p>在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？</p><ul><li>这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；</li></ul><p>再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？</p><ol><li>首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，是可以共享这个 TCP 连接的；</li><li>有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 信息做相应的处理。</li></ol></li></ol><p>Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ControlMessageKey</span> =&gt; <span class="type">Version</span> <span class="type">ControlMessageType</span></span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">ControlMessageType</span> =&gt; int16</span><br><span class="line"></span><br><span class="line"><span class="type">TransactionControlMessageValue</span> =&gt; <span class="type">Version</span> <span class="type">CoordinatorEpoch</span></span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">CoordinatorEpoch</span> =&gt; <span class="built_in">int32</span></span><br></pre></td></tr></table></figure><p>这里再讲一个额外的内容，对于事务写入的数据，为了给消息添加一个标识（标识这条消息是不是来自事务写入的），<strong>数据格式（消息协议）发生了变化</strong>，这个改动主要是在 Attribute 字段，对于 MessageSet，Attribute 是16位，新的格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">| Unused (6-15) | Control (5) | Transactional (4) | Timestamp<span class="built_in"> Type </span>(3) | Compression<span class="built_in"> Type </span>(0-2) |</span><br></pre></td></tr></table></figure><p>对于 Message，也就是单条数据存储时（其中 Marker 数据都是单条存储的），在 Kafka 中，只有 MessageSet 才可以做压缩，所以 Message 就没必要设置压缩字段，其格式如下：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|<span class="string"> Unused (1-7) </span>|<span class="string"> Control Flag(0) </span>|</span><br></pre></td></tr></table></figure><h3 id="Server-端事务状态管理"><a href="#Server-端事务状态管理" class="headerlink" title="Server 端事务状态管理"></a>Server 端事务状态管理</h3><p>TransactionCoordinator 会维护相应的事务的状态信息（也就是 TxnStatus），对于一个事务，总共有以下几种状态：</p><table><thead><tr><th>状态</th><th>状态码</th><th>说明</th></tr></thead><tbody><tr><td>Empty</td><td>0</td><td>Transaction has not existed yet</td></tr><tr><td>Ongoing</td><td>1</td><td>Transaction has started and ongoing</td></tr><tr><td>PrepareCommit</td><td>2</td><td>Group is preparing to commit</td></tr><tr><td>PrepareAbort</td><td>3</td><td>Group is preparing to abort</td></tr><tr><td>CompleteCommit</td><td>4</td><td>Group has completed commit</td></tr><tr><td>CompleteAbort</td><td>5</td><td>Group has completed abort</td></tr><tr><td>Dead</td><td>6</td><td>TransactionalId has expired and is about to be removed from the transaction cache</td></tr><tr><td>PrepareEpochFence</td><td>7</td><td>We are in the middle of bumping the epoch and fencing out older producers</td></tr></tbody></table><p>其相应有效的状态转移图如下：</p><p><a href="http://matt33.com/images/kafka/server-txn.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/server-txn.png" alt="Server 端 Transaction 的状态转移图"></a>Server 端 Transaction 的状态转移图</p><p>正常情况下，对于一个事务而言，其状态状态流程应该是 Empty –&gt; Ongoing –&gt; PrepareCommit –&gt; CompleteCommit –&gt; Empty 或者是 Empty –&gt; Ongoing –&gt; PrepareAbort –&gt; CompleteAbort –&gt; Empty。</p><h3 id="Client-端事务状态管理"><a href="#Client-端事务状态管理" class="headerlink" title="Client 端事务状态管理"></a>Client 端事务状态管理</h3><p>Client 的事务状态信息主要记录本地事务的状态，当然跟其他的系统类似，本地的状态信息与 Server 端的状态信息并不完全一致（状态的设置，就像 GroupCoodinator 会维护一个 Group 的状态，每个 Consumer 也会维护本地的 Consumer 对象的状态一样）。Client 端的事务状态信息主要用于 Client 端的事务状态处理，其主要有以下几种：</p><ol><li>UNINITIALIZED：Transactional Producer 初始化时的状态，此时还没有事务处理；</li><li>INITIALIZING：Transactional Producer 调用 <code>initTransactions()</code> 方法初始化事务相关的内容，比如发送 InitProducerIdRequest 请求；</li><li>READY：对于新建的事务，Transactional Producer 收到来自 TransactionCoordinator 的 InitProducerIdResponse 后，其状态会置为 READY（对于已有的事务而言，是当前事务完成后 Client 的状态会转移为 READY）；</li><li>IN_TRANSACTION：Transactional Producer 调用 <code>beginTransaction()</code> 方法，开始一个事务，标志着一个事务开始初始化；</li><li>COMMITTING_TRANSACTION：Transactional Producer 调用 <code>commitTransaction()</code> 方法时，会先更新本地的状态信息；</li><li>ABORTING_TRANSACTION：Transactional Producer 调用 <code>abortTransaction()</code> 方法时，会先更新本地的状态信息；</li><li>ABORTABLE_ERROR：在一个事务操作中，如果有数据发送失败，本地状态会转移到这个状态，之后再自动 abort 事务；</li><li>FATAL_ERROR：转移到这个状态之后，再进行状态转移时，会抛出异常；</li></ol><p>Client 端状态如下图：</p><p><a href="http://matt33.com/images/kafka/client-txn.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/client-txn.png" alt="Client 端 Transaction 的状态转移图"></a>Client 端 Transaction 的状态转移图</p><h2 id="事务性的整体流程"><a href="#事务性的整体流程" class="headerlink" title="事务性的整体流程"></a>事务性的整体流程</h2><p>有了前面对 Kafka 事务性关键实现的讲述之后，这里详细讲述一个事务操作的处理流程，当然这里只是重点讲述事务性相关的内容，官方版的流程图可参考<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-DataFlow" target="_blank" rel="noopener">Kafka Exactly-Once Data Flow</a>，这里我做了一些改动，其流程图如下：</p><p><a href="http://matt33.com/images/kafka/txn-data-flow.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/txn-data-flow.png" alt="consume-process-produce 事务的处理流程"></a>consume-process-produce 事务的处理流程</p><p>这个流程是以 consume-process-produce 场景为例（主要是 kafka streams 的场景），图中红虚框及 4.3a 部分是关于 consumer 的操作，去掉这部分的话，就是只考虑写入情况的场景。这种只考虑写入场景的事务操作目前在业内应用也是非常广泛的，比如 Flink + Kafka 端到端的 Exactly-Once 实现就是这种场景，下面来详细讲述一下整个流程。</p><h3 id="1-Finding-a-TransactionCoordinator"><a href="#1-Finding-a-TransactionCoordinator" class="headerlink" title="1. Finding a TransactionCoordinator"></a>1. Finding a TransactionCoordinator</h3><p>对于事务性的处理，第一步首先需要做的就是找到这个事务 txn.id 对应的 TransactionCoordinator，Transaction Producer 会向 Broker （随机选择一台 broker，一般选择本地连接最少的这台 broker）发送 FindCoordinatorRequest 请求，获取其 TransactionCoordinator。</p><p>怎么找到对应的 TransactionCoordinator 呢？这个前面已经讲过了，主要是通过下面的方法获取 <code>__transaction_state</code> 的 Partition，该 Partition 对应的 leader 就是这个 txn.id 对应的 TransactionCoordinator。</p><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def partitionFor(transactionalId: <span class="built_in">String</span>): <span class="built_in">Int</span> = Utils.<span class="built_in">abs</span>(transactionalId.hashCode) % transactionTopicPartitionCount</span><br></pre></td></tr></table></figure><h3 id="2-Getting-a-PID"><a href="#2-Getting-a-PID" class="headerlink" title="2. Getting a PID"></a>2. Getting a PID</h3><p>PID 这里就不再介绍了，不了解的可以看前面那篇文章（<a href="http://matt33.com/2018/10/24/kafka-idempotent/#PID" target="_blank" rel="noopener">Producer ID</a>）。</p><p>Transaction Producer 在 <code>initializeTransactions()</code> 方法中会向 TransactionCoordinator 发送 InitPidRequest 请求获取其分配的 PID，有了 PID，事务写入时可以保证幂等性，PID 如何分配可以参考 <a href="http://matt33.com/2018/10/24/kafka-idempotent/#Producer-PID-%E7%94%B3%E8%AF%B7" target="_blank" rel="noopener">PID 分配</a>，但是 TransactionCoordinator 在给事务 Producer 分配 PID 会做一些判断，主要的内容是：</p><ol><li><p>如果这个 txn.id 之前没有相应的事务状态（new txn.id），那么会初始化其事务 meta 信息 TransactionMetadata（会给其分配一个 PID，初始的 epoch 为-1），如果有事务状态，获取之前的状态；</p></li><li><p>校验其 TransactionMetadata 的状态信息（参考下面代码中</p></li></ol>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">prepareInitProduceIdTransit</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法）：</p><ol><li>如果前面还有状态转移正在进行，直接返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果此时的状态为 PrepareAbort 或 PrepareCommit，返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果之前的状态为 CompleteAbort、CompleteCommit 或 Empty，那么先将状态转移为 Empty，然后更新一下 epoch 值；</li><li>如果之前的状态为 Ongoing，状态会转移成 PrepareEpochFence，然后再 abort 当前的事务，并向 client 返回 CONCURRENT_TRANSACTIONS 异常；</li><li>如果状态为 Dead 或 PrepareEpochFence，直接抛出相应的 FATAL 异常；</li></ol><ol start="3"><li>将 txn.id 与相应的 TransactionMetadata 持久化到事务日志中，对于 new txn.id，这个持久化的数据主要时 txn.id 与 pid 关系信息，如图中的 3a 所示。</li></ol><figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: producer 启用事务性的情况下，检测此时事务的状态信息</span></span><br><span class="line"><span class="keyword">private</span> def prepareInitProduceIdTransit(transactionalId: <span class="built_in">String</span>,</span><br><span class="line">                                        transactionTimeoutMs: Int,</span><br><span class="line">                                        coordinatorEpoch: Int,</span><br><span class="line">                                        txnMetadata: TransactionMetadata): ApiResult[(Int, TxnTransitMetadata)] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (txnMetadata.pendingTransitionInProgress) &#123;</span><br><span class="line">    <span class="comment">// return a retriable exception to let the client backoff and retry</span></span><br><span class="line">    Left(Errors.CONCURRENT_TRANSACTIONS)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// caller should have synchronized on txnMetadata already</span></span><br><span class="line">    txnMetadata.state match &#123;</span><br><span class="line">      <span class="keyword">case</span> PrepareAbort | <span class="function"><span class="params">PrepareCommit</span> =&gt;</span></span><br><span class="line">        <span class="comment">// reply to client and let it backoff and retry</span></span><br><span class="line">        Left(Errors.CONCURRENT_TRANSACTIONS)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> CompleteAbort | CompleteCommit | <span class="function"><span class="params">Empty</span> =&gt;</span> <span class="comment">//note: 此时需要将状态转移到 Empty（此时状态并没有转移，只是在 PendingState 记录了将要转移的状态）</span></span><br><span class="line">        val transitMetadata = <span class="keyword">if</span> (txnMetadata.isProducerEpochExhausted) &#123;</span><br><span class="line">          val newProducerId = producerIdManager.generateProducerId()</span><br><span class="line">          txnMetadata.prepareProducerIdRotation(newProducerId, transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: 增加 producer 的 epoch 值</span></span><br><span class="line">          txnMetadata.prepareIncrementProducerEpoch(transactionTimeoutMs, time.milliseconds())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Right(coordinatorEpoch, transitMetadata)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="function"><span class="params">Ongoing</span> =&gt;</span> <span class="comment">//note: abort 当前的事务，并返回一个 CONCURRENT_TRANSACTIONS 异常，强制 client 去重试</span></span><br><span class="line">        <span class="comment">// indicate to abort the current ongoing txn first. Note that this epoch is never returned to the</span></span><br><span class="line">        <span class="comment">// user. We will abort the ongoing transaction and return CONCURRENT_TRANSACTIONS to the client.</span></span><br><span class="line">        <span class="comment">// This forces the client to retry, which will ensure that the epoch is bumped a second time. In</span></span><br><span class="line">        <span class="comment">// particular, if fencing the current producer exhausts the available epochs for the current producerId,</span></span><br><span class="line">        <span class="comment">// then when the client retries, we will generate a new producerId.</span></span><br><span class="line">        Right(coordinatorEpoch, txnMetadata.prepareFenceProducerEpoch())</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> Dead | <span class="function"><span class="params">PrepareEpochFence</span> =&gt;</span> <span class="comment">//note: 返回错误</span></span><br><span class="line">        val errorMsg = s<span class="string">"Found transactionalId $transactionalId with state $&#123;txnMetadata.state&#125;. "</span> +</span><br><span class="line">          s<span class="string">"This is illegal as we should never have transitioned to this state."</span></span><br><span class="line">        fatal(errorMsg)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(errorMsg)</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-Starting-a-Transaction"><a href="#3-Starting-a-Transaction" class="headerlink" title="3. Starting a Transaction"></a>3. Starting a Transaction</h3><p>前面两步都是 Transaction Producer 调用 <code>initTransactions()</code> 部分，到这里，Producer 可以调用 <code>beginTransaction()</code> 开始一个事务操作，其实现方法如下面所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 应该在一个事务操作之前进行调用</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    transactionManager.beginTransaction();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TransactionManager</span></span><br><span class="line"><span class="comment">//note: 在一个事务开始之前进行调用，这里实际上只是转换了状态（只在 producer 本地记录了状态的开始）</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function"><span class="keyword">void</span> <span class="title">beginTransaction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.IN_TRANSACTION);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里只是将本地事务状态转移成 IN_TRANSACTION，并没有与 Server 端进行交互，所以在流程图中没有体现出来（TransactionManager 初始化时，其状态为 UNINITIALIZED，Producer 调用 <code>initializeTransactions()</code> 方法，其状态转移成 INITIALIZING）。</p><h3 id="4-Consume-Porcess-Produce-Loop"><a href="#4-Consume-Porcess-Produce-Loop" class="headerlink" title="4. Consume-Porcess-Produce Loop"></a>4. Consume-Porcess-Produce Loop</h3><p>在这个阶段，Transaction Producer 会做相应的处理，主要包括：从 consumer 拉取数据、对数据做相应的处理、通过 Producer 写入到下游系统中（对于只有写入场景，忽略前面那一步即可），下面有一个示例（start 和 end 中间的部分），是一个典型的 consume-process-produce 场景：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    ConsumerRecords records = consumer.poll(Long.MAX_VALUE);</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    //start</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord <span class="keyword">record</span> : <span class="type">records</span>)&#123;</span><br><span class="line">        producer.send(producerRecord(“outputTopic1”, <span class="keyword">record</span>));</span><br><span class="line">        producer.send(producerRecord(“outputTopic2”, <span class="keyword">record</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    producer.sendOffsetsToTransaction(currentOffsets(consumer), group);</span><br><span class="line">    //<span class="keyword">end</span></span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面来结合前面的流程图来讲述一下这部分的实现。</p><h4 id="4-1-AddPartitionsToTxnRequest"><a href="#4-1-AddPartitionsToTxnRequest" class="headerlink" title="4.1. AddPartitionsToTxnRequest"></a>4.1. AddPartitionsToTxnRequest</h4><p>Producer 在调用 <code>send()</code> 方法时，Producer 会将这个对应的 Topic—Partition 添加到 TransactionManager 的记录中，如下所示：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 如何开启了幂等性或事务性，需要做一些处理</span></span><br><span class="line"><span class="keyword">if</span> (transactionManager != <span class="built_in">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</span><br></pre></td></tr></table></figure><p>如果这个 Topic-Partition 之前不存在，那么就添加到 newPartitionsInTransaction 集合中，如下所示：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 将 tp 添加到 newPartitionsInTransaction 中，记录当前进行事务操作的 tp</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> maybeAddPartitionToTransaction(TopicPartition topicPartition) &#123;</span><br><span class="line">    failIfNotReadyForSend();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//note: 如果 partition 已经添加到 partitionsInTransaction、pendingPartitionsInTransaction、newPartitionsInTransaction中</span></span><br><span class="line">    <span class="keyword">if</span> (isPartitionAdded(topicPartition) || isPartitionPendingAdd(topicPartition))</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">log</span>.debug(<span class="string">"Begin adding new partition &#123;&#125; to transaction"</span>, topicPartition);</span><br><span class="line">    newPartitionsInTransaction.<span class="built_in">add</span>(topicPartition);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Producer 端的 Sender 线程会将这个信息通过 AddPartitionsToTxnRequest 请求发送给 TransactionCoordinator，也就是图中的 4.1 过程，TransactionCoordinator 会将这个 Topic-Partition 列表更新到 txn.id 对应的 TransactionMetadata 中，并且会持久化到事务日志中，也就是图中的 4.1 a 部分，这里持久化的数据主要是 txn.id 与其涉及到的 Topic-Partition 信息。</p><h4 id="4-2-ProduceRequest"><a href="#4-2-ProduceRequest" class="headerlink" title="4.2. ProduceRequest"></a>4.2. ProduceRequest</h4><p>这一步与正常 Producer 写入基本上一样，就是相应的 Leader 在持久化数据时会在头信息中标识这条数据是不是来自事务 Producer 的写入（主要是数据协议有变动，Server 处理并不需要做额外的处理）。</p><h4 id="4-3-AddOffsetsToTxnRequest"><a href="#4-3-AddOffsetsToTxnRequest" class="headerlink" title="4.3. AddOffsetsToTxnRequest"></a>4.3. AddOffsetsToTxnRequest</h4><p>Producer 在调用 <code>sendOffsetsToTransaction()</code> 方法时，第一步会首先向 TransactionCoordinator 发送相应的 AddOffsetsToTxnRequest 请求，如下所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProcducer</span></span><br><span class="line"><span class="comment">//note: 当你需要 batch 的消费-处理-写入消息，这个方法需要被使用</span></span><br><span class="line"><span class="comment">//note: 发送指定的 offset 给 group coordinator，用来标记这些 offset 是作为当前事务的一部分，只有这次事务成功时</span></span><br><span class="line"><span class="comment">//note: 这些 offset 才会被认为 commit 了</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="line">                                     String consumerGroupId) <span class="keyword">throws</span> ProducerFencedException &#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.sendOffsetsToTransaction(offsets, consumerGroupId);</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 发送 AddOffsetsToTxRequest</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> TransactionalRequestResult sendOffsetsToTransaction(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="line">                                                                        String consumerGroupId) &#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.IN_TRANSACTION)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Cannot send offsets to transaction either because the producer is not in an "</span> +</span><br><span class="line">                <span class="string">"active transaction"</span>);</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Begin adding offsets &#123;&#125; for consumer group &#123;&#125; to transaction"</span>, offsets, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnRequest.Builder builder = <span class="keyword">new</span> AddOffsetsToTxnRequest.Builder(transactionalId,</span><br><span class="line">            producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, consumerGroupId);</span><br><span class="line">    AddOffsetsToTxnHandler <span class="keyword">handler</span> = <span class="keyword">new</span> AddOffsetsToTxnHandler(builder, offsets);</span><br><span class="line">    enqueueRequest(<span class="keyword">handler</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">handler</span>.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransactionCoordinator 在收到这个请求时，处理方法与 4.1 中的一样，把这个 group.id 对应的 <code>__consumer_offsets</code> 的 Partition （与写入涉及的 Topic-Partition 一样）保存到事务对应的 meta 中，之后会持久化相应的事务日志，如图中 4.3a 所示。</p><h4 id="4-4-TxnOffsetsCommitRequest"><a href="#4-4-TxnOffsetsCommitRequest" class="headerlink" title="4.4. TxnOffsetsCommitRequest"></a>4.4. TxnOffsetsCommitRequest</h4><p>Producer 在收到 TransactionCoordinator 关于 AddOffsetsToTxnRequest 请求的结果后，后再次发送 TxnOffsetsCommitRequest 请求给对应的 GroupCoordinator，AddOffsetsToTxnHandler 的 <code>handleResponse()</code> 的实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(AbstractResponse response)</span> </span>&#123;</span><br><span class="line">    AddOffsetsToTxnResponse addOffsetsToTxnResponse = (AddOffsetsToTxnResponse) response;</span><br><span class="line">    Errors <span class="keyword">error</span> = addOffsetsToTxnResponse.<span class="keyword">error</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">error</span> == Errors.NONE) &#123;</span><br><span class="line">        log.debug(<span class="string">"Successfully added partition for consumer group &#123;&#125; to transaction"</span>, builder.consumerGroupId());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// note the result is not completed until the TxnOffsetCommit returns</span></span><br><span class="line">        <span class="comment">//note: AddOffsetsToTnxRequest 之后，还会再发送 TxnOffsetCommitRequest</span></span><br><span class="line">        pendingRequests.add(txnOffsetCommitHandler(result, offsets, builder.consumerGroupId()));</span><br><span class="line">        transactionStarted = <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.COORDINATOR_NOT_AVAILABLE || <span class="keyword">error</span> == Errors.NOT_COORDINATOR)</span> </span>&#123;</span><br><span class="line">        lookupCoordinator(FindCoordinatorRequest.CoordinatorType.TRANSACTION, transactionalId);</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.COORDINATOR_LOAD_IN_PROGRESS || <span class="keyword">error</span> == Errors.CONCURRENT_TRANSACTIONS)</span> </span>&#123;</span><br><span class="line">        reenqueue();</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.INVALID_PRODUCER_EPOCH)</span> </span>&#123;</span><br><span class="line">        fatalError(<span class="keyword">error</span>.exception());</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED)</span> </span>&#123;</span><br><span class="line">        fatalError(<span class="keyword">error</span>.exception());</span><br><span class="line">    &#125; <span class="function"><span class="keyword">else</span> <span class="title">if</span> <span class="params">(<span class="keyword">error</span> == Errors.GROUP_AUTHORIZATION_FAILED)</span> </span>&#123;</span><br><span class="line">        abortableError(<span class="keyword">new</span> GroupAuthorizationException(builder.consumerGroupId()));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fatalError(<span class="keyword">new</span> KafkaException(<span class="string">"Unexpected error in AddOffsetsToTxnResponse: "</span> + <span class="keyword">error</span>.message()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>GroupCoordinator 在收到相应的请求后，会将 offset 信息持久化到 consumer offsets log 中（包含对应的 PID 信息），但是<strong>不会更新到缓存</strong>中，除非这个事务 commit 了，这样的话就可以保证这个 offset 信息对 consumer 是不可见的（没有更新到缓存中的数据是不可见的，通过接口是获取的，这是 GroupCoordinator 本身来保证的）。</p><h3 id="5-Committing-or-Aborting-a-Transaction"><a href="#5-Committing-or-Aborting-a-Transaction" class="headerlink" title="5.Committing or Aborting a Transaction"></a>5.Committing or Aborting a Transaction</h3><p>在一个事务操作处理完成之后，Producer 需要调用 <code>commitTransaction()</code> 或者 <code>abortTransaction()</code> 方法来 commit 或者 abort 这个事务操作。</p><h4 id="5-1-EndTxnRequest"><a href="#5-1-EndTxnRequest" class="headerlink" title="5.1. EndTxnRequest"></a>5.1. EndTxnRequest</h4><p>无论是 Commit 还是 Abort，对于 Producer 而言，都是向 TransactionCoordinator 发送 EndTxnRequest 请求，这个请求的内容里会标识是 commit 操作还是 abort 操作，Producer 的 <code>commitTransaction()</code> 方法实现如下所示：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: commit 正在进行的事务操作，这个方法在真正发送 commit 之后将会 flush 所有未发送的数据</span></span><br><span class="line"><span class="comment">//note: 如果在发送中遇到任何一个不能修复的错误，这个方法抛出异常，事务也不会被提交，所有 send 必须成功，这个事务才能 commit 成功</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginCommit();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="comment">//note: 开始 commit，转移本地本地保存的状态以及发送相应的请求</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">TransactionalRequestResult <span class="title">beginCommit</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    maybeFailWithError();</span><br><span class="line">    transitionTo(State.COMMITTING_TRANSACTION);</span><br><span class="line">    <span class="function"><span class="keyword">return</span> <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult.COMMIT)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Producer 的 <code>abortTransaction()</code> 方法实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//class KafkaProducer</span></span><br><span class="line"><span class="comment">//note: 取消正在进行事务，任何没有 flush 的数据都会被丢弃</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException </span>&#123;</span><br><span class="line">    throwIfNoTransactionManager();</span><br><span class="line">    TransactionalRequestResult result = transactionManager.beginAbort();</span><br><span class="line">    sender.wakeup();</span><br><span class="line">    result.await();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class TransactionManager</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="function">TransactionalRequestResult <span class="title">beginAbort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ensureTransactional();</span><br><span class="line">    <span class="keyword">if</span> (currentState != State.ABORTABLE_ERROR)</span><br><span class="line">        maybeFailWithError();</span><br><span class="line">    transitionTo(State.ABORTING_TRANSACTION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We're aborting the transaction, so there should be no need to add new partitions</span></span><br><span class="line">    newPartitionsInTransaction.clear();</span><br><span class="line">    <span class="function"><span class="keyword">return</span> <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult.ABORT)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>它们最终都是调用了 TransactionManager 的 <code>beginCompletingTransaction()</code> 方法，这个方法会向其 待发送请求列表 中添加 EndTxnRequest 请求，其实现如下：</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 发送 EndTxnRequest 请求，添加到 pending 队列中</span></span><br><span class="line"><span class="keyword">private</span> <span class="function">TransactionalRequestResult <span class="title">beginCompletingTransaction</span><span class="params">(TransactionResult transactionResult)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!newPartitionsInTransaction.isEmpty())</span><br><span class="line">        enqueueRequest(addPartitionsToTransactionHandler());</span><br><span class="line">    EndTxnRequest.Builder builder = <span class="keyword">new</span> EndTxnRequest.Builder(transactionalId, producerIdAndEpoch.producerId,</span><br><span class="line">            producerIdAndEpoch.epoch, transactionResult);</span><br><span class="line">    EndTxnHandler <span class="keyword">handler</span> = <span class="keyword">new</span> EndTxnHandler(builder);</span><br><span class="line">    enqueueRequest(<span class="keyword">handler</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">handler</span>.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TransactionCoordinator 在收到 EndTxnRequest 请求后，会做以下处理：</p><ol><li>更新事务的 meta 信息，状态转移成 PREPARE_COMMIT 或 PREPARE_ABORT，并将事务状态信息持久化到事务日志中；</li><li>根据事务 meta 信息，向其涉及到的所有 Topic-Partition 的 leader 发送 Transaction Marker 信息（也就是 WriteTxnMarkerRquest 请求，见下面的 5.2 分析）；</li><li>最后将事务状态更新为 COMMIT 或者 ABORT，并将事务的 meta 持久化到事务日志中，也就是 5.3 步骤。</li></ol><h4 id="5-2-WriteTxnMarkerRquest"><a href="#5-2-WriteTxnMarkerRquest" class="headerlink" title="5.2. WriteTxnMarkerRquest"></a>5.2. WriteTxnMarkerRquest</h4><p>WriteTxnMarkerRquest 是 TransactionCoordinator 收到 Producer 的 EndTxnRequest 请求后向其他 Broker 发送的请求，主要是告诉它们事务已经完成。不论是普通的 Topic-Partition 还是 <code>__consumer_offsets</code>，在收到这个请求后，都会把事务结果（Transaction Marker 的格数据式见前面）持久化到对应的日志文件中，这样下游 Consumer 在消费这个数据时，就知道这个事务是 commit 还是 abort。</p><h4 id="5-3-Writing-the-Final-Commit-or-Abort-Message"><a href="#5-3-Writing-the-Final-Commit-or-Abort-Message" class="headerlink" title="5.3. Writing the Final Commit or Abort Message"></a>5.3. Writing the Final Commit or Abort Message</h4><p>当这个事务涉及到所有 Topic-Partition 都已经把这个 marker 信息持久化到日志文件之后，TransactionCoordinator 会将这个事务的状态置为 COMMIT 或 ABORT，并持久化到事务日志文件中，到这里，这个事务操作就算真正完成了，TransactionCoordinator 缓存的很多关于这个事务的数据可以被清除了。</p><h2 id="小思考"><a href="#小思考" class="headerlink" title="小思考"></a>小思考</h2><p>在上面讲述完 Kafka 事务性处理之后，我们来思考一下以下这些问题，上面的流程可能会出现下面这些问题或者很多人可能会有下面的疑问：</p><ol><li>txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？</li><li>TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？</li><li>对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？</li><li>对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？</li><li>如果 txn.id 长期不使用，server 端怎么处理？</li><li>PID Snapshot 是做什么的？是用来解决什么问题？</li></ol><p>下面，来详细分析一下上面提到的这些问题。</p><h3 id="如果多个-Producer-使用同一个-txn-id-会出现什么情况？"><a href="#如果多个-Producer-使用同一个-txn-id-会出现什么情况？" class="headerlink" title="如果多个 Producer 使用同一个 txn.id 会出现什么情况？"></a>如果多个 Producer 使用同一个 txn.id 会出现什么情况？</h3><p>对于这个情况，我们这里直接做了一个相应的实验，两个 Producer 示例都使用了同一个 txn.id（为 test-transactional-matt），Producer 1 先启动，然后过一会再启动 Producer 2，这时候会发现一个现象，那就是 Producer 1 进程会抛出异常退出进程，其异常信息为：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.KafkaException</span>: Cannot execute transactional method because we are <span class="keyword">in</span> an error state</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.internals</span><span class="selector-class">.TransactionManager</span><span class="selector-class">.maybeFailWithError</span>(TransactionManager<span class="selector-class">.java</span>:<span class="number">784</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.internals</span><span class="selector-class">.TransactionManager</span><span class="selector-class">.beginTransaction</span>(TransactionManager<span class="selector-class">.java</span>:<span class="number">215</span>)</span><br><span class="line">at org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.producer</span><span class="selector-class">.KafkaProducer</span><span class="selector-class">.beginTransaction</span>(KafkaProducer<span class="selector-class">.java</span>:<span class="number">606</span>)</span><br><span class="line">at com<span class="selector-class">.matt</span><span class="selector-class">.test</span><span class="selector-class">.kafka</span><span class="selector-class">.producer</span><span class="selector-class">.ProducerTransactionExample</span><span class="selector-class">.main</span>(ProducerTransactionExample<span class="selector-class">.java</span>:<span class="number">68</span>)</span><br><span class="line">Caused by: org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.errors</span><span class="selector-class">.ProducerFencedException</span>: Producer attempted an operation with an old epoch. Either there is <span class="selector-tag">a</span> newer producer with the same transactionalId, or the producer<span class="string">'s transaction has been expired by the broker.</span></span><br></pre></td></tr></table></figure><p>这里抛出了 ProducerFencedException 异常，如果打开相应的 Debug 日志，在 Producer 1 的日志文件会看到下面的日志信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-03 12:48:52,495] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Transition from state COMMITTING_TRANSACTION to error state FATAL_ERROR (org.apache.kafka.clients.producer.internals.TransactionManager)</span><br><span class="line">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation <span class="keyword">with</span> an <span class="keyword">old</span> epoch. Either there <span class="keyword">is</span> a newer producer <span class="keyword">with</span> the same transactionalId, <span class="keyword">or</span> the producer<span class="string">'s transaction has been expired by the broker.</span></span><br><span class="line"><span class="string">[2018-11-03 12:48:52,498] ERROR [Producer clientId=ProducerTransactionExample, transactionalId=test-transactional-matt] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)</span></span><br><span class="line"><span class="string">org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer'</span>s <span class="keyword">transaction</span> has been expired <span class="keyword">by</span> the broker.</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">599</span>] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Closing the Kafka producer <span class="keyword">with</span> timeoutMillis = <span class="number">9223372036854775807</span> ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">599</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] <span class="keyword">Beginning</span> <span class="keyword">shutdown</span> <span class="keyword">of</span> Kafka producer I/O <span class="keyword">thread</span>, sending remaining records. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">601</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> connections-closed: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">601</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> connections-created: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> successful-<span class="keyword">authentication</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">failed</span>-<span class="keyword">authentication</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">602</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-sent-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">603</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-sent: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">603</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">bytes</span>-received: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> <span class="keyword">select</span>-<span class="built_in">time</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> io-<span class="built_in">time</span>: (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">604</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.bytes-sent (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.bytes-received (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="comment">--1.latency (org.apache.kafka.common.metrics.Metrics)</span></span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">605</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span><span class="keyword">bytes</span>-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span><span class="keyword">bytes</span>-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-33.</span>latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span><span class="keyword">bytes</span>-sent (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span><span class="keyword">bytes</span>-received (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">606</span>] DEBUG Removed sensor <span class="keyword">with</span> <span class="keyword">name</span> node<span class="number">-35.</span>latency (org.apache.kafka.common.metrics.Metrics)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">607</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] <span class="keyword">Shutdown</span> <span class="keyword">of</span> Kafka producer I/O <span class="keyword">thread</span> has completed. (org.apache.kafka.clients.producer.internals.Sender)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">607</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] <span class="keyword">ERROR</span> Forcing producer <span class="keyword">close</span>! (com.matt.test.kafka.producer.ProducerTransactionExample)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] INFO [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Closing the Kafka producer <span class="keyword">with</span> timeoutMillis = <span class="number">9223372036854775807</span> ms. (org.apache.kafka.clients.producer.KafkaProducer)</span><br><span class="line">[<span class="number">2018</span><span class="number">-11</span><span class="number">-03</span> <span class="number">12</span>:<span class="number">48</span>:<span class="number">52</span>,<span class="number">808</span>] DEBUG [Producer clientId=ProducerTransactionExample, transactionalId=<span class="keyword">test</span>-<span class="keyword">transactional</span>-matt] Kafka producer has been closed (org.apache.kafka.clients.producer.KafkaProducer)</span><br></pre></td></tr></table></figure><p>Producer 1 本地事务状态从 COMMITTING_TRANSACTION 变成了 FATAL_ERROR 状态，导致 Producer 进程直接退出了，出现这个异常的原因，就是抛出的 ProducerFencedException 异常，简单来说 Producer 1 被 Fencing 了（这是 Producer Fencing 的情况）。因此，这个问题的答案就很清除了，如果多个 Producer 共用一个 txn.id，那么最后启动的 Producer 会成功运行，会它之前启动的 Producer 都 Fencing 掉（至于为什么会 Fencing 下一小节会做分析）。</p><h3 id="Fencing"><a href="#Fencing" class="headerlink" title="Fencing"></a>Fencing</h3><p>关于 Fencing 这个机制，在分布式系统还是很常见的，我第一个见到这个机制是在 HDFS 中，可以参考我之前总结的一篇文章 <a href="http://matt33.com/2018/07/15/hdfs-architecture-learn/#HDFS-%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">HDFS NN 脑裂问题</a>，Fencing 机制解决的主要也是这种类型的问题 —— 脑裂问题，简单来说就是，本来系统这个组件在某个时刻应该只有一个处于 active 状态的，但是在实际生产环境中，特别是切换期间，可能会同时出现两个组件处于 active 状态，这就是脑裂问题，在 Kafka 的事务场景下，用到 Fencing 机制有两个地方：</p><ol><li>TransactionCoordinator Fencing；</li><li>Producer Fencing；</li></ol><h4 id="TransactionCoordinator-Fencing"><a href="#TransactionCoordinator-Fencing" class="headerlink" title="TransactionCoordinator Fencing"></a>TransactionCoordinator Fencing</h4><p>TransactionCoordinator 在遇到上 long FGC 时，可能会导致 脑裂 问题，FGC 时会 stop-the-world，这时候可能会与 zk 连接超时导致临时节点消失进而触发 leader 选举，如果 <code>__transaction_state</code> 发生了 leader 选举，TransactionCoordinator 就会切换，如果此时旧的 TransactionCoordinator FGC 完成，在还没来得及同步到最细 meta 之前，会有一个短暂的时刻，对于一个 txn.id 而言就是这个时刻可能出现了两个 TransactionCoordinator。</p><p>相应的解决方案就是 TransactionCoordinator Fencing，这里 Fencing 策略不像离线场景 HDFS 这种直接 Kill 旧的 NN 进程或者强制切换状态这么暴力，而是通过 CoordinatorEpoch 来判断，每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 <code>__transaction_state</code>Partition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。</p><p>明白了 TransactionCoordinator 脑裂问题发生情况及解决方案之后，来分析下，Fencing 机制会在哪里发挥作用？仔细想想，是可以推断出来的，只可能是 TransactionCoordinator 向别人发请求时影响才会比较严重（特别是乱发 admin 命令）。有了 CoordinatorEpoch 之后，其他 Server 在收到请求时做相应的判断，如果发现 CoordinatorEpoch 值比缓存的最新的值小，那么 Fencing 就生效，拒绝这个请求，也就是 TransactionCoordinator 发送 WriteTxnMarkerRequest 时可能会触发这一机制。</p><h4 id="Producer-Fencing"><a href="#Producer-Fencing" class="headerlink" title="Producer Fencing"></a>Producer Fencing</h4><p>Producer Fencing 与前面的类似，如果对于相同 PID 和 txn.id 的 Producer，Server 端会记录最新的 Epoch 值，拒绝来自 zombie Producer （Epoch 值小的 Producer）的请求。前面第一个问题的情况，Producer 2 在启动时，会向 TransactionCoordinator 发送 InitPIDRequest 请求，此时 TransactionCoordinator 已经有了这个 txn.id 对应的 meta，会返回之前分配的 PID，并把 Epoch 自增 1 返回，这样 Producer 2 就被认为是最新的 Producer，而 Producer 1 就会被认为是 zombie Producer，因此，TransactionCoordinator 在处理 Producer 1 的事务请求时，会返回相应的异常信息。</p><h3 id="Consumer-端如何消费事务数据"><a href="#Consumer-端如何消费事务数据" class="headerlink" title="Consumer 端如何消费事务数据"></a>Consumer 端如何消费事务数据</h3><p>在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 <code>isolation.level</code> 配置，这个是配置对于事务性数据的消费策略，有以下两种可选配置：</p><ol><li><code>read_committed</code>: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.</li><li><code>read_uncommitted</code>: consume all available messages in offset ordering. This is the <strong>default value</strong>.</li></ol><p>简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），这部分的难点在于 read_committed 机制的实现。</p><h4 id="Last-Stable-Offset（LSO）"><a href="#Last-Stable-Offset（LSO）" class="headerlink" title="Last Stable Offset（LSO）"></a>Last Stable Offset（LSO）</h4><p>在事务机制的实现中，Kafka 又设置了一个新的 offset 概念，那就是 Last Stable Offset，简称 LSO（其他的 Offset 概念可参考 <a href="http://matt33.com/2017/01/16/kafka-group/#offset-%E9%82%A3%E4%BA%9B%E4%BA%8B" target="_blank" rel="noopener">Kafka Offset 那些事</a>），先看下 LSO 的定义：</p><blockquote><p>The LSO is defined as the latest offset such that the status of all transactional messages at lower offsets have been determined (i.e. committed or aborted).</p></blockquote><p>对于一个 Partition 而言，offset 小于 LSO 的数据，全都是已经确定的数据，这个主要是对于事务操作而言，在这个 offset 之前的事务操作都是已经完成的事务（已经 commit 或 abort），如果这个 Partition 没有涉及到事务数据，那么 LSO 就是其 HW（水位）。</p><h4 id="Server-处理-read-committed-类型的-Fetch-请求"><a href="#Server-处理-read-committed-类型的-Fetch-请求" class="headerlink" title="Server 处理 read_committed 类型的 Fetch 请求"></a>Server 处理 read_committed 类型的 Fetch 请求</h4><p>如果 Consumer 的消费策略设置的是 read_committed，其在向 Server 发送 Fetch 请求时，Server 端<strong>只会返回 LSO 之前的数据</strong>，在 LSO 之后的数据不会返回。</p><p>这种机制有没有什么问题呢？我现在能想到的就是如果有一个 long transaction，比如其 first offset 是 1000，另外有几个已经完成的小事务操作，比如：txn1（offset：1100<del>1200）、txn2（offset：1400</del>1500），假设此时的 LSO 是 1000，也就是说这个 long transaction 还没有完成，那么已经完成的 txn1、txn2 也会对 consumer 不可见（假设都是 commit 操作），此时<strong>受 long transaction 的影响可能会导致数据有延迟</strong>。</p><p>那么我们再来想一下，如果不设计 LSO，又会有什么问题呢？可能分两种情况：</p><ol><li>允许读未完成的事务：那么 Consumer 可以直接读取到 Partition 的 HW 位置，对于未完成的事务，因为设置的是 read_committed 机制，所以不能对用户可见，需要在 Consumer 端做缓存，这个缓存应该设置多大？（不限制肯定会出现 OOM 的情况，当然也可以现在 client 端持久化到硬盘，这样的设计太过于复杂，还需要考虑 client 端 IO、磁盘故障等风险），明显这种设计方案是不可行的；</li><li>如果不允许读未完成的事务：相当于还是在 Server 端处理，与前面的区别是，这里需要先把示例中的 txn1、txn2 的数据发送给 Consumer，这样的设计会带来什么问题呢？<ol><li>假设这个 long transaction commit 了，其 end offset 是 2000，这时候有两种方案：第一种是把 1000-2000 的数据全部读出来（可能是磁盘读），把这个 long transaction 的数据过滤出来返回给 Consumer；第二种是随机读，只读这个 long transaction 的数据，无论哪种都有多触发一次磁盘读的风险，可能影响影响 Server 端的性能；</li><li>Server 端需要维护每个 consumer group 有哪些事务读了、哪些事务没读的 meta 信息，因为 consumer 是随机可能挂掉，需要接上次消费的，这样实现就复杂很多了；</li><li>还有一个问题是，消费的顺序性无法保证，两次消费其读取到的数据顺序可能是不同的（两次消费启动时间不一样）；</li></ol></li></ol><p>从这些分析来看，个人认为 LSO 机制还是一种相当来说 实现起来比较简单、而且不影响原来 server 端性能、还能保证顺序性的一种设计方案，它不一定是最好的，但也不会差太多。在实际的生产场景中，尽量避免 long transaction 这种操作，而且 long transaction可能也会容易触发事务超时。</p><h4 id="Consumer-如何过滤-abort-的事务数据"><a href="#Consumer-如何过滤-abort-的事务数据" class="headerlink" title="Consumer 如何过滤 abort 的事务数据"></a>Consumer 如何过滤 abort 的事务数据</h4><p>Consumer 在拉取到相应的数据之后，后面该怎么处理呢？它拉取到的这批数据并不能保证都是完整的事务数据，很有可能是拉取到一个事务的部分数据（marker 数据还没有拉取到），这时候应该怎么办？难道 Consumer 先把这部分数据缓存下来，等后面的 marker 数据到来时再确认数据应该不应该丢弃？（还是又 OOM 的风险）有没有更好的实现方案？</p><p>Kafka 的设计总是不会让我们失望，这部分做的优化也是非常高明，Broker 会追踪每个 Partition 涉及到的 abort transactions，Partition 的每个 log segment 都会有一个单独只写的文件（append-only file）来存储 abort transaction 信息，因为 abort transaction 并不是很多，所以这个开销是可以可以接受的，之所以要持久化到磁盘，主要是为了故障后快速恢复，要不然 Broker 需要把这个 Partition 的所有数据都读一遍，才能直到哪些事务是 abort 的，这样的话，开销太大（如果这个 Partition 没有事务操作，就不会生成这个文件）。这个持久化的文件是以 <code>.txnindex</code> 做后缀，前面依然是这个 log segment 的 offset 信息，存储的数据格式如下：</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">TransactionEntry</span> =&gt;</span><br><span class="line">    <span class="type">Version</span> =&gt; int16</span><br><span class="line">    <span class="type">PID</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">FirstOffset</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">LastOffset</span> =&gt; <span class="built_in">int64</span></span><br><span class="line">    <span class="type">LastStableOffset</span> =&gt; <span class="built_in">int64</span></span><br></pre></td></tr></table></figure><p>有了这个设计，Consumer 在拉取数据时，Broker 会把这批数据涉及到的所有 abort transaction 信息都返回给 Consumer，Server 端会根据拉取的 offset 范围与 abort transaction 的 offset 做对比，返回涉及到的 abort transaction 集合，其实现如下：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> collectAbortedTxns(fetchOffset: <span class="keyword">Long</span>, upperBoundOffset: <span class="keyword">Long</span>): TxnIndexSearchResult = &#123;</span><br><span class="line">  val abortedTransactions = ListBuffer.empty[AbortedTxn]</span><br><span class="line">  <span class="keyword">for</span> ((abortedTxn, _) &lt;- iterator()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastOffset &gt;= fetchOffset &amp;&amp; abortedTxn.firstOffset &lt; upperBoundOffset)</span><br><span class="line">      abortedTransactions += abortedTxn <span class="comment">//note: 这个 abort 的事务有在在这个范围内，就返回</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (abortedTxn.lastStableOffset &gt;= upperBoundOffset)</span><br><span class="line">      <span class="keyword">return</span> TxnIndexSearchResult(abortedTransactions.<span class="keyword">toList</span>, isComplete = <span class="keyword">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  TxnIndexSearchResult(abortedTransactions.<span class="keyword">toList</span>, isComplete = <span class="keyword">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Consumer 在拿到这些数据之后，会进行相应的过滤，大概的判断逻辑如下（Server 端返回的 abort transaction 列表就保存在 <code>abortedTransactions</code> 集合中，<code>abortedProducerIds</code> 最开始时是为空的）：</p><ol><li>如果这个数据是 control msg（也即是 marker 数据），是 ABORT 的话，那么与这个事务相关的 PID 信息从 <code>abortedProducerIds</code> 集合删掉，是 COMMIT 的话，就忽略（每个这个 PID 对应的 marker 数据收到之后，就从 <code>abortedProducerIds</code> 中清除这个 PID 信息）；</li><li>如果这个数据是正常的数据，把它的 PID 和 offset 信息与 <code>abortedTransactions</code> 队列（有序队列，头部 transaction 的 first offset 最小）第一个 transaction 做比较，如果 PID 相同，并且 offset 大于等于这个 transaction 的 first offset，就将这个 PID 信息添加到 <code>abortedProducerIds</code> 集合中，同时从 <code>abortedTransactions</code> 队列中删除这个 transaction，最后再丢掉这个 batch（它是 abort transaction 的数据）；</li><li>检查这个 batch 的 PID 是否在 <code>abortedProducerIds</code> 集合中，在的话，就丢弃，不在的话就返回上层应用。</li></ol><p>这部分的实现确实有些绕（有兴趣的可以慢慢咀嚼一下），它严重依赖了 Kafka 提供的下面两种保证：</p><ol><li>Consumer 拉取到的数据，在处理时，其 offset 是严格有序的；</li><li>同一个 txn.id（PID 相同）在某一个时刻最多只能有一个事务正在进行；</li></ol><p>这部分代码实现如下：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">private Record <span class="built_in">nextFetchedRecord</span>() &#123;</span><br><span class="line">    <span class="keyword">while</span> (true) &#123;</span><br><span class="line">        <span class="keyword">if</span> (records == <span class="built_in">null</span> || !records.hasNext()) &#123; <span class="comment">//note: records 为空（数据全部丢掉了），records 没有数据（是 control msg）</span></span><br><span class="line">            <span class="built_in">maybeCloseRecordStream</span>();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!batches.hasNext()) &#123;</span><br><span class="line">                <span class="comment">// Message format v2 preserves the last offset in a batch even if the last record is removed</span></span><br><span class="line">                <span class="comment">// through compaction. By using the next offset computed from the last offset in the batch,</span></span><br><span class="line">                <span class="comment">// we ensure that the offset of the next fetch will point to the next batch, which avoids</span></span><br><span class="line">                <span class="comment">// unnecessary re-fetching of the same batch (in the worst case, the consumer could get stuck</span></span><br><span class="line">                <span class="comment">// fetching the same batch repeatedly).</span></span><br><span class="line">                <span class="keyword">if</span> (currentBatch != <span class="built_in">null</span>)</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                <span class="built_in">drain</span>();</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            currentBatch = batches.next();</span><br><span class="line">            <span class="built_in">maybeEnsureValid</span>(currentBatch);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isolationLevel == IsolationLevel.READ_COMMITTED &amp;&amp; currentBatch.hasProducerId()) &#123;</span><br><span class="line">                <span class="comment">//note: 需要做相应的判断</span></span><br><span class="line">                <span class="comment">// remove from the aborted transaction queue all aborted transactions which have begun</span></span><br><span class="line">                <span class="comment">// before the current batch's last offset and add the associated producerIds to the</span></span><br><span class="line">                <span class="comment">// aborted producer set</span></span><br><span class="line">                <span class="comment">//note: 如果这个 batch 的 offset 已经大于等于 abortedTransactions 中第一事务的 first offset</span></span><br><span class="line">                <span class="comment">//note: 那就证明下个 abort transaction 的数据已经开始到来，将 PID 添加到 abortedProducerIds 中</span></span><br><span class="line">                <span class="built_in">consumeAbortedTransactionsUpTo</span>(currentBatch.<span class="built_in">lastOffset</span>());</span><br><span class="line"></span><br><span class="line">                long producerId = currentBatch.producerId();</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">containsAbortMarker</span>(currentBatch)) &#123;</span><br><span class="line">                    abortedProducerIds.remove(producerId); <span class="comment">//note: 这个 PID（当前事务）涉及到的数据已经处理完</span></span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">isBatchAborted</span>(currentBatch)) &#123; <span class="comment">//note: 丢弃这个数据</span></span><br><span class="line">                    log.debug(<span class="string">"Skipping aborted record batch from partition &#123;&#125; with producerId &#123;&#125; and "</span> +</span><br><span class="line">                                  <span class="string">"offsets &#123;&#125; to &#123;&#125;"</span>,</span><br><span class="line">                              partition, producerId, currentBatch.baseOffset(), currentBatch.lastOffset());</span><br><span class="line">                    nextFetchOffset = currentBatch.nextOffset();</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            records = currentBatch.streamingIterator(decompressionBufferSupplier);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            Record record = records.next();</span><br><span class="line">            <span class="comment">// skip any records out of range</span></span><br><span class="line">            <span class="keyword">if</span> (record.offset() &gt;= nextFetchOffset) &#123;</span><br><span class="line">                <span class="comment">// we only do validation when the message should not be skipped.</span></span><br><span class="line">                <span class="built_in">maybeEnsureValid</span>(record);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// control records are not returned to the user</span></span><br><span class="line">                <span class="keyword">if</span> (!currentBatch.isControlBatch()) &#123; <span class="comment">//note: 过滤掉 marker 数据</span></span><br><span class="line">                    <span class="keyword">return</span> record;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// Increment the next fetch offset when we skip a control batch.</span></span><br><span class="line">                    nextFetchOffset = record.offset() + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Consumer-消费数据时，其顺序如何保证"><a href="#Consumer-消费数据时，其顺序如何保证" class="headerlink" title="Consumer 消费数据时，其顺序如何保证"></a>Consumer 消费数据时，其顺序如何保证</h3><p>有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。</p><h3 id="如果-txn-id-长期不使用，server-端怎么处理？"><a href="#如果-txn-id-长期不使用，server-端怎么处理？" class="headerlink" title="如果 txn.id 长期不使用，server 端怎么处理？"></a>如果 txn.id 长期不使用，server 端怎么处理？</h3><p>Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 <code>transaction.timeout.ms</code>，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间（参数是 <code>transaction.timeout.ms</code>，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。</p><p>上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，Server 端提供了一个 <code>transaction.id.expiration.ms</code> 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，那么 TransactionCoordinator 将会使这个 txn.id 过期。</p><h3 id="PID-Snapshot-是做什么的？用来解决什么问题？"><a href="#PID-Snapshot-是做什么的？用来解决什么问题？" class="headerlink" title="PID Snapshot 是做什么的？用来解决什么问题？"></a>PID Snapshot 是做什么的？用来解决什么问题？</h3><p>对于每个 Topic-Partition，Broker 都会在内存中维护其 PID 与 sequence number（最后成功写入的 msg 的 sequence number）的对应关系（这个在上面幂等性文章应讲述过，主要是为了不丢补充的实现）。</p><p>Broker 重启时，如果想恢复上面的状态信息，那么它读取所有的 log 文件。相比于之下，定期对这个 state 信息做 checkpoint（Snapshot），明显收益是非常大的，此时如果 Broker 重启，只需要读取最近一个 Snapshot 文件，之后的数据再从 log 文件中恢复即可。</p><p>这个 PID Snapshot 样式如 00000000000235947656.snapshot，以 <code>.snapshot</code> 作为后缀，其数据格式如下：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[matt<span class="meta">@XXX</span><span class="number">-35</span> app.matt_test_transaction_json_3<span class="number">-2</span>]$ <span class="regexp">/usr/</span>local<span class="regexp">/java18/</span>bin<span class="regexp">/java -Djava.ext.dirs=/</span>XXX<span class="regexp">/kafka/</span>libs kafka.tools.DumpLogSegments --files <span class="number">00000000000235947656.</span>snapshot</span><br><span class="line">Dumping <span class="number">00000000000235947656.</span>snapshot</span><br><span class="line"><span class="string">producerId:</span> <span class="number">2000</span> <span class="string">producerEpoch:</span> <span class="number">1</span> <span class="string">coordinatorEpoch:</span> <span class="number">4</span> <span class="string">currentTxnFirstOffset:</span> None <span class="string">firstSequence:</span> <span class="number">95769510</span> <span class="string">lastSequence:</span> <span class="number">95769511</span> <span class="string">lastOffset:</span> <span class="number">235947654</span> <span class="string">offsetDelta:</span> <span class="number">1</span> <span class="string">timestamp:</span> <span class="number">1541325156503</span></span><br><span class="line"><span class="string">producerId:</span> <span class="number">3000</span> <span class="string">producerEpoch:</span> <span class="number">5</span> <span class="string">coordinatorEpoch:</span> <span class="number">6</span> <span class="string">currentTxnFirstOffset:</span> None <span class="string">firstSequence:</span> <span class="number">91669662</span> <span class="string">lastSequence:</span> <span class="number">91669666</span> <span class="string">lastOffset:</span> <span class="number">235947651</span> <span class="string">offsetDelta:</span> <span class="number">4</span> <span class="string">timestamp:</span> <span class="number">1541325156454</span></span><br></pre></td></tr></table></figure><p>在实际的使用中，这个 snapshot 文件一般只会保存最近的两个文件。</p><h3 id="中间流程故障如何恢复"><a href="#中间流程故障如何恢复" class="headerlink" title="中间流程故障如何恢复"></a>中间流程故障如何恢复</h3><p>对于上面所讲述的一个事务操作流程，实际生产环境中，任何一个地方都有可能出现的失败：</p><ol><li>Producer 在发送 <code>beginTransaction()</code> 时，如果出现 timeout 或者错误：Producer 只需要重试即可；</li><li>Producer 在发送数据时出现错误：Producer 应该 abort 这个事务，如果 Produce 没有 abort（比如设置了重试无限次，并且 batch 超时设置得非常大），TransactionCoordinator 将会在这个事务超时之后 abort 这个事务操作；</li><li>Producer 发送 <code>commitTransaction()</code> 时出现 timeout 或者错误：Producer 应该重试这个请求；</li><li>Coordinator Failure：如果 Transaction Coordinator 发生切换（事务 topic leader 切换），Coordinator 可以从日志中恢复。如果发送事务有处于 PREPARE_COMMIT 或 PREPARE_ABORT 状态，那么直接执行 commit 或者 abort 操作，如果是一个正在进行的事务，Coordinator 的失败并不需要 abort 事务，producer 只需要向新的 Coordinator 发送请求即可。</li></ol><p>陆陆续续写了几天，终于把这篇文章总结完了。</p><hr><p>参考：</p><ol><li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="noopener">Idempotent Producer</a>；</li><li><a href="https://www.slideshare.net/ConfluentInc/exactlyonce-semantics-in-apache-kafka" target="_blank" rel="noopener">Exactly-once Semantics in Apache Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="noopener">Transactional Messaging in Kafka</a>；</li><li><a href="https://www.confluent.io/blog/transactions-apache-kafka/" target="_blank" rel="noopener">Transactions in Apache Kafka</a>；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka 事务性之幂等性实现</title>
      <link href="/2018/09/25/Kafka%20%E4%BA%8B%E5%8A%A1%E6%80%A7%E4%B9%8B%E5%B9%82%E7%AD%89%E6%80%A7%E5%AE%9E%E7%8E%B0/"/>
      <url>/2018/09/25/Kafka%20%E4%BA%8B%E5%8A%A1%E6%80%A7%E4%B9%8B%E5%B9%82%E7%AD%89%E6%80%A7%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<p>原作者：王蒙</p><p><a href="http://matt33.com/2018/10/24/kafka-idempotent/#%E5%B9%82%E7%AD%89%E6%80%A7%E8%A6%81%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">原文链接</a></p><p>Apache Kafka 从 0.11.0 开始，支持了一个非常大的 feature，就是对事务性的支持，在 Kafka 中关于事务性，是有三种层面上的含义：一是幂等性的支持；二是事务性的支持；三是 Kafka Streams 的 exactly once 的实现，关于 Kafka 事务性系列的文章我们只重点关注前两种层面上的事务性，与 Kafka Streams 相关的内容暂时不做讨论。社区从开始讨论事务性，前后持续近半年时间，相关的设计文档有六十几页（参考 <a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>）。事务性这部分的实现也是非常复杂的，之前 Producer 端的代码实现其实是非常简单的，增加事务性的逻辑之后，这部分代码复杂度提高了很多，本篇及后面几篇关于事务性的文章会以 2.0.0 版的代码实现为例，对这部分做了一下分析，计划分为五篇文章：</p><ol><li>第一篇：Kafka 幂等性实现；</li><li>第二篇：Kafka 事务性实现；</li><li>第三篇：Kafka 事务性相关处理请求在 Server 端如何处理及其实现细节；</li><li>第四篇：关于 Kafka 事务性实现的一些思考，也会简单介绍一下 RocketMQ 事务性的实现，做一下对比；</li><li>第五篇：Flink + Kafka 如何实现 Exactly Once；</li></ol><p>这篇是 Kafka 事务性系列的第一篇文章，主要讲述幂等性实现的整体流程，幂等性的实现相对于事务性的实现简单很多，也是事务性实现的基础。</p><h2 id="一、Producer-幂等性"><a href="#一、Producer-幂等性" class="headerlink" title="一、Producer 幂等性"></a>一、Producer 幂等性</h2><p>Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重，但是这里的幂等性是有条件的：</p><ul><li>只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;</li><li>幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。</li></ul><p>如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。</p><h2 id="二、幂等性示例"><a href="#二、幂等性示例" class="headerlink" title="二、幂等性示例"></a>二、幂等性示例</h2><p>Producer 使用幂等性的示例非常简单，与正常情况下 Producer 使用相比变化不大，只需要把 Producer 的配置 enable.idempotence 设置为 true 即可，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>); <span class="comment">// 当 enable.idempotence 为 true，这里默认为 all</span></span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(props);</span><br><span class="line"></span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord(topic, <span class="string">"test"</span>);</span><br></pre></td></tr></table></figure><p>Prodcuer 幂等性对外保留的接口非常简单，其底层的实现对上层应用做了很好的封装，应用层并不需要去关心具体的实现细节，对用户非常友好。</p><h2 id="三、幂等性要解决的问题"><a href="#三、幂等性要解决的问题" class="headerlink" title="三、幂等性要解决的问题"></a>三、幂等性要解决的问题</h2><p>在看 Producer 是如何实现幂等性之前，首先先考虑一个问题：<strong>幂等性是来解决什么问题的？</strong> 在 0.11.0 之前，Kafka 通过 Producer 端和 Server 端的相关配置可以做到<strong>数据不丢</strong>，也就是 at least once，但是在一些情况下，可能会导致数据重复，比如：网络请求延迟等导致的重试操作，在发送请求重试时 Server 端并不知道这条请求是否已经处理（没有记录之前的状态信息），所以就会有可能导致数据请求的重复发送，这是 Kafka 自身的机制（异常时请求重试机制）导致的数据重复。</p><p>对于大多数应用而言，数据保证不丢是可以满足其需求的，但是对于一些其他的应用场景（比如支付数据等），它们是要求精确计数的，这时候如果上游数据有重复，下游应用只能在消费数据时进行相应的去重操作，应用在去重时，最常用的手段就是根据唯一 id 键做 check 去重。</p><p>在这种场景下，因为上游生产导致的数据重复问题，会导致所有有精确计数需求的下游应用都需要做这种复杂的、重复的去重处理。试想一下：如果在发送时，系统就能保证 exactly once，这对下游将是多么大的解脱。这就是幂等性要解决的问题，主要是解决数据重复的问题，正如前面所述，数据重复问题，通用的解决方案就是加唯一 id，然后根据 id 判断数据是否重复，Producer 的幂等性也是这样实现的，这一小节就让我们看下 Kafka 的 Producer 如何保证数据的 exactly once 的。</p><h2 id="四、幂等性的实现原理"><a href="#四、幂等性的实现原理" class="headerlink" title="四、幂等性的实现原理"></a>四、幂等性的实现原理</h2><p>在讲述幂等性处理流程之前，先看下 Producer 是如何来保证幂等性的，正如前面所述，幂等性要解决的问题是：Producer 设置 at least once 时，由于异常触发重试机制导致数据重复，幂等性的目的就是为了解决这个数据重复的问题，简单来说就是：</p><p><strong>at least once + 幂等 = exactly once</strong></p><p>通过在 al least once 的基础上加上 幂等性来坐到 exactly once，当然这个层面的 exactly once 是有限制的，比如它会要求单会话内有效或者跨会话使用事务性有效等。这里我们先分析最简单的情况，那就是在单会话内如何做到幂等性，进而保证 exactly once。</p><p>要做到幂等性，要解决下面的问题：</p><ol><li>系统需要有能力鉴别一条数据到底是不是重复的数据？常用的手段是通过 <strong>唯一键/唯一 id</strong> 来判断，这时候系统一般是需要缓存已经处理的唯一键记录，这样才能更有效率地判断一条数据是不是重复；</li><li>唯一键应该选择什么粒度？对于分布式存储系统来说，肯定不能用全局唯一键（全局是针对集群级别），核心的解决思路依然是 <strong>分而治之</strong>，数据密集型系统为了实现分布式都是有分区概念的，而分区之间是有相应的隔离，对于 Kafka 而言，这里的解决方案就是在分区的维度上去做，重复数据的判断让 partition 的 leader 去判断处理，前提是 Produce 请求需要把唯一键值告诉 leader；</li><li>分区粒度实现唯一键会不会有其他问题？这里需要考虑的问题是当一个 Partition 有来自多个 client 写入的情况，这些 client 之间是很难做到使用同一个唯一键（一个是它们之间很难做到唯一键的实时感知，另一个是这样实现是否有必要）。而如果系统在实现时做到了 <strong>client + partition</strong> 粒度，这样实现的好处是每个 client 都是完全独立的（它们之间不需要有任何的联系，这是非常大的优点），只是在 Server 端对不同的 client 做好相应的区分即可，当然同一个 client 在处理多个 Topic-Partition 时是完全可以使用同一个 PID 的。</li></ol><p>有了上面的分析（都是个人见解，如果有误，欢迎指教），就不难理解 Producer 幂等性的实现原理，Kafka Producer 在实现时有以下两个重要机制：</p><ol><li>PID（Producer ID），用来标识每个 producer client；</li><li>sequence numbers，client 发送的每条消息都会带相应的 sequence number，Server 端就是根据这个值来判断数据是否重复。</li></ol><p>下面详细讲述这两个实现机制。</p><h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>每个 Producer 在初始化时都会被分配一个唯一的 PID，这个 PID 对应用是透明的，完全没有暴露给用户。对于一个给定的 PID，sequence number 将会从0开始自增，每个 Topic-Partition 都会有一个独立的 sequence number。Producer 在发送数据时，将会给每条 msg 标识一个 sequence number，Server 也就是通过这个来验证数据是否重复。这里的 PID 是全局唯一的，Producer 故障后重新启动后会被分配一个新的 PID，这也是幂等性无法做到跨会话的一个原因。</p><h4 id="Producer-PID-申请"><a href="#Producer-PID-申请" class="headerlink" title="Producer PID 申请"></a>Producer PID 申请</h4><p>这里看下 PID 在 Server 端是如何分配的？Client 通过向 Server 发送一个 InitProducerIdRequest 请求获取 PID（幂等性时，是选择一台连接数最少的 Broker 发送这个请求），这里看下 Server 端是如何处理这个请求的？KafkaApis 中 <code>handleInitProducerIdRequest()</code> 方法的实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">handleInitProducerIdRequest</span><span class="params">(request: RequestChannel.Request)</span>: Unit </span>= &#123;</span><br><span class="line">  val initProducerIdRequest = request.body[InitProducerIdRequest]</span><br><span class="line">  val transactionalId = initProducerIdRequest.transactionalId</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId != <span class="keyword">null</span>) &#123; <span class="comment">//note: 设置 txn.id 时，验证对 txn.id 的权限</span></span><br><span class="line">    <span class="keyword">if</span> (!authorize(request.session, Write, Resource(TransactionalId, transactionalId, LITERAL))) &#123;</span><br><span class="line">      sendErrorResponseMaybeThrottle(request, Errors.TRANSACTIONAL_ID_AUTHORIZATION_FAILED.exception)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!authorize(request.session, IdempotentWrite, Resource.ClusterResource)) &#123; <span class="comment">//note: 没有设置 txn.id 时，验证对集群是否有幂等性权限</span></span><br><span class="line">    sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">def <span class="title">sendResponseCallback</span><span class="params">(result: InitProducerIdResult)</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="function">def <span class="title">createResponse</span><span class="params">(requestThrottleMs: Int)</span>: AbstractResponse </span>= &#123;</span><br><span class="line">      val responseBody = <span class="keyword">new</span> InitProducerIdResponse(requestThrottleMs, result.error, result.producerId, result.producerEpoch)</span><br><span class="line">      trace(s<span class="string">"Completed $transactionalId's InitProducerIdRequest with result $result from client $&#123;request.header.clientId&#125;."</span>)</span><br><span class="line">      responseBody</span><br><span class="line">    &#125;</span><br><span class="line">    sendResponseMaybeThrottle(request, createResponse)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//note: 生成相应的了 pid，返回给 producer</span></span><br><span class="line">  txnCoordinator.handleInitProducerId(transactionalId, initProducerIdRequest.transactionTimeoutMs, sendResponseCallback)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里实际上是调用了 TransactionCoordinator （Broker 在启动 server 服务时都会初始化这个实例）的 <code>handleInitProducerId()</code> 方法做了相应的处理，其实现如下（这里只关注幂等性的处理）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">handleInitProducerId</span><span class="params">(transactionalId: String,</span></span></span><br><span class="line"><span class="function"><span class="params">                         transactionTimeoutMs: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">                         responseCallback: InitProducerIdCallback)</span>: Unit </span>= &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transactionalId == <span class="keyword">null</span>) &#123; <span class="comment">//note: 只设置幂等性时，直接分配 pid 并返回</span></span><br><span class="line">    <span class="comment">// if the transactional id is null, then always blindly accept the request</span></span><br><span class="line">    <span class="comment">// and return a new producerId from the producerId manager</span></span><br><span class="line">    val producerId = producerIdManager.generateProducerId()</span><br><span class="line">    responseCallback(InitProducerIdResult(producerId, producerEpoch = <span class="number">0</span>, Errors.NONE))</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Server 在给一个 client 初始化 PID 时，实际上是通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID。</p><h4 id="Server-PID-管理"><a href="#Server-PID-管理" class="headerlink" title="Server PID 管理"></a>Server PID 管理</h4><p>如前面所述，在幂等性的情况下，直接通过 ProducerIdManager 的 <code>generateProducerId()</code> 方法产生一个 PID，其中 ProducerIdManager 是在 TransactionCoordinator 对象初始化时初始化的，这个对象主要是用来管理 PID 信息：</p><ul><li>在本地的 PID 端用完了或者处于新建状态时，申请 PID 段（默认情况下，每次申请 1000 个 PID）；</li><li>TransactionCoordinator 对象通过 <code>generateProducerId()</code> 方法获取下一个可以使用的 PID；</li></ul><p><strong>PID 端申请是向 ZooKeeper 申请</strong>，zk 中有一个 <code>/latest_producer_id_block</code> 节点，每个 Broker 向 zk 申请一个 PID 段后，都会把自己申请的 PID 段信息写入到这个节点，这样当其他 Broker 再申请 PID 段时，会首先读写这个节点的信息，然后根据 block_end 选择一个 PID 段，最后再把信息写会到 zk 的这个节点，这个节点信息格式如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"version"</span>:<span class="number">1</span>,<span class="string">"broker"</span>:<span class="number">35</span>,<span class="string">"block_start"</span>:<span class="string">"4000"</span>,<span class="string">"block_end"</span>:<span class="string">"4999"</span>&#125;</span><br></pre></td></tr></table></figure><p>ProducerIdManager 向 zk 申请 PID 段的方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">getNewProducerIdBlock</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">var</span> zkWriteComplete = <span class="keyword">false</span></span><br><span class="line">  <span class="keyword">while</span> (!zkWriteComplete) &#123; <span class="comment">//note: 直到从 zk 拿取到分配的 PID 段</span></span><br><span class="line">    <span class="comment">// refresh current producerId block from zookeeper again</span></span><br><span class="line">    val (dataOpt, zkVersion) = zkClient.getDataAndVersion(ProducerIdBlockZNode.path)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// generate the new producerId block</span></span><br><span class="line">    currentProducerIdBlock = dataOpt match &#123;</span><br><span class="line">      <span class="function"><span class="keyword">case</span> <span class="title">Some</span><span class="params">(data)</span> </span>=&gt;</span><br><span class="line">        <span class="comment">//note: 从 zk 获取当前最新的 pid 信息，如果后面更新失败，这里也会重新从 zk 获取</span></span><br><span class="line">        val currProducerIdBlock = ProducerIdManager.parseProducerIdBlockData(data)</span><br><span class="line">        debug(s<span class="string">"Read current producerId block $currProducerIdBlock, Zk path version $zkVersion"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currProducerIdBlock.blockEndId &gt; Long.MaxValue - ProducerIdManager.PidBlockSize) &#123;<span class="comment">//note: 不足以分配1000个 PID</span></span><br><span class="line">          <span class="comment">// we have exhausted all producerIds (wow!), treat it as a fatal error</span></span><br><span class="line">          <span class="comment">//note: 当 PID 分配超过限制时，直接报错了（每秒分配1个，够用2百亿年了）</span></span><br><span class="line">          fatal(s<span class="string">"Exhausted all producerIds as the next block's end producerId is will has exceeded long type limit (current block end producerId is $&#123;currProducerIdBlock.blockEndId&#125;)"</span>)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Have exhausted all producerIds."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ProducerIdBlock(brokerId, currProducerIdBlock.blockEndId + <span class="number">1L</span>, currProducerIdBlock.blockEndId + ProducerIdManager.PidBlockSize)</span><br><span class="line">      <span class="keyword">case</span> None =&gt; <span class="comment">//note: 该节点还不存在，第一次初始化</span></span><br><span class="line">        debug(s<span class="string">"There is no producerId block yet (Zk path version $zkVersion), creating the first block"</span>)</span><br><span class="line">        ProducerIdBlock(brokerId, <span class="number">0L</span>, ProducerIdManager.PidBlockSize - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val newProducerIdBlockData = ProducerIdManager.generateProducerIdBlockJson(currentProducerIdBlock)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try to write the new producerId block into zookeeper</span></span><br><span class="line">    <span class="comment">//note: 将新的 pid 信息写入到 zk，如果写入失败（写入之前会比对 zkVersion，如果这个有变动，证明这期间有别的 Broker 在操作，那么写入失败），重新申请</span></span><br><span class="line">    val (succeeded, version) = zkClient.conditionalUpdatePath(ProducerIdBlockZNode.path,</span><br><span class="line">      newProducerIdBlockData, zkVersion, Some(checkProducerIdBlockZkData))</span><br><span class="line">    zkWriteComplete = succeeded</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (zkWriteComplete)</span><br><span class="line">      info(s<span class="string">"Acquired new producerId block $currentProducerIdBlock by writing to Zk with path version $version"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ProducerIdManager 申请 PID 段的流程如下：</p><ol><li>先从 zk 的 <code>/latest_producer_id_block</code> 节点读取最新已经分配的 PID 段信息；</li><li>如果该节点不存在，直接从 0 开始分配，选择 0~1000 的 PID 段（ProducerIdManager 的 PidBlockSize 默认为 1000，即是每次申请的 PID 段大小）；</li><li>如果该节点存在，读取其中数据，根据 block_end 选择 这个 PID 段（如果 PID 段超过 Long 类型的最大值，这里会直接返回一个异常）；</li><li>在选择了相应的 PID 段后，将这个 PID 段信息写回到 zk 的这个节点中，如果写入成功，那么 PID 段就证明申请成功，如果写入失败（写入时会判断当前节点的 zkVersion 是否与步骤1获取的 zkVersion 相同，如果相同，那么可以成功写入，否则写入就会失败，证明这个节点被修改过），证明此时可能其他的 Broker 已经更新了这个节点（当前的 PID 段可能已经被其他 Broker 申请），那么从步骤 1 重新开始，直到写入成功。</li></ol><p>明白了 ProducerIdManager 如何申请 PID 段之后，再看 <code>generateProducerId()</code> 这个方法就简单很多了，这个方法在每次调用时，都会更新 nextProducerId 值（下一次可以使用 PID 值），如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">generateProducerId</span><span class="params">()</span>: Long </span>= &#123;</span><br><span class="line">  <span class="keyword">this</span> <span class="keyword">synchronized</span> &#123;</span><br><span class="line">    <span class="comment">// grab a new block of producerIds if this block has been exhausted</span></span><br><span class="line">    <span class="keyword">if</span> (nextProducerId &gt; currentProducerIdBlock.blockEndId) &#123;</span><br><span class="line">      <span class="comment">//note: 如果分配的 pid 用完了，重新再向 zk 申请一批</span></span><br><span class="line">      getNewProducerIdBlock()</span><br><span class="line">      nextProducerId = currentProducerIdBlock.blockStartId + <span class="number">1</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      nextProducerId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    nextProducerId - <span class="number">1</span> <span class="comment">//note: 返回当前分配的 pid</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里就是 Producer PID 如何申请（事务性情况下 PID 的申请会复杂一些，下篇文章再讲述）以及 Server 端如何管理 PID 的。</p><h3 id="sequence-numbers"><a href="#sequence-numbers" class="headerlink" title="sequence numbers"></a>sequence numbers</h3><p>再有了 PID 之后，在 PID + Topic-Partition 级别上添加一个 sequence numbers 信息，就可以实现 Producer 的幂等性了。ProducerBatch 也提供了一个 <code>setProducerState()</code> 方法，它可以给一个 batch 添加一些 meta 信息（pid、baseSequence、isTransactional），这些信息是会伴随着 ProduceRequest 发到 Server 端，Server 端也正是通过这些 meta 来做相应的判断，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ProducerBatch</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(ProducerIdAndEpoch producerIdAndEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    recordsBuilder.setProducerState(producerIdAndEpoch.producerId, producerIdAndEpoch.epoch, baseSequence, isTransactional);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryRecordsBuilder</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setProducerState</span><span class="params">(<span class="keyword">long</span> producerId, <span class="keyword">short</span> producerEpoch, <span class="keyword">int</span> baseSequence, <span class="keyword">boolean</span> isTransactional)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isClosed()) &#123;</span><br><span class="line">        <span class="comment">// Sequence numbers are assigned when the batch is closed while the accumulator is being drained.</span></span><br><span class="line">        <span class="comment">// If the resulting ProduceRequest to the partition leader failed for a retriable error, the batch will</span></span><br><span class="line">        <span class="comment">// be re queued. In this case, we should not attempt to set the state again, since changing the producerId and sequence</span></span><br><span class="line">        <span class="comment">// once a batch has been sent to the broker risks introducing duplicates.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Trying to set producer state of an already closed batch. This indicates a bug on the client."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.producerId = producerId;</span><br><span class="line">    <span class="keyword">this</span>.producerEpoch = producerEpoch;</span><br><span class="line">    <span class="keyword">this</span>.baseSequence = baseSequence;</span><br><span class="line">    <span class="keyword">this</span>.isTransactional = isTransactional;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="五、幂等性实现整体流程"><a href="#五、幂等性实现整体流程" class="headerlink" title="五、幂等性实现整体流程"></a>五、幂等性实现整体流程</h2><p>在前面讲述完 Kafka 幂等性的两个实现机制（PID+sequence numbers）之后，这里详细讲述一下，幂等性时其整体的处理流程，主要讲述幂等性相关的内容，其他的部分会简单介绍（可以参考前面【Kafka 源码分析系列文章】了解 Producer 端处理流程以及 Server 端关于 ProduceRequest 请求的处理流程），其流程如下图所示：</p><p><a href="http://matt33.com/images/kafka/kafka-idemoptent.png" target="_blank" rel="noopener"><img src="http://matt33.com/images/kafka/kafka-idemoptent.png" alt="Producer 幂等性时处理流程"></a>Producer 幂等性时处理流程</p><p>这个图只展示了幂等性情况下，Producer 的大概流程，很多部分在前面的文章中做过分析，本文不再讲述，这里重点关注与幂等性相关的内容（事务性实现更加复杂，后面的文章再讲述），首先 KafkaProducer 在初始化时会初始化一个 TransactionManager 实例，它的作用有以下几个部分：</p><ol><li>记录本地的事务状态（事务性时必须）；</li><li>记录一些状态信息以保证幂等性，比如：每个 topic-partition 对应的下一个 sequence numbers 和 last acked batch（最近一个已经确认的 batch）的最大的 sequence number 等；</li><li>记录 ProducerIdAndEpoch 信息（PID 信息）。</li></ol><h3 id="Client-幂等性时发送流程"><a href="#Client-幂等性时发送流程" class="headerlink" title="Client 幂等性时发送流程"></a>Client 幂等性时发送流程</h3><p>如前面图中所示，幂等性时，Producer 的发送流程如下：</p><ol><li><p>应用通过 KafkaProducer 的 <code>send()</code> 方法将数据添加到 RecordAccumulator 中，添加时会判断是否需要新建一个 ProducerBatch，这时这个 ProducerBatch 还是没有 PID 和 sequence number 信息的；</p></li><li><p>Producer 后台发送线程 Sender，在 <code>run()</code> 方法中，会先根据 TransactionManager 的 <code>shouldResetProducerStateAfterResolvingSequences()</code> 方法判断当前的 PID 是否需要重置，重置的原因是因为：如果有 topic-partition 的 batch 重试多次失败最后因为超时而被移除，这时 sequence number 将无法做到连续，因为 sequence number 有部分已经分配出去，这时系统依赖自身的机制无法继续进行下去（因为幂等性是要保证不丢不重的），相当于程序遇到了一个 fatal 异常，PID 会进行重置，TransactionManager 相关的缓存信息被清空（Producer 不会重启），只是保存状态信息的 TransactionManager 做了 <code>clear+new</code> 操作，遇到这个问题时是无法保证 exactly once 的（有数据已经发送失败了，并且超过了重试次数）；</p></li><li><p>Sender 线程通过 <code>maybeWaitForProducerId()</code> 方法判断是否需要申请 PID，如果需要的话，这里会阻塞直到获取到相应的 PID 信息；</p></li><li><p>Sender 线程通过</p></li></ol>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">sendProducerData</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法发送数据，整体流程与之前的 Producer 流程相似，不同的地方是在 RecordAccumulator 的</p><p>​    </p>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">drain</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法中，在加了幂等性之后，</p>   <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">drain</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><p>​    </p><p>   方法多了如下几步判断：</p><ol><li>常规的判断：判断这个 topic-partition 是否可以继续发送（如果出现前面2中的情况是不允许发送的）、判断 PID 是否有效、如果这个 batch 是重试的 batch，那么需要判断这个 batch 之前是否还有 batch 没有发送完成，如果有，这里会先跳过这个 Topic-Partition 的发送，直到前面的 batch 发送完成，<strong>最坏情况下，这个 Topic-Partition 的 in-flight request 将会减少到1</strong>（这个涉及也是考虑到 server 端的一个设置，文章下面会详细分析）；</li><li>如果这个 ProducerBatch 还没有这个相应的 PID 和 sequence number 信息，会在这里进行相应的设置；</li></ol><ol start="5"><li>最后 Sender 线程再调用 <code>sendProduceRequests()</code> 方法发送 ProduceRequest 请求，后面的就跟之前正常的流程保持一致了。</li></ol><p>这里看下几个关键方法的实现，首先是 Sender 线程获取 PID 信息的方法 <code>maybeWaitForProducerId()</code> ，其实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 等待直到 Producer 获取到相应的 PID 和 epoch 信息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWaitForProducerId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!transactionManager.hasProducerId() &amp;&amp; !transactionManager.hasError()) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Node node = awaitLeastLoadedNodeReady(requestTimeoutMs); <span class="comment">//note: 选取 node（本地连接数最少的 node）</span></span><br><span class="line">            <span class="keyword">if</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">                ClientResponse response = sendAndAwaitInitProducerIdRequest(node); <span class="comment">//note: 发送 InitPidRequest</span></span><br><span class="line">                InitProducerIdResponse initProducerIdResponse = (InitProducerIdResponse) response.responseBody();</span><br><span class="line">                Errors error = initProducerIdResponse.error();</span><br><span class="line">                <span class="keyword">if</span> (error == Errors.NONE) &#123; <span class="comment">//note: 更新 Producer 的 PID 和 epoch 信息</span></span><br><span class="line">                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">new</span> ProducerIdAndEpoch(</span><br><span class="line">                            initProducerIdResponse.producerId(), initProducerIdResponse.epoch());</span><br><span class="line">                    transactionManager.setProducerIdAndEpoch(producerIdAndEpoch);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                    log.debug(<span class="string">"Retriable error from InitProducerId response"</span>, error.message());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    transactionManager.transitionToFatalError(error.exception());</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                log.debug(<span class="string">"Could not find an available broker to send InitProducerIdRequest to. "</span> +</span><br><span class="line">                        <span class="string">"We will back off and try again."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedVersionException e) &#123;</span><br><span class="line">            transactionManager.transitionToFatalError(e);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            log.debug(<span class="string">"Broker &#123;&#125; disconnected while awaiting InitProducerId response"</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">        log.trace(<span class="string">"Retry InitProducerIdRequest in &#123;&#125;ms."</span>, retryBackoffMs);</span><br><span class="line">        time.sleep(retryBackoffMs);</span><br><span class="line">        metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再看下 RecordAccumulator 的 <code>drain()</code> 方法，重点需要关注的是关于幂等性和事务性相关的处理，具体如下所示，这里面关于事务性相关的判断在上面的流程中已经讲述。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Drain all the data for the given nodes and collate them into a list of batches that will fit within the specified</span></span><br><span class="line"><span class="comment"> * size on a per-node basis. This method attempts to avoid choosing the same topic-node over and over.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> nodes The list of node to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> maxSize The maximum number of bytes to drain</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current unix time in milliseconds</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> A list of &#123;<span class="doctag">@link</span> ProducerBatch&#125; for each node specified with total size less than the requested maxSize.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster,</span><br><span class="line">                                               Set&lt;Node&gt; nodes,</span><br><span class="line">                                               <span class="keyword">int</span> maxSize,</span><br><span class="line">                                               <span class="keyword">long</span> now) &#123;</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) &#123;</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</span><br><span class="line">        List&lt;ProducerBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">/* to make starvation less likely this loop doesn't start at 0 */</span></span><br><span class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            PartitionInfo part = parts.get(drainIndex);</span><br><span class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</span><br><span class="line">            <span class="comment">// Only proceed if the partition has no in-flight batches.</span></span><br><span class="line">            <span class="keyword">if</span> (!isMuted(tp, now)) &#123;</span><br><span class="line">                Deque&lt;ProducerBatch&gt; deque = getDeque(tp);</span><br><span class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (deque) &#123; <span class="comment">//note: 先判断有没有数据，然后后面真正处理时再加锁处理</span></span><br><span class="line">                        ProducerBatch first = deque.peekFirst();</span><br><span class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            <span class="keyword">boolean</span> backoff = first.attempts() &gt; <span class="number">0</span> &amp;&amp; first.waitedTimeMs(now) &lt; retryBackoffMs;</span><br><span class="line">                            <span class="comment">// Only drain the batch if it is not during backoff period.</span></span><br><span class="line">                            <span class="keyword">if</span> (!backoff) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (size + first.estimatedSizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) &#123;</span><br><span class="line">                                    <span class="comment">// there is a rare case that a single batch size is larger than the request size due</span></span><br><span class="line">                                    <span class="comment">// to compression; in this case we will still eventually send this batch in a single</span></span><br><span class="line">                                    <span class="comment">// request</span></span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                    ProducerIdAndEpoch producerIdAndEpoch = <span class="keyword">null</span>;</span><br><span class="line">                                    <span class="keyword">boolean</span> isTransactional = <span class="keyword">false</span>;</span><br><span class="line">                                    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123; <span class="comment">//note: 幂等性或事务性时， 做一些检查判断</span></span><br><span class="line">                                        <span class="keyword">if</span> (!transactionManager.isSendToPartitionAllowed(tp))</span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        producerIdAndEpoch = transactionManager.producerIdAndEpoch();</span><br><span class="line">                                        <span class="keyword">if</span> (!producerIdAndEpoch.isValid()) <span class="comment">//note: pid 是否有效</span></span><br><span class="line">                                            <span class="comment">// we cannot send the batch until we have refreshed the producer id</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        isTransactional = transactionManager.isTransactional();</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">if</span> (!first.hasSequence() &amp;&amp; transactionManager.hasUnresolvedSequence(first.topicPartition))</span><br><span class="line">                                            <span class="comment">//note: 当前这个 topic-partition 的数据出现过超时,不能发送,如果是新的 batch 数据直接跳过（没有 seq  number 信息）</span></span><br><span class="line">                                            <span class="comment">// Don't drain any new batches while the state of previous sequence numbers</span></span><br><span class="line">                                            <span class="comment">// is unknown. The previous batches would be unknown if they were aborted</span></span><br><span class="line">                                            <span class="comment">// on the client after being sent to the broker at least once.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                                        <span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line">                                        <span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">                                                &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">                                            <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">                                            <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">                                            <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">                                            <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">                                            <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">                                            <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">                                            <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">                                            <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">                                            <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">                                            <span class="comment">// in flight request count to 1.</span></span><br><span class="line">                                            <span class="keyword">break</span>;</span><br><span class="line">                                    &#125;</span><br><span class="line"></span><br><span class="line">                                    ProducerBatch batch = deque.pollFirst();</span><br><span class="line">                                    <span class="keyword">if</span> (producerIdAndEpoch != <span class="keyword">null</span> &amp;&amp; !batch.hasSequence()) &#123;<span class="comment">//note: batch 的相关信息（seq id）是在这里设置的</span></span><br><span class="line">                                        <span class="comment">//note: 这个 batch 还没有 seq number 信息</span></span><br><span class="line">                                        <span class="comment">// If the batch already has an assigned sequence, then we should not change the producer id and</span></span><br><span class="line">                                        <span class="comment">// sequence number, since this may introduce duplicates. In particular,</span></span><br><span class="line">                                        <span class="comment">// the previous attempt may actually have been accepted, and if we change</span></span><br><span class="line">                                        <span class="comment">// the producer id and sequence here, this attempt will also be accepted,</span></span><br><span class="line">                                        <span class="comment">// causing a duplicate.</span></span><br><span class="line">                                        <span class="comment">//</span></span><br><span class="line">                                        <span class="comment">// Additionally, we update the next sequence number bound for the partition,</span></span><br><span class="line">                                        <span class="comment">// and also have the transaction manager track the batch so as to ensure</span></span><br><span class="line">                                        <span class="comment">// that sequence ordering is maintained even if we receive out of order</span></span><br><span class="line">                                        <span class="comment">// responses.</span></span><br><span class="line">                                        <span class="comment">//note: 给这个 batch 设置相应的 pid、seq id 等信息</span></span><br><span class="line">                                        batch.setProducerState(producerIdAndEpoch, transactionManager.sequenceNumber(batch.topicPartition), isTransactional);</span><br><span class="line">                                        transactionManager.incrementSequenceNumber(batch.topicPartition, batch.recordCount); <span class="comment">//note: 增加 partition 对应的下一个 seq id 值</span></span><br><span class="line">                                        log.debug(<span class="string">"Assigned producerId &#123;&#125; and producerEpoch &#123;&#125; to batch with base sequence "</span> +</span><br><span class="line">                                                        <span class="string">"&#123;&#125; being sent to partition &#123;&#125;"</span>, producerIdAndEpoch.producerId,</span><br><span class="line">                                                producerIdAndEpoch.epoch, batch.baseSequence(), tp);</span><br><span class="line"></span><br><span class="line">                                        transactionManager.addInFlightBatch(batch);</span><br><span class="line">                                    &#125;</span><br><span class="line">                                    batch.close();</span><br><span class="line">                                    size += batch.records().sizeInBytes();</span><br><span class="line">                                    ready.add(batch);</span><br><span class="line">                                    batch.drained(now);</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</span><br><span class="line">        &#125; <span class="keyword">while</span> (start != drainIndex);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="幂等性时-Server-端如何处理-ProduceRequest-请求"><a href="#幂等性时-Server-端如何处理-ProduceRequest-请求" class="headerlink" title="幂等性时 Server 端如何处理 ProduceRequest 请求"></a>幂等性时 Server 端如何处理 ProduceRequest 请求</h3><p>如前面途中所示，当 Broker 收到 ProduceRequest 请求之后，会通过 <code>handleProduceRequest()</code> 做相应的处理，其处理流程如下（这里只讲述关于幂等性相关的内容）：</p><ol><li>如果请求是事务请求，检查是否对 TXN.id 有 Write 权限，没有的话返回 TRANSACTIONAL_ID_AUTHORIZATION_FAILED；</li><li>如果请求设置了幂等性，检查是否对 ClusterResource 有 IdempotentWrite 权限，没有的话返回 CLUSTER_AUTHORIZATION_FAILED；</li><li>验证对 topic 是否有 Write 权限以及 Topic 是否存在，否则返回 TOPIC_AUTHORIZATION_FAILED 或 UNKNOWN_TOPIC_OR_PARTITION 异常；</li><li>检查是否有 PID 信息，没有的话走正常的写入流程；</li><li>LOG 对象会在 <code>analyzeAndValidateProducerState()</code> 方法先根据 batch 的 sequence number 信息检查这个 batch 是否重复（server 端会缓存 PID 对应这个 Topic-Partition 的最近5个 batch 信息），如果有重复，这里当做写入成功返回（不更新 LOG 对象中相应的状态信息，比如这个 replica 的 the end offset 等）；</li><li>有了 PID 信息，并且不是重复 batch 时，在更新 producer 信息时，会做以下校验：<ol><li>检查该 PID 是否已经缓存中存在（主要是在 ProducerStateManager 对象中检查）；</li><li>如果不存在，那么判断 sequence number 是否 从0 开始，是的话，在缓存中记录 PID 的 meta（PID，epoch， sequence number），并执行写入操作，否则返回 UnknownProducerIdException（PID 在 server 端已经过期或者这个 PID 写的数据都已经过期了，但是 Client 还在接着上次的 sequence number 发送数据）；</li><li>如果该 PID 存在，先检查 PID epoch 与 server 端记录的是否相同；</li><li>如果不同并且 sequence number 不从 0 开始，那么返回 OutOfOrderSequenceException 异常；</li><li>如果不同并且 sequence number 从 0 开始，那么正常写入；</li><li>如果相同，那么根据缓存中记录的最近一次 sequence number（currentLastSeq）检查是否为连续（会区分为 0、Int.MaxValue 等情况），不连续的情况下返回 OutOfOrderSequenceException 异常。</li></ol></li><li>下面与正常写入相同。</li></ol><p>幂等性时，Broker 在处理 ProduceRequest 请求时，多了一些校验操作，这里重点看一下其中一些重要实现，先看下 <code>analyzeAndValidateProducerState()</code> 方法的实现，如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">analyzeAndValidateProducerState</span><span class="params">(records: MemoryRecords, isFromClient: Boolean)</span>: <span class="params">(mutable.Map[Long, ProducerAppendInfo], List[CompletedTxn], Option[BatchMetadata])</span> </span>= &#123;</span><br><span class="line">  val updatedProducers = mutable.Map.empty[Long, ProducerAppendInfo]</span><br><span class="line">  val completedTxns = ListBuffer.empty[CompletedTxn]</span><br><span class="line">  <span class="keyword">for</span> (batch &lt;- records.batches.asScala <span class="keyword">if</span> batch.hasProducerId) &#123; <span class="comment">//note: 有 pid 时,才会做相应的判断</span></span><br><span class="line">    val maybeLastEntry = producerStateManager.lastEntry(batch.producerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if this is a client produce request, there will be up to 5 batches which could have been duplicated.</span></span><br><span class="line">    <span class="comment">// If we find a duplicate, we return the metadata of the appended batch to the client.</span></span><br><span class="line">    <span class="keyword">if</span> (isFromClient) &#123;</span><br><span class="line">      maybeLastEntry.flatMap(_.findDuplicateBatch(batch)).foreach &#123; duplicate =&gt;</span><br><span class="line">        <span class="keyword">return</span> (updatedProducers, completedTxns.toList, Some(duplicate)) <span class="comment">//note: 如果这个 batch 已经收到过，这里直接返回</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val maybeCompletedTxn = updateProducers(batch, updatedProducers, isFromClient = isFromClient) <span class="comment">//note: 这里</span></span><br><span class="line">    maybeCompletedTxn.foreach(completedTxns += _)</span><br><span class="line">  &#125;</span><br><span class="line">  (updatedProducers, completedTxns.toList, None)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果这个 batch 有 PID 信息，会首先检查这个 batch 是否为重复的 batch 数据，其实现如下，batchMetadata 会缓存最新 5个 batch 的数据（如果超过5个，添加时会进行删除，这个也是幂等性要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5 的原因，与这个值的设置有关），根据 batchMetadata 缓存的 batch 数据来判断这个 batch 是否为重复的数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">findDuplicateBatch</span><span class="params">(batch: RecordBatch)</span>: Option[BatchMetadata] </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (batch.producerEpoch != producerEpoch)</span><br><span class="line">     None</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    batchWithSequenceRange(batch.baseSequence, batch.lastSequence)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Return the batch metadata of the cached batch having the exact sequence range, if any.</span></span><br><span class="line"><span class="function">def <span class="title">batchWithSequenceRange</span><span class="params">(firstSeq: Int, lastSeq: Int)</span>: Option[BatchMetadata] </span>= &#123;</span><br><span class="line">  val duplicate = batchMetadata.filter &#123; metadata =&gt;</span><br><span class="line">    firstSeq == metadata.firstSeq &amp;&amp; lastSeq == metadata.lastSeq</span><br><span class="line">  &#125;</span><br><span class="line">  duplicate.headOption</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">addBatchMetadata</span><span class="params">(batch: BatchMetadata)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (batchMetadata.size == ProducerStateEntry.NumBatchesToRetain)</span><br><span class="line">    batchMetadata.dequeue() <span class="comment">//note: 只会保留最近 5 个 batch 的记录</span></span><br><span class="line">  batchMetadata.enqueue(batch) <span class="comment">//note: 添加到 batchMetadata 中记录，便于后续根据 seq id 判断是否重复</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果 batch 不是重复的数据，<code>analyzeAndValidateProducerState()</code> 会通过 <code>updateProducers()</code> 更新 producer 的相应记录，在更新的过程中，会做一步校验，校验方法如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 检查 seq number</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> def <span class="title">checkSequence</span><span class="params">(producerEpoch: Short, appendFirstSeq: Int)</span>: Unit </span>= &#123;</span><br><span class="line">  <span class="keyword">if</span> (producerEpoch != updatedEntry.producerEpoch) &#123; <span class="comment">//note: epoch 不同时</span></span><br><span class="line">    <span class="keyword">if</span> (appendFirstSeq != <span class="number">0</span>) &#123; <span class="comment">//note: 此时要求 seq number 必须从0开始（如果不是的话，pid 可能是新建的或者 PID 在 Server 端已经过期）</span></span><br><span class="line">      <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 不是-1，证明时原来的 pid 过期了）</span></span><br><span class="line">      <span class="keyword">if</span> (updatedEntry.producerEpoch != RecordBatch.NO_PRODUCER_EPOCH) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Invalid sequence number for new epoch: $producerEpoch "</span> +</span><br><span class="line">          s<span class="string">"(request epoch), $appendFirstSeq (seq. number)"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//note: pid 已经过期（updatedEntry.producerEpoch 为-1，证明 server 端 meta 新建的，PID 在 server 端已经过期，client 还在接着上次的 seq 发数据）</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnknownProducerIdException(s<span class="string">"Found no record of producerId=$producerId on the broker. It is possible "</span> +</span><br><span class="line">          s<span class="string">"that the last message with t（）he producerId=$producerId has been removed due to hitting the retention limit."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    val currentLastSeq = <span class="keyword">if</span> (!updatedEntry.isEmpty)</span><br><span class="line">      updatedEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (producerEpoch == currentEntry.producerEpoch)</span><br><span class="line">      currentEntry.lastSeq</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      RecordBatch.NO_SEQUENCE</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (currentLastSeq == RecordBatch.NO_SEQUENCE &amp;&amp; appendFirstSeq != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">//note: 此时期望的 seq number 是从 0 开始,因为 currentLastSeq 是 -1,也就意味着这个 pid 还没有写入过数据</span></span><br><span class="line">      <span class="comment">// the epoch was bumped by a control record, so we expect the sequence number to be reset</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Out of order sequence number for producerId $producerId: found $appendFirstSeq "</span> +</span><br><span class="line">        s<span class="string">"(incoming seq. number), but expected 0"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!inSequence(currentLastSeq, appendFirstSeq)) &#123;</span><br><span class="line">      <span class="comment">//note: 判断是否连续</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> OutOfOrderSequenceException(s<span class="string">"Out of order sequence number for producerId $producerId: $appendFirstSeq "</span> +</span><br><span class="line">        s<span class="string">"(incoming seq. number), $currentLastSeq (current end sequence number)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其校验逻辑如前面流程中所述。</p><h2 id="六、小思考"><a href="#六、小思考" class="headerlink" title="六、小思考"></a>六、小思考</h2><p>这里主要思考两个问题：</p><ol><li>Producer 在设置幂等性时，为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5，如果设置大于 5（不考虑 Producer 端参数校验的报错），会带来什么后果？</li><li>Producer 在设置幂等性时，如果我们设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，那么是否可以保证有序，如果可以，是怎么做到的？</li></ol><p>先说一下结论，问题 1 的这个设置要求其实上面分析的时候已经讲述过了，主要跟 server 端只会缓存最近 5 个 batch 的机制有关；问题 2，即使 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 大于 1，幂等性时依然可以做到有序，下面来详细分析一下这两个问题。</p><h3 id="为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5"><a href="#为什么要求-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-小于等于5" class="headerlink" title="为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5"></a>为什么要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于5</h3><p>其实这里，要求 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 小于等于 5 的主要原因是：Server 端的 ProducerStateManager 实例会缓存每个 PID 在每个 Topic-Partition 上发送的最近 5 个batch 数据（这个 5 是写死的，至于为什么是 5，可能跟经验有关，当不设置幂等性时，当这个设置为 5 时，性能相对来说较高，社区是有一个相关测试文档，忘记在哪了），如果超过 5，ProducerStateManager 就会将最旧的 batch 数据清除。</p><p>假设应用将 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 6，假设发送的请求顺序是 1、2、3、4、5、6，这时候 server 端只能缓存 2、3、4、5、6 请求对应的 batch 数据，这时候假设请求 1 发送失败，需要重试，当重试的请求发送过来后，首先先检查是否为重复的 batch，这时候检查的结果是否，之后会开始 check 其 sequence number 值，这时候只会返回一个 OutOfOrderSequenceException 异常，client 在收到这个异常后，会再次进行重试，直到超过最大重试次数或者超时，这样不但会影响 Producer 性能，还可能给 Server 带来压力（相当于client 狂发错误请求）。</p><p>那有没有更好的方案呢？我认为是有的，那就是对于 OutOfOrderSequenceException 异常，再进行细分，区分这个 sequence number 是大于 nextSeq （期望的下次 sequence number 值）还是小于 nextSeq，如果是小于，那么肯定是重复的数据。</p><h3 id="当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序"><a href="#当-MAX-IN-FLIGHT-REQUESTS-PER-CONNECTION-配置大于1时，是否保证有序" class="headerlink" title="当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序"></a>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 配置大于1时，是否保证有序</h3><p>先来分析一下，在什么情况下 Producer 会出现乱序的问题？没有幂等性时，乱序的问题是在重试时出现的，举个例子：client 依然发送了 6 个请求 1、2、3、4、5、6（它们分别对应了一个 batch），这 6 个请求只有 2-6 成功 ack 了，1 失败了，这时候需要重试，重试时就会把 batch 1 的数据添加到待发送的数据列队中），那么下次再发送时，batch 1 的数据将会被发送，这时候数据就已经出现了乱序，因为 batch 1 的数据已经晚于了 batch 2-6。</p><p>当 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION 设置为 1 时，是可以解决这个为题，因为同时只允许一个请求正在发送，只有当前的请求发送完成（成功 ack 后），才能继续下一条请求的发送，类似单线程处理这种模式，每次请求发送时都会等待上次的完成，效率非常差，但是可以解决乱序的问题（当然这里有序只是针对单 client 情况，多 client 并发写是无法做到的）。</p><p>系统能提供的方案，基本上就是有序性与性能之间二选一，无法做到兼容，实际上系统出现请求重试的几率是很小的（一般都是网络问题触发的），可能连 0.1% 的时间都不到，但是就是为了这 0.1% 时间都不到的情况，应用需要牺牲性能问题来解决，在大数据场景下，我们是希望有更友好的方式来解决这个问题。简单来说，就是当出现重试时，max-in-flight-request 可以动态减少到 1，在正常情况下还是按 5 （5是举例说明）来处理，这有点类似于分布式系统 CAP 理论中关于 P 的考虑，当出现问题时，可以容忍性能变差，但是其他的情况下，我们希望的是能拥有原来的性能，而不是一刀切。令人高兴的，在 Kafka 2.0.0 版本中，如果 Producer 开始了幂等性，Kafka 是可以做到这一点的，如果不开启幂等性，是无法做到的，因为它的实现是依赖了 sequence number。</p><p>当请求出现重试时，batch 会重新添加到队列中，这时候是根据 sequence number 添加到队列的合适位置（有些 batch 如果还没有 sequence number，那么就保持其相对位置不变），也就是队列中排在这个 batch 前面的 batch，其 sequence number 都比这个 batch 的 sequence number 小，其实现如下，这个方法保证了在重试时，其 batch 会被放到合适的位置：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Re-enqueue the given record batch in the accumulator to retry</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reenqueue</span><span class="params">(ProducerBatch batch, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    batch.reenqueued(now); <span class="comment">//note: 重试,更新相应的 meta</span></span><br><span class="line">    Deque&lt;ProducerBatch&gt; deque = getOrCreateDeque(batch.topicPartition);</span><br><span class="line">    <span class="keyword">synchronized</span> (deque) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>)</span><br><span class="line">            insertInSequenceOrder(deque, batch); <span class="comment">//note: 将 batch 添加到队列的合适位置（根据 seq num 信息）</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            deque.addFirst(batch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外 Sender 在发送请求时，会首先通过 RecordAccumulator 的 <code>drain()</code> 方法获取其发送的数据，在遍历 Topic-Partition 对应的 queue 中的 batch 时，如果发现 batch 已经有了 sequence number 的话，则证明这个 batch 是重试的 batch，因为没有重试的 batch 其 sequence number 还没有设置，这时候会做一个判断，会等待其 in-flight-requests 中请求发送完成，才允许再次发送这个 Topic-Partition 的数据，其判断实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//note: 获取 inFlightBatches 中第一个 batch 的 baseSequence, inFlightBatches 为 null 的话返回 RecordBatch.NO_SEQUENCE</span></span><br><span class="line"><span class="keyword">int</span> firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);</span><br><span class="line"><span class="keyword">if</span> (firstInFlightSequence != RecordBatch.NO_SEQUENCE &amp;&amp; first.hasSequence()</span><br><span class="line">        &amp;&amp; first.baseSequence() != firstInFlightSequence)</span><br><span class="line">    <span class="comment">//note: 重试操作（seq number 不为0）,如果这个 batch 的 baseSequence 与 in-flight</span></span><br><span class="line">    <span class="comment">//note: queue 中第一个 request batch 的 baseSequence不同的话（证明它前面还有请求未成功）,</span></span><br><span class="line">    <span class="comment">//note: 会等待下次循环再判断, 最坏的情况下会导致 in-flight request 为1（只影响这个 partition）</span></span><br><span class="line">    <span class="comment">//note: 这种情况下,继续发送这个是没有意义的,因为幂等性时保证顺序的,只有前面的都成功,后面的再发送才有意义</span></span><br><span class="line">    <span class="comment">//note: 这里是 break,相当于在这次发送中直接跳过了这个 topic-partition 的发送</span></span><br><span class="line">    <span class="comment">// If the queued batch already has an assigned sequence, then it is being</span></span><br><span class="line">    <span class="comment">// retried. In this case, we wait until the next immediate batch is ready</span></span><br><span class="line">    <span class="comment">// and drain that. We only move on when the next in line batch is complete (either successfully</span></span><br><span class="line">    <span class="comment">// or due to a fatal broker error). This effectively reduces our</span></span><br><span class="line">    <span class="comment">// in flight request count to 1.</span></span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>仅有 client 端这两个机制还不够，Server 端在处理 ProduceRequest 请求时，还会检查 batch 的 sequence number 值，它会要求这个值必须是连续的，如果不连续都会返回异常，Client 会进行相应的重试，举个栗子：假设 Client 发送的请求顺序是 1、2、3、4、5（分别对应了一个 batch），如果中间的请求 2 出现了异常，那么会导致 3、4、5 都返回异常进行重试（因为 sequence number 不连续），也就是说此时 2、3、4、5 都会进行重试操作添加到对应的 queue 中。</p><p>Producer 的 TransactionManager 实例的 inflightBatchesBySequence 成员变量会维护这个 Topic-Partition 与目前正在发送的 batch 的对应关系（通过 <code>addInFlightBatch()</code> 方法添加 batch 记录），只有这个 batch 成功 ack 后，才会通过 <code>removeInFlightBatch()</code> 方法将这个 batch 从 inflightBatchesBySequence 中移除。接着前面的例子，此时 inflightBatchesBySequence 中还有 2、3、4、5 这几个 batch（有顺序的，2 在前面），根据前面的 RecordAccumulator 的 <code>drain()</code> 方法可以知道只有这个 Topic-Partition 下次要发送的 batch 是 batch 2（跟 transactionManager 的这个 <code>firstInFlightSequence()</code> 方法获取 inFlightBatches 中第一个 batch 的 baseSequence 来判断） 时，才可以发送，否则会直接 break，跳过这个 Topic-Partition 的数据发送。这里相当于有一个等待，等待 batch 2 重新加入到 queue 中，才可以发送，不能跳过 batch 2，直接重试 batch 3、4、5，这是不允许的。</p><p>简单来说，其实现机制概括为：</p><ol><li>Server 端验证 batch 的 sequence number 值，不连续时，直接返回异常；</li><li>Client 端请求重试时，batch 在 reenqueue 时会根据 sequence number 值放到合适的位置（有序保证之一）；</li><li>Sender 线程发送时，在遍历 queue 中的 batch 时，会检查这个 batch 是否是重试的 batch，如果是的话，只有这个 batch 是最旧的那个需要重试的 batch，才允许发送，否则本次发送跳过这个 Topic-Partition 数据的发送等待下次发送。</li></ol><hr><p>参考：</p><ol><li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit" target="_blank" rel="noopener">Exactly Once Delivery and Transactional Messaging in Kafka</a>；</li><li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Idempotent+Producer" target="_blank" rel="noopener">Idempotent Producer</a>；</li></ol>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka ISR设计及水印与leader epoch副本同步机制深入剖析</title>
      <link href="/2018/09/23/kafka%20ISR%E8%AE%BE%E8%AE%A1%E5%8F%8A%E6%B0%B4%E5%8D%B0%E4%B8%8Eleader%20epoch%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/"/>
      <url>/2018/09/23/kafka%20ISR%E8%AE%BE%E8%AE%A1%E5%8F%8A%E6%B0%B4%E5%8D%B0%E4%B8%8Eleader%20epoch%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h2 id="帽子理论（Gilbert-和-Lynch-）"><a href="#帽子理论（Gilbert-和-Lynch-）" class="headerlink" title="帽子理论（Gilbert 和 Lynch ）"></a>帽子理论（Gilbert 和 Lynch ）</h2><ul><li><p>一致性</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">any</span> <span class="built_in">read</span> operation that <span class="keyword">begins</span> <span class="keyword">after</span> <span class="keyword">a</span> <span class="built_in">write</span> operation completes must </span><br><span class="line"><span class="literal">return</span> that <span class="built_in">value</span>, <span class="keyword">or</span> <span class="keyword">the</span> <span class="built_in">result</span> <span class="keyword">of</span> <span class="keyword">a</span> later <span class="built_in">write</span> operation</span><br><span class="line"></span><br><span class="line">通过某个节点的写操作结果对后面通过其它节点的读操作可见</span><br><span class="line"></span><br><span class="line">强一致性：</span><br><span class="line">如果更新数据后，并发访问情况下后续读操作可立即感知该更新，称为强一致性。</span><br><span class="line"></span><br><span class="line">弱一致性：</span><br><span class="line">如果允许之后部分或者全部感知不到该更新，称为弱一致性。</span><br><span class="line"></span><br><span class="line">最终一致性：</span><br><span class="line">若在之后的一段时间（通常该时间不固定）后，一定可以感知到该更新，称为最终一致性。</span><br></pre></td></tr></table></figure></li><li><p>可用性（Availability）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">every request received by a non-failing node <span class="keyword">in</span> the<span class="built_in"> system </span>must result <span class="keyword">in</span> a response</span><br><span class="line"></span><br><span class="line">任何一个没有发生故障的节点必须在有限的时间内返回合理的结果。</span><br></pre></td></tr></table></figure></li><li><p>分区容忍性（Partition Tolerance）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">the<span class="built_in"> network </span>will be allowed <span class="keyword">to</span> lose arbitrarily many messages sent <span class="keyword">from</span> one node <span class="keyword">to</span> another</span><br><span class="line"></span><br><span class="line">部分节点宕机或者无法与其它节点通信时，各分区间还可保持分布式系统的功能。</span><br></pre></td></tr></table></figure></li><li><p>悖论总结：</p><p>可用性限定在无论是否集群节点宕机，只要有活着的节点，就会立即返回请求结果。若要限制返回结果必须是最近一次写的结果，就比较悲剧，若允许分区容忍性 =&gt; 分布式系统分区之间就存在数据同步机制，那么就有可能因为分区心跳切断，导致数据不一致。</p></li></ul><h2 id="partition本质就是为了日志备份（对外最小的存储单元）"><a href="#partition本质就是为了日志备份（对外最小的存储单元）" class="headerlink" title="partition本质就是为了日志备份（对外最小的存储单元）"></a>partition本质就是为了日志备份（对外最小的存储单元）</h2><p>Kafka中topic的每个partition有一个预写式的日志文件，虽然partition可以继续细分为若干个segment文件，但是对于上层应用来说可以将partition看成最小的存储单元（一个有多个segment文件拼接的“巨型”文件），每个partition都由一些列有序的、不可变的消息组成，这些消息被连续的追加到partition中。</p><p><img src="https://user-gold-cdn.xitu.io/2018/11/22/1673bba0c1d92e2a?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>partition本质就是为了日志备份，利用多份日志文件的副本（replica）备份来共同提供冗余机制来保持系统的高可用性。</li><li>kafka会把副本均匀的分配到所有的Broker上。在其中所有的副本中，会挑选一个Leader副本来对外提供服务，其他的副本统称为follower副本，只能被动的向leader副本请求数据。</li></ul><h2 id="Partitioner-三分天下"><a href="#Partitioner-三分天下" class="headerlink" title="Partitioner 三分天下"></a>Partitioner 三分天下</h2><p>下图展示了3个Partition把一个Topic主题数据流分成三份，通过Partioner路由依次追加到分区的末尾中。如果partition规则设置的合理，所有消息可以均匀分布到不同的partition里，这样就实现了水平扩展。</p><p><img src="https://user-gold-cdn.xitu.io/2018/11/22/1673bca604b58210?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>config/server.properties可以设置num.partitions参数，实现主题数据分流。</p><h2 id="Leader副本竞选上岗（in-sync-replicas）"><a href="#Leader副本竞选上岗（in-sync-replicas）" class="headerlink" title="Leader副本竞选上岗（in-sync replicas）"></a>Leader副本竞选上岗（in-sync replicas）</h2><ul><li>每一个分区都存在一个in-sync replicas。</li><li>in-sync replicas集合中的每一个副本都与leader保持同步状态，不在里面的保持不了同步状态。</li><li>只有ISR中的副本才有资格被选为leader。</li><li>Producer写入的消息只有被ISR中的副本都接收到，才被视为”已提交”。</li></ul><h2 id="水印HW与末端位移LEO-gt-Leader副本"><a href="#水印HW与末端位移LEO-gt-Leader副本" class="headerlink" title="水印HW与末端位移LEO =&gt; Leader副本"></a>水印HW与末端位移LEO =&gt; Leader副本</h2><ul><li>这里着重强调一下，Leader副本水印HW才真正决定了对外可看到的消息数量。</li><li>所有的副本都有LEO和HW。</li><li>Leader副本水印HW的更新发生在所有的副本都更新了最新的LEO后，Leader副本最终才认为可以更新Leader副本水印。</li></ul><h2 id="ISR设计优化（replica-lag-max-messages废弃）"><a href="#ISR设计优化（replica-lag-max-messages废弃）" class="headerlink" title="ISR设计优化（replica.lag.max.messages废弃）"></a>ISR设计优化（replica.lag.max.messages废弃）</h2><ul><li>解决了producer突然发起一大波消息，从而产生瞬时高峰流量。若设置replica.lag.max.messages=4，则follower副本会被瞬时的拉开距离，从而导致follower副本瞬间被踢出ISR。不过一段时间follower副本同步后，会再次进入ISR。</li><li>同步不同步，同步不同步反复出现，是多大的性能浪费。</li><li>0.9.0.0开始采用 replica. lag. time. max. ms，默认是10s，可谓是明智之举。</li></ul><h2 id="HW同步机制"><a href="#HW同步机制" class="headerlink" title="HW同步机制"></a>HW同步机制</h2><h3 id="HW指向哪里？"><a href="#HW指向哪里？" class="headerlink" title="HW指向哪里？"></a>HW指向哪里？</h3><ul><li>这里重点强调，都是无论HW还是LEO都是指向下一条消息</li><li>举例如下：如果一个普通topic的某个分区副本的LEO是10，那么该副本当前保存了10条消息，位移值范围是[0, 9]。此时若有一个producer向该副本插入一条消息，则该条消息的位移值是10，而副本LEO值则更新成11。</li></ul><h3 id="Leader与follower的HW（两阶段请求定终身）"><a href="#Leader与follower的HW（两阶段请求定终身）" class="headerlink" title="Leader与follower的HW（两阶段请求定终身）"></a>Leader与follower的HW（两阶段请求定终身）</h3><p><img src="https://user-gold-cdn.xitu.io/2018/11/23/1673c6b093d51af0?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>follower 副本会不断地向leader副本发送Fetch请求</li></ul><p><strong>（1）follower 副本对象何时更新LEO？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">follower 副本专属线程不断地向<span class="built_in">leader</span>副本所在broker发送FETCH请求。</span><br><span class="line"></span><br><span class="line"><span class="built_in">leader</span> 副本发送 FETCH response 给follower副本。</span><br><span class="line"></span><br><span class="line">Follower 拿到response之后取出位移数据写入到本地底层日志中，在该过程中其LEO值会被更新。</span><br></pre></td></tr></table></figure><p><strong>（2）leader 端非自己副本对象何时更新LEO？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">leader</span> 端非自己副本对象 LEO值是在<span class="built_in">leader</span>端broker处理FETCH请求过程中被更新的。</span><br></pre></td></tr></table></figure><p><strong>（3） follower 副本对象何时更新HW？</strong></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Follower</span> 副本对象更新HW是在其更新本地LEO之后。</span><br><span class="line"></span><br><span class="line">一旦follower向本地日志写完数据后它就会尝试更新其HW值。</span><br><span class="line">算法为取本地LEO与FETCH response中HW值的较小值</span><br></pre></td></tr></table></figure><p><strong>（4）leader 副本对象何时更新HW？</strong></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Leader</span> 副本对象处理 Follower FETCH请求时在更新完<span class="built_in">leader</span> 端非自己副本对象的LEO后将尝试更新其自己HW值</span><br><span class="line"></span><br><span class="line">producer 端写入消息会更新<span class="built_in">leader</span> Replica的LEO</span><br><span class="line"></span><br><span class="line">副本被踢出ISR时</span><br><span class="line"></span><br><span class="line">某分区变更为<span class="built_in">leader</span>副本后</span><br></pre></td></tr></table></figure><p><strong>（5）两阶段请求定终身：</strong></p><p>第一次fetch请求仅获得了当前的数据，fetchOffset &lt; Leader LEO, 因为leader 端的非自己的副本leo 是根据fetch请求确定的，因此，只有第二次请求时，fetchOffset才会和Leader LEO相等，进而更新 leader HW ，进而响应为 leader HW，进而更新 Folloer HW。</p><h3 id="HW更新延迟带来的刀山火海"><a href="#HW更新延迟带来的刀山火海" class="headerlink" title="HW更新延迟带来的刀山火海"></a>HW更新延迟带来的刀山火海</h3><p><img src="https://user-gold-cdn.xitu.io/2018/11/23/1673c679cfaadb97?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>因为 fetchOffset是实实在在的需要位移。所以只有第二轮请求时，Follower才会在其现有位移的基础上，加1进行请求，从而连锁更新 会更新Leader非自己remoteLEO 和  Leader HW 和 Follower HW。</li><li>刀山火海就在一轮请求和第二轮请求之间发生了。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群Controller竞选与责任设计思路架构详解</title>
      <link href="/2018/09/21/kafka%E9%9B%86%E7%BE%A4Controller%E7%AB%9E%E9%80%89%E4%B8%8E%E8%B4%A3%E4%BB%BB%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3/"/>
      <url>/2018/09/21/kafka%E9%9B%86%E7%BE%A4Controller%E7%AB%9E%E9%80%89%E4%B8%8E%E8%B4%A3%E4%BB%BB%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="无所不能的Controller"><a href="#无所不能的Controller" class="headerlink" title="无所不能的Controller"></a>无所不能的Controller</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676dd7505afda5b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>某一个broker被选举出来承担特殊的角色，就是控制器Controller。</p></li><li><p>Leader会向zookeeper上注册Watcher，其他broker几乎不用监听zookeeper的状态变化。</p></li><li><p>Controller集群就是用来管理和协调Kafka集群的，具体就是管理集群中所有分区的状态和分区对应副本的状态。</p></li><li><p>每一个Kafka集群任意时刻都只能有一个controller，当集群启动的时候，所有的broker都会参与到controller的竞选，最终只能有一个broker胜出。</p></li><li><p>Controller维护的状态分为两类：1：管理每一台Broker上对应的分区副本。2：管理每一个Topic分区的状态。</p></li><li><p>KafkaController 核心代码，其中包含副本状态机和分区状态机</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaController</span>(<span class="params">val config : <span class="type">KafkaConfig</span>, zkClient: <span class="type">ZkClient</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">val brokerState: <span class="type">BrokerState</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.logIdent = <span class="string">"[Controller "</span> + config.brokerId + <span class="string">"]: "</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> isRunning = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> stateChangeLogger = <span class="type">KafkaController</span>.stateChangeLogger</span><br><span class="line">    <span class="keyword">val</span> controllerContext = <span class="keyword">new</span> <span class="type">ControllerContext</span>(zkClient, config.zkSessionTimeoutMs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> partitionStateMachine = <span class="keyword">new</span> <span class="type">PartitionStateMachine</span>(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">val</span> replicaStateMachine = <span class="keyword">new</span> <span class="type">ReplicaStateMachine</span>(<span class="keyword">this</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controllerElector = <span class="keyword">new</span> <span class="type">ZookeeperLeaderElector</span>(controllerContext, <span class="type">ZkUtils</span>.<span class="type">ControllerPath</span>, onControllerFailover,</span><br><span class="line">    onControllerResignation, config.brokerId)</span><br><span class="line">    <span class="comment">// have a separate scheduler for the controller to be able to start and stop independently of the</span></span><br><span class="line">    <span class="comment">// kafka server</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> autoRebalanceScheduler = <span class="keyword">new</span> <span class="type">KafkaScheduler</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">var</span> deleteTopicManager: <span class="type">TopicDeletionManager</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> offlinePartitionSelector = <span class="keyword">new</span> <span class="type">OfflinePartitionLeaderSelector</span>(controllerContext, config)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> reassignedPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ReassignedPartitionLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">PreferredReplicaPartitionLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> controlledShutdownPartitionLeaderSelector = <span class="keyword">new</span> <span class="type">ControlledShutdownLeaderSelector</span>(controllerContext)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> brokerRequestBatch = <span class="keyword">new</span> <span class="type">ControllerBrokerRequestBatch</span>(<span class="keyword">this</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> partitionReassignedListener = <span class="keyword">new</span> <span class="type">PartitionsReassignedListener</span>(<span class="keyword">this</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> preferredReplicaElectionListener = <span class="keyword">new</span> <span class="type">PreferredReplicaElectionListener</span>(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure></li><li><p>KafkaController中共定义了五种selector选举器</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、ReassignedPartitionLeaderSelector</span><br><span class="line">从可用的ISR中选取第一个作为<span class="built_in">leader</span>，把当前的ISR作为新的ISR，将重分配的副本集合作为接收LeaderAndIsr请求的副本集合。</span><br><span class="line"><span class="number">2</span>、PreferredReplicaPartitionLeaderSelector</span><br><span class="line">如果从assignedReplicas取出的第一个副本就是分区<span class="built_in">leader</span>的话，则抛出异常，否则将第一个副本设置为分区<span class="built_in">leader</span>。</span><br><span class="line"><span class="number">3</span>、ControlledShutdownLeaderSelector</span><br><span class="line">将ISR中处于关闭状态的副本从集合中去除掉，返回一个新新的ISR集合，然后选取第一个副本作为<span class="built_in">leader</span>，然后令当前AR作为接收LeaderAndIsr请求的副本。</span><br><span class="line"><span class="number">4</span>、NoOpLeaderSelector</span><br><span class="line">原则上不做任何事情，返回当前的<span class="built_in">leader</span>和isr。</span><br><span class="line"><span class="number">5</span>、OfflinePartitionLeaderSelector</span><br><span class="line">从活着的ISR中选择一个broker作为<span class="built_in">leader</span>，如果ISR中没有活着的副本，则从assignedReplicas中选择一个副本作为<span class="built_in">leader</span>，<span class="built_in">leader</span>选举成功后注册到Zookeeper中，并更新所有的缓存。</span><br></pre></td></tr></table></figure></li><li><p>kafka修改分区和副本数</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">..<span class="regexp">/bin/</span>kafka-topics.sh --zookeeper <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">2181</span> --describe  --topic test1</span><br><span class="line"></span><br><span class="line"><span class="string">Topic:</span>test1       <span class="string">PartitionCount:</span><span class="number">3</span>        <span class="string">ReplicationFactor:</span><span class="number">2</span>     <span class="string">Configs:</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">0</span>    <span class="string">Leader:</span> <span class="number">2</span>       <span class="string">Replicas:</span> <span class="number">2</span>,<span class="number">4</span>   <span class="string">Isr:</span> <span class="number">2</span>,<span class="number">4</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">1</span>    <span class="string">Leader:</span> <span class="number">3</span>       <span class="string">Replicas:</span> <span class="number">3</span>,<span class="number">5</span>   <span class="string">Isr:</span> <span class="number">3</span>,<span class="number">5</span></span><br><span class="line"><span class="string">Topic:</span> test1      <span class="string">Partition:</span> <span class="number">2</span>    <span class="string">Leader:</span> <span class="number">4</span>       <span class="string">Replicas:</span> <span class="number">4</span>,<span class="number">1</span>   <span class="string">Isr:</span> <span class="number">4</span>,<span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>topic 分区扩容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  <span class="string">./kafka-topics.sh</span> <span class="params">--zookeeper</span> 127.0.0.1<span class="function">:2181</span> -alter <span class="params">--partitions</span> 4 <span class="params">--topic</span> test1</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure></li></ul><h2 id="ReplicaStateMachine-（ZK持久化副本分配方案）"><a href="#ReplicaStateMachine-（ZK持久化副本分配方案）" class="headerlink" title="ReplicaStateMachine （ZK持久化副本分配方案）"></a>ReplicaStateMachine （ZK持久化副本分配方案）</h2><ul><li><p>Replica有7种状态:</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="string">NewReplica:</span> 在partition reassignment期间KafkaController创建New replica</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> <span class="string">OnlineReplica:</span> 当一个replica变为一个parition的assingned replicas时</span><br><span class="line">其状态变为OnlineReplica, 即一个有效的OnlineReplica</span><br><span class="line"></span><br><span class="line"><span class="number">3</span> Online状态的parition才能转变为leader或isr中的一员</span><br><span class="line"></span><br><span class="line"><span class="number">4</span> <span class="string">OfflineReplica:</span> 当一个broker down时, 上面的replica也随之die, 其状态转变为Onffline;</span><br><span class="line"><span class="string">ReplicaDeletionStarted:</span> 当一个replica的删除操作开始时,其状态转变为ReplicaDeletionStarted</span><br><span class="line"></span><br><span class="line"><span class="number">5</span> <span class="string">ReplicaDeletionSuccessful:</span> Replica成功删除后,其状态转变为ReplicaDeletionSuccessful</span><br><span class="line"></span><br><span class="line"><span class="number">6</span> <span class="string">ReplicaDeletionIneligible:</span> Replica成功失败后,其状态转变为ReplicaDeletionIneligible</span><br><span class="line"></span><br><span class="line"><span class="number">7</span> <span class="string">NonExistentReplica:</span>  Replica成功删除后, 从ReplicaDeletionSuccessful状态转变为NonExistentReplica状态</span><br></pre></td></tr></table></figure></li><li><p>ReplicaStateMachine 所在文件: core/src/main/scala/kafka/controller/ReplicaStateMachine.scala</p></li><li><p>startup: 启动ReplicaStateMachine</p></li><li><p>initializeReplicaState: 初始化每个replica的状态, 如果replica所在的broker是live状态,则此replica的状态为OnlineReplica。</p></li><li><p>处理可以转换到Online状态的Replica, handleStateChanges(controllerContext.allLiveReplicas(), OnlineReplica), 并且发送LeaderAndIsrRequest到各broker nodes: handleStateChanges(controllerContext.allLiveReplicas(), OnlineReplica)</p></li><li><p>当创建某个topic时，该topic下所有分区的所有副本都是NonExistent。</p></li><li><p>当controller加载Zookeeper中该topic每一个分区的所有副本信息到内存中，同时将副本的状态变更为New。</p></li><li><p>之后controller选择该分区副本列表中的第一个副本作为分区的leader副本并设置所有副本进入ISR，然后在Zookeeper中持久化该决定。</p></li><li><p>一旦确定了分区的Leader和ISR之后，controller会将这些消息以请求的方式发送给所有的副本。</p></li><li><p>同时将这些副本状态同步到集群的所有broker上以便让他们知晓。</p></li><li><p>最后controller 会把分区的所有副本状态设置为Online。</p></li></ul><h2 id="partitionStateMachine-（根据副本分配方案创建分区）"><a href="#partitionStateMachine-（根据副本分配方案创建分区）" class="headerlink" title="partitionStateMachine （根据副本分配方案创建分区）"></a>partitionStateMachine （根据副本分配方案创建分区）</h2><ul><li><p>Partition有如下四种状态</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">NonExistentPartition:</span> 这个partition还没有被创建或者是创建后又被删除了<span class="comment">;</span></span><br><span class="line"><span class="symbol">NewPartition:</span> 这个parition已创建, replicas也已分配好,但leader/isr还未就绪<span class="comment">;</span></span><br><span class="line"><span class="symbol">OnlinePartition:</span> 这个partition的leader选好<span class="comment">;</span></span><br><span class="line"><span class="symbol">OfflinePartition:</span> 这个partition的leader挂了,这个parition状态为OfflinePartition<span class="comment">;</span></span><br></pre></td></tr></table></figure></li><li><p>当创建Topic时，controller负责创建分区对象，它首先会短暂的将所有分区状态设置为NonExistent。</p></li><li><p>之后读取Zookeeper副本分配方案，然后令分区状态设置为NewPartion。</p></li><li><p>处于NewPartion状态的分区尚未有leader和ISR，因此Controller会初始化leader和ISR信息并设置分区状态为OnlinePartion，此时分区正常工作。</p></li></ul><h2 id="Controller职责所在-监听znode状态变化做执行"><a href="#Controller职责所在-监听znode状态变化做执行" class="headerlink" title="Controller职责所在(监听znode状态变化做执行)"></a>Controller职责所在(监听znode状态变化做执行)</h2><ul><li>UpdateMetadataRequest：更新元数据请求（比如：topic有多少个分区，每一个分区的leader在哪一台broker上以及分区的副本列表），随着集群的运行，这部分信息随时都可能变更，一旦发生变更，controller会将最新的元数据广播给所有存活的broker。具体方式就是给所有broker发送UpdateMetadataRequest请求</li><li>CreateTopics: 创建topic请求。当前不管是通过API方式、脚本方式（–create）抑或是CreateTopics请求方式来创建topic，做法几乎都是在Zookeeper的/brokers/topics下创建znode来触发创建逻辑，而controller会监听该path下的变更来执行真正的“创建topic”逻辑</li><li>DeleteTopics：删除topic请求。和CreateTopics类似，也是通过创建Zookeeper下的/admin/delete_topics/节点来触发删除topic，主要逻辑有：1：停止所有副本运行。2：删除所有副本的日志数据。3：移除zk上的 /admin/delete_topics/节点。</li><li>分区重分配：即kafka-reassign-partitions脚本做的事情。同样是与Zookeeper结合使用，脚本写入/admin/reassign_partitions节点来触发，controller负责按照方案分配分区。执行过程是：先扩展再伸缩机制（旧副本和新副本集合同时存在）。</li><li>Preferred leader分配：调整分区leader副本，preferred leader选举当前有两种触发方式：1. 自动触发(auto.leader.rebalance.enable = true)，controller会自动调整Preferred leader。2. kafka-preferred-replica-election脚本触发。两者步骤相同，都是向Zookeeper的/admin/preferred_replica_election写数据，controller提取数据执行preferred leader分配</li><li>分区扩展：即增加topic分区数。标准做法也是通过kafka-reassign-partitions脚本完成，不过用户可直接往Zookeeper中写数据来实现，比如直接把新增分区的副本集合写入到/brokers/topics/下，然后controller会为你自动地选出leader并增加分区</li><li>集群扩展：新增broker时Zookeeper中/brokers/ids下会新增znode，controller自动完成服务发现的工作</li><li>broker崩溃：同样地，controller通过Zookeeper可实时侦测broker状态。一旦有broker挂掉了，controller可立即感知并为受影响分区选举新的leader</li><li>ControlledShutdown：broker除了崩溃，还能“优雅”地退出。broker一旦自行终止，controller会接收到一个ControlledShudownRequest请求，然后controller会妥善处理该请求并执行各种收尾工作</li><li>Controller leader选举：controller必然要提供自己的leader选举以防这个全局唯一的组件崩溃宕机导致服务中断。这个功能也是通过Zookeeper的帮助实现的。</li></ul><h2 id="Controller与Broker之间的通信机制（NIO-select）"><a href="#Controller与Broker之间的通信机制（NIO-select）" class="headerlink" title="Controller与Broker之间的通信机制（NIO select）"></a>Controller与Broker之间的通信机制（NIO select）</h2><ul><li>controller启动时会为集群中的所有Broker创建一个专属的Socket连接，假如有100台broker机器，那么controller会创建100个Socket连接。新版本目前统一使用NIO select ，实际上还是要维护100个线程。</li></ul><h2 id="ControllerContext数据组件"><a href="#ControllerContext数据组件" class="headerlink" title="ControllerContext数据组件"></a>ControllerContext数据组件</h2><ul><li>controller的缓存，可谓是最重要的数据组件了，ControllerContext汇总了Zookeeper中关于kafka集群中所有元数据信息，是controller能够正确提供服务的基础。</li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676dd70918f80a5?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>kafka集群Controller主要通过ZK持久化副本分配方案，根据副本分配方案创建分区，监听ZK znode状态变化做执行处理，维护分区和副本ISR机制稳定运行。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka精确一次语义EOS的原理深入剖析</title>
      <link href="/2018/09/20/kafka%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1%E8%AF%AD%E4%B9%89EOS%E7%9A%84%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/"/>
      <url>/2018/09/20/kafka%E7%B2%BE%E7%A1%AE%E4%B8%80%E6%AC%A1%E8%AF%AD%E4%B9%89EOS%E7%9A%84%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h2 id="Kafka-0-11-0-0版本的逆天之作"><a href="#Kafka-0-11-0-0版本的逆天之作" class="headerlink" title="Kafka 0.11.0.0版本的逆天之作"></a>Kafka 0.11.0.0版本的逆天之作</h2><ul><li>0.11.0.0版本之前默认提供at least once语义，想象这样一种场景，分区的Leader副本所在的Broker成功的将消息写入本地磁盘，然后broker将发送响应给producer，此时假设网络出现故障导致该响应没有发送成功。此种情况下，Producer将认为消息发送请求失败，从而开启重试机制。若此时网络恢复正常，那么同一条消息将会被写入两次。</li><li>基于上述案例：0.11.0.0版本提供幂等性：每个分区中精确一次且有序</li><li>0.11.0.0版本提供事务：跨分区原子写入机制。</li></ul><h2 id="故障类型"><a href="#故障类型" class="headerlink" title="故障类型"></a>故障类型</h2><ul><li>broker可能故障：Kafka是一个高可用、持久化的系统，每一条写入一个分区的消息都会被持久化并且多副本备份（假设有n个副本）。所以，Kafka可以容忍n-1个broker故障，意味着一个分区只要至少有一个broker可用，分区就可用。Kafka的副本协议保证了只要消息被成功写入了主副本，它就会被复制到其他所有的可用副本（ISR）。</li><li>producer到broker的RPC调用可能失败：Kafka的持久性依赖于生产者接收broker的ack响应。没有接收成功ack不代表生产请求本身失败了。broker可能在写入消息后，发送ack给生产者的时候挂了。甚至broker也可能在写入消息前就挂了。由于生产者没有办法知道错误是什么造成的，所以它就只能认为消息没写入成功，并且会重试发送。在一些情况下，这会造成同样的消息在Kafka分区日志中重复，进而造成消费端多次收到这条消息。</li><li>客户端可能会故障：精确一次交付也必须考虑客户端故障。但是我们如何知道一个客户端已经故障而不是暂时和brokers断开，或者经历一个程序短暂的暂停，区分永久性故障和临时故障是很重要的，为了正确性，broker应该丢弃僵住的生产这发送来的消息，同样，也应该不向已经僵住的消费者发送消息。一旦一个新的客户端实例启动，它应该能够从失败的实例留下的任何状态中恢复，从一个安全点开始处理。这意味着，消费的偏移量必须始终与生产的输出保持同步。</li></ul><h2 id="Producer幂等性处理机制"><a href="#Producer幂等性处理机制" class="headerlink" title="Producer幂等性处理机制"></a>Producer幂等性处理机制</h2><ul><li>如果出现导致生产者重试的错误，同样的消息，仍由同样的生产者发送多次，将只被写到kafka broker的日志中一次。对于单个分区，幂等生产者不会因为生产者或broker故障而发送多条重复消息。</li><li>kafka保存序列号仅仅需要几个额外的字段，因此这种机制的开销非常低。</li><li>除了序列号，kafka会为每个Producer实例分配一个Producer id（PID）,每一条消息都会有序列号，并严格递增顺序。若发送的消息的序列号小于或者等于broker端保存的序列号，那么broker会拒绝这条消息的写入操作。</li><li>注意的是：当前的设计只能保证单个producer实例的EOS语义，无法实现多个Producer实例一块提供EOS语义。</li><li>想要开启这个特性，获得每个分区内的精确一次语义，也就是说没有重复，没有丢失，并且有序的语义，只需要设置producer配置中的”enable.idempotence=true”。</li></ul><h2 id="事务：跨分区原子写入"><a href="#事务：跨分区原子写入" class="headerlink" title="事务：跨分区原子写入"></a>事务：跨分区原子写入</h2><ul><li><p>事务：跨分区原子写入</p><p>将允许一个生产者发送一批到不同分区的消息，这些消息要么全部对任何一个消费者可见，要么对任何一个消费者都不可见。这个特性也允许你在一个事务中处理消费数据和提交消费偏移量，从而实现端到端的精确一次语义。</p></li><li><p>主要针对消息经过Partioner分区器到多个分区的情况。</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">producer</span><span class="selector-class">.initTransactions</span>();</span><br><span class="line"><span class="selector-tag">try</span> &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.beginTransaction</span>();</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.send</span>(record1);</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.send</span>(record2);</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.commitTransaction</span>();</span><br><span class="line">&#125; <span class="selector-tag">catch</span>(ProducerFencedException e) &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.close</span>();</span><br><span class="line">&#125; <span class="selector-tag">catch</span>(KafkaException e) &#123;</span><br><span class="line">  <span class="selector-tag">producer</span><span class="selector-class">.abortTransaction</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="消费端的事务支持"><a href="#消费端的事务支持" class="headerlink" title="消费端的事务支持"></a>消费端的事务支持</h2><ul><li><p>在消费者方面，有两种选择来读取事务性消息，通过隔离等级“isolation.level”消费者配置表示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read_commited：除了读取不属于事务的消息之外，还可以读取事务提交后的消息。</span><br><span class="line">read_uncommited：按照偏移位置读取所有消息，而不用等事务提交。这个选项类似Kafka消费者的当前语义。</span><br></pre></td></tr></table></figure></li><li><p>为了使用事务，需要配置消费者使用正确的隔离等级。</p></li><li><p>使用新版生产者，并且将生产者的“transactional . id”配置项设置为某个唯一ID。 需要此唯一ID来提供跨越应用程序重新启动的事务状态的连续性。</p></li></ul><h2 id="消费端精确到一次语义实现"><a href="#消费端精确到一次语义实现" class="headerlink" title="消费端精确到一次语义实现"></a>消费端精确到一次语义实现</h2><p>消费端精确到一次语义实现：consumer通过subscribe方法注册到kafka，精确一次的语义要求必须手动管理offset，按照下述步骤进行设置：</p><ul><li><p>1.设置enable.auto.commit = false;</p></li><li><p>2.处理完消息之后不要手动提交offset，</p></li><li><p>3.通过subscribe方法将consumer注册到某个特定topic，</p></li><li><p>4.实现ConsumerRebalanceListener接口和consumer.seek(topicPartition,offset)方法（读取特定topic和partition的offset）</p></li><li><p>5.将offset和消息一块存储，确保原子性，推荐使用事务机制。</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">ExactlyOnceDynamicConsumer</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> OffsetManager offsetManager = <span class="keyword">new</span> OffsetManager(<span class="string">"storage2"</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(<span class="params">String[] str</span>) throws InterruptedException</span> &#123;</span><br><span class="line"></span><br><span class="line">    System.<span class="keyword">out</span>.println(<span class="string">"Starting ManualOffsetGuaranteedExactlyOnceReadingDynamicallyBalancedPartitionConsumer ..."</span>);</span><br><span class="line"></span><br><span class="line">    readMessages();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  private static void readMessages() throws InterruptedException {</p><pre><code>KafkaConsumer&lt;String, String&gt; consumer = createConsumer();// Manually controlling offset but register consumer to topics to get dynamically assigned partitions.// Inside MyConsumerRebalancerListener use consumer.seek(topicPartition,offset) to control offsetconsumer.subscribe(Arrays.asList(&quot;normal-topic&quot;), new MyConsumerRebalancerListener(consumer));processRecords(consumer);</code></pre><p>  }</p></li></ul><pre><code>private static KafkaConsumer&lt;String, String&gt; createConsumer() {    Properties props = new Properties();    props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);    String consumeGroup = &quot;cg3&quot;;    props.put(&quot;group.id&quot;, consumeGroup);    props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;);    props.put(&quot;heartbeat.interval.ms&quot;, &quot;2000&quot;);    props.put(&quot;session.timeout.ms&quot;, &quot;6001&quot;);    * Control maximum data on each poll, make sure this value is bigger than the maximum single record size    props.put(&quot;max.partition.fetch.bytes&quot;, &quot;140&quot;);    props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);    props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);    return new KafkaConsumer&lt;String, String&gt;(props);}private static void processRecords(KafkaConsumer&lt;String, String&gt; consumer) {    while (true) {        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);        for (ConsumerRecord&lt;String, String&gt; record : records) {            System.out.printf(&quot;offset = %d, key = %s, value = %s\n&quot;, record.offset(), record.key(), record.value());            offsetManager.saveOffsetInExternalStore(record.topic(), record.partition(), record.offset());        }    }}public class MyConsumerRebalancerListener implements org.apache.kafka.clients.consumer.ConsumerRebalanceListener {private OffsetManager offsetManager = new OffsetManager(&quot;storage2&quot;);private Consumer&lt;String, String&gt; consumer;public MyConsumerRebalancerListener(Consumer&lt;String, String&gt; consumer) {    this.consumer = consumer;}public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) {    for (TopicPartition partition : partitions) {        offsetManager.saveOffsetInExternalStore(partition.topic(), partition.partition(), consumer.position(partition));    }}public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) {    for (TopicPartition partition : partitions) {        consumer.seek(partition, offsetManager.readOffsetFromExternalStore(partition.topic(), partition.partition()));    }}public class OffsetManager {  private String storagePrefix;  public OffsetManager(String storagePrefix) {      this.storagePrefix = storagePrefix;  }  void saveOffsetInExternalStore(String topic, int partition, long offset) {      try {          FileWriter writer = new FileWriter(storageName(topic, partition), false);          BufferedWriter bufferedWriter = new BufferedWriter(writer);          bufferedWriter.write(offset + &quot;&quot;);          bufferedWriter.flush();          bufferedWriter.close();      } catch (Exception e) {          e.printStackTrace();          throw new RuntimeException(e);      }  }  long readOffsetFromExternalStore(String topic, int partition) {      try {          Stream&lt;String&gt; stream = Files.lines(Paths.get(storageName(topic, partition)));          return Long.parseLong(stream.collect(Collectors.toList()).get(0)) + 1;      } catch (Exception e) {          e.printStackTrace();      }      return 0;  }  private String storageName(String topic, int partition) {      return storagePrefix + &quot;-&quot; + topic + &quot;-&quot; + partition;  }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群Broker端基于Reactor模式请求处理流程</title>
      <link href="/2018/09/20/kafka%E9%9B%86%E7%BE%A4Broker%E7%AB%AF%E5%9F%BA%E4%BA%8EReactor%E6%A8%A1%E5%BC%8F%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/"/>
      <url>/2018/09/20/kafka%E9%9B%86%E7%BE%A4Broker%E7%AB%AF%E5%9F%BA%E4%BA%8EReactor%E6%A8%A1%E5%BC%8F%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Reactor单线程案例代码"><a href="#Reactor单线程案例代码" class="headerlink" title="Reactor单线程案例代码"></a>Reactor单线程案例代码</h2><ul><li><p>如下是单线程的JAVA NIO编程模型。</p></li><li><p>首先服务端创建ServerSocketChannel对象，并注册到Select上OP_ACCEPT事件，然后ServerSocketChannel负责监听指定端口上的连接请求。</p></li><li><p>客户端一旦连接上ServerSocketChannel，就会触发Acceptor来处理OP_ACCEPT事件，并为来自客户端的连接创建Socket Channel，并设置为非阻塞模式，并在其Selector上注册OP_READ或者OP_WRITE，最终实现客户端与服务端的连接建立和数据通道打通。</p></li><li><p>当客户端向建立的SocketChannel发送请求时，服务端的Selector就会监听到OP_READ事件，并触发相应的处理逻辑。当服务端向客户端写数据时，会触发服务端Selector的OP_WRITE事件，从而执行响应的处理逻辑。</p></li><li><p>这里有一个明显的问题，就是所有时间的处理逻辑都是在Acceptor单线程完成的，在并发连接数较小，数据量较小的场景下，是没有问题的，但是……</p></li><li><p>Selector 允许一个单一的线程来操作多个 Channel. 如果我们的应用程序中使用了多个 Channel, 那么使用 Selector 很方便的实现这样的目的, 但是因为在一个线程中使用了多个 Channel, 因此也会造成了每个 Channel 传输效率的降低.</p></li><li><p>优化点在于：通道连接|读取或写入|业务处理均采用单线程来处理。通过线程池或者MessageQueue共享队列，进一步优化了高并发的处理要求，这样就解决了同一时间出现大量I/O事件时，单独的Select就可能在分发事件时阻塞（或延时），而成为瓶颈的问题。</p><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676e6d0547f52c4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioEchoServer</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">256</span>;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TIMEOUT = <span class="number">3000</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      <span class="comment">// 打开服务端 Socket</span></span><br><span class="line">      ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 打开 Selector</span></span><br><span class="line">      Selector selector = Selector.open();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 服务端 Socket 监听8080端口, 并配置为非阻塞模式</span></span><br><span class="line">      serverSocketChannel.socket().bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">8080</span>));</span><br><span class="line">      serverSocketChannel.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将 channel 注册到 selector 中.</span></span><br><span class="line">      <span class="comment">// 通常我们都是先注册一个 OP_ACCEPT 事件, 然后在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ</span></span><br><span class="line">      <span class="comment">// 注册到 Selector 中.</span></span><br><span class="line">      serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          <span class="comment">// 通过调用 select 方法, 阻塞地等待 channel I/O 可操作</span></span><br><span class="line">          <span class="keyword">if</span> (selector.select(TIMEOUT) == <span class="number">0</span>) &#123;</span><br><span class="line">              System.out.print(<span class="string">"."</span>);</span><br><span class="line">              <span class="keyword">continue</span>;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 获取 I/O 操作就绪的 SelectionKey, 通过 SelectionKey 可以知道哪些 Channel 的哪类 I/O 操作已经就绪.</span></span><br><span class="line">          Iterator&lt;SelectionKey&gt; keyIterator = selector.selectedKeys().iterator();</span><br><span class="line"></span><br><span class="line">          <span class="keyword">while</span> (keyIterator.hasNext()) &#123;</span><br><span class="line"></span><br><span class="line">              SelectionKey key = keyIterator.next();</span><br><span class="line"></span><br><span class="line">              <span class="comment">// 当获取一个 SelectionKey 后, 就要将它删除, 表示我们已经对这个 IO 事件进行了处理.</span></span><br><span class="line">              keyIterator.remove();</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isAcceptable()) &#123;</span><br><span class="line">                  <span class="comment">// 当 OP_ACCEPT 事件到来时, 我们就有从 ServerSocketChannel 中获取一个 SocketChannel,</span></span><br><span class="line">                  <span class="comment">// 代表客户端的连接</span></span><br><span class="line">                  <span class="comment">// 注意, 在 OP_ACCEPT 事件中, 从 key.channel() 返回的 Channel 是 ServerSocketChannel.</span></span><br><span class="line">                  <span class="comment">// 而在 OP_WRITE 和 OP_READ 中, 从 key.channel() 返回的是 SocketChannel.</span></span><br><span class="line">                  SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept();</span><br><span class="line">                  clientChannel.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">                  <span class="comment">//在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ 注册到 Selector 中.</span></span><br><span class="line">                  <span class="comment">// 注意, 这里我们如果没有设置 OP_READ 的话, 即 interest set 仍然是 OP_CONNECT 的话, 那么 select 方法会一直直接返回.</span></span><br><span class="line">                  clientChannel.register(key.selector(), OP_READ, ByteBuffer.allocate(BUF_SIZE));</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isReadable()) &#123;</span><br><span class="line">                  SocketChannel clientChannel = (SocketChannel) key.channel();</span><br><span class="line">                  ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                  <span class="keyword">long</span> bytesRead = clientChannel.read(buf);</span><br><span class="line">                  <span class="keyword">if</span> (bytesRead == -<span class="number">1</span>) &#123;</span><br><span class="line">                      clientChannel.close();</span><br><span class="line">                  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (bytesRead &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                      key.interestOps(OP_READ | SelectionKey.OP_WRITE);</span><br><span class="line">                      System.out.println(<span class="string">"Get data length: "</span> + bytesRead);</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (key.isValid() &amp;&amp; key.isWritable()) &#123;</span><br><span class="line">                  ByteBuffer buf = (ByteBuffer) key.attachment();</span><br><span class="line">                  buf.flip();</span><br><span class="line">                  SocketChannel clientChannel = (SocketChannel) key.channel();</span><br><span class="line"></span><br><span class="line">                  clientChannel.write(buf);</span><br><span class="line"></span><br><span class="line">                  <span class="keyword">if</span> (!buf.hasRemaining()) &#123;</span><br><span class="line">                      key.interestOps(OP_READ);</span><br><span class="line">                  &#125;</span><br><span class="line">                  buf.compact();</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Kafka-Reactor模式设计思路"><a href="#Kafka-Reactor模式设计思路" class="headerlink" title="Kafka Reactor模式设计思路"></a>Kafka Reactor模式设计思路</h2><ul><li><p>SelectionKey.OP_READ：Socket 读事件，以从远程发送过来了相应数据</p></li><li><p>SelectionKey.OP_WRITE：Socket写事件，即向远程发送数据</p></li><li><p>SelectionKey.OP_CONNECT：Socket连接事件，用来客户端同远程Server建立连接的时候注册到Selector，当连接建立以后，即对应的SocketChannel已经准备好了，用户可以从对应的key上取出SocketChannel.</p></li><li><p>SelectionKey.OP_ACCEPT：Socket连接接受事件，用来服务器端通过ServerSocketChannel绑定了对某个端口的监听，然后会让其SocketChannel对应的socket注册到服务端的Selector上，并关注该OP_ACCEPT事件。</p></li><li><p>Kafka的网络层入口类是SocketServer。我们知道，kafka.Kafka是Kafka Broker的入口类，kafka.Kafka.main()是Kafka Server的main()方法，即Kafka Broker的启动入口。我们跟踪代码，即沿着方法调用栈kafka.Kafka.main() -&gt; KafkaServerStartable() -&gt; KafkaServer().startup可以从main()方法入口一直跟踪到SocketServer即网络层对象的创建，这意味着Kafka Server启动的时候会初始化并启动SocketServer。</p></li><li><p>Acceptor的构造方法中，首先通过openServerSocket()打开自己负责的EndPoint的Socket，即打开端口并启动监听。然后，Acceptor会负责构造自己管理的一个或者多个Processor对象。其实，每一个Processor都是一个独立线程。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[kafka] <span class="class"><span class="keyword">class</span> <span class="title">Acceptor</span>(<span class="params">val endPoint: <span class="type">EndPoint</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 val sendBufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 val recvBufferSize: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                                 processors: <span class="type">Array</span>[<span class="type">Processor</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                                 connectionQuotas: <span class="type">ConnectionQuotas</span></span>) <span class="keyword">extends</span> <span class="title">AbstractServerThread</span>(<span class="params">connectionQuotas</span>) <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> nioSelector = <span class="type">NSelector</span>.open()</span><br><span class="line"> <span class="keyword">val</span> serverChannel = openServerSocket(endPoint.host, endPoint.port)<span class="comment">//创建一个ServerSocketChannel，监听endPoint.host, endPoint.port套接字</span></span><br><span class="line">    </span><br><span class="line"> <span class="comment">//Acceptor被构造的时候就会启动所有的processor线程</span></span><br><span class="line"> <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">   <span class="comment">//每个processor创建一个单独线程</span></span><br><span class="line">   processors.foreach &#123; processor =&gt;</span><br><span class="line">     <span class="type">Utils</span>.newThread(<span class="string">"kafka-network-thread-%d-%s-%d"</span>.format(brokerId, endPoint.protocolType.toString, processor.id), processor, <span class="literal">false</span>).start()</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>Acceptor线程的run()方法，是不断监听对应ServerChannel上的连接请求，如果有新的连接请求，就选择出一个Processor，用来处理这个请求，将这个新连接交付给Processor是在方法Acceptor.accept()</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]<span class="comment">//取出channel</span></span><br><span class="line">    <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()<span class="comment">//创建socketChannel，专门负责与这个客户端的连接</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//socketChannel参数设置</span></span><br><span class="line">      processor.accept(socketChannel)<span class="comment">//将SocketChannel交给process进行处理</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">//异常处理</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Processor.accept():</span></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Queue up a new connection for reading</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</span><br><span class="line">    newConnections.add(socketChannel)</span><br><span class="line">    wakeup()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>每一个Processor都维护了一个单独的KSelector对象，这个KSelector只负责这个Processor上所有channel的监听。这样最大程度上保证了不同Processor线程之间的完全并行和业务隔离，尽管，在异步IO情况下，一个Selector负责成百上千个socketChannel的状态监控也不会带来效率问题。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">   startupComplete()<span class="comment">//表示初始化流程已经结束，通过这个CountDownLatch代表初始化已经结束，这个Processor已经开始正常运行了</span></span><br><span class="line">   <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="comment">// setup any new connections that have been queued up</span></span><br><span class="line">       configureNewConnections()<span class="comment">//为已经接受的请求注册OR_READ事件</span></span><br><span class="line">       <span class="comment">// register any new responses for writing</span></span><br><span class="line">       processNewResponses()<span class="comment">//处理响应队列,这个响应队列是Handler线程处理以后的结果，会交付给RequestChannel.responseQueue.同时调用unmute，开始接受请求</span></span><br><span class="line">       poll()  <span class="comment">//调用KSelector.poll(),进行真正的数据读写</span></span><br><span class="line">       processCompletedReceives()<span class="comment">//调用mute，停止接受新的请求</span></span><br><span class="line">       processCompletedSends()</span><br><span class="line">       processDisconnected()</span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">       <span class="comment">//异常处理 略</span></span><br><span class="line">   &#125;</span><br><span class="line">    </span><br><span class="line">   debug(<span class="string">"Closing selector - processor "</span> + id)</span><br><span class="line">   swallowError(closeAll())</span><br><span class="line">   shutdownComplete()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>KSelector.register()方法，开始对远程客户端或者其它服务器的读请求(OP_READ)进行绑定和处理。KSelect.register()方法，会将服务端的SocketChannel注册到服务器端的nioSelector，并关注SelectionKey.OP_READ，即，如果发生读请求，可以取出对应的Channel进行处理。这里的Channel也是Kafka经过封装以后的KafkaChannel对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">register</span><span class="params">(String id, SocketChannel socketChannel)</span> <span class="keyword">throws</span> ClosedChannelException </span>&#123;</span><br><span class="line">        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_READ);</span><br><span class="line">        <span class="comment">//如果是SocketServer创建的这个对象并且是纯文本，则channelBuilder是@Code PlainTextChannelBuilder</span></span><br><span class="line">        KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize);<span class="comment">//构造一个KafkaChannel</span></span><br><span class="line">        key.attach(channel);<span class="comment">//将KafkaChannel对象attach到这个registration，以后可以通过调用SelectionKey.attachment()获得这个对象</span></span><br><span class="line">        <span class="keyword">this</span>.channels.put(id, channel);<span class="comment">//记录这个Channel</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>Processor.processCompletedReceives()通过遍历completedReceives，对于每一个已经完成接收的数据，对数据进行解析和封装，交付给RequestChannel，RequestChannel会交付给具体的业务处理层进行处理。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 将completedReceived中的对象进行封装，交付给requestQueue.completRequets</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processCompletedReceives</span></span>() &#123;</span><br><span class="line">  selector.completedReceives.asScala.foreach &#123; receive =&gt;<span class="comment">//每一个receive是一个NetworkReceivedui'xiagn</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//receive.source代表了这个请求的发送者的身份，KSelector保存了channel另一端的身份和对应的SocketChannel之间的对应关系</span></span><br><span class="line">      <span class="keyword">val</span> channel = selector.channel(receive.source)</span><br><span class="line">      <span class="keyword">val</span> session = <span class="type">RequestChannel</span>.<span class="type">Session</span>(<span class="keyword">new</span> <span class="type">KafkaPrincipal</span>(<span class="type">KafkaPrincipal</span>.<span class="type">USER_TYPE</span>, channel.principal.getName),</span><br><span class="line">        channel.socketAddress)</span><br><span class="line">      <span class="keyword">val</span> req = <span class="type">RequestChannel</span>.<span class="type">Request</span>(processor = id, connectionId = receive.source, session = session, buffer = receive.payload, startTimeMs = time.milliseconds, securityProtocol = protocol)</span><br><span class="line">      requestChannel.sendRequest(req)<span class="comment">//将请求通过RequestChannel.requestQueue交付给Handler</span></span><br><span class="line">      selector.mute(receive.source)<span class="comment">//不再接受Read请求,发送响应之前，不可以再接收任何请求</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">//异常处理 略</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676ed08083c5576?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>详情源码剖析请参考如下博客，讲解非常详细。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/zhanyuanlin/</span>article<span class="regexp">/details/</span><span class="number">76556578</span></span><br><span class="line">https:<span class="regexp">//</span>blog.csdn.net<span class="regexp">/zhanyuanlin/</span>article<span class="regexp">/details/</span><span class="number">76906583</span></span><br></pre></td></tr></table></figure></li><li><p>RequestChannel 负责消息从网络层转接到业务层，以及将业务层的处理结果交付给网络层进而返回给客户端。每一个SocketServer只有一个RequestChannel对象，在SocketServer中构造。RequestChannel构造方法中初始化了requestQueue，用来存放网络层接收到的请求，这些请求即将交付给业务层进行处理。同时，初始化了responseQueues，为每一个Processor建立了一个response队列，用来存放这个Processor的一个或者多个Response，这些response即将交付给网络层返回给客户端。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RequestChannel,有totalProcessorThreads个responseQueue队列，</span></span><br><span class="line">    <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(totalProcessorThreads, maxQueuedRequests)</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val numProcessors: <span class="type">Int</span>, val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> responseListeners: <span class="type">List</span>[(<span class="type">Int</span>) =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></span><br><span class="line">    <span class="comment">//request存放了所有Processor接收到的远程请求，负责把requestQueue中的请求交付给具体业务逻辑进行处理</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Request</span>](queueSize)</span><br><span class="line">    <span class="comment">//responseQueues存放了所有Processor的带出来的response，即每一个Processor都有一个response queue</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> responseQueues = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]](numProcessors)</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numProcessors) <span class="comment">//初始化responseQueues</span></span><br><span class="line">      responseQueues(i) = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">RequestChannel</span>.<span class="type">Response</span>]()</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//一些metrics用来监控request和response的数量，代码略</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>KafkaApis是Kafka的API接口层，可以理解为一个工具类，职责就是解析请求然后获取请求类型，根据请求类型将请求交付给对应的业务层</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            val requestChannel: <span class="type">RequestChannel</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            val apis: <span class="type">KafkaApis</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                            numThreads: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">      </span><br><span class="line">        <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">"RequestHandlerAvgIdlePercent"</span>, <span class="string">"percent"</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">this</span>.logIdent = <span class="string">"[Kafka Request Handler on Broker "</span> + brokerId + <span class="string">"], "</span></span><br><span class="line">        <span class="keyword">val</span> threads = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Thread</span>](numThreads)</span><br><span class="line">        <span class="comment">//初始化由KafkaRequestHandler线程构成的线程数组</span></span><br><span class="line">        <span class="keyword">val</span> runnables = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</span><br><span class="line">        <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">          runnables(i) = <span class="keyword">new</span> <span class="type">KafkaRequestHandler</span>(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis)</span><br><span class="line">          threads(i) = <span class="type">Utils</span>.daemonThread(<span class="string">"kafka-request-handler-"</span> + i, runnables(i))</span><br><span class="line">          threads(i).start()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>KafkaRequestHandler.run()方法，就是不断从requestQueue中取出请求，调用API层业务处理逻辑进行处理</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">   <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">var</span> req : <span class="type">RequestChannel</span>.<span class="type">Request</span> = <span class="literal">null</span></span><br><span class="line">       <span class="keyword">while</span> (req == <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="comment">//略</span></span><br><span class="line">       req = requestChannel.receiveRequest(<span class="number">300</span>)<span class="comment">//从RequestChannel.requestQueue中取出请求</span></span><br><span class="line">       <span class="comment">//略</span></span><br><span class="line">       apis.handle(req)<span class="comment">//调用KafkaApi.handle()，将请求交付给业务</span></span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;&#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="参数调优设置"><a href="#参数调优设置" class="headerlink" title="参数调优设置"></a>参数调优设置</h2><ul><li>numProcessorThreads：通过num.network.threads进行配置，单个Acceptor所管理的Processor对象的数量。</li><li>maxQueuedRequests：通过queued.max.requests进行配置，请求队列所允许的最大的未响应请求的数量，用来给ConnectionQuotas进行请求限额控制，避免Kafka Server产生过大的网络负载；</li><li>totalProcessorThreads：计算方式为numProcessorThreads * endpoints.size，即单台机器总的Processor的数量；</li><li>maxConnectionsPerIp：配置项为max.connections.per.ip，单个IP上的最大连接数，用来给ConnectionQuotas控制连接数；</li><li>num.io.threads:表示KafkaRequestHander实际从队列中获取请求进行执行的线程数，默认是8个。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>通过Acceptor、Processor、RequestChannel、KafkaRequestHandler以及KafkaApis多个角色的解析，完成了整个Kafka的消息流通闭环，即从客户端建立连接、发送请求给Kafka Server的Acceptor进行处理，进一步交由Processor、Kafka Server将请求交付给KafkaRequestHandler具体业务进行处理、业务将处理结果返回给网络层、网络层将结果通过NIO返回给客户端。</li><li>由于多Processor线程、以及KafkaRequestHandlerPoll线程池的存在，通过交付-获取的方式而不是阻塞等待的方式，让整个消息处理实现完全的异步化，各个角色各司其职，模块之间无耦合，线程之间或者相互竞争任务，或者被上层安排处理部分任务，整个效率非常高，结构也相当清晰</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka rebalance机制与Consumer多种消费模式案例</title>
      <link href="/2018/09/19/kafka%E9%9B%86%E7%BE%A4Producer%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/"/>
      <url>/2018/09/19/kafka%E9%9B%86%E7%BE%A4Producer%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Producer端基本数据结构"><a href="#Producer端基本数据结构" class="headerlink" title="Producer端基本数据结构"></a>Producer端基本数据结构</h2><ul><li><p>ProducerRecord: 一个ProducerRecord表示一条待发送的消息记录，主要由5个字段构成：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">topic          所属topic</span><br><span class="line"><span class="built_in">partition</span>      所属分区</span><br><span class="line"><span class="built_in">key</span>            键值</span><br><span class="line">value          消息体</span><br><span class="line">timestamp      时间戳</span><br></pre></td></tr></table></figure></li><li><p>RecordMetadata: Kafka服务器端返回给客户端的消息的元数据信息,前3项相对比较重要，Producer端可以使用这些消息做一些消息发送成功之后的处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">offset                   该条消息的位移</span><br><span class="line">timestamp                消息时间戳</span><br><span class="line">topic + partition        所属topic的分区</span><br><span class="line"><span class="keyword">checksum</span>                 消息<span class="keyword">CRC32</span>码</span><br><span class="line">serializedKeySize        序列化后的消息键字节数</span><br><span class="line">serializedValueSize      序列化后的消息体字节数</span><br></pre></td></tr></table></figure></li></ul><h2 id="Producer端消息发送流程"><a href="#Producer端消息发送流程" class="headerlink" title="Producer端消息发送流程"></a>Producer端消息发送流程</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/2/1676fa43d76b5554?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li><p>在send()的发送消息动作触发之前，通过props属性中指定的servers连接到broker集群，从Zookeeper收集集群Metedata信息，从而了解哪些broker掌管哪一个Topic的哪一个partition，以及brokers的健康状态。</p></li><li><p>下面就是流水线操作，ProducerRecord对象携带着topic，partition，message等信息，在Serializer这个“车间”被序列化。</p></li><li><p>序列化过后的ProducerRecord对象进入Partitioner“车间”，按照上文所述的Partitioning 策略决定这个消息将被分配到哪个Partition中。</p></li><li><p>确定partition的ProducerRecord进入一个缓冲区，通过减少IO来提升性能，在这个“车间”，消息被按照TopicPartition信息进行归类整理，相同Topic且相同parition的ProducerRecord被放在同一个RecordBatch中，等待被发送。什么时候发送？都在Producer的props中被指定了，有默认值，显然我们可以自己指定。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>) batch<span class="selector-class">.size</span>:设置每个RecordBatch可以缓存的最大字节数 </span><br><span class="line">(<span class="number">2</span>) buffer<span class="selector-class">.memory</span>:设置所有RecordBatch的总共最大字节数 </span><br><span class="line">(<span class="number">3</span>) linger.ms设置每个RecordBatch的最长延迟发送时间 </span><br><span class="line">(<span class="number">4</span>) max<span class="selector-class">.block</span><span class="selector-class">.ms</span> 设置每个RecordBatch的最长阻塞时间</span><br></pre></td></tr></table></figure></li><li><p>一旦，当单个RecordBatch的linger.ms延迟到达或者batch.size达到上限，这个 RecordBatch会被立即发送。另外，如果所有RecordBatch作为一个整体，达到了buffer.memroy或者max.block.ms上限，所有的RecordBatch都会被发送。</p></li><li><p>ProducerRecord消息按照分配好的Partition发送到具体的broker中,broker接收保存消息，更新Metadata信息，同步给Zookeeper。</p></li><li><p>Producer端其他优化点：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">5</span>) acks：Producer的数据确认阻塞设置，<span class="number">0</span>表示不管任何响应，只管发，发完了立即执行下个任务，这种方式最快，但是很不保险。<span class="number">1</span>表示只确保leader成功响应，接收到数据。<span class="number">2</span>表示确保leader及其所有follwer成功接收保存消息，也可以用”all”。</span><br><span class="line">(<span class="number">6</span>) retries：消息发送失败重试的次数。</span><br><span class="line">(<span class="number">7</span>) retry<span class="selector-class">.backoff</span><span class="selector-class">.ms</span>：失败补偿时间，每次失败重试的时间间隔，不可设置太短，避免第一条消息的响应还没返回，第二条消息又发出去了，造成逻辑错误。</span><br><span class="line">(<span class="number">8</span>) max<span class="selector-class">.in</span><span class="selector-class">.flight</span><span class="selector-class">.request</span><span class="selector-class">.per</span><span class="selector-class">.connection</span>：同一时间，每个Producer能够发送的消息上限。</span><br><span class="line">(<span class="number">9</span>) compression<span class="selector-class">.type</span>  producer所使用的压缩器，目前支持gzip, snappy和lz4。压缩是在用户主线程完成的，通常都需要花费大量的CPU时间，但对于减少网络IO来说确实利器。生产环境中可以结合压力测试进行适当配置</span><br></pre></td></tr></table></figure></li></ul><h2 id="消息缓冲区-accumulator-再剖析"><a href="#消息缓冲区-accumulator-再剖析" class="headerlink" title="消息缓冲区(accumulator)再剖析"></a>消息缓冲区(accumulator)再剖析</h2><ul><li><p>producer创建时会创建一个默认32MB(由buffer.memory参数指定)的accumulator缓冲区，专门保存待发送的消息。</p></li><li><p>该数据结构中还包含了一个特别重要的集合信息：消息批次信息(batches)。该集合本质上是一个HashMap，里面分别保存了每个topic分区下的batch队列，即前面说的批次是按照topic分区进行分组的。这样发往不同分区的消息保存在对应分区下的batch队列中。</p></li><li><p>假设消息M1, M2被发送到test的0分区但属于不同的batch，M3分送到test的1分区，那么batches中包含的信息就是：{“test-0” -&gt; [batch1, batch2], “test-1” -&gt; [batch3]}</p></li><li><p>每个batch中最重要的3个组件包括：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">compressor: 负责执行追加写入操作</span><br><span class="line"><span class="keyword">batch</span>缓冲区：由<span class="keyword">batch</span>.size参数控制，消息被真正追加写入到的地方</span><br><span class="line">thunks：保存消息回调逻辑的集合</span><br></pre></td></tr></table></figure><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fac83237a750?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p></li><li><p>Sender线程自KafkaProducer创建后就一直都在运行着 。它的工作流程基本上是这样的：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)不断轮询缓冲区寻找已做好发送准备的分区 </span><br><span class="line">(<span class="number">2</span>)将轮询获得的各个batch按照目标分区所在的leader broker进行分组</span><br><span class="line">(<span class="number">3</span>)将分组后的batch通过底层创建的Socket连接发送给各个broker</span><br><span class="line">(<span class="number">4</span>)等待服务器端发送response回来</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fb148a5190e4?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>Sender线程会发送PRODUCE请求给对应的broker，broker处理完毕之后发送对应的PRODUCE response。一旦Sender线程接收到response将依次(按照消息发送顺序)调用batch中的回调方法</li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1676fb1b3a94adb8?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Sender线程自KafkaProducer创建后就一直都在运行着，单个RecordBatch的linger.ms延迟到达或者batch.size达到上限，作为后台线程就会检测到立即发送。</li><li>accumulator缓冲器按照Topic partion进行分组，来进行集中向某一个Broker发送。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka rebalance机制与Consumer多种消费模式案例</title>
      <link href="/2018/09/17/kafka%20rebalance%20%E6%9C%BA%E5%88%B6%E4%B8%8EConsumer%E5%A4%9A%E7%A7%8D%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F%E6%A1%88%E4%BE%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/"/>
      <url>/2018/09/17/kafka%20rebalance%20%E6%9C%BA%E5%88%B6%E4%B8%8EConsumer%E5%A4%9A%E7%A7%8D%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F%E6%A1%88%E4%BE%8B%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h2 id="rebalance-何时触发？到底干嘛？流程如何？"><a href="#rebalance-何时触发？到底干嘛？流程如何？" class="headerlink" title="rebalance 何时触发？到底干嘛？流程如何？"></a>rebalance 何时触发？到底干嘛？流程如何？</h2><h3 id="reblance-何时触发"><a href="#reblance-何时触发" class="headerlink" title="reblance 何时触发"></a>reblance 何时触发</h3><ul><li>组订阅发生变更，比如基于正则表达式订阅，当匹配到新的topic创建时，组的订阅就会发生变更。</li><li>组的topic分区数发生变更，通过命令行脚本增加了订阅topic的分区数。</li><li>组成员发生变更：新加入组以及离开组。</li></ul><h3 id="reblance-到底干嘛"><a href="#reblance-到底干嘛" class="headerlink" title="reblance 到底干嘛"></a>reblance 到底干嘛</h3><p>一句话：多个Consumer订阅了一个Topic时，根据分区策略进行消费者订阅分区的重分配</p><h3 id="Coordinator-到底在那个Broker"><a href="#Coordinator-到底在那个Broker" class="headerlink" title="Coordinator 到底在那个Broker"></a>Coordinator 到底在那个Broker</h3><p>找到Coordinator的算法 与 找到_consumer_offsets目标分区的算法是一致的。</p><ul><li>第一步：确定目标分区：Math.abs(groupId.hashCode)%50，假设是12。</li><li>第二步：找到_consumer_offsets分区为10的Leader副本所在的Broker，那么该broker即为Group Coordinator。</li></ul><h3 id="reblance-流程如何"><a href="#reblance-流程如何" class="headerlink" title="reblance 流程如何"></a>reblance 流程如何</h3><p>reblance 流程流程整体如下图所示，值得强调的几点如下：</p><ul><li><p>Coordinator的角色由Broker端担任。</p></li><li><p>Group Leader 的角色主要有Consumer担任。</p></li><li><p>加入组请求（JoinGroup）=&gt;作用在于选择Group Leader。</p></li><li><p>同步组请求（SyncGroup）=&gt;作用在于确定分区分配方案给Coordinator，把方案响应给所有Consumer。</p></li></ul><h3 id="reblance-机制的好处"><a href="#reblance-机制的好处" class="headerlink" title="reblance 机制的好处"></a>reblance 机制的好处</h3><ul><li>分区分配权利下放给客户端consumer，因此系统不用重启，既可以实现分区策略的变更。</li><li>用户可以自行实现机架感知分配方案。</li></ul><h3 id="reblance-generation-过滤无用请求"><a href="#reblance-generation-过滤无用请求" class="headerlink" title="reblance generation 过滤无用请求"></a>reblance generation 过滤无用请求</h3><ul><li>kafka引入 reblance generation ，就是为了防止Consumer group的无效Offset提交。若因为某些原因，consumer延迟提交了Offset，而该consumer被踢出了消费组，那么该Consumer再次提交位移时，携带的就是旧的generation了。</li></ul><h2 id="reblance-监听器应用级别实战"><a href="#reblance-监听器应用级别实战" class="headerlink" title="reblance 监听器应用级别实战"></a>reblance 监听器应用级别实战</h2><ul><li><p>reblance 监听器解决用户   把位移提交到外部存储的情况，在监听器中实现位移保存和位移的重定向。</p></li><li><p>onPartitionsRevoked :  rebalance开启新一轮的重平衡前会调用，一般用于手动提交位移，及审计功能</p></li><li><p>onPartitionsAssigned ：rebalance在重平衡结束后会调用，一般用于消费逻辑处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"> props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line"> props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line"> props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line"> props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"> props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"> KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"> </span><br><span class="line"> 统计rebalance总时长</span><br><span class="line"> <span class="keyword">final</span> AtomicLong totalRebalanceTimeMs =<span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"> </span><br><span class="line"> 统计rebalance开始时刻</span><br><span class="line"> <span class="keyword">final</span> AtomicLong rebalanceStart =<span class="keyword">new</span> AtomicLong(<span class="number">0L</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> <span class="number">1</span> 重平衡监听</span><br><span class="line"> consumer.subscribe(Arrays.asList(<span class="string">"test-topic"</span>), <span class="keyword">new</span> ConsumerRebalanceListener()&#123;</span><br><span class="line">     </span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(TopicPartition tp : partitions)&#123;</span><br><span class="line">        </span><br><span class="line">            <span class="number">1</span> 保存到外部存储</span><br><span class="line">            saveToExternalStore(consumer.position(tp)) </span><br><span class="line">            </span><br><span class="line">            <span class="number">2</span> 手动提交位移</span><br><span class="line">            <span class="comment">//consumer.commitSync(toCommit);</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        rebalanceStart.set(System.currentTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    totalRebalanceTimeMs.addAndGet(System.currentTimeMillis()-rebalanceStart.get())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : partitions) &#123;</span><br><span class="line">           </span><br><span class="line">           consumer.seek(tp，readFromExternalStore(tp))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> &#125;);</span><br><span class="line"></span><br><span class="line"><span class="number">2</span> 消息处理</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"> ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line"> <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">     buffer.add(record);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">     insertIntoDb(buffer);</span><br><span class="line">     consumer.commitSync();</span><br><span class="line">     buffer.clear();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h2 id="Consumer组内消息均衡实战"><a href="#Consumer组内消息均衡实战" class="headerlink" title="Consumer组内消息均衡实战"></a>Consumer组内消息均衡实战</h2><h3 id="Consumer-单线程封装，实现多个消费者来消费（浪费资源）"><a href="#Consumer-单线程封装，实现多个消费者来消费（浪费资源）" class="headerlink" title="Consumer 单线程封装，实现多个消费者来消费（浪费资源）"></a>Consumer 单线程封装，实现多个消费者来消费（浪费资源）</h3><p>实例主题：</p><ul><li>ConsumerGroup 实现组封装</li><li>ConsumerRunnable 每个线程维护私有的KafkaConsumer实例</li></ul><hr><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span>  <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> void main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line">        <span class="keyword">String</span> brokerList = <span class="string">"localhost:9092"</span>;</span><br><span class="line">        <span class="keyword">String</span> groupId = <span class="string">"testGroup1"</span>;</span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"test-topic"</span>;</span><br><span class="line">        int consumerNum = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">        核心对外封装</span><br><span class="line">        ConsumerGroup consumerGroup = <span class="keyword">new</span> <span class="type">ConsumerGroup</span>(consumerNum, groupId, topic, brokerList);</span><br><span class="line">        consumerGroup.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerGroup</span> </span>&#123;    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;ConsumerRunnable&gt; consumers;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> ConsumerGroup(int consumerNum, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic, <span class="keyword">String</span> brokerList) &#123;</span><br><span class="line">        consumers = <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(consumerNum);</span><br><span class="line">        <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; consumerNum; ++i) &#123;</span><br><span class="line">            ConsumerRunnable consumerThread = <span class="keyword">new</span> <span class="type">ConsumerRunnable</span>(brokerList, groupId, topic);</span><br><span class="line">            consumers.add(consumerThread);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> void execute() &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRunnable task : <span class="type">consumers</span>) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>(task).start();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class ConsumerRunnable implements Runnable &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> final KafkaConsumer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; consumer;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> ConsumerRunnable(<span class="keyword">String</span> brokerList, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic) &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);        <span class="comment">//本例使用自动提交位移</span></span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        <span class="keyword">this</span>.consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));   <span class="comment">// 本例使用分区副本自动分配策略</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">run</span>() &#123;</span><br><span class="line">        <span class="built_in">while</span> (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; records = consumer.poll(<span class="number">200</span>);   </span><br><span class="line">            <span class="built_in">for</span> (ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record : records) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(Thread.currentThread().getName() + <span class="string">" consumed "</span> + record.partition() +</span><br><span class="line">                        <span class="string">"th message with offset: "</span> + record.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="一个Consumer，内部实现多线程消费（consumer压力过大）"><a href="#一个Consumer，内部实现多线程消费（consumer压力过大）" class="headerlink" title="一个Consumer，内部实现多线程消费（consumer压力过大）"></a>一个Consumer，内部实现多线程消费（consumer压力过大）</h3><p>实例主题：</p><ul><li>ConsumerHandler 单一的Consumer实例，poll后里面会跑一个线程池，执行多个Processor线程来处理</li><li>Processor 业务逻辑处理方法</li></ul><p>进一步优化建议；</p><ul><li>ConsumerHandler 设置手动提交位移，负责最终位移提交consumer.commitSync();。</li><li>ConsumerHandler设置一个全局的Map&lt;TopicPartion,OffsetAndMetadata&gt; offsets，来管理Processor消费的位移。</li><li>Processor 负责批处理完消息后，得到消息的最大位移，并更新offsets数组</li><li>ConsumerHandler 根据 offsets，位移提交后会清空offsets集合。</li><li>ConsumerHandler设置重平衡监听</li></ul><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> class Main &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">String</span> brokerList = <span class="string">"localhost:9092,localhost:9093,localhost:9094"</span>;</span><br><span class="line">        <span class="keyword">String</span> groupId = <span class="string">"group2"</span>;</span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"test-topic"</span>;</span><br><span class="line">        <span class="keyword">int</span> workerNum = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">        ConsumerHandler consumers = <span class="keyword">new</span> ConsumerHandler(brokerList, groupId, topic);</span><br><span class="line">        consumers.execute(workerNum);</span><br><span class="line">        <span class="built_in">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000000</span>);</span><br><span class="line">        &#125; <span class="built_in">catch</span> (InterruptedException ignored) &#123;&#125;</span><br><span class="line">        consumers.<span class="built_in">shutdown</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ArrayBlockingQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ThreadPoolExecutor;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class ConsumerHandler &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> final KafkaConsumer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; consumer;</span><br><span class="line">    <span class="keyword">private</span> ExecutorService executors;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> ConsumerHandler(<span class="keyword">String</span> brokerList, <span class="keyword">String</span> groupId, <span class="keyword">String</span> topic) &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        consumer.subscribe(Arrays.asList(topic));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> execute(<span class="keyword">int</span> workerNum) &#123;</span><br><span class="line">    </span><br><span class="line">        executors = <span class="keyword">new</span> ThreadPoolExecutor(workerNum, workerNum, <span class="number">0</span>L, TimeUnit.MILLISECONDS,</span><br><span class="line">                <span class="keyword">new</span> ArrayBlockingQueue&lt;&gt;(<span class="number">1000</span>), <span class="keyword">new</span> ThreadPoolExecutor.CallerRunsPolicy());</span><br><span class="line">                </span><br><span class="line">        <span class="built_in">while</span> (true) &#123;</span><br><span class="line">            ConsumerRecords&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; records = consumer.poll(<span class="number">200</span>);</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">for</span> (final ConsumerRecord record : records) &#123;</span><br><span class="line">                executors.submit(<span class="keyword">new</span> Processor(record));</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="built_in">shutdown</span>() &#123;</span><br><span class="line">        <span class="built_in">if</span> (consumer != null) &#123;</span><br><span class="line">            consumer.<span class="built_in">close</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">if</span> (executors != null) &#123;</span><br><span class="line">            executors.<span class="built_in">shutdown</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">try</span> &#123;</span><br><span class="line">            <span class="built_in">if</span> (!executors.awaitTermination(<span class="number">10</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line">                System.out.<span class="built_in">println</span>(<span class="string">"Timeout.... Ignore for this case"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="built_in">catch</span> (InterruptedException ignored) &#123;</span><br><span class="line">            System.out.<span class="built_in">println</span>(<span class="string">"Other thread interrupted this shutdown, ignore for this case."</span>);</span><br><span class="line">            Thread.currentThread().interrupt();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Processor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ConsumerRecord&lt;String, String&gt; consumerRecord;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Processor</span><span class="params">(ConsumerRecord record)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.consumerRecord = record;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(Thread.currentThread().getName() + <span class="string">" consumed "</span> + consumerRecord.partition()</span><br><span class="line">            + <span class="string">"th message with offset: "</span> + consumerRecord.offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="方案对比"><a href="#方案对比" class="headerlink" title="方案对比"></a>方案对比</h3><ul><li>第一种方案：建议采用 Consumer 单线程封装，实现多个消费者来消费（浪费资源），这样能很好地保证分区内消费的顺序，同时也没有线程切换的开销。</li><li>第二种方案：实现复杂，问题在于可能无法维护分区内的消息顺序，注意消息处理和消息接收解耦了。</li></ul><h3 id="Consumer指定分区消费案例实战（Standalone-Consumer）"><a href="#Consumer指定分区消费案例实战（Standalone-Consumer）" class="headerlink" title="Consumer指定分区消费案例实战（Standalone  Consumer）"></a>Consumer指定分区消费案例实战（Standalone  Consumer）</h3><ul><li><p>Standalone Consumer  assign 用于接收指定分区列表的消息和Subscribe是矛盾的。只能二选一。</p></li><li><p>多个 Consumer 实例消费一个 Topic 借助于 group reblance可谓是天作之合。</p></li><li><p>若要精准控制，assign逃不了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">poperties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">List&lt;TopicPartion&gt; partitions = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">List&lt;PartitionInfo&gt;  allPartitions = consumer.partitionsFor(<span class="string">"kaiXinTopic"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(allPartitions != <span class="keyword">null</span> &amp;&amp; !allPartitions.isEmpty)&#123;</span><br><span class="line">    <span class="keyword">for</span>(PartitionInfo partitionInfo : allPartitions)&#123;</span><br><span class="line">    </span><br><span class="line">        partitions.add(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(),partitionInfo.partition()))</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    consumer.assign(partitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">   ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">   <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">       buffer.add(record);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">       insertIntoDb(buffer);</span><br><span class="line">       </span><br><span class="line">       consumer.commitSync();</span><br><span class="line">       </span><br><span class="line">       buffer.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka Poll轮询机制与消费者组的重平衡分区策略剖析</title>
      <link href="/2018/09/16/kafka%20Poll%E8%BD%AE%E8%AF%A2%E6%9C%BA%E5%88%B6%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E9%87%8D%E5%B9%B3%E8%A1%A1%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E5%89%96%E6%9E%90/"/>
      <url>/2018/09/16/kafka%20Poll%E8%BD%AE%E8%AF%A2%E6%9C%BA%E5%88%B6%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E9%87%8D%E5%B9%B3%E8%A1%A1%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E5%89%96%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>注意本文采用最新版本进行Kafka的内核原理剖析，新版本每一个Consumer通过独立的线程，来管理多个Socket连接，即同时与多个broker通信实现消息的并行读取。这就是新版的技术革新。类似于Linux I/O模型或者Select NIO 模型。</p><h2 id="Poll为什么要设置一个超时参数"><a href="#Poll为什么要设置一个超时参数" class="headerlink" title="Poll为什么要设置一个超时参数"></a>Poll为什么要设置一个超时参数</h2><ul><li>条件：</li><li>1：获取足够多的可用数据</li><li>2：等待时间超过指定的超时时间。</li><li>目的在于让Consumer主线程定期的””苏醒”去做其他事情。比如：定期的执行常规任务，（比如写日志，写库等）。</li><li>获取消息，然后执行业务逻辑。</li></ul><h2 id="位移精度"><a href="#位移精度" class="headerlink" title="位移精度"></a>位移精度</h2><ul><li>最少一次 -&gt;  消息会被重复处理</li><li>最多一次 -&gt;  消息会丢失，但不会被重复处理。</li><li>精确一次 -&gt; 一定会被处理，且也只会处理一次。</li></ul><h2 id="位移角色"><a href="#位移角色" class="headerlink" title="位移角色"></a>位移角色</h2><ul><li>上次提交位移 ：last committed offset</li><li>当前位置 ：current position</li><li>水位 ： High watermark</li><li>日志终端位移： （Log End Offset）</li></ul><h2 id="位移管理"><a href="#位移管理" class="headerlink" title="位移管理"></a>位移管理</h2><p>consumer的位移提交最终会向group coordinator来提交，不过这里重点需要重新说明一下：组协调者coordinator负责管理所有的Consumer实例。而且coordinator运行在broker上（通过选举出某个broker），不过请注意新版本coordinator只负责做组管理。</p><h4 id="但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。"><a href="#但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。" class="headerlink" title="但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。"></a>但是具体的reblance分区分配策略目前已经交由Consumer客户端。这样就解耦了组管理和分区分配。</h4><p>权利下放的优势：</p><ul><li>如果需要分配就貌似需要重启整个kafka集群。</li><li>在Consumer端可以定制分区分配策略。</li><li>每一个consumer位移提交时，都会向_consumer_offsets对应的分区上追加写入一条消息。如果某一个consumer为同一个group的同一个topic同一个分区提交多次位移，很显然我们只关心最新一次提交的位移。</li></ul><h2 id="reblance的触发条件"><a href="#reblance的触发条件" class="headerlink" title="reblance的触发条件"></a>reblance的触发条件</h2><ul><li>组订阅发生变更，比如基于正则表达式订阅，当匹配到新的topic创建时，组的订阅就会发生变更。</li><li>组的topic分区数发生变更，通过命令行脚本增加了订阅topic的分区数。</li><li>组成员发生变更：新加入组以及离开组。</li></ul><h2 id="reblance-分配策略"><a href="#reblance-分配策略" class="headerlink" title="reblance 分配策略"></a>reblance 分配策略</h2><h3 id="range分区分配策略"><a href="#range分区分配策略" class="headerlink" title="range分区分配策略"></a>range分区分配策略</h3><p>举例如下：一个拥有十个分区（0,1,2…..,9）的topic，相同group拥有三个consumerid为a,b,c的消费者：</p><ul><li><p>consumer a分配对应的分区号为[0,4),即0，1，2，3前面四个分区</p></li><li><p>consumer b 分配对应分区4，5，6中间三个分区</p></li><li><p>consumer c 分配对应分区7，8，9最后三个分区。</p><p>class RangeAssignor() extends PartitionAssignor with Logging {</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign</span></span>(ctx: <span class="type">AssignmentContext</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> valueFactory = (topic: <span class="type">String</span>) =&gt; <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicAndPartition</span>, <span class="type">ConsumerThreadId</span>]</span><br><span class="line">  <span class="keyword">val</span> partitionAssignment =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">TopicAndPartition</span>, <span class="type">ConsumerThreadId</span>]](<span class="type">Some</span>(valueFactory))</span><br><span class="line">  <span class="keyword">for</span> (topic &lt;- ctx.myTopicThreadIds.keySet) &#123;</span><br><span class="line">    <span class="keyword">val</span> curConsumers = ctx.consumersForTopic(topic)</span><br><span class="line">    <span class="keyword">val</span> curPartitions: <span class="type">Seq</span>[<span class="type">Int</span>] = ctx.partitionsForTopic(topic)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> nPartsPerConsumer = curPartitions.size / curConsumers.size</span><br><span class="line">    <span class="keyword">val</span> nConsumersWithExtraPart = curPartitions.size % curConsumers.size</span><br><span class="line"></span><br><span class="line">    info(<span class="string">"Consumer "</span> + ctx.consumerId + <span class="string">" rebalancing the following partitions: "</span> + curPartitions +</span><br><span class="line">      <span class="string">" for topic "</span> + topic + <span class="string">" with consumers: "</span> + curConsumers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (consumerThreadId &lt;- curConsumers) &#123;</span><br><span class="line">      <span class="keyword">val</span> myConsumerPosition = curConsumers.indexOf(consumerThreadId)</span><br><span class="line">      assert(myConsumerPosition &gt;= <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> startPart = nPartsPerConsumer * myConsumerPosition + myConsumerPosition.min(nConsumersWithExtraPart)</span><br><span class="line">      <span class="keyword">val</span> nParts = nPartsPerConsumer + (<span class="keyword">if</span> (myConsumerPosition + <span class="number">1</span> &gt; nConsumersWithExtraPart) <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       *   Range-partition the sorted partitions to consumers for better locality.</span></span><br><span class="line"><span class="comment">       *  The first few consumers pick up an extra partition, if any.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">if</span> (nParts &lt;= <span class="number">0</span>)</span><br><span class="line">        warn(<span class="string">"No broker partitions consumed by consumer thread "</span> + consumerThreadId + <span class="string">" for topic "</span> + topic)</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- startPart until startPart + nParts) &#123;</span><br><span class="line">          <span class="keyword">val</span> partition = curPartitions(i)</span><br><span class="line">          info(consumerThreadId + <span class="string">" attempting to claim partition "</span> + partition)</span><br><span class="line">          <span class="comment">// record the partition ownership decision</span></span><br><span class="line">          <span class="keyword">val</span> assignmentForConsumer = partitionAssignment.getAndMaybePut(consumerThreadId.consumer)</span><br><span class="line">          assignmentForConsumer += (<span class="type">TopicAndPartition</span>(topic, partition) -&gt; consumerThreadId)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>源码剖析如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">curConsumers=(a,b,c)</span><br><span class="line">curPartitions=(0,1,2,3,4,5,6,7,8,9)</span><br><span class="line">nPartsPerConsumer=10/3  =3</span><br><span class="line">nConsumersWithExtraPart=10%3  =1</span><br><span class="line"></span><br><span class="line"><span class="section">a:</span></span><br><span class="line">myConsumerPosition= curConsumers.indexof(a) =0</span><br><span class="line">startPart= 3*0+0.min(1) = 0</span><br><span class="line">nParts = 3+(if (0 + 1 &gt; 1) 0 <span class="keyword">else</span> 1)=3+1=4</span><br><span class="line"><span class="section">b:</span></span><br><span class="line">myConsumerPosition=1</span><br><span class="line"><span class="section">c:</span></span><br><span class="line">myConsumerPosition</span><br></pre></td></tr></table></figure></li></ul><h3 id="round-robin分区分配策略"><a href="#round-robin分区分配策略" class="headerlink" title="round-robin分区分配策略"></a>round-robin分区分配策略</h3><p>如果同一个消费组内所有的消费者的订阅信息都是相同的，那么RoundRobinAssignor策略的分区分配会是均匀的。举例如下：假设消费组中有2个消费者C0和C1，都订阅了主题topic0 和 topic1，并且每个主题都有3个分区，进行hashCode 排序 后，顺序为：topic0_0、topic0_1、topic0_2、topic1_0、topic1_1、topic1_2。最终的分配结果为：</p><p>消费者consumer0：topic0_0、topic0_2 、 topic1_1</p><p>消费者consumer1：topic0_1、topic1_0、 topic1_2</p><p>使用RoundRobin策略有两个前提条件必须满足：</p><ul><li>同一个Consumer Group里面的所有消费者的num.streams必须相等；</li><li>每个消费者订阅的主题必须相同。</li></ul><p>所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，最后按照round-robin风格将分区分别分配给不同的消费者线程。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> allTopicPartitions = ctx.partitionsForTopic.flatMap &#123; <span class="keyword">case</span>(topic, partitions) =&gt;</span><br><span class="line">  info(<span class="string">"Consumer %s rebalancing the following partitions for topic %s: %s"</span></span><br><span class="line">       .format(ctx.consumerId, topic, partitions))</span><br><span class="line">  partitions.map(partition =&gt; &#123;</span><br><span class="line">    <span class="type">TopicAndPartition</span>(topic, partition)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;.toSeq.sortWith((topicPartition1, topicPartition2) =&gt; &#123;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending</span></span><br><span class="line"><span class="comment">   * up on one consumer (if it has a high enough stream count).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  topicPartition1.toString.hashCode &lt; topicPartition2.toString.hashCode</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h3 id="StickyAssignor分区分配策略（摘录）"><a href="#StickyAssignor分区分配策略（摘录）" class="headerlink" title="StickyAssignor分区分配策略（摘录）"></a>StickyAssignor分区分配策略（摘录）</h3><ul><li>分区的分配要尽可能的均匀；</li><li>分区的分配尽可能的与上次分配的保持相同。当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂很多。</li></ul><p>假设消费组内有3个消费者：C0、C1和C2，它们都订阅了4个主题：t0、t1、t2、t3，并且每个主题有2个分区，也就是说整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p1、t3p0</span><br><span class="line">消费者C1：t0p1、t2p0、t3p1</span><br><span class="line">消费者C2：t1p0、t2p1</span><br></pre></td></tr></table></figure><p>假设此时消费者C1脱离了消费组，那么消费组就会执行再平衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor策略，那么此时的分配结果如下：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p0、t2p0、t3p0</span><br><span class="line">消费者C2：t0p1、t1p1、t2p1、t3p1</span><br></pre></td></tr></table></figure><p>RoundRobinAssignor策略会按照消费者C0和C2进行重新轮询分配。而如果此时使用的是StickyAssignor策略，那么分配结果为：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消费者C0：t0p0、t1p1、t3p0、t2p0</span><br><span class="line">消费者C2：t1p0、t2p1、t0p1、t3p1</span><br></pre></td></tr></table></figure><p>可以看到分配结果中保留了上一次分配中对于消费者C0和C2的所有分配结果，并将原来消费者C1的“负担”分配给了剩余的两个消费者C0和C2，最终C0和C2的分配仍然保持了均衡。</p><p>如果发生分区重分配，那么对于同一个分区而言有可能之前的消费者和新指派的消费者不是同一个，对于之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor策略如同其名称中的“sticky”一样，让分配策略具备一定的“粘性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗以及其它异常情况的发生。</p><h2 id="reblance-generation-（代代不同）"><a href="#reblance-generation-（代代不同）" class="headerlink" title="reblance generation （代代不同）"></a>reblance generation （代代不同）</h2><p>主要作用在于防止无效的offset提交，原因在于若上一届的consumer成员因为某些原因延迟提交了offset,同时被踢出group组，那么新一届的group组成员分区分配结束后，老一届的consumer再次提交老的offset就会出问题。因此采用reblance generation ，老的请求就会被拒绝。</p><h2 id="reblance-扫尾工作"><a href="#reblance-扫尾工作" class="headerlink" title="reblance 扫尾工作"></a>reblance 扫尾工作</h2><p>每一次reblance操作之前，都会检查用户是否设置了自动提交位移，如果是，则帮助用户提交。如没有设置，会在监听器中回调用户的提交程序。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka之Producer同步与异步消息发送及事务幂等性案例</title>
      <link href="/2018/09/15/kafka%E4%B9%8BProducer%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E5%8F%8A%E4%BA%8B%E5%8A%A1%E5%B9%82%E7%AD%89%E6%80%A7%E6%A1%88%E4%BE%8B/"/>
      <url>/2018/09/15/kafka%E4%B9%8BProducer%E5%90%8C%E6%AD%A5%E4%B8%8E%E5%BC%82%E6%AD%A5%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E5%8F%8A%E4%BA%8B%E5%8A%A1%E5%B9%82%E7%AD%89%E6%80%A7%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h2><p>kafka的Producer是线程安全的，用户可以非常非常放心的在多线程中使用。</p><p>但是官方建议：通常情况下，一个线程维护一个kafka 的producer的效率会更高。</p><h2 id="Producer-消息发送流程"><a href="#Producer-消息发送流程" class="headerlink" title="Producer 消息发送流程"></a>Producer 消息发送流程</h2><ul><li>第一步：封装ProducerRecord</li><li>第二步：分区器Partioner进行数据路由，选择某一个Topic分区。如果没有指定key，消息会被均匀的分配到所有分区。</li><li>第三步：确定好分区，就会找分区对应的leader，接下来就是副本同步机制。</li></ul><h2 id="Producer官方实例"><a href="#Producer官方实例" class="headerlink" title="Producer官方实例"></a>Producer官方实例</h2><h3 id="Fire-and-Fogret案例-（无所谓心态）"><a href="#Fire-and-Fogret案例-（无所谓心态）" class="headerlink" title="Fire and Fogret案例 （无所谓心态）"></a>Fire and Fogret案例 （无所谓心态）</h3><ul><li><h5 id="发送之后便不再理会发送结果"><a href="#发送之后便不再理会发送结果" class="headerlink" title="发送之后便不再理会发送结果"></a>发送之后便不再理会发送结果</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"> props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line"> props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line"> props.put(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line"> props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line"> props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line"> props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line"> props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"> props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line"> Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">     producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"my-topic"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line"></span><br><span class="line"> producer.close();</span><br></pre></td></tr></table></figure></li></ul><h3 id="异步回调官方案例-（不阻塞）"><a href="#异步回调官方案例-（不阻塞）" class="headerlink" title="异步回调官方案例 （不阻塞）"></a>异步回调官方案例 （不阻塞）</h3><ul><li><h5 id="JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。"><a href="#JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。" class="headerlink" title="JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。"></a>JavaProducer的send方法会返回一个JavaFuture对象供用户稍后获取发送结果。这就是回调机制。</h5></li><li><p>Fully non-blocking usage can make use of the Callback parameter to provide a callback that will be invoked when the request is complete.</p></li><li><p>RecordMetadata 和 Exception 不可能同时为空，消息发送成功时，Exception为null，消息发送失败时，metadata为空。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">byte</span>[],<span class="keyword">byte</span>[]&gt;(<span class="string">"the-topic"</span>, key, value);</span><br><span class="line"></span><br><span class="line">producer.send(myRecord,</span><br><span class="line">              <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception e)</span> </span>&#123;</span><br><span class="line">                      <span class="keyword">if</span>(e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                         e.printStackTrace();</span><br><span class="line">                      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                         System.out.println(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset());</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;);</span><br></pre></td></tr></table></figure></li></ul><h3 id="同步发送官方案例-（阻塞）"><a href="#同步发送官方案例-（阻塞）" class="headerlink" title="同步发送官方案例 （阻塞）"></a>同步发送官方案例 （阻塞）</h3><ul><li><p>通过 producer.send（record)返回Future对象，通过调用Future.get()进行无限等待结果返回。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">producer.send（<span class="built_in">record</span>).<span class="keyword">get</span>()</span><br></pre></td></tr></table></figure></li></ul><h3 id="基于事务发送官方案例-（原子性和幂等性）"><a href="#基于事务发送官方案例-（原子性和幂等性）" class="headerlink" title="基于事务发送官方案例 （原子性和幂等性）"></a>基于事务发送官方案例 （原子性和幂等性）</h3><ul><li><p>From Kafka 0.11, the KafkaProducer supports two additional modes: the idempotent producerand the transactional producer. The idempotent producer strengthens Kafka’s deliverysemantics from at least once to exactly once delivery. In particular producer retries willno longer introduce duplicates. The transactional producer allows an application to sendmessages to multiple partitions (and topics!) atomically.</p></li><li><p>To enable idempotence, the enable.idempotence configuration must be set to true. If set,the retries config will default to Integer.MAX_VALUE and the acks config will default to all.There are no API changes for the idempotent producer, so existing applications will not need to be modified to take advantage of this feature.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"transactional.id"</span>, <span class="string">"my-transactional-id"</span>);</span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props, <span class="keyword">new</span> StringSerializer(), <span class="keyword">new</span> StringSerializer());</span><br><span class="line"></span><br><span class="line">producer.initTransactions();</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    producer.beginTransaction();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">        producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my-topic"</span>, Integer.toString(i), Integer.toString(i)));</span><br><span class="line">    producer.commitTransaction();</span><br><span class="line">&#125; <span class="keyword">catch</span> (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line">    <span class="comment">// We can't recover from these exceptions, so our only option is to close the producer and exit.</span></span><br><span class="line">    producer.close();</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">    <span class="comment">// For all other exceptions, just abort the transaction and try again.</span></span><br><span class="line">    producer.abortTransaction();</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure></li><li><p>As is hinted at in the example, there can be only one open transaction per producer. All messages sent between the beginTransaction() and commitTransaction() calls will be part of a single transaction. When the <a href="https://link.juejin.im?target=http%3A%2F%2Ftransactional.id" target="_blank" rel="noopener">transactional.id</a> is specified, all messages sent by the producer must be part of a transaction.</p></li></ul><h3 id="可重试异常（继承RetriableException）"><a href="#可重试异常（继承RetriableException）" class="headerlink" title="可重试异常（继承RetriableException）"></a>可重试异常（继承RetriableException）</h3><ul><li>LeaderNotAvailableException :分区的Leader副本不可用，这可能是换届选举导致的瞬时的异常，重试几次就可以恢复</li><li>NotControllerException:Controller主要是用来选择分区副本和每一个分区leader的副本信息，主要负责统一管理分区信息等，也可能是选举所致。</li><li>NetWorkerException :瞬时网络故障异常所致。</li></ul><h3 id="不可重试异常"><a href="#不可重试异常" class="headerlink" title="不可重试异常"></a>不可重试异常</h3><ul><li>SerializationException:序列化失败异常</li><li>RecordToolLargeException:消息尺寸过大导致。</li></ul><h3 id="异常的区别对待"><a href="#异常的区别对待" class="headerlink" title="异常的区别对待"></a>异常的区别对待</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">producer.send(myRecord,</span><br><span class="line">              <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception e)</span> </span>&#123;</span><br><span class="line">                      <span class="keyword">if</span>(e ==<span class="keyword">null</span>)&#123;</span><br><span class="line">                          <span class="comment">//正常处理逻辑</span></span><br><span class="line">                          System.out.println(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset()); </span><br><span class="line">                          </span><br><span class="line">                      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                              </span><br><span class="line">                            <span class="keyword">if</span>(e <span class="keyword">instanceof</span> RetriableException) &#123;</span><br><span class="line">                               <span class="comment">//处理可重试异常</span></span><br><span class="line">                               ......</span><br><span class="line">                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                               <span class="comment">//处理不可重试异常</span></span><br><span class="line">                               ......</span><br><span class="line">                            &#125;</span><br><span class="line">                      &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;);</span><br></pre></td></tr></table></figure><h3 id="Producer的绅士关闭"><a href="#Producer的绅士关闭" class="headerlink" title="Producer的绅士关闭"></a>Producer的绅士关闭</h3><ul><li>producer.close()：优先把消息处理完毕，优雅退出。</li><li>producer.close(timeout): 超时时，强制关闭。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用ogg实现oracle到kafka的增量数据实时同步</title>
      <link href="/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"/>
      <url>/2018/09/13/%E5%88%A9%E7%94%A8ogg%E5%AE%9E%E7%8E%B0oracle%E5%88%B0kafka%E7%9A%84%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Oracle里存储的结构化数据导出到Hadoop体系做离线计算是一种常见数据处置手段。近期有场景需要做Oracle到Kafka的实时导入，这里以此案例进行介绍。</p><p>ogg即Oracle GoldenGate是Oracle的同步工具，本文讲如何配置ogg以实现Oracle数据库增量数据实时同步到kafka中，其中同步消息格式为json。</p><p>下面是我的源端和目标端的一些配置信息：</p><table><thead><tr><th align="center">-</th><th align="center">版本</th><th align="center">OGG版本</th><th align="center">ip</th><th align="center">主机名</th></tr></thead><tbody><tr><td align="center">源端</td><td align="center">OracleRelease 11.2.0.1.0</td><td align="center">Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64</td><td align="center">192.168.23.167</td><td align="center">cdh01</td></tr><tr><td align="center">目标端</td><td align="center">kafka_2.11-0.11.0.1</td><td align="center">Oracle GoldenGate for Big Data 12.3.0.1.0 on Linux x86-64</td><td align="center">192.168.23.168</td><td align="center">cdh02</td></tr></tbody></table><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>注意：源端和目标端的文件不一样，目标端需要下载Oracle GoldenGate for Big Data,源端需要下载Oracle GoldenGate for Oracle具体下载方法见最后的附录截图。</p><p>目标端在<a href="http://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html" target="_blank" rel="noopener">这里</a>查询下载，源端在<a href="https://edelivery.oracle.com/osdc/faces/SoftwareDelivery" target="_blank" rel="noopener">旧版本</a>查询下载。</p><h2 id="源端（Oracle）配置"><a href="#源端（Oracle）配置" class="headerlink" title="源端（Oracle）配置"></a>源端（Oracle）配置</h2><p>注意：源端是创建了oracle用户且安装了oracle数据库，oracle环境变量之前都配置好了</p><p>（后面只要涉及到源端均在oracle用户下操作）</p><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><p>先建立ogg目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /oracledata/data/ogg</span><br><span class="line">unzip Oracle GoldenGate_11.2.1.0.3.zip</span><br></pre></td></tr></table></figure><p>解压后得到一个tar包，再解压这个tar</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /oracledata/data/ogg</span><br></pre></td></tr></table></figure><h3 id="配置ogg环境变量"><a href="#配置ogg环境变量" class="headerlink" title="配置ogg环境变量"></a>配置ogg环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export OGG_HOME=/oracledata/data/ogg</span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/usr/lib</span><br><span class="line">export PATH=$OGG_HOME:$PATH</span><br></pre></td></tr></table></figure><p>使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><p>如果命令成功即可进行下一步，不成功请检查前面的步骤。</p><h3 id="oracle打开归档模式"><a href="#oracle打开归档模式" class="headerlink" title="oracle打开归档模式"></a>oracle打开归档模式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 以DBA身份连接数据库</span></span><br><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>执行下面的命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><p>若显示如下，则说明当前未开启归档模式</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Database <span class="built_in">log</span> <span class="built_in">mode</span>       No Archive <span class="built_in">Mode</span></span><br><span class="line">Automatic archival       Disabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online <span class="built_in">log</span> sequence     <span class="number">12</span></span><br><span class="line">Current <span class="built_in">log</span> sequence       <span class="number">14</span></span><br></pre></td></tr></table></figure><p>手动打开即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 立即关闭数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动实例并加载数据库，但不打开</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup mount</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更改数据库为归档模式</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database archivelog;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 打开数据库</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用自动归档</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system archive <span class="built_in">log</span> start;</span></span><br></pre></td></tr></table></figure><p>再执行一下命令查看当前是否为归档模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> archive <span class="built_in">log</span> list</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Database log mode       Archive Mode</span><br><span class="line">Automatic archival       Enabled</span><br><span class="line">Archive destination       USE_DB_RECOVERY_FILE_DEST</span><br><span class="line">Oldest online log sequence     12</span><br><span class="line">Next log sequence to archive   14</span><br><span class="line">Current log sequence       14</span><br></pre></td></tr></table></figure><p>可以看到为Enabled，则成功打开归档模式。</p><h3 id="Oracle打开日志相关"><a href="#Oracle打开日志相关" class="headerlink" title="Oracle打开日志相关"></a>Oracle打开日志相关</h3><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">NO</span>     <span class="literal">NO</span></span><br></pre></td></tr></table></figure><p>若为NO，则需要通过命令修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database force logging;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database add supplemental <span class="built_in">log</span> data;</span></span><br></pre></td></tr></table></figure><p>再查看一下为YES即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select force_logging, supplemental_log_data_min from v<span class="variable">$database</span>;</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">FORCE_</span> <span class="string">SUPPLEMENTAL_LOG</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-----</span> <span class="bullet">----------------</span></span><br><span class="line"><span class="literal">YES</span>    <span class="literal">YES</span></span><br></pre></td></tr></table></figure><p>上述操作只是开启了最小补充日志，如果要抽取全部字段需要开启全列补充日志,否则值为null的字段不会在抽取日志中显示！！！</p><p>补充日志开启命令参考：<a href="https://blog.csdn.net/aaron8219/article/details/16825963" target="_blank" rel="noopener">https://blog.csdn.net/aaron8219/article/details/16825963</a></p><p><strong>注：开启全列补充日志会导致磁盘快速增长，LGWR进程繁忙，不建议使用。大家可根据自己的情况使用。</strong></p><p>查看数据库是否开启了全列补充日志</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;  </span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">NO</span></span><br></pre></td></tr></table></figure><p>若未开启可以通过以下命令开启。</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; alter database add supplemental log data(all) columns;</span><br><span class="line"></span><br><span class="line">Database altered.</span><br><span class="line"></span><br><span class="line">SQL&gt; select supplemental<span class="emphasis">_log_</span>data<span class="emphasis">_all from v$database;</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">SUPPLE</span></span><br><span class="line"><span class="emphasis">------</span></span><br><span class="line"><span class="emphasis">YES</span></span><br></pre></td></tr></table></figure><h3 id="oracle创建复制用户"><a href="#oracle创建复制用户" class="headerlink" title="oracle创建复制用户"></a>oracle创建复制用户</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p <span class="regexp">/oracledata/</span>data<span class="regexp">/tablespace/</span>dbsrv2</span><br></pre></td></tr></table></figure><p>然后执行下面sql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create tablespace oggtbs datafile <span class="string">'/oracledata/data/tablespace/dbsrv2/oggtbs01.dbf'</span> size 1000M autoextend on;</span></span><br><span class="line">控制台显示的内容：Tablespace created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">  create user ogg identified by 123456 default tablespace oggtbs;</span></span><br><span class="line">控制台显示的内容：User created.</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to ogg;</span></span><br><span class="line">控制台显示的内容：Grant succeeded.</span><br></pre></td></tr></table></figure><h3 id="OGG初始化"><a href="#OGG初始化" class="headerlink" title="OGG初始化"></a>OGG初始化</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">GGSCI (cdh01) 1&gt; create subdirs</span><br></pre></td></tr></table></figure><p>控制台显示的内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Creating subdirectories under current directory /oracledata/data/ogg</span><br><span class="line"></span><br><span class="line">Parameter files                /oracledata/data/ogg/dirprm: created</span><br><span class="line">Report files                   /oracledata/data/ogg/dirrpt: created</span><br><span class="line">Checkpoint files               /oracledata/data/ogg/dirchk: created</span><br><span class="line">Process status files           /oracledata/data/ogg/dirpcs: created</span><br><span class="line">SQL script files               /oracledata/data/ogg/dirsql: created</span><br><span class="line">Database definitions files     /oracledata/data/ogg/dirdef: created</span><br><span class="line">Extract data files             /oracledata/data/ogg/dirdat: created</span><br><span class="line">Temporary files                /oracledata/data/ogg/dirtmp: created</span><br><span class="line">Stdout files                   /oracledata/data/ogg/dirout: created</span><br></pre></td></tr></table></figure><h3 id="Oracle创建测试表"><a href="#Oracle创建测试表" class="headerlink" title="Oracle创建测试表"></a>Oracle创建测试表</h3><p>创建一个用户,在该用户下新建测试表，用户名、密码、表名均为 test_ogg。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create user test_ogg identified by test_ogg default tablespace users;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> grant dba to test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> conn test_ogg/test_ogg;</span></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> create table test_ogg(id int,name varchar(20),sex varchar(4),primary key(id));</span></span><br></pre></td></tr></table></figure><h2 id="目标端（kafka）配置"><a href="#目标端（kafka）配置" class="headerlink" title="目标端（kafka）配置"></a>目标端（kafka）配置</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -<span class="selector-tag">p</span> /data/apps/ogg</span><br><span class="line">unzip OGG_BigData_12.<span class="number">3.0</span>.<span class="number">1.0</span>_Release.zip</span><br><span class="line">tar xf ggs_Adapters_Linux_x64<span class="selector-class">.tar</span>  -C /data/apps/ogg</span><br></pre></td></tr></table></figure><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> /etc/<span class="keyword">profile</span></span><br></pre></td></tr></table></figure><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=<span class="regexp">/opt/java</span><span class="regexp">/jdk1.8.0_211</span></span><br><span class="line"><span class="regexp">export PATH=$JAVA_HOME/bin</span>:$PATH</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/<span class="class"><span class="keyword">lib</span>/<span class="title">dt</span>.<span class="title">jar</span>:$<span class="title">JAVA_HOME</span>/<span class="title">lib</span>/<span class="title">tools</span>.<span class="title">jar</span></span></span><br><span class="line"></span><br><span class="line">export OGG_HOME=<span class="regexp">/data/apps</span><span class="regexp">/ogg</span></span><br><span class="line"><span class="regexp">export LD_LIBRARY_PATH=$JAVA_HOME/jre</span><span class="regexp">/lib/amd</span>64:$JAVA_HOME/jre/<span class="class"><span class="keyword">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">libjsig</span>.<span class="title">so</span>:$<span class="title">JAVA_HOME</span>/<span class="title">jre</span>/<span class="title">lib</span>/<span class="title">amd64</span>/<span class="title">server</span>/<span class="title">libjvm</span>.<span class="title">so</span>:$<span class="title">OGG_HOME</span>/<span class="title">lib</span></span></span><br><span class="line">export PATH=$<span class="symbol">OGG_HOME:</span>$PATH</span><br></pre></td></tr></table></figure><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">source</span> <span class="regexp">/etc/</span>profile</span><br></pre></td></tr></table></figure><p>同样测试一下ogg命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><h3 id="初始化目录"><a href="#初始化目录" class="headerlink" title="初始化目录"></a>初始化目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create subdirs</span><br></pre></td></tr></table></figure><h2 id="OGG源端配置"><a href="#OGG源端配置" class="headerlink" title="OGG源端配置"></a>OGG源端配置</h2><p>Oracle实时传输到Hadoop集群（HDFS，<a href="http://lib.csdn.net/base/hive" target="_blank" rel="noopener">Hive</a>，Kafka等）的基本原理如图：<br><img src="https://mc.qcloudimg.com/static/img/dd548277beb41f51d0e5914dccda9134/image.png" alt="img"><br>根据如上原理，配置大概分为如下步骤：源端目标端配置ogg管理器（mgr）；源端配置extract进程进行Oracle日志抓取；源端配置pump进程传输抓取内容到目标端；目标端配置replicate进程复制日志到Kafka集群。</p><h3 id="配置OGG的全局变量"><a href="#配置OGG的全局变量" class="headerlink" title="配置OGG的全局变量"></a>配置OGG的全局变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">ggsci</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 1&gt; dblogin userid ogg password 123456</span><br><span class="line">控制台显示的内容：Successfully logged into database.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 2&gt; edit param ./globals</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">oggschema ogg</span></span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr"><a href="#配置管理器mgr" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">3</span>&gt; edit param mgr</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><p>说明：PORT即mgr的默认监听端口；</p><p>DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；</p><p>AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；</p><p>PURGEOLDEXTRACTS即TRAIL文件的定期清理</p><h3 id="添加复制表"><a href="#添加复制表" class="headerlink" title="添加复制表"></a>添加复制表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 4&gt; add trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo data enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line"></span><br><span class="line">GGSCI (cdh01) 5&gt; info trandata test_ogg.test_ogg</span><br><span class="line">控制台显示的内容：Logging of supplemental redo log data is enabled for table TEST_OGG.TEST_OGG.</span><br><span class="line">控制台显示的内容：Columns supplementally logged for table TEST_OGG.TEST_OGG: ID</span><br></pre></td></tr></table></figure><h3 id="配置extract进程"><a href="#配置extract进程" class="headerlink" title="配置extract进程"></a>配置extract进程</h3><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">6</span>&gt; edit param extkafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">extract</span> extkafka</span><br><span class="line"><span class="attribute">dynamicresolution</span></span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (ORACLE_SID = <span class="string">"dbsrv2"</span>)</span><br><span class="line"><span class="attribute"><span class="nomarkup">SETENV</span></span> (NLS_LANG = <span class="string">"american_america.AL32UTF8"</span>)</span><br><span class="line"><span class="attribute">GETUPDATEBEFORES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSDELETES</span></span><br><span class="line"><span class="attribute">NOCOMPRESSUPDATES</span></span><br><span class="line"><span class="attribute">userid</span> ogg,password 123456</span><br><span class="line"><span class="attribute">exttrail</span> /oracledata/data/ogg/dirdat/to</span><br><span class="line"><span class="attribute">table</span> test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>dynamicresolution动态解析；</p><p>SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；</p><p>userid ogg,password 123456即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；</p><p>table即复制表的表名，支持*通配，必须以;结尾</p><p>添加extract进程：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) <span class="number">7</span>&gt; <span class="keyword">add </span><span class="keyword">extract </span><span class="keyword">extkafka,tranlog,begin </span>now</span><br><span class="line">控制台显示的内容：<span class="keyword">EXTRACT </span><span class="keyword">added.</span></span><br></pre></td></tr></table></figure><p>(注：若报错</p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ERROR: </span>Could not create checkpoint file /opt/ogg/dirchk/EXTKAFKA.cpe (error 2, No such file or directory).</span><br></pre></td></tr></table></figure><p>执行下面的命令再重新添加即可。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create <span class="keyword">subdirs</span></span><br></pre></td></tr></table></figure><p>)</p><p>添加trail文件的定义与extract进程绑定：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 8&gt; <span class="builtin-name">add</span> exttrail /oracledata/data/ogg/dirdat/<span class="keyword">to</span>,extract extkafka</span><br><span class="line">控制台显示的内容：EXTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置pump进程"><a href="#配置pump进程" class="headerlink" title="配置pump进程"></a>配置pump进程</h3><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">9</span>&gt; edit param pukafka</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">extract pukafka</span><br><span class="line">passthru</span><br><span class="line">dynamicresolution</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">rmthost <span class="number">192.168</span><span class="number">.23</span><span class="number">.168</span> mgrport <span class="number">7809</span></span><br><span class="line">rmttrail /data/apps/ogg/dirdat/to</span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>第一行指定extract进程名称；</p><p>passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；</p><p>dynamicresolution动态解析；</p><p>userid ogg,password ogg即OGG连接Oracle数据库的帐号密码</p><p>rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口；</p><p>rmttrail即目标端trail文件存储位置以及名称。<strong>(注意，这里很容易犯错！！！注意是目标端的路径！！！)</strong></p><p>分别将本地trail文件和目标端的trail文件绑定到extract进程：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh01) 10&gt; <span class="builtin-name">add</span> extract pukafka,exttrailsource /oracledata/data/ogg/dirdat/<span class="keyword">to</span></span><br><span class="line">控制台显示的内容：EXTRACT added.</span><br><span class="line">GGSCI (cdh01) 11&gt; <span class="builtin-name">add</span> rmttrail /data/apps/ogg/dirdat/<span class="keyword">to</span>,extract pukafka</span><br><span class="line">控制台显示的内容：RMTTRAIL added.</span><br></pre></td></tr></table></figure><h3 id="配置defgen文件"><a href="#配置defgen文件" class="headerlink" title="配置defgen文件"></a>配置defgen文件</h3><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">12</span>&gt; edit param test_ogg</span><br></pre></td></tr></table></figure><p>然后和用vim编辑一样添加</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">defsfile <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password <span class="number">123456</span></span><br><span class="line">table test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>退出GGSCI</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh01</span>) <span class="number">13</span>&gt; quit</span><br></pre></td></tr></table></figure><p>进行OGG主目录下执行以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $OGG_HOME</span><br><span class="line">./defgen paramfile dirprm/test_ogg.prm</span><br></pre></td></tr></table></figure><p>输出以下内容则执行成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">        Oracle GoldenGate Table Definition Generator for Oracle</span><br><span class="line"> Version 11.2.1.0.3 14400833 OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258</span><br><span class="line">   Linux, x64, 64bit (optimized), Oracle 11g on Aug 23 2012 16:58:29</span><br><span class="line"> </span><br><span class="line">Copyright (C) 1995, 2012, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    Starting at 2018-05-23 05:03:04</span><br><span class="line">***********************************************************************</span><br><span class="line"></span><br><span class="line">Operating System Version:</span><br><span class="line">Linux</span><br><span class="line">Version #1 SMP Wed Apr 12 15:04:24 UTC 2017, Release 3.10.0-514.16.1.el7.x86_64</span><br><span class="line">Node: ambari.master.com</span><br><span class="line">Machine: x86_64</span><br><span class="line">                         soft limit   hard limit</span><br><span class="line">Address Space Size   :    unlimited    unlimited</span><br><span class="line">Heap Size            :    unlimited    unlimited</span><br><span class="line">File Size            :    unlimited    unlimited</span><br><span class="line">CPU Time             :    unlimited    unlimited</span><br><span class="line"></span><br><span class="line">Process id: 13126</span><br><span class="line"></span><br><span class="line">***********************************************************************</span><br><span class="line">**            Running with the following parameters                  **</span><br><span class="line">***********************************************************************</span><br><span class="line">defsfile /opt/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">userid ogg,password ***</span><br><span class="line">table test_ogg.test_ogg;</span><br><span class="line">Retrieving definition for TEST_OGG.TEST_OGG</span><br><span class="line"></span><br><span class="line">Definitions generated for 1 table in /oracledata/data/ogg/dirdef/test_ogg.test_ogg</span><br></pre></td></tr></table></figure><p>将生成的/oracledata/data/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /oracledata/data/ogg/dirdef/test_ogg.test_ogg root@cdh02:/data/apps/ogg/dirdef/</span><br></pre></td></tr></table></figure><h2 id="OGG目标端配置"><a href="#OGG目标端配置" class="headerlink" title="OGG目标端配置"></a>OGG目标端配置</h2><h3 id="开启kafka服务"><a href="#开启kafka服务" class="headerlink" title="开启kafka服务"></a>开启kafka服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 开启Zookeeper</span></span><br><span class="line">/data/apps/apache-zookeeper-3.5.5-bin/bin/zkServer.sh start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 开启Kafka</span></span><br><span class="line">/data/apps/kafka_2.11-0.11.0.1/bin/kafka-server-start.sh -daemon config/server.properties</span><br></pre></td></tr></table></figure><h3 id="配置管理器mgr-1"><a href="#配置管理器mgr-1" class="headerlink" title="配置管理器mgr"></a>配置管理器mgr</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt;  edit param mgr</span><br><span class="line">PORT 7809</span><br><span class="line">DYNAMICPORTLIST 7810-7909</span><br><span class="line">AUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3</span><br><span class="line">PURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</span><br></pre></td></tr></table></figure><h3 id="配置checkpoint"><a href="#配置checkpoint" class="headerlink" title="配置checkpoint"></a>配置checkpoint</h3><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 2&gt; edit param ./GLOBALS</span><br><span class="line">CHECKPOINTTABLE test_ogg.checkpoint</span><br></pre></td></tr></table></figure><h3 id="配置replicate进程"><a href="#配置replicate进程" class="headerlink" title="配置replicate进程"></a>配置replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 3&gt; edit param rekafka</span><br><span class="line">REPLICAT rekafka</span><br><span class="line">sourcedefs /data/apps/ogg/dirdef/test_ogg.test_ogg</span><br><span class="line">TARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props</span><br><span class="line">REPORTCOUNT EVERY 1 MINUTES, RATE </span><br><span class="line">GROUPTRANSOPS 10000</span><br><span class="line">MAP test_ogg.test_ogg, TARGET test_ogg.test_ogg;</span><br></pre></td></tr></table></figure><p>说明：</p><p>REPLICATE rekafka定义rep进程名称；</p><p>sourcedefs即在4.6中在源服务器上做的表映射文件；</p><p>TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props；</p><p>REPORTCOUNT即复制任务的报告生成频率；</p><p>GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；</p><p>MAP即源端与目标端的映射关系</p><h3 id="配置kafka-props"><a href="#配置kafka-props" class="headerlink" title="配置kafka.props"></a>配置kafka.props</h3><p><strong>本环节配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirprm/</span><br><span class="line">vim kafka.props</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> handler类型</span></span><br><span class="line">gg.handlerlist=kafkahandler</span><br><span class="line">gg.handler.kafkahandler.type=kafka</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kafka生产者配置文件</span></span><br><span class="line">gg.handler.kafkahandler.KafkaProducerConfigFile=custom_kafka_producer.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka的topic名称，无需手动创建</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gg.handler.kafkahandler.topicMappingTemplate=test_ogg（新版topicName属性的设置方式）</span></span><br><span class="line">gg.handler.kafkahandler.topicName=test_ogg</span><br><span class="line"><span class="meta">#</span><span class="bash"> 传输文件的格式，支持json，xml等</span></span><br><span class="line">gg.handler.kafkahandler.format=json</span><br><span class="line">gg.handler.kafkahandler.format.insertOpKey = I  </span><br><span class="line">gg.handler.kafkahandler.format.updateOpKey = U  </span><br><span class="line">gg.handler.kafkahandler.format.deleteOpKey = D</span><br><span class="line">gg.handler.kafkahandler.format.truncateOpKey=T</span><br><span class="line">gg.handler.kafkahandler.format.includePrimaryKeys=true</span><br><span class="line"><span class="meta">#</span><span class="bash"> OGG <span class="keyword">for</span> Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次</span></span><br><span class="line">gg.handler.kafkahandler.mode=op</span><br><span class="line"><span class="meta">#</span><span class="bash"> 类路径</span></span><br><span class="line">gg.classpath=dirprm/:/data/apps/kafka_2.11-0.11.0.1/libs/*:/data/apps/ogg/:/data/apps/ogg/lib/*</span><br></pre></td></tr></table></figure><p>紧接着创建Kafka生产者配置文件：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim custom_kafk<span class="built_in">a_producer</span>.properties</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kafkabroker的地址</span></span><br><span class="line">bootstrap.servers=cdh01:9092,cdh02:9092,cdh03:9092</span><br><span class="line">acks=1</span><br><span class="line"><span class="meta">#</span><span class="bash"> 压缩类型</span></span><br><span class="line">compression.type=gzip</span><br><span class="line"><span class="meta">#</span><span class="bash"> 重连延时</span></span><br><span class="line">reconnect.backoff.ms=1000</span><br><span class="line">value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">key.serializer=org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line">batch.size=102400</span><br><span class="line">linger.ms=10000</span><br></pre></td></tr></table></figure><p><strong>配置时把注释都去掉，ogg不识别注释，如果不去掉会报错！！！</strong></p><h3 id="添加trail文件到replicate进程"><a href="#添加trail文件到replicate进程" class="headerlink" title="添加trail文件到replicate进程"></a>添加trail文件到replicate进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (cdh02) 1&gt; add replicat rekafka exttrail /data/apps/ogg/dirdat/to,checkpointtable test_ogg.checkpoint</span><br><span class="line">控制台显示的内容：REPLICAT added.</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="启动所有进程"><a href="#启动所有进程" class="headerlink" title="启动所有进程"></a>启动所有进程</h3><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。<br>启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。<br>全部需要在ogg目录下执行ggsci目录进入ogg命令行。<br>源端依次是</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> extkafka</span><br><span class="line"><span class="literal">start</span> pukafka</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">start</span> mgr</span><br><span class="line"><span class="literal">start</span> rekafka</span><br></pre></td></tr></table></figure><p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功<br>源端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.master.com) 5&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">EXTRACT     RUNNING     EXTKAFKA    04:50:21      00:00:03    </span><br><span class="line">EXTRACT     RUNNING     PUKAFKA     00:00:00      00:00:03</span><br></pre></td></tr></table></figure><p>目标端</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (ambari.slave1.com) 3&gt; <span class="builtin-name">info</span> all</span><br><span class="line"></span><br><span class="line">Program     Status     <span class="built_in"> Group </span>      Lag at Chkpt  Time Since Chkpt</span><br><span class="line"></span><br><span class="line">MANAGER     RUNNING                                           </span><br><span class="line">REPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</span><br></pre></td></tr></table></figure><h3 id="异常解决"><a href="#异常解决" class="headerlink" title="异常解决"></a>异常解决</h3><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ggser.<span class="built_in">log</span></span><br></pre></td></tr></table></figure><p>或者ogg命令行,以rekafka进程为例</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGSCI (<span class="name">cdh02</span>) <span class="number">2</span>&gt; view report rekafka</span><br></pre></td></tr></table></figure><h3 id="测试同步更新效果"><a href="#测试同步更新效果" class="headerlink" title="测试同步更新效果"></a>测试同步更新效果</h3><p>现在源端执行sql语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conn test_ogg/test_ogg</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_ogg <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">'test'</span>,<span class="literal">null</span>);</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">update</span> test_ogg <span class="keyword">set</span> <span class="keyword">name</span>=<span class="string">'zhangsan'</span> <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">delete</span> test_ogg <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">3</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure><p>查看源端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/oracledata/</span>data<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-rw-rw- <span class="number">1</span> oracle oinstall <span class="number">1464</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看目标端trail文件状态</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls -l <span class="meta-keyword">/data/</span>apps<span class="meta-keyword">/ogg/</span>dirdat/to*</span><br><span class="line">-rw-r----- <span class="number">1</span> root root <span class="number">1504</span> May <span class="number">23</span> <span class="number">10</span>:<span class="number">31</span> <span class="meta-keyword">/opt/</span>ogg<span class="meta-keyword">/dirdat/</span>to000000</span><br></pre></td></tr></table></figure><p>查看kafka是否自动建立对应的主题</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.<span class="keyword">sh</span> --<span class="keyword">list</span> --zookeeper localhos<span class="variable">t:2181</span></span><br></pre></td></tr></table></figure><p>在列表中显示有test_ogg则表示没问题<br>通过消费者看是否有同步消息</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server <span class="number">192.168</span><span class="number">.44</span><span class="number">.129</span>:<span class="number">9092</span> --topic test_ogg --<span class="keyword">from</span>-beginning</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"I"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:04:39.001362"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:04:44.610000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001246"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"U"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:05:44.000411"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:05:50.764000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001541"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"test"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;,<span class="string">"after"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"table"</span>:<span class="string">"TEST_OGG.TEST_OGG"</span>,<span class="string">"op_type"</span>:<span class="string">"D"</span>,<span class="string">"op_ts"</span>:<span class="string">"2019-08-17 22:06:33.000312"</span>,<span class="string">"current_ts"</span>:<span class="string">"2019-08-17T22:06:39.845000"</span>,<span class="string">"pos"</span>:<span class="string">"00000000010000001670"</span>,<span class="string">"primary_keys"</span>:[<span class="string">"ID"</span>],<span class="string">"before"</span>:&#123;<span class="string">"ID"</span>:<span class="number">3</span>,<span class="string">"NAME"</span>:<span class="string">"zhangsan"</span>,<span class="string">"SEX"</span>:<span class="literal">null</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>before代表操作之前的数据，after代表操作后的数据，现在已经可以从kafka获取到同步的json数据了，后面可以用SparkStreaming和Storm等解析然后存到hadoop等大数据平台里</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果想通配整个库的话，只需要把上面的配置所有表名改为*，如test_ogg<span class="selector-class">.test_ogg</span> 改为 test_ogg.*,但是kafka的topic不能通配，所以需要把所有表的数据放在一个topic，后面再用程序解析表名即可。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">若后期因业务需要导致表结构发生改变，需要重新生成源端表结构的defgen定义文件，再把定义文件通过scp放到目标端。defgen文件的作用是，记录了源端的表结构，然后我们再把这个文件放到目标端，在目标端应用SQL时就能根据defgen文件与目标端表结构，来做一定的转换。</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/purpleraintear/p/6071038.html" target="_blank" rel="noopener">基于OGG的Oracle与Hadoop集群准实时同步介绍</a></p><p><a href="https://docs.oracle.com/goldengate/bd1221/gg-bd/GADBD/GUID-2561CA12-9BAC-454B-A2E3-2D36C5C60EE5.htm#GADBD449" target="_blank" rel="noopener">Fusion Middleware Integrating Oracle GoldenGate for Big Data</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据的导入导出 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
            <tag> Kafka </tag>
            
            <tag> ogg </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群基于延时指标进行性能调优</title>
      <link href="/2018/09/10/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%BB%B6%E6%97%B6%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/10/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%BB%B6%E6%97%B6%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="缓解延时症状"><a href="#缓解延时症状" class="headerlink" title="缓解延时症状"></a>缓解延时症状</h2><ul><li>不要创建具备超多分区数的topic，因为适当增加分区数的确可以提升TPS，但是大量的分区的存在对于延时确实损害，分区数越多，broker端就需要越长的时间实现follower与leader的同步。</li><li>适当增加Broker数来分散分区数，从而限制了单台Broker上的总分区数，减轻了单台Broker端分区访问压力。</li><li>增加num.replica.fetchers参数提升broker端的I/O并行度。该值增加了broker端follower副本从leader副本处获取的最大线程数。默认值是1。</li><li>和调节吞吐量相反，调优延时要求producer端尽量不要缓存消息，而是尽快地把消息发送出去。</li></ul><h2 id="实际可行性调优"><a href="#实际可行性调优" class="headerlink" title="实际可行性调优"></a>实际可行性调优</h2><ul><li>producer端尽量不要缓存消息，而是尽快的将消息发送出去，又重复了一遍。</li><li>设置linger.ms参数设置为0，不要让producer花费额外的时间去缓存待发送的消息。</li><li>压缩是一种时间换空间的一种优化方式，为了减少网络I/O传输量，推荐关闭。compression.type=none。</li><li>Producer端的acks参数也是优化延时的重要手段之一，leader.broker越快的发送response，producer端就能越快地发送下一批消息。该参数默认值是1，实际上已经是非常好的设置了。</li><li>调整leader副本返回的最小数据量来间接的影响Consumer端的延时，即fetch.min.bytes参数值。默认值是1，已经是非常好的选择。</li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p>broker端</p><ul><li>适当增加num.replica.fetchers(broker端follower副本从leader副本处获取的最大线程数)</li><li>避免创建过多的topic分区。</li></ul><p>producer端</p><ul><li>设置linger.ms=0</li><li>设置compression.type=none</li><li>设置acks=1或者0。</li></ul><p>consumer端</p><ul><li>设置fetch.min.bytes=1</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记住，调优延时要求producer端尽量不要缓存消息，而是尽快地把消息发送出去。又重复了一遍。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群基于吞吐量指标进行性能调优</title>
      <link href="/2018/09/09/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%90%9E%E5%90%90%E9%87%8F%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/09/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%90%9E%E5%90%90%E9%87%8F%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><ul><li>性能<ul><li>吞吐量：broker或者clients应用程序每秒能处理多少字节（或消息）</li><li>延时：通常指Producer发送到broker端持久化保存消息之间的时间间隔。</li></ul></li><li>可用性：系统和组件正常运行的概率或时间比率，业界一般N个9来量化可用性，比如年度4个9表示53分钟（365* 24 * 60 * 0.01%=53分钟）</li><li>持久性：已提交的消息需要被持久化到Broker端底层的文件系统的物理日志而不能丢失。</li></ul><h2 id="kafka-基础设施优化"><a href="#kafka-基础设施优化" class="headerlink" title="kafka 基础设施优化"></a>kafka 基础设施优化</h2><ul><li><p>磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10TB数据，单个broker能存储 2TB，那么我们需要的最小Kafka集群大小5 个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</p></li><li><p>文件系统在文件被访问、创建、修改等的时候会记录文件的一些时间戳，比如：文件创建时间（ctime）、最近一次修改时间（mtime）和最近一次访问时间（atime）。默认情况下，atime的更新会有一次读操作，这会产生大量的磁盘读写，然而atime</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mount -o noatime</span></span><br></pre></td></tr></table></figure></li><li><p>绝大多数运行在Linux上的软件都是基于EXT4构建和测试的，因此兼容性上EXT4要优于其他文件系统。</p></li><li><p>作为高性能的64位日志文件系统（journaling file system），XFS表现出高性能，高伸缩性，特别适应于生产服务器，特别是大文件（30+GB）操作。很多存储类的应用都适合选择XFS作为底层文件系统。</p></li><li><p>计算机的内存分为虚拟内存和物理内存。物理内存是真实的内存，虚拟内存是用磁盘来代替内存。并通过swap机制实现磁盘到物理内存的加载和替换,这里面用到的磁盘我们称为swap磁盘。在写文件的时候，Linux首先将数据写入没有被使用的内存中，这些内存被叫做内存页（page cache）。然后读的时候，Linux会优先从page cache中查找，如果找不到就会从硬盘中查找。当物理内存使用达到一定的比例后，Linux就会使用进行swap，使用磁盘作为虚拟内存。通过cat /proc/sys/vm/swappiness可以看到swap参数。这个参数表示虚拟内存中swap磁盘占了多少百分比。0表示最大限度的使用内存，100表示尽量使用swap磁盘。系统默认的参数是60，当物理内存使用率达到40%，就会频繁进行swap，影响系统性能，推荐将vm.swappiness 设置为较低的值1。最终我设置为10，因为我们的机器的内存还是比较小的，只有40G，设置的太小，可能会影响到虚拟内存的使用吧。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">临时修改：sudo sysctl vm.<span class="attribute">swappiness</span>=N</span><br><span class="line">永久修改（/etc/sysctl.conf）：vm.<span class="attribute">swappiness</span>=N</span><br></pre></td></tr></table></figure></li></ul><h2 id="kafka-JVM设置"><a href="#kafka-JVM设置" class="headerlink" title="kafka JVM设置"></a>kafka JVM设置</h2><ul><li><p>PermGen space : 全称是Permanent Generation  space，是指内存的永久保存区域，为什么会发生内存溢出？这一部分用于存放Class和Meta的信息, Class在被 Load的时候被放入PermGen space区域，它和存放Instance的Heap区域不同,所以如果你的APP会LOAD很多CLASS的话,就很可能出现PermGen space错误。</p></li><li><p>G1算法将堆划分为若干个区域（Region），它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。</p></li><li><p>在G1中，还有一种特殊的区域，叫Humongous区域。 如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。这些巨型对象，默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。</p><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/1677483bb9496fd9?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>​</p></li><li><p>G1采用内存分区(Region)的思路，将内存划分为一个个相等大小的内存分区，回收时则以分区为单位进行回收，存活的对象复制到另一个空闲分区中。由于都是以相等大小的分区为单位进行操作，因此G1天然就是一种压缩方案(局部压缩)；</p></li><li><p>G1虽然也是分代收集器，但整个内存分区不存在物理上的年轻代与老年代的区别，也不需要完全独立的survivor(to space)堆做复制准备。G1只有逻辑上的分代概念，或者说每个分区都可能随G1的运行在不同代之间前后切换；</p></li><li><p>G1的收集都是STW的，但年轻代和老年代的收集界限比较模糊，采用了混合(mixed)收集的方式。即每次收集既可能只收集年轻代分区(年轻代收集)，也可能在收集年轻代的同时，包含部分老年代分区(混合收集)，这样即使堆内存很大时，也可以限制收集范围，从而降低停顿。</p></li><li><p>堆内存中一个Region的大小可以通过-XX:G1HeapRegionSize参数指定，大小区间只能是1M、2M、4M、8M、16M和32M，总之是2的幂次方，如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实践大小，默认把堆内存按照2048份均分，最后得到一个合理的大小。</p></li><li><p>JVM 8 metaSpace 诞生了: 不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小：-XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。</p></li><li><p>-XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集-XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集</p></li><li><p>XX:MaxGCPauseMillis=n : 设置最大GC停顿时间(GC pause time)指标(target). 这是一个软性指标(soft goal)， JVM 会尽量去达成这个目标。</p></li><li><p>InitiatingHeapOccupancyPercent： 整个堆栈使用达到百分之多少的时候，启动GC周期. 基于整个堆，不仅仅是其中的某个代的占用情况，G1根据这个值来判断是否要触发GC周期, 0表示一直都在GC，默认值是45（即45%慢了，或者说占用了)</p><p>   -XX:G1NewSizePercent    新生代最小值，默认值5%</p></li><li><p>-XX:G1MaxNewSizePercent 新生代最大值，默认值60%</p></li><li><p>MetaspaceSize: 这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># export JAVA_HOME=/usr/java/jdk1.8.0_51</span></span><br><span class="line"><span class="comment"># export KAFKA_HEAP_OPTS="</span></span><br><span class="line">-Xmx6g -Xms6g -XX:<span class="attribute">MetaspaceSize</span>=128m </span><br><span class="line">-XX:<span class="attribute">MaxMetaspaceSize</span>=128m -XX:+UseG1GC -XX:<span class="attribute">MaxGCPauseMillis</span>=20</span><br><span class="line">-XX:<span class="attribute">InitiatingHeapOccupancyPercent</span>=35 -XX:+<span class="attribute">G1HeapRegionSize</span>=16M</span><br><span class="line">-XX:<span class="attribute">MinMetaspaceFreeRatio</span>=50 <span class="string">"</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="文件调优"><a href="#文件调优" class="headerlink" title="文件调优"></a>文件调优</h2><ul><li><p>若出现”too many files open”错误时，就需要为Broker所在的机器调优最大文件部署符上限。文件句柄个数计算方法如下：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker上可能最大分区数*（每个分区平均数据量/平均的日志段大小+<span class="number">3</span>）其中<span class="number">3</span> 表示索引文件个数，</span><br><span class="line">假设<span class="number">20</span>个分区，分割分区总数据量为<span class="number">100</span>GB，每一个日志段大小是<span class="number">1</span>GB，那么这台机器最大文件部署符大小应该是：</span><br><span class="line">            <span class="number">20</span>*（<span class="number">100</span>/<span class="number">1</span>+<span class="number">3</span>）=<span class="number">2060</span></span><br><span class="line">因此该参数一定要设置足够大。比如<span class="number">100000</span></span><br></pre></td></tr></table></figure></li><li><p>若出现”java.lang.OutOfMemoryError:Map failed的严重错误，主要原因是大量创建topic将极大消耗操作系统内存，用户可以适当调整vm.max.map.count参数，具体方法如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sbin/sysctl -w vm<span class="selector-class">.max_map_count</span> = N ，该参数默认值是<span class="number">65535</span>，可以考虑线上环境设置更大的值。</span><br></pre></td></tr></table></figure></li></ul><h2 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h2><p>Broker端；</p><ul><li>适当增加num.replica.fetchers，但不要超过CPU核数，该值控制了broker端follower副本从leader副本处获取消息的最大线程数。默认值是1，表明follower副本只使用一个线程实时拉取leader处的最新消息。对于设置了acks=all的producer而言，主要延时可能耽误在follower与leader同步过程，所以增加该值可以缩短同步的时间，从而间接的提升Producer端的TPS。</li><li>调优GC避免经常性的Full GC。老版本过渡依赖Zookeeper来表征Consumer还活着，若GC时间过长，会导致Zookeeper会话过期，kafka会立即对group进行rebalance。新版本上已经弃用了对Zookeeper的依赖。</li></ul><p>Producer端：</p><ul><li>适当增加batch.size，比如100-512KB。</li><li><a href="https://link.juejin.im?target=http%3A%2F%2Fxn--linger-2e4j090brvo308i.ms" target="_blank" rel="noopener">适当增加linger.ms</a>，比如10-100毫秒Producer端是批量发送消息的，更大的batch size可以令更多的消息封装进同一个请求，故发送给broker端的总请求数就会减少，此举可以减轻Producer的负载，也降低了broker端的CPU请求处理开销。而更大的linger.ms使producer等待更长的时间才发送消息，这样就能够缓存更多的消息填满batch，从而从整体上提升TPS。但是延时肯定增加了。</li><li>设置压缩类型compression.type=lz4，目前支持的压缩方式有：GZIP，Snappy，LZ4，在CPU资源丰富的情况下compression.type=lz4的效果是最好的。</li><li>acks=0或1</li><li>retries=0</li><li>若多线程共享Produer或分区数很多时，增加buffer.memory。因为每一个分区都会占用一个batch.size。</li></ul><p>consumer端</p><ul><li>采用多Consumer实例，共同消费多分区数据，这些实例共享相同的group.id</li><li>增加fetch.min.bytes，比如10000，表征每次leader副本所在的broker所返回的最小数据量来间接影响TPS,通过增加该值，Kafka会为每一个FETCH请求的response填入更多的数据。</li></ul><h2 id="悖论存在（分区数越多，TPS越高）"><a href="#悖论存在（分区数越多，TPS越高）" class="headerlink" title="悖论存在（分区数越多，TPS越高）"></a>悖论存在（分区数越多，TPS越高）</h2><p><img src="https://user-gold-cdn.xitu.io/2018/12/3/16774b2833e5a82e?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><ul><li>Kafka基本的并行单元就是分区，producer在设计时实现了能够同时向多个分区发送消息，从而这些消息最终写入到多个broker上，最终可以被多个consumer同时消费。通常来说，分区数越多，TPS越高</li><li>分区数越多，占用的缓冲区越多，因为缓冲区是以分区为粒度的，所以Server/clients端将占用更多的内存。</li><li>每一个分区在底层文件系统中都有专属目录，除了3个索引文件外还保存日志段文件，会占用大量的文件句柄。</li><li>每一个分区都有若干个副本保存在不同的broker上，当broker挂掉后，因为需要Controller进行处理leader变更请求，该处理线程是单线程，疯了吧，亚历山大。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>num.replica.fetchers倒是一个新颖的选手，可以好好试试，acks重点关注，其他都中规中矩。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群基于可用性指标进行性能调优</title>
      <link href="/2018/09/07/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%8F%AF%E7%94%A8%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/07/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E5%8F%AF%E7%94%A8%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h2><ul><li>可用性反映了kafka集群应对崩溃的能力，调优可用性就是为了让Kafka更快的从崩溃中恢复过来。</li></ul><h2 id="Topic角度看问题"><a href="#Topic角度看问题" class="headerlink" title="Topic角度看问题"></a>Topic角度看问题</h2><ul><li>Controller就是broker，主要负责副本分配方案和分区上线的工作（根据副本的分配方案）。</li><li>副本状态机制</li><li>分区状态机制</li><li>注意Topic的分区数越多，一旦分区的leader副本所在broker发生崩溃，就需要进行leader的选举，虽然Leader的选举时间很短，大概几毫秒，但是Controller处理请求时单线程的，controller通过Zookeeper可实时侦测broker状态。一旦有broker挂掉了，controller可立即感知并为受影响分区选举新的leader，但是在新的分配方案出来后，发布到各个broker进行元数据更新就要浪费网络I/O了。</li><li>建议分区不要过大，可能会影响可用性。</li></ul><h2 id="Producer角度看问题"><a href="#Producer角度看问题" class="headerlink" title="Producer角度看问题"></a>Producer角度看问题</h2><ul><li>首推参数是acks，当acks设置为all时，broker端参数min.insync.replicas的效果会影响Producer端的可用性。该参数越大，kafka会强制进行越多的follower副本同步写入日志，当出现ISR缩减到min.insync.replicas值时，producer会停止对特定分区发送消息，从而影响了可用性。</li></ul><h2 id="Broker角度看问题"><a href="#Broker角度看问题" class="headerlink" title="Broker角度看问题"></a>Broker角度看问题</h2><ul><li>Broker端高可用性直接的表现形式就是broker崩溃，崩溃之后leader选举有两种：1：从ISR中选择。2：通过unclean.leader.election.enable值决定是否从ISR中选择。第二种情况会出现数据丢失的风险。</li><li>另一个参数是broker崩溃恢复调优，即num.recovery.threads.per.data.dir。场景是这样的，当broker从崩溃中重启恢复后，broker会扫描并加载底层的分区数据执行清理和与其他broker保持同步。这一工程被称为日志加载和日志恢复。默认情况下是单线程的。假设某个Broker的log.dirs配置了10个日志目录，那么单线程可能就吭哧吭哧扫描加载，太慢了。实际使用中，建议配置为日志的log.dirs磁盘数量。</li></ul><h2 id="Consumer角度看问题"><a href="#Consumer角度看问题" class="headerlink" title="Consumer角度看问题"></a>Consumer角度看问题</h2><ul><li><p>对于Consumer而言，高可用性主要是基于组管理的consumer group来体现的，当group下某个或某些consumer实例“挂了”，group的coordinator能够自动检测出这种崩溃并及时的开启rebalance，进而将崩溃的消费分区分配到其他存活的consumer上。</p></li><li><p>consumer端参数session.timeout.ms就是定义了能检测出failure的时间间隔。若要实现高可用，必须设置较低的值，比如5-10秒。一旦超过该值，就会开启新一轮的reblance。</p></li><li><p>消息处理时间参数很牛，<code>max.poll.interval.ms</code>，参数设置了consumer实例所需要的消息处理时间，一旦超过该值，就是高负荷状态，此时consumer将停止发送心跳，并告知coordinator要离开group。<code>消费者在创建时会有一个属性max.poll.interval.ms</code>，该属性意思为kafka消费者在每一轮poll()调用之间的最大延迟,消费者在获取更多记录之前可以空闲的时间量的上限。如果此超时时间期满之前poll()没有被再次调用，则消费者被视为失败，并且分组将重新平衡，以便将分区重新分配给别的成员</p></li><li><p><code>hearbeat.interval.ms</code> 当coordinator决定开启新一轮的reblance时，他会把这个决定以REBALANCE_IN_PROCESS异常的形式塞进consumer心跳的请求响应中，这样就可以飞快的感知到新的分区分配方案。</p></li><li><p>kafka调优经典的独白：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">上线两个月都没有问题，为什么最近突然出现问题了。我想肯定是业务系统有什么动作，我就去问了一个下，</span><br><span class="line">果然头一天风控系统kafka挂掉了，并进行了数据重推，导致了数据阻塞。但是我又想即使阻塞了也会慢慢消费掉啊，不应</span><br><span class="line">该报错呀。后来我看了一下kafka官网上的参数介绍，发现max.poll.records默认是<span class="number">2147483647</span> </span><br><span class="line">（<span class="number">0.10</span><span class="number">.0</span><span class="number">.1</span>版本），也就是kafka里面有多少poll多少，如果消费者拿到的这些数据在制定时间内消费不完，就会手动提交</span><br><span class="line">失败，数据就会回滚到kafka中，会发生重复消费的情况。如此循环，数据就会越堆越多。后来咨询了公司的kafka大神，他</span><br><span class="line">说我的kafka版本跟他的集群版本不一样让我升级kafka版本。于是我就升级到了<span class="number">0.10</span><span class="number">.2</span><span class="number">.1</span>，查阅官网发现这个版本的max.po</span><br><span class="line">ll.records默认是<span class="number">500</span>，可能kafka开发团队也意识到了这个问题。并且这个版本多了一个max.poll.interval.ms这个参数，</span><br><span class="line">默认是<span class="number">300</span>s。这个参数的大概意思就是kafka消费者在一次poll内，业务处理时间不能超过这个时间。后来升级了kafka版本</span><br><span class="line">，把max.poll.records改成了<span class="number">50</span>个之后，上了一次线，准备观察一下。上完线已经晚上<span class="number">9</span>点了，于是就打卡回家了，明天看</span><br><span class="line">结果。第二天早起满心欢喜准备看结果，以为会解决这个问题，谁曾想还是堆积。我的天，思来想去，也想不出哪里有问题</span><br><span class="line">。于是就把处理各个业务的代码前后执行时间打印出来看一下，添加代码，提交上线。然后观察结果，发现大部分时间都用</span><br><span class="line">在数据库IO上了，并且执行时间很慢，大部分都是<span class="number">2</span>s。于是想可能刚上线的时候数据量比较小，查询比较快，现在数据量大</span><br><span class="line">了，就比较慢了。当时脑子里第一想法就是看了一下常用查询字段有没有添加索引，一看没有，然后马上添加索引。加完索</span><br><span class="line">引观察了一下，处理速度提高了好几倍。虽然单条业务处理的快了， </span><br><span class="line">但是堆积还存在，后来发现，业务系统大概<span class="number">1</span>s推送<span class="number">3</span>、<span class="number">4</span>条数据，但是我kafka现在是单线程消费，速度大概也是这么多。再</span><br><span class="line">加上之前的堆积，所以消费还是很慢。于是业务改成多线程消费，利用线程池，开启了<span class="number">10</span>个线程，上线观察。几分钟就消费</span><br><span class="line">完了。大功告成，此时此刻，心里舒坦了好多。不容易呀！</span><br></pre></td></tr></table></figure></li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p><strong>Broker端</strong></p><ul><li>避免过多分区</li><li>设置unclean.leader.election.enable=true（为了可用性，数据丢失不考虑）</li><li>设置min.insync.replicas=1（减轻同步压力）</li><li>设置num.recovery.threads.per.data.dir=broker 日志的log.dirs磁盘数</li></ul><p><strong>Producer端</strong></p><ul><li>设置acks=1，设置为all时，遵循min.insync.replicas=1</li></ul><p><strong>consumer端</strong></p><ul><li>设置session.timeout.ms为较低的值，比如100000</li><li>设置max.poll.interval.ms消息平均处理时间，可适当调大。</li><li>设置max.poll.records和max.partition.fetch.bytes减少消息处理总时长，避免频繁的rebalance。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>num.recovery.threads.per.data.dir以及max.partition.fetch.bytes以及max.poll.records重点关注一下。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群基于持久性指标进行性能调优</title>
      <link href="/2018/09/06/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E6%8C%81%E4%B9%85%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/06/kafka%E9%9B%86%E7%BE%A4%E5%9F%BA%E4%BA%8E%E6%8C%81%E4%B9%85%E6%80%A7%E6%8C%87%E6%A0%87%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h2><ul><li>持久性定义了kafka集群中消息不容易丢失的程度，持久性越高表明Kafka越不会丢失消息。</li><li>持久性通常是通过冗余来实现，而kafka实现冗余的手段就是备份机制，该备份机制保证了Kafka的每一个消息都会被保存到多台Broker上。</li></ul><h2 id="实际可行性调优"><a href="#实际可行性调优" class="headerlink" title="实际可行性调优"></a>实际可行性调优</h2><ul><li>分区数和副本因子由broker端参数num.partitions和default.replication.factor指定，这两个参数默认值都是1。</li><li>禁止kafka自动创建topic，可以参数设置auto.create.topics.enable=false。</li><li>新版本已经把consumer端位移信息的存储从Zookeeper转移到了内部的topic <strong>consumer_offsets中，注意这个也同样会受到default.replication.factor参数影响。0.11.0.0要求</strong>consumer_offsets创建时必须满足default.replication.factor，若Broker小于default.replication.factor会直接报错。</li><li>若要达到最高的持久性，必须设置acks=all(或者acks=-1)，即强制leader副本等待的ISR中所有副本都响应了某条消息后发送response给producer。</li><li>producer发送失败后，会进行重试机制，比如网络临时抖动。</li><li>当Producer重试后，待发送的消息可能已经成功，假如数据已经落盘写入日志，在发送response给Producer时出现了故障，重试时就会再次发送同一条消息，也就出现了重复的消息。自从0.11.0.0版本开始，kafka提供了幂等性Producer，实现了精确一次的处理语义。幂等性Producer保证同一条消息只会被broker写入一次。启动这样设置：enable.idempotence=true。</li><li>消息乱序处理机制，producer依次发送消息m1和m2，若m1写入失败但m2写入成功，一旦重试m1，那么m1就位于m2的后面了。如果对消息的顺序要求比较严格的话，max.in.flight.requests.per.connection=1来规避这个问题。该参数目的保证单个producer连接上在任意某个时刻只处理一个请求。</li><li>当某个Broker出现出现崩溃时，controller会自动检测出宕机的broker并为该Broker上的所有分区重新选举leader，选择的范围是从ISR副本中挑选。若ISR副本所在的broker全部宕机，Kafka就会从非ISR副本中选举leader。这种情况下就会造成不同步和消息丢失。是否允许Unclean选举，借助unclean.leader.election.enable参数控制，0.11.0.0版本默认是false。</li><li>min.insync.replicas，若成功写入消息时则必须等待响应完成的最少ISR副本，该参数是在acks=all时才会生效。</li><li>自0.10.0.0版本开始，kafka仿照Hadoop为broker引入机架属性信息（rack）,该参数由broker.rack设定。Kafka集群在后台会收集所有的机架信息仔仔创建分区时将分区分散到不同的机架上。</li><li>日志持久化：两个重要参数log.flush.interval.ms和log.flush.interval.message。前者指定了Kafka多长时间执行一次消息“落盘”，后者是写入多少消息后执行一次消息“落盘”。默认情况下，log.flush.interval.ms是空而将log.flush.interval.message设置为Long.MAX_VALUE，这个表示Kafka实际上不会自动执行消息的“冲刷”操作，事实上这也是kafka的设计初衷。即把消息同步到磁盘的工作交由操作系统来完成，由操作系统控制页缓存数据到物理文件的同步。但是若用户希望每一条消息都冲刷一次，及时持久化，可以设置log.flush.interval.message=1。</li><li>提交位移的持久化，实际应用中设置auto.commit.enable=false，同时用户需要使用commitSync方法来提交位移，而非commitAsync方法。</li></ul><h2 id="参数清单"><a href="#参数清单" class="headerlink" title="参数清单"></a>参数清单</h2><p>​         <strong>broker端</strong></p><ul><li><p>设置unclean.leader.election.enable=false(0.11.0.0之前版本)</p></li><li><p>设置auto.create.topics.enable=false</p></li><li><p>设置replication.factor=3，min.insync.replicas=1</p></li><li><p>设置default.replication.factor=3</p></li><li><p>设置broker.rack属性分散分区数据到不同的机架。</p></li><li><p>设置log.flush.interval.ms和log.flush.interval.message为一个较小的值。</p><p>​</p><p><strong>producer端</strong></p></li><li><p>设置acks=all</p></li><li><p>设置retries为一个较大的值，比如10-30。</p></li><li><p>设置max.in.flight.request.per.connection=1</p></li><li><p>设置enable.idempotence=true</p><p>​</p></li></ul><p>​         <strong>consumer端</strong></p><ul><li>设置auto.commit.enable=false</li><li>消息消费成功后，调用commitSync提交位移。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>enable.idempotence=true 和max.in.flight.request.per.connection=1以及broker.rack属性比较新颖，值得一试。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka消费者Consumer参数设置及调优</title>
      <link href="/2018/09/04/kafka%E6%B6%88%E8%B4%B9%E8%80%85Consumer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/04/kafka%E6%B6%88%E8%B4%B9%E8%80%85Consumer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="消息的接收-gt-基于Consumer-Group"><a href="#消息的接收-gt-基于Consumer-Group" class="headerlink" title="消息的接收-&gt;基于Consumer Group"></a>消息的接收-&gt;基于Consumer Group</h2><p>Consumer Group 主要用于实现高伸缩性，高容错性的Consumer机制。因此，消息的接收是基于Consumer Group 的。组内多个Consumer实例可以同时读取Kafka消息，同一时刻一条消息只能被一个消费者消费，而且一旦某一个consumer “挂了”， Consumer Group 会立即将已经崩溃的Consumer负责的分区转交给其他Consumer来负责。从而保证 Consumer Group 能够正常工作。</p><h2 id="位移保存-gt-基于Consumer-Group"><a href="#位移保存-gt-基于Consumer-Group" class="headerlink" title="位移保存-&gt;基于Consumer Group"></a>位移保存-&gt;基于Consumer Group</h2><p>说来奇怪，位移保存是基于Consumer Group，同时引入检查点模式，定期实现offset的持久化。</p><h2 id="位移提交-gt-抛弃ZooKeeper"><a href="#位移提交-gt-抛弃ZooKeeper" class="headerlink" title="位移提交-&gt;抛弃ZooKeeper"></a>位移提交-&gt;抛弃ZooKeeper</h2><p>Consumer会定期向kafka集群汇报自己消费数据的进度，这一过程叫做位移的提交。这一过程已经抛弃Zookeeper，因为Zookeeper只是一个协调服务组件，不能作为存储组件，高并发的读取势必造成Zk的压力。</p><ul><li>新版本位移提交是在kafka内部维护了一个内部Topic(_consumer_offsets)。</li><li>在kafka内部日志目录下面，总共有50个文件夹，每一个文件夹包含日志文件和索引文件。日志文件主要是K-V结构，（group.id,topic,分区号）。</li><li>假设线上有很多的consumer和ConsumerGroup，通过对group.id做Hash求模运算，这50个文件夹就可以分散同时位移提交的压力。</li></ul><h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><h3 id="自动提交位移"><a href="#自动提交位移" class="headerlink" title="自动提交位移"></a>自动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        System.out.printf(<span class="string">"offset = %d, key = %s, value = %s%n"</span>, record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="手动提交位移"><a href="#手动提交位移" class="headerlink" title="手动提交位移"></a>手动提交位移</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"test"</span>);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"foo"</span>, <span class="string">"bar"</span>));</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> minBatchSize = <span class="number">200</span>;</span><br><span class="line">List&lt;ConsumerRecord&lt;String, String&gt;&gt; buffer = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        buffer.add(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (buffer.size() &gt;= minBatchSize) &#123;</span><br><span class="line">        insertIntoDb(buffer);</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">        buffer.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="kafka-Consumer参数设置"><a href="#kafka-Consumer参数设置" class="headerlink" title="kafka Consumer参数设置"></a>kafka Consumer参数设置</h2><ul><li>consumer.poll(1000) 重要参数</li><li>新版本的Consumer的Poll方法使用了类似于Select I/O机制，因此所有相关事件（包括reblance，消息获取等）都发生在一个事件循环之中。</li><li>1000是一个超时时间，一旦拿到足够多的数据（参数设置），consumer.poll(1000)会立即返回 ConsumerRecords&lt;String, String&gt; records。</li><li>如果没有拿到足够多的数据，会阻塞1000ms，但不会超过1000ms就会返回。</li></ul><hr><ul><li>session. timeout. ms &lt;=  coordinator检测失败的时间</li><li>默认值是10s</li><li>该参数是 Consumer Group 主动检测 (组内成员consummer)崩溃的时间间隔。若设置10min，那么Consumer Group的管理者（group coordinator）可能需要10分钟才能感受到。太漫长了是吧。</li></ul><hr><ul><li>max. poll. interval. ms &lt;= 处理逻辑最大时间</li><li>这个参数是0.10.1.0版本后新增的，可能很多地方看不到喔。这个参数需要根据实际业务处理时间进行设置，一旦Consumer处理不过来，就会被踢出Consumer Group。</li><li>注意：如果业务平均处理逻辑为1分钟，那么max. poll. interval. ms需要设置稍微大于1分钟即可，但是session. timeout. ms可以设置小一点（如10s），用于快速检测Consumer崩溃。</li></ul><hr><ul><li>auto.offset.reset</li><li>该属性指定了消费者在读取一个没有偏移量或者偏移量无效（消费者长时间失效当前的偏移量已经过时并且被删除了）的分区的情况下，应该作何处理，默认值是latest，也就是从最新记录读取数据（消费者启动之后生成的记录），另一个值是earliest，意思是在偏移量无效的情况下，消费者从起始位置开始读取数据。</li></ul><hr><ul><li>enable.auto.commit</li><li>对于精确到一次的语义，最好手动提交位移</li></ul><hr><ul><li>fetch.max.bytes</li><li>单次获取数据的最大消息数。</li></ul><hr><ul><li>max.poll.records  &lt;=  吞吐量</li><li>单次poll调用返回的最大消息数，如果处理逻辑很轻量，可以适当提高该值。</li><li>一次从kafka中poll出来的数据条数,max.poll.records条数据需要在在session.timeout.ms这个时间内处理完</li><li>默认值为500</li></ul><hr><ul><li>heartbeat. interval. ms &lt;= 居然拖家带口</li><li>heartbeat心跳主要用于沟通交流，及时返回请求响应。这个时间间隔真是越快越好。因为一旦出现reblance,那么就会将新的分配方案或者通知重新加入group的命令放进心跳响应中。</li></ul><hr><ul><li>connection. max. idle. ms &lt;= socket连接</li><li>kafka会定期的关闭空闲Socket连接。默认是9分钟。如果不在乎这些资源开销，推荐把这些参数值为-1，即不关闭这些空闲连接。</li></ul><hr><ul><li>request. timeout. ms</li><li>这个配置控制一次请求响应的最长等待时间。如果在超时时间内未得到响应，kafka要么重发这条消息，要么超过重试次数的情况下直接置为失败。</li><li>消息发送的最长等待时间.需大于session.timeout.ms这个时间</li></ul><hr><ul><li>fetch.min.bytes</li><li>server发送到消费端的最小数据，若是不满足这个数值则会等待直到满足指定大小。默认为1表示立即接收。</li></ul><hr><ul><li><p>fetch.wait.max.ms</p></li><li><p>若是不满足fetch.min.bytes时，等待消费端请求的最长等待时间</p></li></ul><hr><ul><li>0.11 新功能</li><li>空消费组延时rebalance，主要在server.properties文件配置</li><li>group.initial.rebalance.delay.ms&lt;= ，防止成员加入请求后本应立即开启的rebalance</li><li>对于用户来说，这个改进最直接的效果就是新增了一个broker配置：group.initial.rebalance.delay.ms，</li><li>默认是3秒钟。</li><li>主要作用是让coordinator推迟空消费组接收到成员加入请求后本应立即开启的rebalance。在实际使用时，假设你预估你的所有consumer组成员加入需要在10s内完成，那么你就可以设置该参数=10000。</li></ul><h2 id="线上采坑"><a href="#线上采坑" class="headerlink" title="线上采坑"></a>线上采坑</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.clients</span><span class="selector-class">.consumer</span><span class="selector-class">.CommitFailedException</span>:</span><br><span class="line"> Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. </span><br><span class="line">This means that the <span class="selector-tag">time</span> between subsequent calls to poll() was longer than the configured session<span class="selector-class">.timeout</span><span class="selector-class">.ms</span>, which typically implies that the poll loop is spending too much <span class="selector-tag">time</span> message processing. </span><br><span class="line">You can <span class="selector-tag">address</span> this either by increasing the session timeout or by reducing the maximum size of batches returned <span class="keyword">in</span> poll() with max<span class="selector-class">.poll</span><span class="selector-class">.records</span>. [com<span class="selector-class">.bonc</span><span class="selector-class">.framework</span><span class="selector-class">.server</span><span class="selector-class">.kafka</span><span class="selector-class">.consumer</span><span class="selector-class">.ConsumerLoop</span>]</span><br><span class="line">复制代码</span><br></pre></td></tr></table></figure><p>基于最新版本10，注意此版本session. timeout. ms 与max.poll.interval.ms进行功能分离了。</p><ul><li>可以发现频繁reblance，并伴随者重复性消费，这是一个很严重的问题，就是处理逻辑过重,max.poll. interval.ms过小导致。发生的原因就是 poll（）的循环调用时间过长，出现了处理超时。此时只用调大max.poll. interval.ms ，调小max.poll.records即可，同时要把request. timeout. ms设置大于max.poll. interval.ms</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化会继续，暂时把核心放在request. timeout. ms, max. poll. interval. ms，max.poll.records 上，避免因为处理逻辑过重，导致Consumer被频繁的踢出Consumer group。</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产者Producer参数设置及调优</title>
      <link href="/2018/09/03/kafka%E7%94%9F%E4%BA%A7%E8%80%85Producer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/03/kafka%E7%94%9F%E4%BA%A7%E8%80%85Producer%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="Producer核心工作流程"><a href="#Producer核心工作流程" class="headerlink" title="Producer核心工作流程"></a>Producer核心工作流程</h2><ul><li>Producer首先使用用户主线程将待发送的消息封装进一个ProducerRecord类实例中。</li><li>进行序列化后，发送给Partioner，由Partioner确定目标分区后，发送到Producer程序中的一块内存缓冲区中。</li><li>Producer的另一个工作线程（即Sender线程），则负责实时地从该缓冲区中提取出准备好的消息封装到一个批次，统一发送给对应的broker中。</li></ul><h2 id="producer-主要参数设置"><a href="#producer-主要参数设置" class="headerlink" title="producer 主要参数设置"></a>producer 主要参数设置</h2><h3 id="producer-参数acks-设置（无数据丢失）"><a href="#producer-参数acks-设置（无数据丢失）" class="headerlink" title="producer 参数acks 设置（无数据丢失）"></a>producer 参数acks 设置（无数据丢失）</h3><p>在消息被认为是“已提交”之前，producer需要leader确认的produce请求的应答数。该参数用于控制消息的持久性，目前提供了3个取值：</p><p>acks = 0: 表示produce请求立即返回，不需要等待leader的任何确认。这种方案有最高的吞吐率，但是不保证消息是否真的发送成功。</p><p>acks = -1: 表示分区leader必须等待消息被成功写入到所有的ISR副本(同步副本)中才认为produce请求成功。这种方案提供最高的消息持久性保证，但是理论上吞吐率也是最差的。</p><p>acks = 1: 表示leader副本必须应答此produce请求并写入消息到本地日志，之后produce请求被认为成功。如果此时leader副本应答请求之后挂掉了，消息会丢失。这是个较好的方案，提供了不错的持久性保证和吞吐。</p><p><strong>商业环境推荐：</strong></p><p>如果要较高的持久性要求以及无数据丢失的需求，设置acks = -1。其他情况下设置acks = 1</p><h3 id="producer参数-buffer-memory-设置（吞吐量）"><a href="#producer参数-buffer-memory-设置（吞吐量）" class="headerlink" title="producer参数 buffer.memory 设置（吞吐量）"></a>producer参数 buffer.memory 设置（吞吐量）</h3><p>该参数用于指定Producer端用于缓存消息的缓冲区大小，单位为字节，默认值为：33554432合计为32M。kafka采用的是异步发送的消息架构，prducer启动时会首先创建一块内存缓冲区用于保存待发送的消息，然后由一个专属线程负责从缓冲区读取消息进行真正的发送。</p><p><strong>商业环境推荐：</strong></p><ul><li>消息持续发送过程中，当缓冲区被填满后，producer立即进入阻塞状态直到空闲内存被释放出来，这段时间不能超过max.blocks.ms设置的值，一旦超过，producer则会抛出TimeoutException 异常，因为Producer是线程安全的，若一直报TimeoutException，需要考虑调高buffer.memory 了。</li><li>用户在使用多个线程共享kafka producer时，很容易把 buffer.memory 打满。</li></ul><h3 id="producer参数-compression-type-设置（lZ4）"><a href="#producer参数-compression-type-设置（lZ4）" class="headerlink" title="producer参数 compression.type 设置（lZ4）"></a>producer参数 compression.type 设置（lZ4）</h3><p>producer压缩器，目前支持none（不压缩），gzip，snappy和lz4。</p><p><strong>商业环境推荐：</strong></p><p>基于公司的大数据平台，试验过目前lz4的效果最好。当然2016年8月，FaceBook开源了Ztandard。官网测试： Ztandard压缩率为2.8，snappy为2.091，LZ4 为2.101 。</p><h3 id="producer参数-retries设置-注意消息乱序-EOS"><a href="#producer参数-retries设置-注意消息乱序-EOS" class="headerlink" title="producer参数 retries设置(注意消息乱序,EOS)"></a>producer参数 retries设置(注意消息乱序,EOS)</h3><p>producer重试的次数设置。重试时producer会重新发送之前由于瞬时原因出现失败的消息。瞬时失败的原因可能包括：元数据信息失效、副本数量不足、超时、位移越界或未知分区等。倘若设置了retries &gt; 0，那么这些情况下producer会尝试重试。</p><p><strong>商业环境推荐：</strong></p><ul><li>producer还有个参数：max.in.flight.requests.per.connection。如果设置该参数大约1，那么设置retries就有可能造成发送消息的乱序。</li><li>版本为0.11.0.1的kafka已经支持”精确到一次的语义”，因此消息的重试不会造成消息的重复发送。</li></ul><h3 id="producer参数batch-size设置-吞吐量和延时性能"><a href="#producer参数batch-size设置-吞吐量和延时性能" class="headerlink" title="producer参数batch.size设置(吞吐量和延时性能)"></a>producer参数batch.size设置(吞吐量和延时性能)</h3><p>producer都是按照batch进行发送的，因此batch大小的选择对于producer性能至关重要。producer会把发往同一分区的多条消息封装进一个batch中，当batch满了后，producer才会把消息发送出去。但是也不一定等到满了，这和另外一个参数linger.ms有关。默认值为16K，合计为16384.</p><p><strong>商业环境推荐：</strong></p><ul><li>batch 越小，producer的吞吐量越低，越大，吞吐量越大。</li></ul><h3 id="producer参数linger-ms设置-吞吐量和延时性能"><a href="#producer参数linger-ms设置-吞吐量和延时性能" class="headerlink" title="producer参数linger.ms设置(吞吐量和延时性能)"></a>producer参数linger.ms设置(吞吐量和延时性能)</h3><p>producer是按照batch进行发送的，但是还要看linger.ms的值，默认是0，表示不做停留。这种情况下，可能有的batch中没有包含足够多的produce请求就被发送出去了，造成了大量的小batch，给网络IO带来的极大的压力。</p><p><strong>商业环境推荐：</strong></p><ul><li>为了减少了网络IO，提升了整体的TPS。假设设置linger.ms=5，表示producer请求可能会延时5ms才会被发送。</li></ul><h3 id="producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能"><a href="#producer参数max-in-flight-requests-per-connection设置-吞吐量和延时性能" class="headerlink" title="producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)"></a>producer参数max.in.flight.requests.per.connection设置(吞吐量和延时性能)</h3><p>producer的IO线程在单个Socket连接上能够发送未应答produce请求的最大数量。增加此值应该可以增加IO线程的吞吐量，从而整体上提升producer的性能。不过就像之前说的如果开启了重试机制，那么设置该参数大于1的话有可能造成消息的乱序。</p><p><strong>商业环境推荐：</strong></p><ul><li>默认值5是一个比较好的起始点,如果发现producer的瓶颈在IO线程，同时各个broker端负载不高，那么可以尝试适当增加该值.</li></ul><ul><li>过大增加该参数会造成producer的整体内存负担，同时还可能造成不必要的锁竞争反而会降低TPS</li></ul><h2 id="Java客户端"><a href="#Java客户端" class="headerlink" title="Java客户端"></a>Java客户端</h2><p>KafkaProducer(org.apache.kafka.clients.producer.KafkaProducer)是一个用于向kafka集群发送数据的Java客户端。该Java客户端是线程安全的，多个线程可以共享同一个producer实例，而且这通常比在多个线程中每个线程创建一个实例速度要快些。本文介绍的内容来自于kafka官方文档，详情参见<a href="http://kafka.apache.org/0110/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html" target="_blank" rel="noopener">KafkaProducer</a> </p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">package com.test.kafkaProducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.PartitionInfo;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> class TestProducer &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"bootstrap.servers"</span>, <span class="string">"192.168.137.200:9092"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.<span class="built_in">put</span>(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="comment">//生产者发送消息 </span></span><br><span class="line">        <span class="keyword">String</span> topic = <span class="string">"mytopic"</span>;</span><br><span class="line">        Producer&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; procuder = <span class="keyword">new</span> KafkaProducer&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(props);</span><br><span class="line">        <span class="built_in">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">String</span> value = <span class="string">"value_"</span> + i;</span><br><span class="line">            ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; msg = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(topic, value);</span><br><span class="line">            procuder.send(msg);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//列出topic的相关信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = <span class="keyword">new</span> ArrayList&lt;PartitionInfo&gt;() ;</span><br><span class="line">        partitions = procuder.partitionsFor(topic);</span><br><span class="line">        <span class="built_in">for</span>(PartitionInfo p:partitions)</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.<span class="built_in">println</span>(p);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.<span class="built_in">println</span>(<span class="string">"send message over."</span>);</span><br><span class="line">        procuder.<span class="built_in">close</span>(<span class="number">100</span>,TimeUnit.MILLISECONDS);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>producer包含一个用于保存待发送消息的缓冲池，缓冲池中消息是还没来得及传输到kafka集群的消息。位于底层的kafka I/O线程负责将缓冲池中的消息转换成请求发送到集群。如果在结束produce时，没有调用close()方法，那么这些资源会发生泄露。<br>用于建立消费者的相关参数说明及其默认值参见producerconfigs，此处对代码中用到的几个参数进行解释：<br>bootstrap.servers:用于初始化时建立链接到kafka集群，以host:port形式，多个以逗号分隔host1:port1,host2:port2；<br>acks:生产者需要server端在接收到消息后，进行反馈确认的尺度，主要用于消息的可靠性传输；acks=0表示生产者不需要来自server的确认；acks=1表示server端将消息保存后即可发送ack，而不必等到其他follower角色的都收到了该消息；acks=all(or acks=-1)意味着server端将等待所有的副本都被接收后才发送确认。<br>retries:生产者发送失败后，重试的次数<br>batch.size:当多条消息发送到同一个partition时，该值控制生产者批量发送消息的大小，批量发送可以减少生产者到服务端的请求数，有助于提高客户端和服务端的性能。<br>linger.ms:默认情况下缓冲区的消息会被立即发送到服务端，即使缓冲区的空间并没有被用完。可以将该值设置为大于0的值，这样发送者将等待一段时间后，再向服务端发送请求，以实现每次请求可以尽可能多的发送批量消息。<br>batch.size和linger.ms是两种实现让客户端每次请求尽可能多的发送消息的机制，它们可以并存使用，并不冲突。<br>buffer.memory:生产者缓冲区的大小，保存的是还未来得及发送到server端的消息，如果生产者的发送速度大于消息被提交到server端的速度，该缓冲区将被耗尽。<br>key.serializer,value.serializer说明了使用何种序列化方式将用户提供的key和vaule值序列化成字节。</p></blockquote><p><strong>kafka客户端的API</strong>  </p><p><strong>KafkaProducer对象实例化方法</strong>,可以使用map形式的键值对或者Properties对象来配置客户端的属性</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keySerializer:发送数据key值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> *valueSerializer:发送数据value值的序列化方法，该方法实现了Serializer接口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Map&lt;String,Object&gt; configs, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(Properties properties, Serializer&lt;K&gt; keySerializer,Serializer&lt;V&gt; valueSerializer)</span></span>;</span><br></pre></td></tr></table></figure><p><strong>消息发送方法send()</strong></p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *record:key-value形式的待发送数据</span></span><br><span class="line"><span class="comment"> *callback:到发送的消息被borker端确认后的回调函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record</span>)</span>; <span class="comment">// Equivalent to send(record, null)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span>(<span class="params">ProducerRecord&lt;K,V&gt; record,Callback callback</span>)</span>;</span><br></pre></td></tr></table></figure><p>send方法负责将缓冲池中的消息异步的发送到broker的指定topic中。异步发送是指，方法将消息存储到底层待发送的I/O缓存后，将立即返回，这可以实现并行无阻塞的发送更多消息。send方法的返回值是RecordMetadata类型，它含有消息将被投递的partition信息，该条消息的offset，以及时间戳。<br>因为send返回的是Future对象，因此在该对象上调用get()方法将阻塞，直到相关的发送请求完成并返回元数据信息；或者在发送时抛出异常而退出。<br>阻塞发送的方法如下：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">String</span> <span class="built_in">key</span> = <span class="string">"Key"</span>;</span><br><span class="line"><span class="keyword">String</span> value = <span class="string">"Value"</span>;</span><br><span class="line">ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(<span class="built_in">key</span>, value);</span><br><span class="line">producer.send(record).<span class="built_in">get</span>();</span><br></pre></td></tr></table></figure><p>可以充分利用回调函数和异步发送方式来确认消息发送的进度:</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt; record = <span class="keyword">new</span> ProducerRecord&lt;<span class="keyword">String</span>,<span class="keyword">String</span>&gt;(<span class="string">"the-topic"</span>, <span class="built_in">key</span>, value);</span><br><span class="line">producer.send(myRecord, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> onCompletion(RecordMetadata metadata, Exception e) &#123;</span><br><span class="line">                        <span class="keyword">if</span>(e != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            e.printStackTrace();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            System.out.<span class="built_in">println</span>(<span class="string">"The offset of the record we just sent is: "</span> + metadata.offset());</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure><p><strong>flush</strong> </p><p>立即发送缓存数据</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flush</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure><p>调用该方法将使得缓冲区的所有消息被立即发送（即使linger.ms参数被设置为大于0），且会阻塞直到这些相关消息的发送请求完成。flush方法的前置条件是：之前发送的所有消息请求已经完成。一个请求被视为完成是指：根据acks参数配置项收到了相应的确认，或者发送中抛出异常失败了。<br>下面的例子展示了从一个topic消费后发到另一个topic，flush方法在此非常有用，它提供了一种方便的方法来确保之前发送的消息确实已经完成了：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">for</span>(ConsumerRecord&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; record: consumer.poll(<span class="number">100</span>))</span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord(<span class="string">"my-topic"</span>, record.key(), record.value());</span><br><span class="line">producer.<span class="built_in">flush</span>();  <span class="comment">//将缓冲区的消息立即发送</span></span><br><span class="line">consumer.commit(); <span class="comment">//消费者手动确认消费进度</span></span><br></pre></td></tr></table></figure><p><strong>partitionsFor</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取指定topic的partition元数据信息</span></span><br><span class="line"><span class="keyword">public</span> <span class="built_in">List</span>&lt;PartitionInfo&gt; partitionsFor(<span class="built_in">String</span> topic);</span><br></pre></td></tr></table></figure><p><strong>close</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//关闭producer，方法将被阻塞，直到之前的发送请求已经完成</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;<span class="comment">//  equivalent to close(Long.MAX_VALUE, TimeUnit.MILLISECONDS)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout,TimeUnit timeUnit)</span></span>; <span class="comment">//同上，方法将等待timeout时长，以让未完成的请求完成发送</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka集群Broker端参数设置及调优</title>
      <link href="/2018/09/02/kafka%E9%9B%86%E7%BE%A4Broker%E7%AB%AF%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/"/>
      <url>/2018/09/02/kafka%E9%9B%86%E7%BE%A4Broker%E7%AB%AF%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E5%8F%8A%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h2 id="Distributed-streaming-platform"><a href="#Distributed-streaming-platform" class="headerlink" title="Distributed streaming platform"></a>Distributed streaming platform</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Apache Kafka® is <span class="keyword">a</span> distributed streaming <span class="built_in">platform</span>. What exactly does that mean?</span><br><span class="line">A streaming <span class="built_in">platform</span> has <span class="literal">three</span> key capabilities:</span><br><span class="line">   -  Publish <span class="keyword">and</span> subscribe <span class="built_in">to</span> streams <span class="keyword">of</span> records, similar <span class="built_in">to</span> <span class="keyword">a</span> message queue <span class="keyword">or</span> enterprise messaging <span class="keyword">system</span>.</span><br><span class="line">   - similar <span class="built_in">to</span> <span class="keyword">a</span> message queue <span class="keyword">or</span> enterprise messaging <span class="keyword">system</span>.</span><br><span class="line">   -  Process streams <span class="keyword">of</span> records <span class="keyword">as</span> they occur.</span><br><span class="line"></span><br><span class="line">Kafka is generally used <span class="keyword">for</span> <span class="literal">two</span> broad classes <span class="keyword">of</span> applications:</span><br><span class="line">- Building real-<span class="built_in">time</span> streaming data pipelines that reliably <span class="built_in">get</span> data between systems <span class="keyword">or</span> applications</span><br><span class="line">- Building real-<span class="built_in">time</span> streaming applications that transform <span class="keyword">or</span> react <span class="built_in">to</span> <span class="keyword">the</span> streams <span class="keyword">of</span> data</span><br><span class="line"></span><br><span class="line">To understand how Kafka does these things, let<span class="string">'s dive in and explore Kafka'</span>s capabilities <span class="built_in">from</span> <span class="keyword">the</span> bottom up.</span><br><span class="line">First <span class="keyword">a</span> few concepts:</span><br><span class="line"></span><br><span class="line">- Kafka is run <span class="keyword">as</span> <span class="keyword">a</span> cluster <span class="keyword">on</span> <span class="title">one</span> <span class="title">or</span> <span class="title">more</span> <span class="title">servers</span> <span class="title">that</span> <span class="title">can</span> <span class="title">span</span> <span class="title">multiple</span> <span class="title">datacenters</span>.</span><br><span class="line">- The Kafka cluster stores streams <span class="keyword">of</span> records <span class="keyword">in</span> categories called topics.</span><br><span class="line">- Each record consists <span class="keyword">of</span> <span class="keyword">a</span> key, <span class="keyword">a</span> <span class="built_in">value</span>, <span class="keyword">and</span> <span class="keyword">a</span> timestamp.</span><br></pre></td></tr></table></figure><h2 id="Kafka-as-a-Storage-System"><a href="#Kafka-as-a-Storage-System" class="headerlink" title="Kafka as a Storage System"></a>Kafka as a Storage System</h2><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Any message queue <span class="keyword">that</span> allows publishing messages decoupled <span class="keyword">from</span> consuming them </span><br><span class="line"><span class="keyword">is</span> effectively acting <span class="keyword">as</span> a storage system <span class="keyword">for</span> <span class="keyword">the</span> <span class="keyword">in</span>-flight messages. What <span class="keyword">is</span> </span><br><span class="line">different <span class="keyword">about</span> Kafka <span class="keyword">is</span> <span class="keyword">that</span> <span class="keyword">it</span> <span class="keyword">is</span> a very good storage system.</span><br><span class="line"></span><br><span class="line">- Data written <span class="keyword">to</span> Kafka <span class="keyword">is</span> written <span class="keyword">to</span> disk <span class="keyword">and</span> replicated <span class="keyword">for</span> fault-tolerance. </span><br><span class="line">Kafka allows producers <span class="keyword">to</span> wait <span class="keyword">on</span> acknowledgement so <span class="keyword">that</span> a <span class="built_in">write</span> <span class="keyword">isn't</span> considered</span><br><span class="line">complete <span class="keyword">until</span> <span class="keyword">it</span> <span class="keyword">is</span> fully replicated <span class="keyword">and</span> guaranteed <span class="keyword">to</span> persist even <span class="keyword">if</span> <span class="keyword">the</span> server </span><br><span class="line">written <span class="keyword">to</span> fails.</span><br><span class="line"></span><br><span class="line">- The disk structures Kafka uses scale well，Kafka will perform <span class="keyword">the</span> same whether you </span><br><span class="line">have <span class="number">50</span> KB <span class="keyword">or</span> <span class="number">50</span> TB <span class="keyword">of</span> persistent data <span class="keyword">on</span> <span class="keyword">the</span> server.</span><br><span class="line"></span><br><span class="line">- As a <span class="literal">result</span> <span class="keyword">of</span> taking storage seriously <span class="keyword">and</span> allowing <span class="keyword">the</span> clients <span class="keyword">to</span> control </span><br><span class="line">their <span class="built_in">read</span> position, you can think <span class="keyword">of</span> Kafka <span class="keyword">as</span> a kind <span class="keyword">of</span> special purpose </span><br><span class="line">distributed filesystem dedicated <span class="keyword">to</span> high-performance, low-latency commit </span><br><span class="line"><span class="built_in">log</span> storage, replication, <span class="keyword">and</span> propagation.</span><br></pre></td></tr></table></figure><h2 id="kafka实现高吞吐率的秘密"><a href="#kafka实现高吞吐率的秘密" class="headerlink" title="kafka实现高吞吐率的秘密"></a>kafka实现高吞吐率的秘密</h2><ul><li>一个用户程序要把文件内容发送到网络，这个用户程序是工作在用户空间，文件和网络socket属于硬件资源，两者之间有一个内核空间。</li></ul><p>​        在Linux kernel2.2 之后出现了一种叫做”零拷贝(zero-copy)”系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”</p><ul><li>kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中，通过分段的方式，每次文件操作都是对一个小文件的操作，增加了并行处理能力</li></ul><ul><li>kafka允许进行批量发送消息，先将消息缓存在内存中，然后通过一次请求批量把消息发送出去，比如：可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去，如100条消息就发送，或者每5秒发送一次这种策略将大大减少服务端的I/O次数。</li><li>kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式或LZ4对消息集合进行压缩,压缩的好处就是减少传输的数据量，减轻对网络传输的压力。</li></ul><h2 id="kafka集群Broker端全局参数设置"><a href="#kafka集群Broker端全局参数设置" class="headerlink" title="kafka集群Broker端全局参数设置"></a>kafka集群Broker端全局参数设置</h2><hr><ul><li>broker. id</li></ul><p>唯一的整数来标识每个broker，不能与其他broker冲突，建议从0开始。</p><hr><ul><li>log.dirs    &lt;= 吞吐量提升</li></ul><p>确保该目录有比较大的硬盘空间。如果需要指定多个目录，以逗号分隔即可，比如/xin/kafka1,/xin/kafka2。这样做的好处是Kafka会力求均匀地在多个目录下存放分区(partition)数据。如果挂载多块磁盘，那么会有多个磁头同时执行写操作。对吞吐量具有非常强的提升。</p><hr><ul><li>zookeeper.connect</li></ul><p>该参数则完全没有默认值，必须要配置。这个参数也可以是一个逗号分隔值的列表，比如zk1:2181,zk2:2181,zk3:2181/kafka。注意结尾的/kafka，它是zookeeper的chroot，是可选的配置，如果不指定的话就默认使用zookeeper的根路径。</p><hr><ul><li>listeners</li></ul><p>协议配置包括PLAINTEXT，SSL, SASL_SSL等，格式是[协议]://[主机名]:[端口],[[协议]://[主机名]:[端口]]，该参数是Brocker端开发给clients的监听端口。建议配置：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">PLAINTEXT:</span><span class="comment">//hostname:port（未启用安全认证）</span></span><br><span class="line"><span class="symbol">SSL:</span><span class="comment">//hostname:port（启用安全认证）</span></span><br></pre></td></tr></table></figure><hr><ul><li>unclean.leader.election.enable  &lt;= 数据的完整性保证</li></ul><p>解决ISR所有副本为空，leader又出现宕机的情况。此时leader该如何选择呢？截止kafka 1.0.0版本，该参数默认为false，表示不允许选择非ISR副本集之外的broker。因为高可用性与数据的完整性，kafka官方选择了后者。</p><hr><ul><li>delete.topic.enable</li></ul><p>不多说，是否允许删除topic，鉴于0.9.0.0新增了ACL机制权限机制，误操作基本是不存在的。</p><hr><ul><li>log.retention.{hours|minutes|ms}  &lt;=时间维度</li></ul><p>优先选取ms的配置，minutes次之，hours最后，默认留存机制是7天。如何判断：</p><p>新版本：基于消息中的时间戳来进行判断。老版本：根据日志文件的最新修改时间进行比较.</p><hr><ul><li>log.retention.bytes  &lt;=空间维度</li></ul><p>Kafka会定期删除那些大小超过该参数值的日志文件。默认值是-1，表示Kafka永远不会根据大小来删除日志</p><hr><ul><li>min.insync.replicas &lt;=     与acks=-1 搭配使用</li></ul><p>持久化级别，用于最少需要多少副本同步。在acks=all(或-1) 时才有意义。min.insync.replicas指定了必须要应答写请求的最小数量的副本数。如果不能满足，producer将会抛出NotEnoughReplicas或NotEnoughReplicasAfterAppend异常。该参数用于实现更好的消息持久性。</p><p>举例如下：</p><p>5台broker  ack =-1  min.insync.replicas = 3</p><p>上述表示最少需要3个副本同步后，Broker才能够对外提供服务,否则将会抛出异常。若3台Broker宕机，即使剩余2台全部同步结束，满足了 ack =-1也要报错。</p><hr><ul><li>num.network.threads  &lt;= 请求转发线程数量</li></ul><p>默认值为3，主要负责转发来自broker和clients发送过来的各种请求。强调这里只是转发，真实环境下，需要监听NetWorkerProcessorAvgIdlePercent JMX指标，若指标低于0.3，则建议调高该值。</p><hr><ul><li>num.io.threads  &lt;= 实际处理线程数量</li></ul><p>默认是8，也即broker后端有8个线程以轮询的方式不停的监听转发过来的网络请求并进行实时处理。</p><hr><ul><li>message.max.bytes</li></ul><p>broker能够接收的最大消息大小，默认是977KB。因此注意，生产环境应该调高该值。</p><h2 id="kafka集群Broker端Topic级别参数设置"><a href="#kafka集群Broker端Topic级别参数设置" class="headerlink" title="kafka集群Broker端Topic级别参数设置"></a>kafka集群Broker端Topic级别参数设置</h2><ul><li>delete.topic.enable</li><li>message.max.bytes</li><li>log.retention.bytes</li></ul><h2 id="操作系统参数设置"><a href="#操作系统参数设置" class="headerlink" title="操作系统参数设置"></a>操作系统参数设置</h2><ul><li>OS页缓存刷盘flush时间  &lt;= 提升吞吐量</li></ul><p>默认是5秒，间隔实在太短，适当增加该值可以很高的提行OS物理写入操作的性能。LinkedIn设置为2分钟来提升吞吐量。</p><ul><li>文件系统选择</li></ul><p>官方测试XFS文件系统写入时间为160秒，Ext4大约是250毫秒。建议生产环境最好使用XFS文件系统。</p><ul><li>OS文件描述符限制</li></ul><p>OS系统最大打开的文件描述符是有上限的，举例：一个kafka集群主要有3个副本，50个分区，若每一个分区文件大小为10G，而分区内日志段大小为1GB，则一个Broker需要维护1500个左右的文件描述符。因此根据需要设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ulimit</span> -n 100000</span><br></pre></td></tr></table></figure><ul><li>OS 操作系统缓冲区设置 （尚不确定）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产者和消费者吞吐量测试</title>
      <link href="/2018/09/02/kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%9E%E5%90%90%E9%87%8F%E6%B5%8B%E8%AF%95/"/>
      <url>/2018/09/02/kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E5%90%9E%E5%90%90%E9%87%8F%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h2 id="测试集群配置："><a href="#测试集群配置：" class="headerlink" title="测试集群配置："></a>测试集群配置：</h2><blockquote><p><strong>三台4核cpu/8G内存/50G硬盘的CentOS7机器</strong></p><p><strong>Kafka版本：Kafka_2.11-0.11.0.1</strong></p></blockquote><h2 id="kafka生产者吞吐量测试"><a href="#kafka生产者吞吐量测试" class="headerlink" title="kafka生产者吞吐量测试"></a>kafka生产者吞吐量测试</h2><p><strong>使用kafka-producer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic test --num-records 500000 --record-size 200 --throughput -1 --producer-props bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092 acks=1</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">500000</span> records sent ,<span class="number">180701.120347</span> records/sec (<span class="number">34.47</span> MB/sec),<span class="number">315.17</span> ms/avg latency ,<span class="number">982.00</span> ms max latency ,<span class="number">167</span>ms <span class="number">50</span>h ,<span class="number">902</span>ms <span class="number">95</span>th ,<span class="number">969</span> ms <span class="number">99</span>h, <span class="number">981</span>ms <span class="number">99.9</span>th</span><br></pre></td></tr></table></figure><blockquote><p>测试结果显示：</p><p>发送了500000条消息，kafka 的平均吞吐量是34.47 MB/sec ，即占用275Mb/s左右的带宽，平均每秒发送180701条消息。</p><p>平均延时为315 ms，最大延时为982 ms，</p><p>发送50%的消息需要167ms，发送95%的消息需要902ms，发送99%的消息需要969ms，发送99.9%的消息需要981ms。</p></blockquote><h2 id="kafka消费者吞吐量测试"><a href="#kafka消费者吞吐量测试" class="headerlink" title="kafka消费者吞吐量测试"></a>kafka消费者吞吐量测试</h2><p><strong>使用kafka-consumer-perf-test测试脚本，总共测试50万条数据量</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --message-size 200 --messages 500000 --topic test</span><br></pre></td></tr></table></figure><p><strong>测试结果分析如下：</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:41</span><span class="selector-pseudo">:388</span>, 2019<span class="selector-tag">-09-18</span> 17<span class="selector-pseudo">:56</span><span class="selector-pseudo">:43</span><span class="selector-pseudo">:906</span>, 95<span class="selector-class">.3674</span>, 37<span class="selector-class">.8743</span>, 500000, 198570<span class="selector-class">.2939</span></span><br></pre></td></tr></table></figure><blockquote><p>看测试结果显示：</p><p>消费了95.3674MB消息，吞吐量为37.8743MB/s,也即303Mb/s，</p><p>消费了500000条消息，吞吐量为198570条/s</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka0.11版本集群重要操作指令</title>
      <link href="/2018/09/01/kafka0.11%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4%E9%87%8D%E8%A6%81%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4/"/>
      <url>/2018/09/01/kafka0.11%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4%E9%87%8D%E8%A6%81%E6%93%8D%E4%BD%9C%E6%8C%87%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h2><p>前台启动broker</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="keyword">server</span>-start.sh &lt;path&gt;/<span class="keyword">server</span>.properties</span><br></pre></td></tr></table></figure><p>Ctrl + C 关闭</p><p>后台启动broker</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="keyword">server</span>-start.sh -daemon &lt;path&gt;/<span class="keyword">server</span>.properties</span><br></pre></td></tr></table></figure><p>后台启动broker并开放JMX端口</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JMX_PORT=<span class="number">9999</span> bin/kafka-<span class="keyword">server</span>-start.sh -daemon &lt;path&gt;/<span class="keyword">server</span>.properties</span><br></pre></td></tr></table></figure><p>关闭broker</p><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">server</span>-<span class="keyword">stop</span>.sh</span><br></pre></td></tr></table></figure><h2 id="Topic管理"><a href="#Topic管理" class="headerlink" title="Topic管理"></a>Topic管理</h2><p>创建topic</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">3</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">3</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span></span><br></pre></td></tr></table></figure><p>删除topic</p><p>(需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启)</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">delete</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span></span><br></pre></td></tr></table></figure><p>查询topic列表</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.<span class="keyword">sh</span> --zookeeper localhos<span class="variable">t:2181</span> --<span class="keyword">list</span></span><br></pre></td></tr></table></figure><p>查询topic详情</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">describe</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span></span><br></pre></td></tr></table></figure><p>修改topic</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">alter</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">6</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span></span><br></pre></td></tr></table></figure><h2 id="Consumer-Groups管理"><a href="#Consumer-Groups管理" class="headerlink" title="Consumer-Groups管理"></a>Consumer-Groups管理</h2><p>查询消费者组</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.<span class="keyword">sh</span> --<span class="keyword">bootstrap</span>-server localhost:9092 --<span class="keyword">list</span></span><br></pre></td></tr></table></figure><p>查询消费者组详情</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">describe</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span></span><br></pre></td></tr></table></figure><p>重设消费者组位移</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">最早处</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">reset</span><span class="literal">-</span><span class="comment">offsets</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">all</span><span class="literal">-</span><span class="comment">topics</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">to</span><span class="literal">-</span><span class="comment">earliest</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">execute</span></span><br><span class="line"><span class="comment">最新处</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">reset</span><span class="literal">-</span><span class="comment">offsets</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">all</span><span class="literal">-</span><span class="comment">topics</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">to</span><span class="literal">-</span><span class="comment">latest</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">execute</span></span><br><span class="line"><span class="comment">某个位置</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">reset</span><span class="literal">-</span><span class="comment">offsets</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">all</span><span class="literal">-</span><span class="comment">topics</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">to</span><span class="literal">-</span><span class="comment">offset</span> <span class="comment">2000</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">execute</span></span><br><span class="line"><span class="comment">调整到某个时间之后得最早位移</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">reset</span><span class="literal">-</span><span class="comment">offsets</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">all</span><span class="literal">-</span><span class="comment">topics</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">to</span><span class="literal">-</span><span class="comment">datetime</span> <span class="comment">2019</span><span class="literal">-</span><span class="comment">09</span><span class="literal">-</span><span class="comment">15T00:00:00</span><span class="string">.</span><span class="comment">000</span></span><br></pre></td></tr></table></figure><p>删除消费者组</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">groups</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">delete</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">group</span> <span class="comment">groupname</span></span><br></pre></td></tr></table></figure><h2 id="脚本工具"><a href="#脚本工具" class="headerlink" title="脚本工具"></a>脚本工具</h2><p>producer脚本</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh <span class="params">--broker-list</span> localhost<span class="function">:9092</span> <span class="params">--topic</span> topicname </span><br><span class="line">参数含义：</span><br><span class="line"><span class="params">--compression-codec</span> lz4  压缩类型</span><br><span class="line"><span class="params">--request-required-acks</span> all acks的值</span><br><span class="line"><span class="params">--timeout</span> 3000  linger.ms的值</span><br><span class="line"><span class="params">--message-send-max-retries</span> 10   retries的值</span><br><span class="line"><span class="params">--max-partition-memory-bytes</span> <span class="keyword">batch</span>.size值</span><br></pre></td></tr></table></figure><p>consumer脚本</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">console</span><span class="literal">-</span><span class="comment">consumer</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">from</span><span class="literal">-</span><span class="comment">beginning</span></span><br><span class="line"><span class="comment">指定groupid</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">console</span><span class="literal">-</span><span class="comment">consumer</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">from</span><span class="literal">-</span><span class="comment">beginning</span></span><br><span class="line"><span class="comment"></span><span class="literal">-</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">property</span> <span class="comment">group</span><span class="string">.</span><span class="comment">id=old</span><span class="literal">-</span><span class="comment">consumer</span><span class="literal">-</span><span class="comment">group</span></span><br><span class="line"><span class="comment">指定分区</span></span><br><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">console</span><span class="literal">-</span><span class="comment">consumer</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9092</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">topicname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">from</span><span class="literal">-</span><span class="comment">beginning</span></span><br><span class="line"><span class="comment"></span><span class="literal">-</span><span class="literal">-</span><span class="comment">partition</span> <span class="comment">0</span></span><br></pre></td></tr></table></figure><p>kafka-run-class脚本</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kafka-run-class<span class="selector-class">.sh</span> kafka<span class="selector-class">.tools</span><span class="selector-class">.ConsoleConsumer</span>   就是 kafka-console-consumer.sh</span><br><span class="line">kafka-run-class<span class="selector-class">.sh</span> kafka<span class="selector-class">.tools</span><span class="selector-class">.ConsoleProducer</span>   就是 kafka-console-producer.sh</span><br></pre></td></tr></table></figure><p>获取topic当前消息数</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-run-class<span class="selector-class">.sh</span> kafka<span class="selector-class">.tools</span><span class="selector-class">.GetOffsetShell</span> --broker-list localhost:<span class="number">9092</span> --topic topicname --<span class="selector-tag">time</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure><p>–time -1表示最大位移 –time -2表示最早位移</p><p>查询_consumer_offsets</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">bin/</span><span class="string">kafka-simple-</span><span class="string">consumer-shell.</span><span class="string">sh </span><span class="built_in">--topic</span> <span class="string">_consumer_offsets </span><span class="built_in">--partition</span> <span class="string">12 </span><span class="built_in">--broker-list</span> <span class="string">localhost:9092 </span><span class="built_in">--formatter</span> <span class="string">"kafka.coorfinator.GroupMetadataManager\$OffsetsMessageFormatter"</span></span><br></pre></td></tr></table></figure><h2 id="MirrorMaker"><a href="#MirrorMaker" class="headerlink" title="MirrorMaker"></a>MirrorMaker</h2><p>跨机房灾备工具</p><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">mirror</span><span class="literal">-</span><span class="comment">maker</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">consumer</span><span class="string">.</span><span class="comment">config</span> <span class="comment">consumer</span><span class="string">.</span><span class="comment">properties</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">producer</span><span class="string">.</span><span class="comment">config</span> <span class="comment">producer</span><span class="string">.</span><span class="comment">properties</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">whitelist</span> <span class="comment">topicA|topicB</span></span><br></pre></td></tr></table></figure><h2 id="Zookeeper常用命令"><a href="#Zookeeper常用命令" class="headerlink" title="Zookeeper常用命令"></a>Zookeeper常用命令</h2><p>查看znode中的内容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ls</span>  /</span><br></pre></td></tr></table></figure><p>创建普通的节点 </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create</span></span><br></pre></td></tr></table></figure><p>获得节点的信息</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">get</span></span><br></pre></td></tr></table></figure><p>创建临时节点  </p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> -<span class="built_in">e</span></span><br></pre></td></tr></table></figure><p>编号节点： </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create -s</span></span><br></pre></td></tr></table></figure><p> 删除一个节点 </p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span></span><br></pre></td></tr></table></figure><p>递归删除节点</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rmr</span></span><br></pre></td></tr></table></figure><p>修改节点内容</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span></span><br></pre></td></tr></table></figure><p>监听节点  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">get</span> /test watch</span><br></pre></td></tr></table></figure><p>在其他节点进行修改  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> /<span class="built_in">test</span> 555</span><br></pre></td></tr></table></figure><p>监听节点上收到WatchedEvent state:SyncConnected type:NodeDataChanged path:/test</p>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka生产环境规划</title>
      <link href="/2018/09/01/kafka%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92/"/>
      <url>/2018/09/01/kafka%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h2 id="操作系统选型"><a href="#操作系统选型" class="headerlink" title="操作系统选型"></a>操作系统选型</h2><p>因为kafka服务端代码是Scala语言开发的，因此属于JVM系的大数据框架，目前部署最多的3类操作系统主要由Linux ，OS X 和Windows,但是部署在Linux数量最多，为什么呢？因为I/O模型的使用和数据网络传输效率两点。</p><ul><li>第一：Kafka新版本的Clients在设计底层网络库时采用了Java的Select模型，而在Linux实现机制是epoll,感兴趣的读者可以查询一下epoll和select的区别，明确一点就是：kafka跑在Linux上效率更高，因为epoll取消了轮询机制，换成了回调机制，当底层连接socket数较多时，可以避免CPU的时间浪费。</li><li>第二：网络传输效率上。kafka需要通过网络和磁盘进行数据传输，而大部分操作系统都是通过Java的FileChannel.transferTo方法实现，而Linux操作系统则会调用sendFile系统调用，也即零拷贝（Zero Copy 技术），避免了数据在内核地址空间和用户程序空间进行重复拷贝。</li></ul><h2 id="磁盘类型规划"><a href="#磁盘类型规划" class="headerlink" title="磁盘类型规划"></a>磁盘类型规划</h2><ul><li>机械磁盘（HDD）一般机械磁盘寻道时间是毫秒级的，若有大量随机I/O，则将会出现指数级的延迟，但是kafka是顺序读写的，因此对于机械磁盘的性能也是不弱的，所以，基于成本问题可以考虑。</li><li>固态硬盘（SSD）读写速度可观，没有成本问题可以考虑。</li><li>JBOD (Just Bunch Of Disks ) 经济实惠的方案，对数据安全级别不是非常非常高的情况下可以采用，建议用户在Broker服务器上设置多个日志路径，每个路径挂载在不同磁盘上，可以极大提升并发的日志写入速度。</li><li>RAID 磁盘阵列常见的RAID是RAID10，或者称为（RAID 1+0） 这种磁盘阵列结合了磁盘镜像和磁盘带化技术来保护数据，因为使用了磁盘镜像技术，使用率只有50%，注意，LinkedIn公司采用的就是RAID作为存储来提供服务的。那么弊端在什么地方呢？如果Kafka副本数量设置为3，那么实际上数据将存在6倍的冗余数据，利用率实在太低。因此，LinkedIn正在计划更改方案为JBOD.</li></ul><h2 id="磁盘容量规划"><a href="#磁盘容量规划" class="headerlink" title="磁盘容量规划"></a>磁盘容量规划</h2><p>假设每天大约能够产生一亿条消息，假设副本replica设置为2 （其实我们设置为3），数据留存时间为1周，平均每条上报事件消息为1K左右，那么每天产生的消息总量为：1亿 乘 2 乘  1K 除以 1000 除以 1000 =200G磁盘。预留10%的磁盘空间，为210G。一周大约为1.5T。采用压缩，平均压缩比为0.5，整体磁盘容量为0.75T。关联因素主要有：</p><ul><li>新增消息数</li><li>副本数</li><li>是否启用压缩</li><li>消息大小</li><li>消息保留时间</li></ul><h2 id="内存容量规划"><a href="#内存容量规划" class="headerlink" title="内存容量规划"></a>内存容量规划</h2><p>kafka对于内存的使用，并不过多依赖JVM 内存,而是更多的依赖操作系统的页缓存，consumer若命中页缓存，则不用消耗物理I/O操作。一般情况下，java堆内存的使用属于朝生夕灭的，很快会被GC,一般情况下，不会超过6G，对于16G内存的机器，文件系统page cache 可以达到10-14GB。</p><ul><li>怎么设计page cache，可以设置为单个日志段文件大小，若日志段为10G,那么页缓存应该至少设计为10G以上。</li><li>堆内存最好不要超过6G。</li></ul><h2 id="CPU选择规划"><a href="#CPU选择规划" class="headerlink" title="CPU选择规划"></a>CPU选择规划</h2><p>kafka不属于计算密集型系统，因此CPU核数够多就可以，而不必追求时钟频率，因此核数选择最好大于8。</p><h2 id="网络带宽决定Broker数量"><a href="#网络带宽决定Broker数量" class="headerlink" title="网络带宽决定Broker数量"></a>网络带宽决定Broker数量</h2><p>带宽主要有1Gb/s 和10 Gb/s 。我们可以称为千兆位网络和万兆位网络。举例如下：我们的系统一天每小时都要处理1Tb的数据，我们选择1Gb/b带宽，那么需要选择多少机器呢？</p><ul><li>假设网络带宽kafka专用，且分配给kafka服务器70%带宽，那么单台Borker带宽就是710Mb/s，但是万一出现突发流量问题，很容易把网卡打满，因此在降低1/3,也即240Mb/s。因为1小时处理1TTB数据，每秒需要处理292MB,1MB=8Mb，也就是2336Mb数据，那么一小时处理1TB数据至少需要2336/240=10台Broker数据。冗余设计，最终可以定为20台机器。</li></ul><h2 id="典型推荐"><a href="#典型推荐" class="headerlink" title="典型推荐"></a>典型推荐</h2><ul><li>cpu 核数 32</li><li>内存 32GB</li><li>磁盘 3TB 7200转 SAS盘三块</li><li>带宽 1Gb/s</li></ul>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka0.11版本+Zookeeper-3.4.10集群部署</title>
      <link href="/2018/09/01/Kafka0.11%E7%89%88%E6%9C%AC+Zookeeper-3.4.10%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/09/01/Kafka0.11%E7%89%88%E6%9C%AC+Zookeeper-3.4.10%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.shields.io/badge/Author-Joker-green" alt></p><p><img src="https://img.shields.io/badge/Email-gaojintao999%40163.com-blue" alt></p><h2 id="集群环境："><a href="#集群环境：" class="headerlink" title="集群环境："></a>集群环境：</h2><table><thead><tr><th align="center">IP</th><th align="center">hostname</th><th align="center">配置</th></tr></thead><tbody><tr><td align="center">192.168.23.167</td><td align="center">kafka01</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.168</td><td align="center">kafka02</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr><tr><td align="center">192.168.23.169</td><td align="center">kafka03</td><td align="center">4核cpu/8G内存/50G硬盘</td></tr></tbody></table><p><strong>集群安装目录：/data/apps</strong></p><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><h3 id="root用户配置主机映射"><a href="#root用户配置主机映射" class="headerlink" title="root用户配置主机映射"></a>root用户配置主机映射</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.23.167  kafka01</span><br><span class="line">192.168.23.168  kafka02</span><br><span class="line">192.168.23.169  kafka03</span><br></pre></td></tr></table></figure><h3 id="在root用户下新建kafka用户（kafka-admin）"><a href="#在root用户下新建kafka用户（kafka-admin）" class="headerlink" title="在root用户下新建kafka用户（kafka/admin）"></a>在root用户下新建kafka用户（kafka/admin）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">adduser kafka -g kafka</span><br><span class="line">passwd kafka</span><br></pre></td></tr></table></figure><h3 id="在root用户下将apps目录下的用户及用户组均更改为kafka"><a href="#在root用户下将apps目录下的用户及用户组均更改为kafka" class="headerlink" title="在root用户下将apps目录下的用户及用户组均更改为kafka"></a>在root用户下将apps目录下的用户及用户组均更改为kafka</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R kafka:kafka /data/apps/</span><br></pre></td></tr></table></figure><h3 id="切换到kafka用户（之后的操作均在kafka用户下）"><a href="#切换到kafka用户（之后的操作均在kafka用户下）" class="headerlink" title="切换到kafka用户（之后的操作均在kafka用户下）"></a>切换到kafka用户（之后的操作均在kafka用户下）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su kafka</span><br></pre></td></tr></table></figure><h3 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id kafka@kafka01</span><br><span class="line">ssh-copy-id kafka@kafka02</span><br><span class="line">ssh-copy-id kafka@kafka03</span><br></pre></td></tr></table></figure><h3 id="将所有安装资源上传到kafka01节点的kafka用户的家目录下"><a href="#将所有安装资源上传到kafka01节点的kafka用户的家目录下" class="headerlink" title="将所有安装资源上传到kafka01节点的kafka用户的家目录下"></a>将所有安装资源上传到kafka01节点的kafka用户的家目录下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> zookeeper-3.4.10.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka_2.11-0.11.0.1.tar.gz (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka-manager-1.3.3.22 (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> influxdb-1.7.5.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> jmxtrans-270.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> grafana-6.0.2-1.x86_64.rpm (官网下载)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-kafka.sh (文件内容见附录)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> install-zookeeper.sh (文件内容见附录)</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行zookeeper集群安装脚本"><a href="#在kafka01节点上执行zookeeper集群安装脚本" class="headerlink" title="在kafka01节点上执行zookeeper集群安装脚本"></a>在kafka01节点上执行zookeeper集群安装脚本</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">sh</span> ~/install-zookeeper.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure><h3 id="在kafka01节点上执行kafka集群安装脚本"><a href="#在kafka01节点上执行kafka集群安装脚本" class="headerlink" title="在kafka01节点上执行kafka集群安装脚本"></a>在kafka01节点上执行kafka集群安装脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh ~/install-kafka.sh</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br><span class="line"><span class="meta">#</span><span class="bash"> 内容如下：</span></span><br><span class="line">export ZOOKEEPER_HOME=/data/apps/zookeeper-3.4.10</span><br><span class="line">export KAFKA_HOME=/data/apps/kafka_0.11.0.1</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin:$KAFKA_HOME/bin</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使环境变量生效</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动zookeeper集群"><a href="#在kafka01、02、03节点上执行命令启动zookeeper集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动zookeeper集群"></a>在kafka01、02、03节点上执行命令启动zookeeper集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="在kafka01、02、03节点上执行命令启动kafka集群"><a href="#在kafka01、02、03节点上执行命令启动kafka集群" class="headerlink" title="在kafka01、02、03节点上执行命令启动kafka集群"></a>在kafka01、02、03节点上执行命令启动kafka集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JMX_PORT=9999 kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties</span><br></pre></td></tr></table></figure><h3 id="Zookeeper常用命令"><a href="#Zookeeper常用命令" class="headerlink" title="Zookeeper常用命令"></a>Zookeeper常用命令</h3><p>查看znode中的内容</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ls</span>  /</span><br></pre></td></tr></table></figure><p>创建普通的节点 </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create</span></span><br></pre></td></tr></table></figure><p>获得节点的信息</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">get</span></span><br></pre></td></tr></table></figure><p>创建临时节点  </p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> -<span class="built_in">e</span></span><br></pre></td></tr></table></figure><p>编号节点： </p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">create -s</span></span><br></pre></td></tr></table></figure><p> 删除一个节点 </p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span></span><br></pre></td></tr></table></figure><p>递归删除节点</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">rmr</span></span><br></pre></td></tr></table></figure><p>修改节点内容</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span></span><br></pre></td></tr></table></figure><p>监听节点  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">get</span> /test watch</span><br></pre></td></tr></table></figure><p>在其他节点进行修改  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> /<span class="built_in">test</span> 555</span><br></pre></td></tr></table></figure><p>监听节点上收到WatchedEvent state:SyncConnected type:NodeDataChanged path:/test</p><h3 id="Kafka常用命令"><a href="#Kafka常用命令" class="headerlink" title="Kafka常用命令"></a>Kafka常用命令</h3><p>关闭Kafka（关闭Kafka之前禁止关闭Zookeeper）</p><figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-<span class="built_in">server</span>-<span class="keyword">stop</span>.sh</span><br></pre></td></tr></table></figure><p>创建Topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --replication-factor 3 --partitions 3 --topic test</span><br></pre></td></tr></table></figure><p>查看Topic列表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper kafka01:2181,kafka02:2181,kafka03:2181</span><br></pre></td></tr></table></figure><p>查看Topic详细信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><p>建立发布者console-producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --topic test</span><br></pre></td></tr></table></figure><p>建立订阅者console-consumer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server kafka01:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure><p>删除topic(需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --delete --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --topic test</span><br></pre></td></tr></table></figure><h2 id="使用kafka-manager管理kafka集群"><a href="#使用kafka-manager管理kafka集群" class="headerlink" title="使用kafka-manager管理kafka集群"></a>使用kafka-manager管理kafka集群</h2><p><strong>注：以下均在kafka用户下搭建，仅在kafka01节点上安装kafka-manager</strong></p><h3 id="将kafka-manager的安装包放到-data-apps目录下"><a href="#将kafka-manager的安装包放到-data-apps目录下" class="headerlink" title="将kafka-manager的安装包放到/data/apps目录下"></a>将kafka-manager的安装包放到/data/apps目录下</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv ~/kafka-manager<span class="number">-1.3</span><span class="number">.3</span><span class="number">.22</span> /data/apps</span><br></pre></td></tr></table></figure><h3 id="配置kafka-manager"><a href="#配置kafka-manager" class="headerlink" title="配置kafka-manager"></a>配置kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd conf</span><br><span class="line">vim application.conf</span><br><span class="line">修改kafka-manager.zkhosts="kafka01:2181,kafka02:2181,kafka03:2181"</span><br></pre></td></tr></table></figure><h3 id="启动kafka-manager"><a href="#启动kafka-manager" class="headerlink" title="启动kafka-manager"></a>启动kafka-manager</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;</span><br></pre></td></tr></table></figure><h3 id="kafka-manager的WebUI"><a href="#kafka-manager的WebUI" class="headerlink" title="kafka-manager的WebUI"></a>kafka-manager的WebUI</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka01:8080</span><br></pre></td></tr></table></figure><h2 id="使用jmxtrans-influxdb-grafana监控JMX指标"><a href="#使用jmxtrans-influxdb-grafana监控JMX指标" class="headerlink" title="使用jmxtrans+influxdb+grafana监控JMX指标"></a>使用jmxtrans+influxdb+grafana监控JMX指标</h2><p><strong>注：以下均在root用户下搭建，除了jmxtrans，其他组件仅在kafka01节点上安装</strong></p><h3 id="开启Kafka-JMX端口"><a href="#开启Kafka-JMX端口" class="headerlink" title="开启Kafka JMX端口"></a>开启Kafka JMX端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd bin</span><br><span class="line">vim kafka-run-class.sh</span><br></pre></td></tr></table></figure><p><strong>第一行增加<code>JMX_PORT=9999</code>即可</strong></p><p><strong>修改好后重启kafka，查看Kafka以及JMX端口状态</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ps</span> -ef | <span class="keyword">grep</span> kafka</span><br><span class="line">netstat -anop | <span class="keyword">grep</span> <span class="number">9999</span></span><br></pre></td></tr></table></figure><h3 id="安装InfluxDB"><a href="#安装InfluxDB" class="headerlink" title="安装InfluxDB"></a>安装InfluxDB</h3><p><strong>下载InfluxDB rpm安装包</strong></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http<span class="variable">s:</span>//<span class="keyword">dl</span>.influxdata.<span class="keyword">com</span>/influxdb/releases/influxdb-<span class="number">1.7</span>.<span class="number">5</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">influxdb-1</span><span class="selector-class">.7</span><span class="selector-class">.5</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动InfluxDB</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service <span class="literal">inf</span>luxdb <span class="literal">start</span>（systemctl <span class="literal">start</span> <span class="literal">inf</span>luxdb）</span><br></pre></td></tr></table></figure><p><strong>查看InfluxDB状态</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep influxdb</span><br><span class="line">service influxdb status（systemctl status influxdb）</span><br></pre></td></tr></table></figure><p><strong>使用InfluxDB客户端</strong></p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">influx</span></span><br></pre></td></tr></table></figure><p><strong>创建用户和数据库</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">"admin"</span> <span class="keyword">WITH</span> <span class="keyword">PASSWORD</span> <span class="string">'admin'</span> <span class="keyword">WITH</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="string">"jmxDB"</span>;</span><br></pre></td></tr></table></figure><p><strong>创建完成InfluxDB的用户和数据库暂时就够用了，其它简单操作如下，后面会用到</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建数据库</span></span><br><span class="line">create database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示所有的数据库</span></span><br><span class="line">show databases</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除数据库</span></span><br><span class="line">drop database "db_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用数据库</span></span><br><span class="line">use db_name</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示该数据库中所有的表</span></span><br><span class="line">show measurements</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建表，直接在插入数据的时候指定表名</span></span><br><span class="line">insert test,host=127.0.0.1,monitor_name=test count=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop measurement "measurement_name"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出</span></span><br><span class="line">quit</span><br></pre></td></tr></table></figure><h3 id="安装jmxtrans（所有kafka节点均安装）"><a href="#安装jmxtrans（所有kafka节点均安装）" class="headerlink" title="安装jmxtrans（所有kafka节点均安装）"></a>安装jmxtrans（所有kafka节点均安装）</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http:<span class="regexp">//</span>central.maven.org<span class="regexp">/maven2/</span>org<span class="regexp">/jmxtrans/</span>jmxtrans<span class="regexp">/270/</span>jmxtrans-<span class="number">270</span>.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">jmxtrans-270</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>jmxtrans相关路径</strong></p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jmxtrans安装目录：/usr/share/jmxtrans</span><br><span class="line">json文件默认目录：/var/<span class="class"><span class="keyword">lib</span>/<span class="title">jmxtrans</span>/</span></span><br><span class="line">日志路径：/var/log/jmxtrans/jmxtrans.log</span><br></pre></td></tr></table></figure><p><strong>配置json，jmxtrans的github上有一段示例配置</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"servers"</span> : [ &#123;</span><br><span class="line">    <span class="attr">"port"</span> : <span class="string">"9999"</span>,</span><br><span class="line">    <span class="attr">"host"</span> : <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="attr">"queries"</span> : [ &#123;</span><br><span class="line">      <span class="attr">"obj"</span> : <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line">      <span class="attr">"attr"</span> : [ <span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span> ],</span><br><span class="line">      <span class="attr">"resultAlias"</span>:<span class="string">"jvmMemory"</span>,</span><br><span class="line">      <span class="attr">"outputWriters"</span> : [ &#123;</span><br><span class="line">        <span class="attr">"@class"</span> : <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line">        <span class="attr">"url"</span> : <span class="string">"http://127.0.0.1:8086/"</span>,</span><br><span class="line">        <span class="attr">"username"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"password"</span> : <span class="string">"admin"</span>,</span><br><span class="line">        <span class="attr">"database"</span> : <span class="string">"jmxDB"</span>,</span><br><span class="line">        <span class="attr">"tags"</span>     : &#123;<span class="attr">"application"</span> : <span class="string">"kafka"</span>&#125;</span><br><span class="line">      &#125; ]</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>host：监控服务器</li><li>port：jmx端口</li><li>obj：对应jmx的ObjectName，就是我们要监控的指标</li><li>attr：对应ObjectName的属性，可以理解为我们要监控的指标的值</li><li>resultAlias：对应metric 的名称，在InfluxDB里面就是MEASUREMENTS名</li><li>tags：对应InfluxDB的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</li></ul><p><strong>附上两段配置的json示例文件（完整的均放在了三台节点的/var/lib/jmxtrans/目录下）</strong></p><p>base_10.164.204.248.json</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesOutPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"BytesRejectedPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesRejectedPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"OneMinuteRate"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MessagesInPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"FetchFollower"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"RequestsPerSec"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"request"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Memory"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"HeapMemoryUsage"</span>, <span class="string">"NonHeapMemoryUsage"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"MemoryUsage"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MemoryUsage"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=GarbageCollector,name=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"CollectionCount"</span>, <span class="string">"CollectionTime"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"GC"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"GC"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"java.lang:type=Threading"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"PeakThreadCount"</span>, <span class="string">"ThreadCount"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"Thread"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Thread"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaFetcherManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MaxLag"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=PartitionCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"PartitionCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"UnderReplicatedPartitions"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=LeaderCount"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LeaderCount"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"FetchConsumer"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>, <span class="string">"Max"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"TotalTimeMs"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"Produce"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=ReplicaManager,name=IsrShrinksPerSec"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"ReplicaManager"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"IsrShrinksPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>flink_sf_lx_248.json</strong></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"servers"</span>: [&#123;</span><br><span class="line"><span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line"><span class="attr">"host"</span>: <span class="string">"10.164.204.248"</span>,</span><br><span class="line"><span class="attr">"queries"</span>: [&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"BytesOutPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Count"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"MessagesInPerSec"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"obj"</span>: <span class="string">"kafka.log:type=Log,name=LogEndOffset,topic=FLINK_SF_LX,partition=*"</span>,</span><br><span class="line"><span class="attr">"attr"</span>: [<span class="string">"Value"</span>],</span><br><span class="line"><span class="attr">"resultAlias"</span>: <span class="string">"FLINK_SF_LX"</span>,</span><br><span class="line"><span class="attr">"outputWriters"</span>: [&#123;</span><br><span class="line"><span class="attr">"@class"</span>: <span class="string">"com.googlecode.jmxtrans.model.output.InfluxDbWriterFactory"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://10.164.204.248:8086/"</span>,</span><br><span class="line"><span class="attr">"username"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"password"</span>: <span class="string">"admin"</span>,</span><br><span class="line"><span class="attr">"database"</span>: <span class="string">"jmxDB"</span>,</span><br><span class="line"><span class="attr">"tags"</span>: &#123;</span><br><span class="line"><span class="attr">"application"</span>: <span class="string">"LogEndOffset"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br><span class="line">]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>配置说明：</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">1、全局指标</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesOutPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒输入的流量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"BytesRejectedPerSec"</span><br><span class="line">"tags" : &#123;"application" : "BytesRejectedPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒的消息写入总量</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"MessagesInPerSec"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchFollower的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">每秒FetchConsumer的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">每秒Produce的请求次数</span><br><span class="line"></span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"RequestsPerSec"</span><br><span class="line">"tags" : &#123;"request" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line">内存使用的使用情况</span><br><span class="line">"obj" : "java.lang:type=Memory"</span><br><span class="line">"attr" : [ "HeapMemoryUsage", "NonHeapMemoryUsage" ]</span><br><span class="line">"resultAlias":"MemoryUsage"</span><br><span class="line">"tags" : &#123;"application" : "MemoryUsage"&#125;</span><br><span class="line"></span><br><span class="line">GC的耗时和次数</span><br><span class="line">"obj" : "java.lang:type=GarbageCollector,name=*"</span><br><span class="line">"attr" : [ "CollectionCount","CollectionTime" ]</span><br><span class="line">"resultAlias":"GC"</span><br><span class="line">"tags" : &#123;"application" : "GC"&#125;</span><br><span class="line"></span><br><span class="line">线程的使用情况</span><br><span class="line">"obj" : "java.lang:type=Threading"</span><br><span class="line">"attr" : [ "PeakThreadCount","ThreadCount" ]</span><br><span class="line">"resultAlias":"Thread"</span><br><span class="line">"tags" : &#123;"application" : "Thread"&#125;</span><br><span class="line"></span><br><span class="line">副本落后主分片的最大消息数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaFetcherManager,name=MaxLag,clientId=Replica"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaFetcherManager"</span><br><span class="line">"tags" : &#123;"application" : "MaxLag"&#125;</span><br><span class="line"></span><br><span class="line">该broker上的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=PartitionCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "PartitionCount"&#125;</span><br><span class="line"></span><br><span class="line">正在做复制的partition的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "UnderReplicatedPartitions"&#125;</span><br><span class="line"></span><br><span class="line">Leader的replica的数量</span><br><span class="line">"obj" : "kafka.server:type=ReplicaManager,name=LeaderCount"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"ReplicaManager"</span><br><span class="line">"tags" : &#123;"application" : "LeaderCount"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchConsumer耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchConsumer"&#125;</span><br><span class="line"></span><br><span class="line">一个请求FetchFollower耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchFollower"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "FetchFollower"&#125;</span><br><span class="line"></span><br><span class="line">一个请求Produce耗费的所有时间</span><br><span class="line">"obj" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce"</span><br><span class="line">"attr" : [ "Count","Max" ]</span><br><span class="line">"resultAlias":"TotalTimeMs"</span><br><span class="line">"tags" : &#123;"application" : "Produce"&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2、topic的监控指标</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的写入流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒的输出流量</span><br><span class="line"><span class="string">"kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topic=falcon_monitor_us"</span></span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "BytesOutPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us每秒写入消息的数量</span><br><span class="line">"obj" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec,topic=falcon_monitor_us"</span><br><span class="line">"attr" : [ "Count" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "MessagesInPerSec"&#125;</span><br><span class="line"></span><br><span class="line">falcon_monitor_us在每个分区最后的Offset</span><br><span class="line">"obj" : "kafka.log:type=Log,name=LogEndOffset,topic=falcon_monitor_us,partition=*"</span><br><span class="line">"attr" : [ "Value" ]</span><br><span class="line">"resultAlias":"falcon_monitor_us"</span><br><span class="line">"tags" : &#123;"application" : "LogEndOffset"&#125;</span><br><span class="line"></span><br><span class="line">PS：</span><br><span class="line">1、参数说明</span><br><span class="line">"obj"对应jmx的ObjectName，就是我们要监控的指标</span><br><span class="line">"attr"对应ObjectName的属性，可以理解为我们要监控的指标的值</span><br><span class="line">"resultAlias"对应metric 的名称，在InfluxDb里面就是MEASUREMENTS名</span><br><span class="line">"tags" 对应InfluxDb的tag功能，对与存储在同一个MEASUREMENTS里面的不同监控指标可以做区分，我们在用Grafana绘图的时候会用到，建议对每个监控指标都打上tags</span><br><span class="line"></span><br><span class="line">2、对于全局监控，每一个监控指标对应一个MEASUREMENTS，所有的kafka节点同一个监控指标数据写同一个MEASUREMENTS ，对于topc监控的监控指标，同一个topic所有kafka节点写到同一个MEASUREMENTS，并且以topic名称命名</span><br></pre></td></tr></table></figure><p><strong>启动jmxtrans</strong></p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service jmxtrans <span class="literal">start</span>(systemctl <span class="literal">start</span> jmxtrans)</span><br></pre></td></tr></table></figure><p><strong>查看日志没有报错即为成功</strong></p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail /<span class="built_in">var</span>/<span class="keyword">log</span>/jmxtrans/jmxtrans.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><h3 id="安装Grafana"><a href="#安装Grafana" class="headerlink" title="安装Grafana"></a>安装Grafana</h3><p><strong>下载jmxtrans rpm安装包</strong></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://s3-us-west<span class="string">-2</span>.amazonaws.com/grafana-releases/release/grafana<span class="string">-6</span>.0.2<span class="string">-1</span>.x86_64.rpm</span><br></pre></td></tr></table></figure><p><strong>安装rpm包（如果缺少依赖，下载依赖）</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rpm</span> <span class="selector-tag">-ivh</span> <span class="selector-tag">grafana-6</span><span class="selector-class">.0</span><span class="selector-class">.2-1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install --downloadonly --downloaddir=./ fontconfig</span><br><span class="line"></span><br><span class="line">yum localinstall fontconfig-<span class="number">2.13</span>.<span class="number">0</span>-<span class="number">4.3</span><span class="selector-class">.el7</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br><span class="line"></span><br><span class="line">yum install --downloadonly --downloaddir=./ urw-fonts</span><br><span class="line"></span><br><span class="line">yum localinstall urw-fonts-<span class="number">2.4</span>-<span class="number">11</span><span class="selector-class">.el6</span><span class="selector-class">.noarch</span><span class="selector-class">.rpm</span> </span><br><span class="line"></span><br><span class="line">rpm -ivh grafana-<span class="number">6.0</span>.<span class="number">2</span>-<span class="number">1</span><span class="selector-class">.x86_64</span><span class="selector-class">.rpm</span></span><br></pre></td></tr></table></figure><p><strong>启动Grafana</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service grafana-server <span class="keyword">start</span>（systemctl <span class="keyword">start</span> grafana-<span class="keyword">server</span>）</span><br></pre></td></tr></table></figure><p><strong>打开浏览器</strong></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//127.0.0.1:3000</span></span><br></pre></td></tr></table></figure><p><strong>先输入默认用户名密码admin/admin</strong></p><p><strong>点击Add data source，选择InfluxDB</strong></p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Url、Database、User、Password需要和jmxtrans采集数据配置文件里面的写一致，然后点击<span class="keyword">Save</span>&amp;<span class="keyword">Test</span>，提示成功就正常了</span><br></pre></td></tr></table></figure><p><strong>通过后点击Back返回</strong></p><p><strong>左侧 + 可以创建或引入仪表盘，创建一个dashboard，然后在这里配置每一个监控指标的图</strong></p><p><strong>主要配置项说明：</strong></p><table><thead><tr><th>配置项</th><th>说明</th></tr></thead><tbody><tr><td>DataSource</td><td>选择Grafana已配置的数据源</td></tr><tr><td>FROM-Default</td><td>默认Schema，保持不变即可</td></tr><tr><td>FROM-measurement</td><td>对应的InfluxDB的表名</td></tr><tr><td>WHERE</td><td>WHERE条件，根据自己需求选择</td></tr><tr><td>SELECT-Field</td><td>对应选的字段，可根据需求增减</td></tr><tr><td>SELECT-mean()</td><td>选择的字段对应的InfluxDB的函数</td></tr><tr><td>GroupBY-time()</td><td>根据时间分组</td></tr><tr><td>GROUPBY-fill()</td><td>当不存在数据时，以null为默认值填充</td></tr></tbody></table><p><strong>要点说明：</strong></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、对于监控指标为Count的监控项，需要通过Grafana做计算得到我们想要的监控，比如BytesInPerSec这个指标，它的监控值是一个累计值，我们想要取到每秒的流量，肯定需要计算，(本次采集的值-上次采集的值)/<span class="number">60</span> ,jmxtrans是一分钟采集一次数据，具体配置参考下面截图：</span><br><span class="line"></span><br><span class="line">因为我们是一分钟采集一次数据，所以group <span class="keyword">by</span> 和derivative选<span class="number">1</span>分钟；因为我们要每秒的流量，所以math这里除以<span class="number">60</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、X轴的单位选择，比如流量的单位、时间的单位、每秒消息的个数无单位等等，下面分布举一个例子介绍说明</span><br><span class="line"></span><br><span class="line">设置流量的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》data（Metric）--》bytes</span></span><br><span class="line"></span><br><span class="line">设置时间的单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》time--》milliseconds(ms)</span></span><br><span class="line"></span><br><span class="line">设置按原始值展示，无单位 ，点击需要设置的图，选择<span class="string">"Edit"</span>进入编辑页面，切到Axes这个<span class="literal">tab</span>页，Unit<span class="comment">--》none--》none</span></span><br></pre></td></tr></table></figure><h2 id="附录（kafka配置说明）"><a href="#附录（kafka配置说明）" class="headerlink" title="附录（kafka配置说明）"></a>附录（kafka配置说明）</h2><h3 id="server-properties"><a href="#server-properties" class="headerlink" title="server.properties"></a>server.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.server.KafkaConfig <span class="keyword">for</span> additional details and defaults</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Server Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The id of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的ID，必须与其它节点不同</span></span><br><span class="line">broker.id=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Switch to <span class="built_in">enable</span> topic deletion or not, default value is <span class="literal">false</span></span></span><br><span class="line">delete.topic.enable=false</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Socket Server Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> java.net.InetAddress.getCanonicalHostName() <span class="keyword">if</span> not configured.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器监听的地址。如果没有配置，就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   FORMAT:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   EXAMPLE:</span></span><br><span class="line"><span class="meta">#</span><span class="bash">     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listeners=PLAINTEXT://:9092</span></span><br><span class="line">listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Hostname and port the broker will advertise to producers and consumers. If not <span class="built_in">set</span>, </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> it uses the value <span class="keyword">for</span> <span class="string">"listeners"</span> <span class="keyword">if</span> configured.  Otherwise, it will use the value</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> returned from java.net.InetAddress.getCanonicalHostName().</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 节点的主机名会通知给生产者和消费者。如果没有设置，如果配置了<span class="string">"listeners"</span>就使用<span class="string">"listeners"</span>的值。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则就使用java.net.InetAddress.getCanonicalHostName()的返回值</span></span><br><span class="line"><span class="meta">#</span><span class="bash">advertised.listeners=PLAINTEXT://your.host.name:9092</span></span><br><span class="line">advertised.listeners=PLAINTEXT://kafka01:9092</span><br><span class="line"><span class="meta">#</span><span class="bash"> Maps listener names to security protocols, the default is <span class="keyword">for</span> them to be the same. See the config documentation <span class="keyword">for</span> more details</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将侦听器的名称映射到安全协议，默认情况下它们是相同的。有关详细信息，请参阅配置文档</span></span><br><span class="line"><span class="meta">#</span><span class="bash">listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> receiving requests from the network and sending responses to the network</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来接受请求或者发送响应的线程数</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads that the server uses <span class="keyword">for</span> processing requests, <span class="built_in">which</span> may include disk I/O</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 服务器用来处理请求的线程数，可能包括磁盘IO</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The send buffer (SO_SNDBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的发送缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The receive buffer (SO_RCVBUF) used by the socket server</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 套接字服务器使用的接收缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a request that the socket server will accept (protection against OOM)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单个请求最大能接收的数据量</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A comma seperated list of directories under <span class="built_in">which</span> to store <span class="built_in">log</span> files</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个逗号分隔的目录列表，用来存储日志文件</span></span><br><span class="line">log.dirs=/data/apps/kafkaapp/logs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default number of <span class="built_in">log</span> partitions per topic. More partitions allow greater</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> parallelism <span class="keyword">for</span> consumption, but this will also result <span class="keyword">in</span> more files across</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the brokers.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个主题的日志分区的默认数量。更多的分区允许更大的并行操作，但是它会导致节点产生更多的文件</span></span><br><span class="line">num.partitions=6</span><br><span class="line">default.replication.factor=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of threads per data directory to be used <span class="keyword">for</span> <span class="built_in">log</span> recovery at startup and flushing at shutdown.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This value is recommended to be increased <span class="keyword">for</span> installations with data <span class="built_in">dirs</span> located <span class="keyword">in</span> RAID array.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每个数据目录中的线程数，用于在启动时日志恢复，并在关闭时刷新。</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Internal Topic Settings  #############################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The replication factor <span class="keyword">for</span> the group metadata internal topics <span class="string">"__consumer_offsets"</span> and <span class="string">"__transaction_state"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For anything other than development testing, a value greater than 1 is recommended <span class="keyword">for</span> to ensure availability such as 3.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于除了开发测试之外的其他任何东西，group元数据内部主题的复制因子“__consumer_offsets”和“__transaction_state”，建议值大于1，以确保可用性(如3)。</span></span><br><span class="line">offsets.topic.replication.factor=3</span><br><span class="line">transaction.state.log.replication.factor=3</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Flush Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Messages are immediately written to the filesystem but by default we only fsync() to sync</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the OS cache lazily. The following configurations control the flush of data to disk.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 消息直接被写入文件系统，但是默认情况下我们仅仅调用fsync()以延迟的同步系统缓存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There are a few important trade-offs here:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这些有一些重要的权衡</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    1. Durability: Unflushed data may be lost <span class="keyword">if</span> you are not using replication.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. 持久性:如果不使用复制，未刷新的数据可能会丢失。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 延迟:非常大的刷新间隔可能会在刷新时导致延迟，因为将会有大量数据刷新。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3. 吞吐量:刷新通常是最昂贵的操作，而一个小的刷新间隔可能会导致过多的搜索。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The settings below allow one to configure the flush policy to flush data after a period of time or</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> every N messages (or both). This can be <span class="keyword">done</span> globally and overridden on a per-topic basis.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面的设置允许你去配置刷新策略，每隔一段时间刷新或者一次N个消息（或者两个都配置）。这可以在全局范围内完成，并在每个主题的基础上重写。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The number of messages to accept before forcing a flush of data to disk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.messages=10000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum amount of time a message can sit <span class="keyword">in</span> a <span class="built_in">log</span> before we force a flush</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.flush.interval.ms=1000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Log Retention Policy #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configurations control the disposal of <span class="built_in">log</span> segments. The policy can</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> be <span class="built_in">set</span> to delete segments after a period of time, or after a given size has accumulated.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以下的配置控制了日志段的处理。策略可以配置为每隔一段时间删除片段或者到达一定大小之后。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A segment will be deleted whenever *either* of these criteria are met. Deletion always happens</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> from the end of the <span class="built_in">log</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当满足这些条件时，将会删除一个片段。删除总是发生在日志的末尾。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The minimum age of a <span class="built_in">log</span> file to be eligible <span class="keyword">for</span> deletion due to age</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个日志的最小存活时间，可以被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A size-based retention policy <span class="keyword">for</span> logs. Segments are pruned from the <span class="built_in">log</span> as long as the remaining</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> segments don<span class="string">'t drop below log.retention.bytes. Functions independently of log.retention.hours.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">log.retention.bytes=1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The maximum size of a <span class="built_in">log</span> segment file. When this size is reached a new <span class="built_in">log</span> segment will be created.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 每一个日志段大小的最大值。当到达这个大小时，会生成一个新的片段。</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The interval at <span class="built_in">which</span> <span class="built_in">log</span> segments are checked to see <span class="keyword">if</span> they can be deleted according</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to the retention policies</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查日志段的时间间隔，看是否可以根据保留策略删除它们</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Zookeeper #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string (see zookeeper docs <span class="keyword">for</span> details).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper连接字符串（具体见Zookeeper文档）</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This is a comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这是一个以逗号为分割的部分，每一个都匹配一个Zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can also append an optional chroot string to the urls to specify the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 您还可以将一个可选的chroot字符串附加到url，以指定所有kafka znode的根目录。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> root directory <span class="keyword">for</span> all kafka znodes.</span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 连接到Zookeeper的超时时间</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Group Coordinator Settings #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configuration specifies the time, <span class="keyword">in</span> milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default value <span class="keyword">for</span> this is 3 seconds.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> We override this to 0 here as it makes <span class="keyword">for</span> a better out-of-the-box experience <span class="keyword">for</span> development and testing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> However, <span class="keyword">in</span> production environments the default value of 3 seconds is more suitable as this will <span class="built_in">help</span> to avoid unnecessary, and potentially expensive, rebalances during application startup.</span></span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br></pre></td></tr></table></figure><h3 id="producer-properties"><a href="#producer-properties" class="headerlink" title="producer.properties"></a>producer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.producer.ProducerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Producer Basics #############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> list of brokers used <span class="keyword">for</span> bootstrapping knowledge about the rest of the cluster</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> format: host1:port1,host2:port2 ...</span></span><br><span class="line">bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> specify the compression codec <span class="keyword">for</span> all data generated: none, gzip, snappy, lz4</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否压缩，默认0表示不压缩，1表示用gzip压缩，2表示用snappy压缩。压缩后消息中会有头来指明消息压缩类型，故在消费者端消息解压是透明的无需指定。</span></span><br><span class="line">compression.type=none</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> name of the partitioner class <span class="keyword">for</span> partitioning events; default partition spreads data randomly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定分区处理类。默认kafka.producer.DefaultPartitioner，表通过key哈希到对应分区</span></span><br><span class="line">partitioner.class=kafka.producer.DefaultPartitioner</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum amount of time the client will <span class="built_in">wait</span> <span class="keyword">for</span> the response of a request</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在向producer发送ack之前,broker允许等待的最大时间 ，如果超时,broker将会向producer发送一个error ACK.意味着上一次消息因为某种原因未能成功(比如follower未能同步成功)</span></span><br><span class="line">request.timeout.ms=10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> how long `KafkaProducer.send` and `KafkaProducer.partitionsFor` will block <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.block.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the producer will <span class="built_in">wait</span> <span class="keyword">for</span> up to the given delay to allow other records to be sent so that the sends can be batched together</span></span><br><span class="line"><span class="meta">#</span><span class="bash">linger.ms=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the maximum size of a request <span class="keyword">in</span> bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash">max.request.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the default batch size <span class="keyword">in</span> bytes when batching multiple records sent to a partition</span></span><br><span class="line"><span class="meta">#</span><span class="bash">batch.size=</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> the total bytes of memory the producer can use to buffer records waiting to be sent to the server</span></span><br><span class="line"><span class="meta">#</span><span class="bash">buffer.memory=</span></span><br></pre></td></tr></table></figure><h3 id="consumer-properties"><a href="#consumer-properties" class="headerlink" title="consumer.properties"></a>consumer.properties</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this work <span class="keyword">for</span> additional information regarding copyright ownership.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (the <span class="string">"License"</span>); you may not use this file except <span class="keyword">in</span> compliance with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> see kafka.consumer.ConsumerConfig <span class="keyword">for</span> more details</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper connection string</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> comma separated host:port pairs, each corresponding to a zk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server. e.g. <span class="string">"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002"</span></span></span><br><span class="line">zookeeper.connect=kafka01:2181,kafka02:2181,kafka03:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> timeout <span class="keyword">in</span> ms <span class="keyword">for</span> connecting to zookeeper</span></span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer group id</span></span><br><span class="line">group.id=test-consumer-group</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer timeout</span></span><br><span class="line"><span class="meta">#</span><span class="bash">consumer.timeout.ms=500</span></span><br></pre></td></tr></table></figure><h3 id="install-kafka-sh"><a href="#install-kafka-sh" class="headerlink" title="install-kafka.sh"></a>install-kafka.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置KAFKA的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入kafka的安装目录,不存在脚本自动创建,最后一个/不要写,如 /data/apps"</span><br><span class="line">read kafkainstallpath</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建kafka安装的目录</span></span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">   mkdir -p $kafkainstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkainstallpath ]; then</span><br><span class="line">  echo "创建目录$kafkainstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'kafka.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   # 当前目录没有kafka的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现kafka*.tar.gz,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   # 解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'kafka.*[gz]$') -C $kafkainstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka版本全称</span></span><br><span class="line">kafkaversion=`ls $kafkainstallpath| grep 'kafka_2.*'`</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka配置文件存储路径</span></span><br><span class="line">confpath=$kafkainstallpath/$kafkaversion/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改配置文件</span></span><br><span class="line">echo -e "请输入当前kafka节点的broker.id：唯一值 例如 0"</span><br><span class="line">read brokerid</span><br><span class="line">sed -i "s/^broker.id=0/broker.id=$&#123;brokerid&#125;/g" $confpath/server.properties</span><br><span class="line"></span><br><span class="line">echo -e "请输入当前kafka节点的hostname: 例如kafka01"</span><br><span class="line">read hostname</span><br><span class="line">sed -i "s/^#listeners=PLAINTEXT:\/\/:9092/listeners=PLAINTEXT:\/\/$hostname:9092/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e "请输入kafka消息存储目录：例如 /data/apps/kafkaapp/log"</span><br><span class="line">read kafkalogspath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建KAFKA日志存储目录</span></span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">   mkdir -p $kafkalogspath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $kafkalogspath ]; then</span><br><span class="line">  echo "创建目录$kafkalogspath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='log.dirs=/tmp/kafka-logs'</span><br><span class="line">new_dir='log.dirs='$kafkalogspath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line">echo -e '请输入zookeeper集群的所有节点：（严格按照示例格式） 例如kafka01:2181,kafka02:2181,kafka03:2181'</span><br><span class="line">read allhosts</span><br><span class="line">sed -i "s/^zookeeper.connect=localhost:2181/zookeeper.connect=$allhosts/g" $confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 关闭删除topic的权限</span></span><br><span class="line">sed -i 's/^#delete.topic.enable=true/delete.topic.enable=false/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置topic的默认分区数量为6</span></span><br><span class="line">sed -i 's/^num.partitions=1/num.partitions=6/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 一个基于大小的日志保留策略。段将被从日志中删除只要剩下的部分段不低于log.retention.bytes。</span></span><br><span class="line">sed -i 's/^#log.retention.bytes=1073741824/log.retention.bytes=1073741824/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新数据到磁盘之前允许接收消息的数量</span></span><br><span class="line">sed -i 's/^#log.flush.interval.messages=10000/log.flush.interval.messages=10000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在强制刷新之前，消息可以在日志中停留的最长时间</span></span><br><span class="line">sed -i 's/^#log.flush.interval.ms=1000/log.flush.interval.ms=1000/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置偏移量topic的复制因子为3</span></span><br><span class="line">sed -i 's/^offsets.topic.replication.factor=1/offsets.topic.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置事务topic的复制因子为3</span></span><br><span class="line">sed -i 's/^transaction.state.log.replication.factor=1/transaction.state.log.replication.factor=3/g' $confpath/server.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置默认副本因子为3</span></span><br><span class="line">echo ""&gt;&gt;$confpath/server.properties</span><br><span class="line">echo "default.replication.factor=3" &gt;&gt;$confpath/server.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> <span class="string">"log.cleanup.policy=delete"</span> &gt;&gt;<span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash">kafka参数优化</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">'s/^log.retention.hours=16/log.retention.hours=72/g'</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> param=`cat /proc/cpuinfo | grep <span class="string">"cpu cores"</span>| uniq`</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_count=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_count=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+1))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_count&#125;</span>!<span class="variable">$&#123;new_count&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bak_io=<span class="string">"num.network.threads=3"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> new_io=<span class="string">"num.network.threads="</span>$((<span class="variable">$&#123;param:0-1:1&#125;</span>+<span class="variable">$&#123;param:0-1:1&#125;</span>))</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sed -i <span class="string">"s!<span class="variable">$&#123;bak_io&#125;</span>!<span class="variable">$&#123;new_io&#125;</span>!g"</span> <span class="variable">$confpath</span>/server.properties</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#KAFKA <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$KAFKA_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"> </span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">kafkapath=$kafkainstallpath/$kafkaversion</span><br><span class="line">kafkapathtemp=$kafkainstallpath/$kafkaversion-temp</span><br><span class="line">cp -r $kafkapath $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$&#123;hostname&#125;),严格符合以下格式hostname:brokerid,空格隔开， 如kafka02:1 kafka03:2'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> kafkahostname=$&#123;array3[0]&#125;</span><br><span class="line"> kafkabrokerid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $kafkahostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $kafkahostname "rm -rf $kafkapath $kafkalogspath"</span><br><span class="line"> ssh $kafkahostname "mkdir -p $kafkapath $kafkalogspath"</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改broker.id</span></span><br><span class="line"> old_brokerid="broker.id=$brokerid"</span><br><span class="line"> new_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"><span class="meta"> #</span><span class="bash">修改listeners</span></span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$&#123;hostname&#125;:9092"</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$&#123;kafkahostname&#125;:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> scp -r $kafkapathtemp/* $&#123;user&#125;@$kafkahostname:$kafkapath/</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo '#KAFKA <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"echo 'export KAFKA_HOME=<span class="variable">$kafkainstallpath</span>/<span class="variable">$kafkaversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">'echo "export PATH=\$PATH:\$KAFKA_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$kafkahostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">再次修改回来 防止修改错误</span></span><br><span class="line"> new_brokerid="broker.id=$brokerid"</span><br><span class="line"> old_brokerid="broker.id=$kafkabrokerid"</span><br><span class="line"> sed -i "s!$&#123;old_brokerid&#125;!$&#123;new_brokerid&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> new_listeners="listeners=PLAINTEXT:\/\/$hostname:9092"</span><br><span class="line"> old_listeners="listeners=PLAINTEXT:\/\/$kafkahostname:9092"</span><br><span class="line"> sed -i "s!$&#123;old_listeners&#125;!$&#123;new_listeners&#125;!g" $kafkapathtemp/config/server.properties</span><br><span class="line"> </span><br><span class="line"> echo ======= $kafkahostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $kafkapathtemp</span><br><span class="line"> </span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h3 id="install-zookeeper-sh"><a href="#install-zookeeper-sh" class="headerlink" title="install-zookeeper.sh"></a>install-zookeeper.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author gaojintao999@163.com</span></span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "~~~ 运行后续操作前请仔细阅读以下内容！ Author gaojintao999@163.com ~~~"</span><br><span class="line">echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"</span><br><span class="line">echo "(1)已配置主机映射！"</span><br><span class="line">echo "(2)已永久关闭防火墙！"</span><br><span class="line">echo "(3)已配置免密登录！"</span><br><span class="line">echo "(4)当前执行脚本和相关的安装包资源在同一路径下！"</span><br><span class="line">read -p "上述条件是否都满足？(y or n)" yesorno</span><br><span class="line"></span><br><span class="line">if [[ $yesorno = "y" || $yesorno = "Y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">配置zk的安装目录</span></span><br><span class="line">currentTime=$(date '+%Y-%m-%d %H:%M:%S')</span><br><span class="line">echo -e "请输入zk的安装目录,不存在脚本自动创建,最后一个/不要写 如/data/apps"</span><br><span class="line">read zkinstallpath</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk安装的目录</span></span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">   mkdir -p $zkinstallpath</span><br><span class="line">fi </span><br><span class="line">if [ ! -d $zkinstallpath ]; then</span><br><span class="line">  echo "创建目录$zkinstallpath失败！请检查目录是否有权限"</span><br><span class="line">  exit:</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">解压tar包</span></span><br><span class="line">currentdir=$(cd $(dirname $0); pwd)</span><br><span class="line">ls | grep 'zookeeper-.*[gz]$'</span><br><span class="line">if [ $? -ne 0 ]; then</span><br><span class="line">   #当前目录没有zk的压缩包</span><br><span class="line">   echo "在$currentdir下没有发现zookeeper的gz压缩包,请自行上传!"</span><br><span class="line">   exit</span><br><span class="line">else</span><br><span class="line">   #解压</span><br><span class="line">   tar -zxvf $currentdir/$(ls | grep 'zookeeper-.*[gz]$') -C $zkinstallpath</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">zkversion=`ls $zkinstallpath| grep 'zookeeper-.*'`</span><br><span class="line"></span><br><span class="line">confpath=$zkinstallpath/$zkversion/conf</span><br><span class="line"></span><br><span class="line">cp $confpath/zoo_sample.cfg  $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line">echo -e "请输入zk数据存储目录：例如 /data/apps/zookeeperapp"</span><br><span class="line">read zkdatapath</span><br><span class="line"><span class="meta">#</span><span class="bash">创建zk数据的目录</span></span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">   mkdir -p $zkdatapath</span><br><span class="line">fi</span><br><span class="line">if [ ! -d $zkdatapath ]; then</span><br><span class="line">  echo "创建目录$zkdatapath失败！请检查目录是否有权限"</span><br><span class="line">  exit</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">bak_dir='dataDir=/tmp/zookeeper'</span><br><span class="line">new_dir='dataDir='$zkdatapath</span><br><span class="line">sed -i "s!$&#123;bak_dir&#125;!$&#123;new_dir&#125;!g" $confpath/zoo.cfg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">echo  "请输入所有的zk集群节点：（按照空格分割） 例如 zk01 zk02 zk03"</span><br><span class="line">read zkNodes</span><br><span class="line">array=(`echo $zkNodes | tr ' ' ' '` )</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line">echo ""&gt;&gt;$confpath/zoo.cfg</span><br><span class="line">for i in `seq 0 $(($&#123;#array[@]&#125;-1))`</span><br><span class="line">do</span><br><span class="line"> echo "server.$(($&#123;i&#125;+1))=$&#123;array[$&#123;i&#125;]&#125;:2888:3888" &gt;&gt;$confpath/zoo.cfg</span><br><span class="line">done </span><br><span class="line"></span><br><span class="line">echo  "请输入zk的myid,不能重复,唯一值 例如 1" </span><br><span class="line">read myid</span><br><span class="line">echo $myid &gt; $zkdatapath/myid</span><br><span class="line"></span><br><span class="line">binpath=$zkinstallpath/$zkversion/bin</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG_DIR=\".\"/ZOO_LOG_DIR=\"$&#123;ZOOKEEPER_PREFIX&#125;\/logs\"/g' $binpath/zkEnv.sh</span><br><span class="line"></span><br><span class="line">echo "ZOO_LOG_DIR修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/ZOO_LOG4J_PROP=\"INFO,CONSOLE\"/ZOO_LOG4J_PROP=\"INFO,ROLLINGFILE\"/g' $binpath/zkEnv.sh</span><br><span class="line">echo "ZOO_LOG4J_PROP修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.out\"/_ZOO_DAEMON_OUT=\"$ZOO_LOG_DIR\/zookeeper.log\"/g' $binpath/zkServer.sh</span><br><span class="line">echo "_ZOO_DAEMON_OUT修改成功"</span><br><span class="line"></span><br><span class="line">sed -i 's/zookeeper.root.logger=INFO, CONSOLE/zookeeper.root.logger=INFO, ROLLINGFILE/g' $confpath/log4j.properties</span><br><span class="line">echo "zookeeper.root.logger修改成功"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">PATH设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash">末行插入</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">""</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"#zookeeper <span class="variable">$currentTime</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">"export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>"</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">echo</span> <span class="string">'export PATH=$PATH:$ZK_HOME/bin'</span>&gt;&gt;~/.bash_profile</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">source</span> ~/.bash_profile</span></span><br><span class="line"></span><br><span class="line">echo -e "是否远程复制 请输入y/n"</span><br><span class="line">read flag</span><br><span class="line">if [[ $flag == "y" ]]; then</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">修改并分发安装文件</span></span><br><span class="line">zkpath=$zkinstallpath/$zkversion</span><br><span class="line">zkpathtemp=$zkinstallpath/$zkversion-temp</span><br><span class="line">cp -r $zkpath $zkpathtemp</span><br><span class="line"></span><br><span class="line">echo "以下输入的节点必须做免密登录"</span><br><span class="line">currentnode=`hostname`</span><br><span class="line">echo -e '请输入除当前节点之外的节点(当前节点$currentnode),严格符合以下格式hostname:zkID,空格隔开， 如zk02:2 zk03:3 zk04:4 zk05:5 zk06:6'</span><br><span class="line">read allnodes</span><br><span class="line">user=`whoami`</span><br><span class="line">array2=($&#123;allnodes// / &#125;)</span><br><span class="line">for allnode in $&#123;array2[@]&#125;</span><br><span class="line">do</span><br><span class="line"> array3=($&#123;allnode//:/ &#125;)</span><br><span class="line"> hostname=$&#123;array3[0]&#125;</span><br><span class="line"> zkid=$&#123;array3[1]&#125;</span><br><span class="line"> echo ======= $hostname  =======</span><br><span class="line"> </span><br><span class="line"><span class="meta"> #</span><span class="bash">修改文件</span></span><br><span class="line"> ssh $hostname "mkdir -p $zkpath"</span><br><span class="line"> ssh $hostname "mkdir -p $zkdatapath"</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">修改zk的myid唯一值</span></span><br><span class="line"> ssh $hostname "echo $zkid &gt; $zkdatapath/myid"</span><br><span class="line"></span><br><span class="line"> scp -r $zkpathtemp/* $&#123;user&#125;@$hostname:$zkpath/</span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo ''&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo '#zk <span class="variable">$currentTime</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"echo 'export ZK_HOME=<span class="variable">$zkinstallpath</span>/<span class="variable">$zkversion</span>'&gt;&gt;~/.bash_profile"</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">'echo "export PATH=\$PATH:\$ZK_HOME/bin"&gt;&gt;~/.bash_profile'</span></span></span><br><span class="line"><span class="meta"> #</span><span class="bash">ssh <span class="variable">$hostname</span> <span class="string">"source ~/.bash_profile"</span></span></span><br><span class="line"></span><br><span class="line"> echo ======= $hostname 远程复制完成  =======</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">删除临时文件</span></span><br><span class="line">rm -rf $zkpathtemp</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">echo "退出当前程序！"</span><br><span class="line">exit</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 消息中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
            <tag> 消息中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JsonSchema全套解决方案</title>
      <link href="/2018/08/17/JsonSchema%E5%85%A8%E5%A5%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
      <url>/2018/08/17/JsonSchema%E5%85%A8%E5%A5%97%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Json-Schema说明"><a href="#Json-Schema说明" class="headerlink" title="Json Schema说明"></a>Json Schema说明</h1><ol><li>json schema 本身也是一个json串；</li><li>每个schema可以描述一个json实例，并且该json实例里每一个节点都可以用一个schema来描述，因此schema与json一样，本身也是一个层级结构，一个schema中可能嵌套着另外若干层schema；</li><li>json schema 定义的检查规则以数据格式验证为主（字段存在性、字段类型），并可以支持一些简单的数据正确性验证（例如数值范围、字符串的模式等），但不能进行复杂的逻辑校验（例如进价必须小于售价等）；</li></ol><h1 id="Json-Schema-格式"><a href="#Json-Schema-格式" class="headerlink" title="Json Schema 格式"></a>Json Schema 格式</h1><p>Json schema 本身遵循Json规范，本身就是一个Json字符串，先来看一个例子</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来看一下json schema 最外层包含以下几个字段</p><table><thead><tr><th>$schema</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>$schema</td><td>$schema 关键字状态，表示这个模式与 v4 规范草案书写一致。</td><td></td></tr><tr><td>title</td><td>标题，用来描述结构</td><td></td></tr><tr><td>description</td><td>描述</td><td></td></tr><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr></tbody></table><p>上面只是一个简单的例子，从上面可以看出Json schema 本身是一个JSON字符串，由通过key-value的形式进行标示。<br>type 和 properties 用来定义json 属性的类型。required 是对Object字段的必段性进行约束。事实上,json Schema定义了json所支持的类型，每种类型都有0-N种约束方式。下一节我们来，细致介绍一下。</p><hr><h1 id="Json-Schema-类型"><a href="#Json-Schema-类型" class="headerlink" title="Json Schema 类型"></a>Json Schema 类型</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"id"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>object类型有三个关键字:type（限定类型）,properties(定义object的各个字段),required（限定必需字段）,如下：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>type</td><td>类型</td><td>.</td></tr><tr><td>properties</td><td>定义属性</td><td></td></tr><tr><td>required</td><td>必需属性</td><td></td></tr><tr><td>maxProperties</td><td>最大属性个数</td><td></td></tr><tr><td>minProperties</td><td>最小属性个数</td><td></td></tr><tr><td>additionalProperties</td><td>true or false or object</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html" target="_blank" rel="noopener">参考</a></td></tr></tbody></table><p>properties 定义每个属性的名字和类型，方式如上例。</p><h2 id="array"><a href="#array" class="headerlink" title="array"></a>array</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">     &#125;,</span><br><span class="line">     <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">     <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>array有三个单独的属性:items,minItems,uniqueItems:</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>items</td><td>array 每个元素的类型</td><td>.</td></tr><tr><td>minItems</td><td>约束属性，数组最小的元素个数</td><td></td></tr><tr><td>maxItems</td><td>约束属性，数组最大的元素个数</td><td></td></tr><tr><td>uniqueItems</td><td>约束属性，每个元素都不相同</td><td></td></tr><tr><td>additionalProperties</td><td>约束items的类型，不建议使用</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/array.html" target="_blank" rel="noopener">示例</a></td></tr><tr><td>Dependencies</td><td>属性依赖</td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=additionalproperties" target="_blank" rel="noopener">用法</a></td></tr><tr><td>patternProperties</td><td></td><td><a href="https://link.jianshu.com/?t=https://spacetelescope.github.io/understanding-json-schema/reference/object.html?highlight=patternproperties" target="_blank" rel="noopener">用法</a></td></tr></tbody></table><h2 id="string"><a href="#string" class="headerlink" title="string"></a>string</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"ip"</span>: &#123;</span><br><span class="line">            <span class="attr">"mail"</span>: <span class="string">"string"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"w+([-+.]w+)*@w+([-.]w+)*.w+([-.]w+)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"host"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"phoneNumber"</span>,</span><br><span class="line">            <span class="attr">"pattern"</span>:<span class="string">"((d&#123;3,4&#125;)|d&#123;3,4&#125;-)?d&#123;7,8&#125;(-d&#123;3&#125;)*"</span></span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"ip"</span>, <span class="string">"host"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>maxLength</td><td>定义字符串的最大长度，&gt;=0</td><td>.</td></tr><tr><td>minLength</td><td>定义字符串的最小长度，&gt;=0</td><td></td></tr><tr><td>pattern</td><td>用正则表达式约束字符串</td><td></td></tr></tbody></table><h2 id="integer"><a href="#integer" class="headerlink" title="integer"></a>integer</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr><tr><td>multipleOf</td><td>是某数的倍数，必须大于0的整数</td><td></td></tr></tbody></table><h2 id="number"><a href="#number" class="headerlink" title="number"></a>number</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A product from Acme's catalog"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">    <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"name"</span>: &#123;</span><br><span class="line">            <span class="attr">"description"</span>: <span class="string">"Name of the product"</span>,</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"price"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>number 关键字可以描述任意长度，任意小数点的数字。number类型的约束有以下几个：</p><table><thead><tr><th>关键字</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td>minimum</td><td>最小值</td><td>.</td></tr><tr><td>exclusiveMinimum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上大于 “minimum” 的值则实例有效。</td><td></td></tr><tr><td>maximum</td><td>约束属性，最大值</td><td></td></tr><tr><td>exclusiveMaximum</td><td>如果存在 “exclusiveMinimum” 并且具有布尔值 true，如果它严格意义上小于 “maximum” 的值则实例有效。</td><td></td></tr></tbody></table><h2 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"boolean"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>true or false</p><h2 id="enum"><a href="#enum" class="headerlink" title="enum"></a>enum</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                     <span class="attr">"enum"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]</span><br><span class="line">                   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以这么做</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="attr">"properties"</span>: &#123;</span><br><span class="line">    <span class="attr">"number"</span>:      &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_name"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    <span class="attr">"street_type"</span>: [<span class="string">"Street"</span>, <span class="string">"Avenue"</span>, <span class="string">"Boulevard"</span>]                   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="null"><a href="#null" class="headerlink" title="null"></a>null</h2><h1 id="Json-Schema进阶"><a href="#Json-Schema进阶" class="headerlink" title="Json Schema进阶"></a>Json Schema进阶</h1><p>了解了上面的各个类型的定义及约定条件，就可以满足大部分情况了。但为了写出更好的json schema,我们再学习几个关键字</p><h2 id="ref"><a href="#ref" class="headerlink" title="$ref"></a>$ref</h2><p>$ref 用来引用其它schema,<br>示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"$schema"</span>: <span class="string">"http://json-schema.org/draft-04/schema#"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Product set"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123;</span><br><span class="line">        <span class="attr">"title"</span>: <span class="string">"Product"</span>,</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">        <span class="attr">"properties"</span>: &#123;</span><br><span class="line">            <span class="attr">"id"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"The unique identifier for a product"</span>,</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"name"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"price"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"number"</span>,</span><br><span class="line">                <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"tags"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">                <span class="attr">"items"</span>: &#123;</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"string"</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"minItems"</span>: <span class="number">1</span>,</span><br><span class="line">                <span class="attr">"uniqueItems"</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"dimensions"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">                <span class="attr">"properties"</span>: &#123;</span><br><span class="line">                    <span class="attr">"length"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"width"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;,</span><br><span class="line">                    <span class="attr">"height"</span>: &#123;<span class="attr">"type"</span>: <span class="string">"number"</span>&#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="attr">"required"</span>: [<span class="string">"length"</span>, <span class="string">"width"</span>, <span class="string">"height"</span>]</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">"warehouseLocation"</span>: &#123;</span><br><span class="line">                <span class="attr">"description"</span>: <span class="string">"Coordinates of the warehouse with the product"</span>,</span><br><span class="line">                <span class="attr">"$ref"</span>: <span class="string">"http://json-schema.org/geo"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"required"</span>: [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"price"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="definitions"><a href="#definitions" class="headerlink" title="definitions"></a>definitions</h2><p>当一个schema写的很大的时候，可能需要创建内部结构体，再使用$ref进行引用，示列如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"array"</span>,</span><br><span class="line">    <span class="attr">"items"</span>: &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/positiveInteger"</span> &#125;,</span><br><span class="line">    <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">        <span class="attr">"positiveInteger"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"integer"</span>,</span><br><span class="line">            <span class="attr">"minimum"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="attr">"exclusiveMinimum"</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="allOf"><a href="#allOf" class="headerlink" title="allOf"></a>allOf</h2><p>意思是展示全部属性，建议用requires替代</p><p>不建议使用，示例如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"definitions"</span>: &#123;</span><br><span class="line">    <span class="attr">"address"</span>: &#123;</span><br><span class="line">      <span class="attr">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">      <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"street_address"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"city"</span>:           &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">        <span class="attr">"state"</span>:          &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"required"</span>: [<span class="string">"street_address"</span>, <span class="string">"city"</span>, <span class="string">"state"</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"allOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"$ref"</span>: <span class="string">"#/definitions/address"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"properties"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: &#123; <span class="attr">"enum"</span>: [ <span class="string">"residential"</span>, <span class="string">"business"</span> ] &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="anyOf"><a href="#anyOf" class="headerlink" title="anyOf"></a>anyOf</h2><p>意思是展示任意属性，建议用requires替代和minProperties替代，示例如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"anyOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="oneOf"><a href="#oneOf" class="headerlink" title="oneOf"></a>oneOf</h2><p>其中之一</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"oneOf"</span>: [</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">5</span> &#125;,</span><br><span class="line">    &#123; <span class="attr">"type"</span>: <span class="string">"number"</span>, <span class="attr">"multipleOf"</span>: <span class="number">3</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="not"><a href="#not" class="headerlink" title="not"></a>not</h2><p>非 * 类型<br>示例</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">"not"</span>: &#123; <span class="attr">"type"</span>: <span class="string">"string"</span> &#125; &#125;</span><br></pre></td></tr></table></figure><h1 id="Java-Json-Schema库"><a href="#Java-Json-Schema库" class="headerlink" title="Java Json Schema库"></a>Java Json Schema库</h1><p>表中给出了两种java中使用的JSON Schema库</p><table><thead><tr><th>库名称</th><th>地址</th><th>支持草案</th></tr></thead><tbody><tr><td>fge</td><td><a href="https://github.com/daveclayton/json-schema-validator" target="_blank" rel="noopener">https://github.com/daveclayton/json-schema-validator</a></td><td>draft-04 draft-03</td></tr><tr><td>everit</td><td><a href="https://github.com/everit-org/json-schema" target="_blank" rel="noopener">https://github.com/everit-org/json-schema</a></td><td>draft-04</td></tr></tbody></table><p>建议：</p><ol><li><p>如果在项目中使用了jackson json，那么使用fge是一个好的选择，因为fge就是使用的jackson json。</p></li><li><p>如果项目中使用的是org.json API，那么使用everit会更好。</p></li><li><p>如果是使用以上两个库以外的库，那么就使用everit，因为everit会比fge的性能好上两倍。</p></li></ol><h2 id="fge的使用："><a href="#fge的使用：" class="headerlink" title="fge的使用："></a>fge的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.github.fge&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;json-schema-validator&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.2.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  JsonNode schema = readJsonFile(<span class="string">"src/main/resources/Schema.json"</span>);</span><br><span class="line">  JsonNode data = readJsonFile(<span class="string">"src/main/resources/failure.json"</span>);</span><br><span class="line">  ProcessingReport report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  Assert.assertTrue(report.isSuccess());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> JsonNode <span class="title">readJsonFile</span><span class="params">(String filePath)</span> </span>&#123;</span><br><span class="line">  JsonNode instance = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">instance = <span class="keyword">new</span> JsonNodeReader().fromReader(<span class="keyword">new</span> FileReader(filePath));</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> instance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>真正的调用只有一行代码，需要传入验证规则和数据。分别有validate和validateUnchecked两种方法，区别在于validateUnchecked方法不会抛出ProcessingException异常。</p><p>还可以从字符串中读取json，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  String failure = <span class="keyword">new</span> String(<span class="string">"&#123;\"foo\":1234&#125;"</span>);</span><br><span class="line">  String Schema = <span class="string">"&#123;\"type\": \"object\", \"properties\" : &#123;\"foo\" : &#123;\"type\" : \"string\"&#125;&#125;&#125;"</span>;</span><br><span class="line">  ProcessingReport report = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">JsonNode data = JsonLoader.fromString(failure);</span><br><span class="line">JsonNode schema = JsonLoader.fromString(Schema);</span><br><span class="line">report = JsonSchemaFactory.byDefault().getValidator().validateUnchecked(schema, data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//Assert.assertTrue(report.isSuccess());</span></span><br><span class="line">  Iterator&lt;ProcessingMessage&gt; it = report.iterator();</span><br><span class="line">  <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">System.out.println(it.next());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中ProcessingReport对象中维护了一共迭代器，如果执行失败（执行成功时没有信息），其提供了一些高级故障信息。每个错误可能包含以下属性：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">level: 错误级别（应该就是error）</span><br><span class="line">schema：引起故障的模式的所在位置的 URI</span><br><span class="line">instance：错误对象</span><br><span class="line">domain：验证域</span><br><span class="line">keyword：引起错误的约束key</span><br><span class="line">found：现在类型</span><br><span class="line">expected：期望类型</span><br></pre></td></tr></table></figure><p>以上代码的json信息为：</p><p>failure.json ：  </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"foo"</span> : <span class="number">1234</span>&#125;</span><br></pre></td></tr></table></figure><p>Schema.json ：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">  <span class="string">"properties"</span> : &#123;</span><br><span class="line">    <span class="string">"foo"</span> : &#123;</span><br><span class="line">      <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行错误信息为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">error: <span class="function">instance <span class="title">type</span> <span class="params">(integer)</span> does not match any allowed primitive <span class="title">type</span> <span class="params">(allowed: [<span class="string">"string"</span>])</span></span></span><br><span class="line"><span class="function">level: "error"</span></span><br><span class="line"><span class="function">schema: </span>&#123;<span class="string">"loadingURI"</span>:<span class="string">"#"</span>,<span class="string">"pointer"</span>:<span class="string">"/properties/foo"</span>&#125;</span><br><span class="line">instance: &#123;<span class="string">"pointer"</span>:<span class="string">"/foo"</span>&#125;</span><br><span class="line">domain: <span class="string">"validation"</span></span><br><span class="line">keyword: <span class="string">"type"</span></span><br><span class="line">found: <span class="string">"integer"</span></span><br><span class="line">expected: [<span class="string">"string"</span>]</span><br></pre></td></tr></table></figure><h2 id="everit的使用："><a href="#everit的使用：" class="headerlink" title="everit的使用："></a>everit的使用：</h2><p>maven配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.everit.json&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;org.everit.json.schema&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.3.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>测试代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testJsonSchema3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  InputStream inputStream = getClass().getResourceAsStream(<span class="string">"/Schema.json"</span>);</span><br><span class="line">  JSONObject Schema = <span class="keyword">new</span> JSONObject(<span class="keyword">new</span> JSONTokener(inputStream));</span><br><span class="line">  JSONObject data = <span class="keyword">new</span> JSONObject(<span class="string">"&#123;\"foo\" : 1234&#125;"</span>);</span><br><span class="line">  Schema schema = SchemaLoader.load(Schema);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">schema.validate(data);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ValidationException e) &#123;</span><br><span class="line">System.out.println(e.getMessage());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果验证失败会抛出一个ValidationException异常，然后在catch块中打印出错误信息。everit中的错误信息想比fge来说比较简单，相同的json测试文件，打印的信息如下：</p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#/foo: expected <span class="keyword">type</span>: <span class="built_in">String</span>, found: <span class="built_in">Integer</span></span><br></pre></td></tr></table></figure><p>此外everit提供了一个format关键字，可以自定义validator来校验json中一些复杂数据，比如IP地址，电话号码等。具体请参考官方文档。</p><h2 id="性能测试："><a href="#性能测试：" class="headerlink" title="性能测试："></a>性能测试：</h2><p>1、一共执行1000次，成功和失败分开执行，每种情况执行250次。然后记录下每次的执行时间，执行10次，取平均值。</p><p>fge每1000次的执行时间(ms)：1158, 1122, 1120, 1042, 1180, 1254, 1198，1126，1177，1192<br>everit每1000次的执行时间(ms)：33, 49, 54, 57, 51, 47, 48, 52, 53, 44</p><p>2、一共执行10000次，成功和失败分开执行，每种情况执行2500次。</p><table><thead><tr><th>方法/场景</th><th>每次执行时间(ms)</th></tr></thead><tbody><tr><td>fge/场景1</td><td>1.1569</td></tr><tr><td>fge/场景2</td><td>0.3407</td></tr><tr><td>everit/场景1</td><td>0.0488</td></tr><tr><td>everit/场景2</td><td>0.0206</td></tr></tbody></table><p><strong>使用对比：</strong></p><p>​    从性能上来说everit完全是碾压fge，官方说的至少两倍，实际测试过程中，差不多有20倍的差距。虽然fge使用的是jackson json，相对来说学习成本可能较低，但是使用下来发现everit的使用也并不复杂，需要注意的是包需要导入正确（org.json）。fge唯一的优势在于错误信息比较详细。还有一点区别在于，everit验证失败是抛出异常，而fge是判断返回一个boolean类型的值。</p>]]></content>
      
      
      <categories>
          
          <category> 数据存储格式 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Json </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：Session Window</title>
      <link href="/2018/05/21/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9ASession%20Window/"/>
      <url>/2018/05/21/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9ASession%20Window/</url>
      
        <content type="html"><![CDATA[<p>在上一篇文章《Window机制》中，我们介绍了窗口的概念和底层实现，以及 Flink 一些内建的窗口，包括滑动窗口、翻滚窗口。本文将深入讲解一种较为特殊的窗口：会话窗口（session window）。建议您在阅读完上一篇文章的基础上再阅读本文。</p><p>当我们需要分析用户的一段交互的行为事件时，通常的想法是将用户的事件流按照“session”来分组。session 是指一段持续活跃的期间，由活跃间隙分隔开。通俗一点说，消息之间的间隔小于超时阈值（sessionGap）的，则被分配到同一个窗口，间隔大于阈值的，则被分配到不同的窗口。目前开源领域大部分的流计算引擎都有窗口的概念，但是没有对 session window 的支持，要实现 session window，需要用户自己去做完大部分事情。而当 Flink 1.1.0 版本正式发布时，Flink 将会是开源流计算领域第一个内建支持 session window 的引擎。</p><p>在 Flink 1.1.0 之前，Flink 也可以通过自定义的window assigner和trigger来实现一个基本能用的session window。<code>release-1.0</code> 版本中提供了一个实现 session window 的 example：<a href="https://github.com/apache/flink/blob/release-1.0/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/windowing/SessionWindowing.java" target="_blank" rel="noopener">SessionWindowing</a>。这个session window范例的实现原理是，基于GlobleWindow这个window assigner，将所有元素都分配到同一个窗口中，然后指定一个自定义的trigger来触发执行窗口。这个trigger的触发机制是，对于每个到达的元素都会根据其时间戳（timestamp）注册一个会话超时的定时器（timestamp+sessionTimeout），并移除上一次注册的定时器。最新一个元素到达后，如果超过 sessionTimeout 的时间还没有新元素到达，那么trigger就会触发，当前窗口就会是一个session window。处理完窗口后，窗口中的数据会清空，用来缓存下一个session window的数据。</p><p>但是这种session window的实现是非常弱的，无法应用到实际生产环境中的。因为它无法处理乱序 event time 的消息。 而在即将到来的 Flink 1.1.0 版本中，Flink 提供了对 session window 的直接支持，用户可以通过<code>SessionWindows.withGap()</code>来轻松地定义 session widnow，而且能够处理乱序消息。Flink 对 session window 的支持主要借鉴自 Google 的 DataFlow 。</p><h2 id="Session-Window-in-Flink"><a href="#Session-Window-in-Flink" class="headerlink" title="Session Window in Flink"></a>Session Window in Flink</h2><p>假设有这么个场景，用户点开手机淘宝后会进行一系列的操作（点击、浏览、搜索、购买、切换tab等），这些操作以及对应发生的时间都会发送到服务器上进行用户行为分析。那么用户的操作行为流的样例可能会长下面这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1rvs8KXXXXXXiXVXXXXXXXXXX" alt="img"></a></p><p>通过上图，我们可以很直观地观察到，用户的行为是一段一段的，每一段内的行为都是连续紧凑的，段内行为的关联度要远大于段之间行为的关联度。我们把每一段用户行为称之为“session”，段之间的空档我们称之为“session gap”。所以，理所当然地，我们应该按照 session window 对用户的行为流进行切分，并计算每个session的结果。如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1jB3_KXXXXXcgXFXXXXXXXXXX" alt="img"></a></p><p>为了定义上述的窗口切分规则，我们可以使用 Flink 提供的 <code>SessionWindows</code> 这个 widnow assigner API。如果你用过 <code>SlidingEventTimeWindows</code>、<code>TumlingProcessingTimeWindows</code>等，你会对这个很熟悉。</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataStream <span class="built_in">input</span> = …</span><br><span class="line">DataStream result = <span class="built_in">input</span></span><br><span class="line">  .keyBy(&lt;key selector&gt;)</span><br><span class="line">  .window(SessionWindows.withGap(Time.seconds(&lt;seconds&gt;))</span><br><span class="line">  .apply(&lt;window <span class="function"><span class="keyword">function</span>&gt;)</span> // <span class="keyword">or</span> reduce() <span class="keyword">or</span> fold()</span><br></pre></td></tr></table></figure><p>这样，Flink 就会基于元素的时间戳，自动地将元素放到不同的session window中。如果两个元素的时间戳间隔小于 session gap，则会在同一个session中。如果两个元素之间的间隔大于session gap，且没有元素能够填补上这个gap，那么它们会被放到不同的session中。</p><h2 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h2><p>为了实现 session window，我们需要扩展 Flink 中的窗口机制，使得能够支持窗口合并。要理解其原因，我们需要先了解窗口的现状。在上一篇文章中，我们谈到了 Flink 中 WindowAssigner 负责将元素分配到哪个/哪些窗口中去，Trigger 决定了一个窗口何时能够被计算或清除。当元素被分配到窗口之后，这些窗口是固定的不会改变的，而且窗口之间不会相互作用。</p><p>对于session window来说，我们需要窗口变得更灵活。基本的思想是这样的：<code>SessionWindows</code> assigner 会为每个进入的元素分配一个窗口，该窗口以元素的时间戳作为起始点，时间戳加会话超时时间为结束点，也就是该窗口为<code>[timestamp, timestamp+sessionGap)</code>。比如我们现在到了两个元素，它们被分配到两个独立的窗口中，两个窗口目前不相交，如图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1174pKpXXXXbVXXXXXXXXXXXX" alt="img"></a></p><p>当第三个元素进入时，分配到的窗口与现有的两个窗口发生了叠加，情况变成了这样：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB19.g5KXXXXXX.XVXXXXXXXXXX" alt="img"></a></p><p>由于我们支持了窗口的合并，<code>WindowAssigner</code>可以合并这些窗口。它会遍历现有的窗口，并告诉系统哪些窗口需要合并成新的窗口。Flink 会将这些窗口进行合并，合并的主要内容有两部分：</p><ol><li>需要合并的窗口的底层状态的合并（也就是窗口中缓存的数据，或者对于聚合窗口来说是一个聚合值）</li><li>需要合并的窗口的Trigger的合并（比如对于EventTime来说，会删除旧窗口注册的定时器，并注册新窗口的定时器）</li></ol><p>总之，结果是三个元素现在在同一个窗口中了：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1iFw0KXXXXXcoXVXXXXXXXXXX" alt="img"></a></p><p>需要注意的是，对于每一个新进入的元素，都会分配一个属于该元素的窗口，都会检查并合并现有的窗口。在触发窗口计算之前，每一次都会检查该窗口是否可以和其他窗口合并，直到trigger触发后，会将该窗口从窗口列表中移除。对于 event time 来说，窗口的触发是要等到大于窗口结束时间的 watermark 到达，当watermark没有到，窗口会一直缓存着。所以基于这种机制，可以做到对乱序消息的支持。</p><p>这里有一个优化点可以做，因为每一个新进入的元素都会创建属于该元素的窗口，然后合并。如果新元素连续不断地进来，并且新元素的窗口一直都是可以和之前的窗口重叠合并的，那么其实这里多了很多不必要的创建窗口、合并窗口的操作，我们可以直接将新元素放到那个已存在的窗口，然后扩展该窗口的大小，看起来就像和新元素的窗口合并了一样。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p><a href="https://issues.apache.org/jira/browse/FLINK-3174" target="_blank" rel="noopener">FLINK-3174</a> 这个JIRA中有对 Flink 如何支持 session window 的详细说明，以及代码更新。建议可以结合该 <a href="https://github.com/apache/flink/pull/1460" target="_blank" rel="noopener">PR</a>的代码来理解本文讨论的实现原理。</p><p>为了扩展 Flink 中的窗口机制，使得能够支持窗口合并，首先 window assigner 要能合并现有的窗口，Flink 增加了一个新的抽象类 <code>MergingWindowAssigner</code> 继承自 <code>WindowAssigner</code>，这里面主要多了一个 <code>mergeWindows</code> 的方法，用来决定哪些窗口是可以合并的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MergingWindowAssigner&lt;T</span>, <span class="title">W</span> <span class="keyword">extends</span> <span class="title">Window&gt;</span> <span class="keyword">extends</span> <span class="title">WindowAssigner&lt;T</span>, <span class="title">W&gt;</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> static <span class="keyword">final</span> long serialVersionUID = <span class="number">1</span>L;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 决定哪些窗口需要被合并。对于每组需要合并的窗口, 都会调用 callback.merge(toBeMerged, mergeResult)</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param windows 现存的窗口集合 The window candidates.</span></span><br><span class="line"><span class="comment">   * @param callback 需要被合并的窗口会回调 callback.merge 方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  public <span class="keyword">abstract</span> void mergeWindows(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; windows, <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; callback);</span><br><span class="line"></span><br><span class="line">  public interface <span class="type">MergeCallback</span>&lt;<span class="type">W</span>&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用来声明合并窗口的具体动作（合并窗口底层状态、合并窗口trigger等）。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @param toBeMerged  需要被合并的窗口列表</span></span><br><span class="line"><span class="comment">     * @param mergeResult 合并后的窗口</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    void merge(<span class="type">Collection</span>&lt;<span class="type">W</span>&gt; toBeMerged, <span class="type">W</span> mergeResult);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有已经存在的 assigner 都继承自 <code>WindowAssigner</code>，只有新加入的 session window assigner 继承自 <code>MergingWindowAssigner</code>，如：<code>ProcessingTimeSessionWindows</code>和<code>EventTimeSessionWindows</code>。</p><p>另外，Trigger 也需要能支持对合并窗口后的响应，所以 Trigger 添加了一个新的接口 <code>onMerge(W window, OnMergeContext ctx)</code>，用来响应发生窗口合并之后对trigger的相关动作，比如根据合并后的窗口注册新的 event time 定时器。</p><p>OK，接下来我们看下最核心的代码，也就是对于每个进入的元素的处理，代码位于<code>WindowOperator.processElement</code>方法中，如下所示：</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">public void processElement(StreamRecord<span class="variable">&lt;IN&gt;</span> element) throws Exception &#123;</span><br><span class="line">  Collection<span class="variable">&lt;W&gt;</span> elementWindows = windowAssigner.assignWindows(element.getValue(), element.getTimestamp());</span><br><span class="line">  final K key = (K) getStateBackend().getCurrentKey();</span><br><span class="line">  if (windowAssigner instanceof MergingWindowAssigner) &#123;</span><br><span class="line">    // 对于session window 的特殊处理，我们只关注该条件块内的代码</span><br><span class="line">    MergingWindowSet<span class="variable">&lt;W&gt;</span> mergingWindows = getMergingWindowSet();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line">      final Tuple1<span class="variable">&lt;TriggerResult&gt;</span> mergeTriggerResult = new Tuple1<span class="variable">&lt;&gt;</span>(TriggerResult.CONTINUE);</span><br><span class="line">      </span><br><span class="line">      // 加入新窗口, 如果没有合并发生,那么actualWindow就是新加入的窗口</span><br><span class="line">      // 如果有合并发生, 那么返回的actualWindow即为合并后的窗口,</span><br><span class="line">      // 并且会调用 MergeFunction.merge 方法, 这里方法中的内容主要是更新trigger, 合并旧窗口中的状态到新窗口中</span><br><span class="line">      W actualWindow = mergingWindows.addWindow(window, new MergingWindowSet.MergeFunction<span class="variable">&lt;W&gt;</span>() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void merge(W mergeResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedWindows, W <span class="keyword">state</span>WindowResult,</span><br><span class="line">            Collection<span class="variable">&lt;W&gt;</span> mergedStateWindows) throws Exception &#123;</span><br><span class="line">          context.key = key;</span><br><span class="line">          context.window = mergeResult;</span><br><span class="line"></span><br><span class="line">          // 这里面会根据新窗口的结束时间注册新的定时器</span><br><span class="line">          mergeTriggerResult.f0 = context.<span class="keyword">on</span>Merge(mergedWindows);</span><br><span class="line"></span><br><span class="line">          // 删除旧窗口注册的定时器</span><br><span class="line">          <span class="keyword">for</span> (W m: mergedWindows) &#123;</span><br><span class="line">            context.window = m;</span><br><span class="line">            context.clear();</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          // 合并旧窗口(mergedStateWindows)中的状态到新窗口（<span class="keyword">state</span>WindowResult）中</span><br><span class="line">          getStateBackend().mergePartitionedStates(<span class="keyword">state</span>WindowResult,</span><br><span class="line">              mergedStateWindows,</span><br><span class="line">              windowSerializer,</span><br><span class="line">              (StateDescriptor<span class="variable">&lt;? extends MergingState&lt;?,?&gt;</span>, ?&gt;) windowStateDescriptor);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">      // 取 actualWindow 对应的用来存状态的窗口</span><br><span class="line">      W <span class="keyword">state</span>Window = mergingWindows.getStateWindow(actualWindow);</span><br><span class="line">      // 从状态后端拿出对应的状态 </span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(<span class="keyword">state</span>Window, windowSerializer, windowStateDescriptor);</span><br><span class="line">      // 将新进入的元素数据加入到新窗口（或者说合并后的窗口）中对应的状态中</span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = actualWindow;</span><br><span class="line"></span><br><span class="line">      // 检查是否需要fire or purge </span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      TriggerResult combinedTriggerResult = TriggerResult.merge(triggerResult, mergeTriggerResult.f0);</span><br><span class="line"></span><br><span class="line">      // 根据trigger结果决定怎么处理窗口中的数据</span><br><span class="line">      processTriggerResult(combinedTriggerResult, actualWindow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 对于普通window assigner的处理， 这里我们不关注</span><br><span class="line">    <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">      AppendingState<span class="variable">&lt;IN, ACC&gt;</span> windowState = getPartitionedState(window, windowSerializer,</span><br><span class="line">          windowStateDescriptor);</span><br><span class="line"></span><br><span class="line">      windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">      context.key = key;</span><br><span class="line">      context.window = window;</span><br><span class="line">      TriggerResult triggerResult = context.<span class="keyword">on</span>Element(element);</span><br><span class="line"></span><br><span class="line">      processTriggerResult(triggerResult, window);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实这段代码写的并不是很clean，并且不是很好理解。在第六行中有用到<code>MergingWindowSet</code>，这个类很重要所以我们先介绍它。这是一个用来跟踪窗口合并的类。比如我们有A、B、C三个窗口需要合并，合并后的窗口为D窗口。这三个窗口在底层都有对应的状态集合，为了避免代价高昂的状态替换（创建新状态是很昂贵的），我们保持其中一个窗口作为原始的状态窗口，其他几个窗口的数据合并到该状态窗口中去，比如随机选择A作为状态窗口，那么B和C窗口中的数据需要合并到A窗口中去。这样就没有新状态产生了，但是我们需要额外维护窗口与状态窗口之间的映射关系（D-&gt;A），这就是<code>MergingWindowSet</code>负责的工作。这个映射关系需要在失败重启后能够恢复，所以<code>MergingWindowSet</code>内部也是对该映射关系做了容错。状态合并的工作示意图如下所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15U4lKpXXXXc9XXXXXXXXXXXX" alt="img"></a></p><p>然后我们来解释下processElement的代码，首先根据window assigner为新进入的元素分配窗口集合。接着进入第一个条件块，取出当前的<code>MergingWindowSet</code>。对于每个分配到的窗口，我们就会将其加入到<code>MergingWindowSet</code>中（<code>addWindow</code>方法），由<code>MergingWindowSet</code>维护窗口与状态窗口之间的关系，并在需要窗口合并的时候，合并状态和trigger。然后根据映射关系，取出结果窗口对应的状态窗口，根据状态窗口取出对应的状态。将新进入的元素数据加入到该状态中。最后，根据trigger结果来对窗口数据进行处理，对于session window来说，这里都是不进行任何处理的。真正对窗口处理是由定时器超时后对完成的窗口调用<code>processTriggerResult</code>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文在上一篇文章《Window机制》的基础上，深入讲解了 Flink 是如何支持 session window 的，核心的原理是窗口的合并。Flink 对于 session window 的支持很大程度上受到了 Google DataFlow 的启发，所以也建议阅读下 DataFlow 的论文。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：Window 机制</title>
      <link href="/2018/05/20/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9AWindow%20%E6%9C%BA%E5%88%B6/"/>
      <url>/2018/05/20/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9AWindow%20%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>Flink 认为 Batch 是 Streaming 的一个特例，所以 Flink 底层引擎是一个流式引擎，在上面实现了流处理和批处理。而窗口（window）就是从 Streaming 到 Batch 的一个桥梁。Flink 提供了非常完善的窗口机制，这是我认为的 Flink 最大的亮点之一（其他的亮点包括消息乱序处理，和 checkpoint 机制）。本文我们将介绍流式处理中的窗口概念，介绍 Flink 内建的一些窗口和 Window API，最后讨论下窗口在底层是如何实现的。</p><h2 id="什么是-Window"><a href="#什么是-Window" class="headerlink" title="什么是 Window"></a>什么是 Window</h2><p>在流处理应用中，数据是连续不断的，因此我们不可能等到所有数据都到了才开始处理。当然我们可以每来一个消息就处理一次，但是有时我们需要做一些聚合类的处理，例如：在过去的1分钟内有多少用户点击了我们的网页。在这种情况下，我们必须定义一个窗口，用来收集最近一分钟内的数据，并对这个窗口内的数据进行计算。</p><p>窗口可以是时间驱动的（Time Window，例如：每30秒钟），也可以是数据驱动的（Count Window，例如：每一百个元素）。一种经典的窗口分类可以分成：翻滚窗口（Tumbling Window，无重叠），滚动窗口（Sliding Window，有重叠），和会话窗口（Session Window，活动间隙）。</p><p>我们举个具体的场景来形象地理解不同窗口的概念。假设，淘宝网会记录每个用户每次购买的商品个数，我们要做的是统计不同窗口中用户购买商品的总数。下图给出了几种经典的窗口切分概述图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1bwsTJVXXXXaBaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1bwsTJVXXXXaBaXXXXXXXXXXX" alt="img"></a></p><p>上图中，raw data stream 代表用户的购买行为流，圈中的数字代表该用户本次购买的商品个数，事件是按时间分布的，所以可以看出事件之间是有time gap的。Flink 提供了上图中所有的窗口类型，下面我们会逐一进行介绍。</p><h3 id="Time-Window"><a href="#Time-Window" class="headerlink" title="Time Window"></a>Time Window</h3><p>就如名字所说的，Time Window 是根据时间对数据流进行分组的。这里我们涉及到了流处理中的时间问题，时间问题和消息乱序问题是紧密关联的，这是流处理中现存的难题之一，我们将在后续的 <a href="http://wuchong.me/blog/2016/05/25/flink-internals-window-mechanism/#" target="_blank" rel="noopener">EventTime 和消息乱序处理</a>中对这部分问题进行深入探讨。这里我们只需要知道 Flink 提出了三种时间的概念，分别是event time（事件时间：事件发生时的时间），ingestion time（摄取时间：事件进入流处理系统的时间），processing time（处理时间：消息被计算处理的时间）。Flink 中窗口机制和时间类型是完全解耦的，也就是说当需要改变时间类型时不需要更改窗口逻辑相关的代码。</p><ul><li><p><strong>Tumbling Time Window</strong><br>如上图，我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnt)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by userId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  <span class="comment">// tumbling time window of 1 minute length</span></span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">1</span>))</span><br><span class="line">  <span class="comment">// compute sum over buyCnt</span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Sliding Time Window</strong><br>但是对于某些应用，它们需要的窗口是不间断的，需要平滑地进行窗口聚合。比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val slidingCn<span class="symbol">ts:</span> DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>) </span><br><span class="line">  // sliding <span class="built_in">time</span> window of <span class="number">1</span> <span class="built_in">minute</span> length <span class="built_in">and</span> <span class="number">30</span> secs trigger interval</span><br><span class="line">  .timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>))</span><br><span class="line">  .<span class="built_in">sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Count-Window"><a href="#Count-Window" class="headerlink" title="Count Window"></a>Count Window</h3><p>Count Window 是根据元素个数对数据流进行分组的。</p><ul><li><p><strong>Tumbling Count Window</strong><br>当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window），上图所示窗口大小为3个。通过使用 DataStream API，我们可以这样实现：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> tumblingCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = buyCnts</span><br><span class="line">  <span class="comment">// key stream by sensorId</span></span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// tumbling count window of 100 elements size</span></span><br><span class="line">  .countWindow(<span class="number">100</span>)</span><br><span class="line">  <span class="comment">// compute the buyCnt sum </span></span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Sliding Count Window</strong><br>当然Count Window 也支持 Sliding Window，虽在上图中未描述出来，但和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和，代码示例如下。</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val slidingCnts: DataStream[(<span class="keyword">Int</span>, <span class="keyword">Int</span>)] = vehicleCnts</span><br><span class="line"><span class="meta">  .keyBy</span>(<span class="number">0</span>)</span><br><span class="line">  // sliding count window of <span class="number">100</span> elements size <span class="keyword">and</span> <span class="number">10</span> elements trigger interval</span><br><span class="line"><span class="meta">  .countWindow</span>(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">  .sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Session-Window"><a href="#Session-Window" class="headerlink" title="Session Window"></a>Session Window</h3><p>在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）。Session Window 的示例代码如下：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stream of (userId, buyCnts)</span></span><br><span class="line"><span class="keyword">val</span> buyCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = ...</span><br><span class="line">  </span><br><span class="line"><span class="keyword">val</span> sessionCnts: DataStream[(<span class="built_in">Int</span>, <span class="built_in">Int</span>)] = vehicleCnts</span><br><span class="line">  .keyBy(<span class="number">0</span>)</span><br><span class="line">  <span class="comment">// session window based on a 30 seconds session gap interval </span></span><br><span class="line">  .window(ProcessingTimeSessionWindows.withGap(Time.seconds(<span class="number">30</span>)))</span><br><span class="line">  .sum(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>一般而言，window 是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink 的 DataStream API 提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义窗口分配逻辑。下面我们会对 Flink 窗口相关的 API 进行剖析。</p><h2 id="剖析-Window-API"><a href="#剖析-Window-API" class="headerlink" title="剖析 Window API"></a>剖析 Window API</h2><p>得益于 Flink Window API 松耦合设计，我们可以非常灵活地定义符合特定业务的窗口。Flink 中定义一个窗口主要需要以下三个组件。</p><ul><li><p><strong>Window Assigner：</strong>用来决定某个元素被分配到哪个/哪些窗口中去。</p><p>如下类图展示了目前内置实现的 Window Assigners：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1plkxJVXXXXXqXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1plkxJVXXXXXqXpXXXXXXXXXX" alt="img"></a></p></li><li><p><strong>Trigger：</strong>触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。</p><p>如下类图展示了目前内置实现的 Triggers：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15yMeJVXXXXbbXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15yMeJVXXXXbbXFXXXXXXXXXX" alt="img"></a></p></li><li><p><strong>Evictor：</strong>可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。</p><p>如下类图展示了目前内置实现的 Evictors：<br><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1OCT6JVXXXXcjXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1OCT6JVXXXXcjXVXXXXXXXXXX" alt="img"></a></p></li></ul><p>上述三个组件的不同实现的不同组合，可以定义出非常复杂的窗口。Flink 中内置的窗口也都是基于这三个组件构成的，当然内置窗口有时候无法解决用户特殊的需求，所以 Flink 也暴露了这些窗口机制的内部接口供用户实现自定义的窗口。下面我们将基于这三者探讨窗口的实现机制。</p><h2 id="Window-的实现"><a href="#Window-的实现" class="headerlink" title="Window 的实现"></a>Window 的实现</h2><p>下图描述了 Flink 的窗口机制以及各组件之间是如何相互工作的。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1swNgKXXXXXc4XpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1swNgKXXXXXc4XpXXXXXXXXXX" alt="img"></a></p><p>首先上图中的组件都位于一个算子（window operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给 WindowAssigner。WindowAssigner 会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，<code>Window</code>本身只是一个ID标识符，其内部可能存储了一些元数据，如<code>TimeWindow</code>中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在 Key/Value State 中，key为<code>Window</code>，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了 Flink 的 State 机制（参见 <a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/state.html" target="_blank" rel="noopener">state 文档</a>）。</p><p>每一个窗口都拥有一个属于自己的 Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者 fire + purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被 purge 了。在purge之前，窗口会一直占用着内存。</p><p>当Trigger fire了，窗口中的元素集合就会交给<code>Evictor</code>（如果指定了的话）。Evictor 主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有 Evictor 的话，窗口中的所有元素会一起交给函数进行计算。</p><p>计算函数收到了窗口的元素（可能经过了 Evictor 的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的<code>sum()</code>,<code>min()</code>,<code>max()</code>，还有 <code>ReduceFunction</code>，<code>FoldFunction</code>，还有<code>WindowFunction</code>。WindowFunction 是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。</p><p>Flink 对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了 Evictor，则不会启用对聚合窗口的优化，因为 Evictor 需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>上述的三个组件构成了 Flink 的窗口机制。为了更清楚地描述窗口机制，以及解开一些疑惑（比如 purge 和 Evictor 的区别和用途），我们将一步步地解释 Flink 内置的一些窗口（Time Window，Count Window，Session Window）是如何实现的。</p><h3 id="Count-Window-实现"><a href="#Count-Window-实现" class="headerlink" title="Count Window 实现"></a>Count Window 实现</h3><p>Count Window 是使用三组件的典范，我们可以在 <code>KeyedStream</code> 上创建 Count Window，其源码如下所示：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// <span class="keyword">tumbling</span><span class="built_in"> count</span> <span class="keyword">window</span></span><br><span class="line">public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">window</span>(GlobalWindows.create())  // create <span class="keyword">window</span> stream using GlobalWindows</span><br><span class="line">      .trigger(PurgingTrigger.<span class="keyword">of</span>(CountTrigger.<span class="keyword">of</span>(size))); // trigger <span class="literal">is</span> <span class="keyword">window</span> size</span><br><span class="line">&#125;</span><br><span class="line">// <span class="keyword">sliding</span><span class="built_in"> count</span> <span class="keyword">window</span></span><br><span class="line">public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">window</span>(GlobalWindows.create())</span><br><span class="line">    .evictor(CountEvictor.<span class="keyword">of</span>(size))  // evictor <span class="literal">is</span> <span class="keyword">window</span> size</span><br><span class="line">    .trigger(CountTrigger.<span class="keyword">of</span>(slide)); // trigger <span class="literal">is</span> slide size</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个函数是申请翻滚计数窗口，参数为窗口大小。第二个函数是申请滑动计数窗口，参数分别为窗口大小和滑动大小。它们都是基于 <code>GlobalWindows</code> 这个 WindowAssigner 来创建的窗口，该assigner会将所有元素都分配到同一个global window中，所有<code>GlobalWindows</code>的返回值一直是 <code>GlobalWindow</code> 单例。基本上自定义的窗口都会基于该assigner实现。</p><p>翻滚计数窗口并不带evictor，只注册了一个trigger。该trigger是带purge功能的 CountTrigger。也就是说每当窗口中的元素数量达到了 window-size，trigger就会返回fire+purge，窗口就会执行计算并清空窗口中的所有元素，再接着储备新的元素。从而实现了tumbling的窗口之间无重叠。</p><p>滑动计数窗口的各窗口之间是有重叠的，但我们用的 GlobalWindows assinger 从始至终只有一个窗口，不像 sliding time assigner 可以同时存在多个窗口。所以trigger结果不能带purge，也就是说计算完窗口后窗口中的数据要保留下来（供下个滑窗使用）。另外，trigger的间隔是slide-size，evictor的保留的元素个数是window-size。也就是说，每个滑动间隔就触发一次窗口计算，并保留下最新进入窗口的window-size个元素，剔除旧元素。</p><p>假设有一个滑动计数窗口，每2个元素计算一次最近4个元素的总和，那么窗口工作示意图如下所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB15vcUJVXXXXcGXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB15vcUJVXXXXcGXVXXXXXXXXXX" alt="img"></a></p><p>图中所示的各个窗口逻辑上是不同的窗口，但在物理上是同一个窗口。该滑动计数窗口，trigger的触发条件是元素个数达到2个（每进入2个元素就会触发一次），evictor保留的元素个数是4个，每次计算完窗口总和后会保留剩余的元素。所以第一次触发trigger是当元素5进入，第三次触发trigger是当元素2进入，并驱逐5和2，计算剩余的4个元素的总和（22）并发送出去，保留下2,4,9,7元素供下个逻辑窗口使用。</p><h3 id="Time-Window-实现"><a href="#Time-Window-实现" class="headerlink" title="Time Window 实现"></a>Time Window 实现</h3><p>同样的，我们也可以在 <code>KeyedStream</code> 上申请 Time Window，其源码如下所示：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tumbling time window</span></span><br><span class="line">public WindowedStream&lt;T, <span class="built_in">KEY</span>, TimeWindow&gt; <span class="built_in">timeWindow</span>(<span class="built_in">Time</span> size) &#123;</span><br><span class="line">  <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(TumblingProcessingTimeWindows.<span class="built_in">of</span>(size));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(TumblingEventTimeWindows.<span class="built_in">of</span>(size));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// sliding time window</span></span><br><span class="line">public WindowedStream&lt;T, <span class="built_in">KEY</span>, TimeWindow&gt; <span class="built_in">timeWindow</span>(<span class="built_in">Time</span> size, <span class="built_in">Time</span> slide) &#123;</span><br><span class="line">  <span class="keyword">if</span> (environment.getStreamTimeCharacteristic() == TimeCharacteristic.ProcessingTime) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(SlidingProcessingTimeWindows.<span class="built_in">of</span>(size, slide));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">window</span>(SlidingEventTimeWindows.<span class="built_in">of</span>(size, slide));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在方法体内部会根据当前环境注册的时间类型，使用不同的WindowAssigner创建window。可以看到，EventTime和IngestTime都使用了<code>XXXEventTimeWindows</code>这个assigner，因为EventTime和IngestTime在底层的实现上只是在Source处为Record打时间戳的实现不同，在window operator中的处理逻辑是一样的。</p><p>这里我们主要分析sliding process time window，如下是相关源码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SlidingProcessingTimeWindows</span> <span class="keyword">extends</span> <span class="title">WindowAssigner</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> size;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> slide;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="title">SlidingProcessingTimeWindows</span><span class="params">(<span class="keyword">long</span> size, <span class="keyword">long</span> slide)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.size = size;</span><br><span class="line">    <span class="keyword">this</span>.slide = slide;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title">assignWindows</span><span class="params">(Object element, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    timestamp = System.currentTimeMillis();</span><br><span class="line">    List&lt;TimeWindow&gt; windows = <span class="keyword">new</span> ArrayList&lt;&gt;((<span class="keyword">int</span>) (size / slide));</span><br><span class="line">    <span class="comment">// 对齐时间戳</span></span><br><span class="line">    <span class="keyword">long</span> lastStart = timestamp - timestamp % slide;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> start = lastStart;</span><br><span class="line">      start &gt; timestamp - size;</span><br><span class="line">      start -= slide) &#123;</span><br><span class="line">      <span class="comment">// 当前时间戳对应了多个window</span></span><br><span class="line">      windows.add(<span class="keyword">new</span> TimeWindow(start, start + size));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> windows;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessingTimeTrigger</span> <span class="keyword">extends</span> <span class="title">Trigger</span>&lt;<span class="title">Object</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="comment">// 每个元素进入窗口都会调用该方法</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 注册定时器，当系统时间到达window end timestamp时会回调该trigger的onProcessingTime方法</span></span><br><span class="line">    ctx.registerProcessingTimeTimer(window.getEnd());</span><br><span class="line">    <span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="comment">// 返回结果表示执行窗口计算并清空窗口</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> time, TimeWindow window, TriggerContext ctx)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> TriggerResult.FIRE_AND_PURGE;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先，<code>SlidingProcessingTimeWindows</code>会对每个进入窗口的元素根据系统时间分配到<code>(size / slide)</code>个不同的窗口，并会在每个窗口上根据窗口结束时间注册一个定时器（相同窗口只会注册一份），当定时器超时时意味着该窗口完成了，这时会回调对应窗口的Trigger的<code>onProcessingTime</code>方法，返回FIRE_AND_PURGE，也就是会执行窗口计算并清空窗口。整个过程示意图如下：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1iihcKXXXXXavXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1iihcKXXXXXavXFXXXXXXXXXX" alt="img"></a></p><p>如上图所示横轴代表时间戳（为简化问题，时间戳从0开始），第一条record会被分配到[-5,5)和[0,10)两个窗口中，当系统时间到5时，就会计算[-5,5)窗口中的数据，并将结果发送出去，最后清空窗口中的数据，释放该窗口资源。</p><h3 id="Session-Window-实现"><a href="#Session-Window-实现" class="headerlink" title="Session Window 实现"></a>Session Window 实现</h3><p>Session Window 是一个需求很强烈的窗口机制，但Session也比之前的Window更复杂，所以 Flink 也是在即将到来的 1.1.0 版本中才支持了该功能。由于篇幅问题，我们将在后续的Session Window 的实现中深入探讨 Session Window 的实现。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：生成 JobGraph</title>
      <link href="/2018/05/17/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20JobGraph/"/>
      <url>/2018/05/17/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20JobGraph/</url>
      
        <content type="html"><![CDATA[<p>继前文<a href="http://wuchong.me/blog/2016/05/03/flink-internals-overview/" target="_blank" rel="noopener">Flink 原理与实现：架构和拓扑概览</a>中介绍了Flink的四层执行图模型，本文将主要介绍 Flink 是如何将 StreamGraph 转换成 JobGraph 的。根据用户用Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。以 WordCount 为例，转换图如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1DzYXJFXXXXXJXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1DzYXJFXXXXXJXVXXXXXXXXXX" alt="img"></a></p><p>StreamGraph 和 JobGraph 都是在 Client 端生成的，也就是说我们可以在 IDE 中通过断点调试观察 StreamGraph 和 JobGraph 的生成过程。</p><p>JobGraph 的相关数据结构主要在 <code>org.apache.flink.runtime.jobgraph</code> 包中。构造 JobGraph 的代码主要集中在 <code>StreamingJobGraphGenerator</code> 类中，入口函数是 <code>StreamingJobGraphGenerator.createJobGraph()</code>。我们首先来看下<code>StreamingJobGraphGenerator</code>的核心源码：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">StreamingJobGraphGenerator</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> StreamGraph streamGraph;</span><br><span class="line">  <span class="keyword">private</span> JobGraph jobGraph;</span><br><span class="line">  <span class="comment">// id -&gt; JobVertex</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, JobVertex&gt; jobVertices;</span><br><span class="line">  <span class="comment">// 已经构建的JobVertex的id集合</span></span><br><span class="line">  <span class="keyword">private</span> Collection&lt;Integer&gt; builtVertices;</span><br><span class="line">  <span class="comment">// 物理边集合（排除了chain内部的边）, 按创建顺序排序</span></span><br><span class="line">  <span class="keyword">private</span> List&lt;StreamEdge&gt; physicalEdgesInOrder;</span><br><span class="line">  <span class="comment">// 保存chain信息，部署时用来构建 OperatorChain，startNodeId -&gt; (currentNodeId -&gt; StreamConfig)</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, Map&lt;Integer, StreamConfig&gt;&gt; chainedConfigs;</span><br><span class="line">  <span class="comment">// 所有节点的配置信息，id -&gt; StreamConfig</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, StreamConfig&gt; vertexConfigs;</span><br><span class="line">  <span class="comment">// 保存每个节点的名字，id -&gt; chainedName</span></span><br><span class="line">  <span class="keyword">private</span> Map&lt;Integer, String&gt; chainedNames;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 构造函数，入参只有 StreamGraph</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">StreamingJobGraphGenerator</span>(<span class="params">StreamGraph streamGraph</span>)</span> &#123;</span><br><span class="line">    <span class="keyword">this</span>.streamGraph = streamGraph;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 根据 StreamGraph，生成 JobGraph</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> JobGraph <span class="title">createJobGraph</span>(<span class="params"></span>)</span> &#123;</span><br><span class="line">    jobGraph = <span class="keyword">new</span> JobGraph(streamGraph.getJobName());</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// streaming 模式下，调度模式是所有节点（vertices）一起启动</span></span><br><span class="line">    jobGraph.setScheduleMode(ScheduleMode.ALL);</span><br><span class="line">    <span class="comment">// 初始化成员变量</span></span><br><span class="line">    init();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 广度优先遍历 StreamGraph 并且为每个SteamNode生成hash id，</span></span><br><span class="line">    <span class="comment">// 保证如果提交的拓扑没有改变，则每次生成的hash都是一样的</span></span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes = traverseStreamGraphAndGenerateHashes();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 最重要的函数，生成JobVertex，JobEdge等，并尽可能地将多个节点chain在一起</span></span><br><span class="line">    setChaining(hashes);</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 将每个JobVertex的入边集合也序列化到该JobVertex的StreamConfig中</span></span><br><span class="line">    <span class="comment">// (出边集合已经在setChaining的时候写入了)</span></span><br><span class="line">    setPhysicalEdges();</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 根据group name，为每个 JobVertex 指定所属的 SlotSharingGroup </span></span><br><span class="line">    <span class="comment">// 以及针对 Iteration的头尾设置  CoLocationGroup</span></span><br><span class="line">    setSlotSharing();</span><br><span class="line">    <span class="comment">// 配置checkpoint</span></span><br><span class="line">    configureCheckpointing();</span><br><span class="line">    <span class="comment">// 配置重启策略（不重启，还是固定延迟重启）</span></span><br><span class="line">    configureRestartStrategy();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 将 StreamGraph 的 ExecutionConfig 序列化到 JobGraph 的配置中</span></span><br><span class="line">      InstantiationUtil.writeObjectToConfig(<span class="keyword">this</span>.streamGraph.getExecutionConfig(), <span class="keyword">this</span>.jobGraph.getJobConfiguration(), ExecutionConfig.CONFIG_KEY);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Config object could not be written to Job Configuration: "</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> jobGraph;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>StreamingJobGraphGenerator</code>的成员变量都是为了辅助生成最终的JobGraph。<code>createJobGraph()</code>函数的逻辑也很清晰，首先为所有节点生成一个唯一的hash id，如果节点在多次提交中没有改变（包括并发度、上下游等），那么这个id就不会改变，这主要用于故障恢复。这里我们不能用 <code>StreamNode.id</code>来代替，因为这是一个从1开始的静态计数变量，同样的Job可能会得到不一样的id，如下代码示例的两个job是完全一样的，但是source的id却不一样了。然后就是最关键的chaining处理，和生成JobVetex、JobEdge等。之后就是写入各种配置相关的信息。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 范例1：A.id=1  B.id=2</span></span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; A = ...</span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; B = ...</span><br><span class="line">A.<span class="keyword">union</span>(B).<span class="built_in">print</span>();</span><br><span class="line"><span class="comment">// 范例2：A.id=2  B.id=1</span></span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; B = ...</span><br><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; A = ...</span><br><span class="line">A.<span class="keyword">union</span>(B).<span class="built_in">print</span>();</span><br></pre></td></tr></table></figure><p>下面具体分析下关键函数 <code>setChaining</code> 的实现：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从source开始建立 node chains</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> setChaining(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</span><br><span class="line">  <span class="built_in">for</span> (Integer sourceNodeId : streamGraph.getSourceIDs()) &#123;</span><br><span class="line">    createChain(sourceNodeId, sourceNodeId, hashes);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建node chains，返回当前节点的物理出边</span></span><br><span class="line"><span class="comment">// startNodeId != currentNodeId 时,说明currentNode是chain中的子节点</span></span><br><span class="line"><span class="keyword">private</span> List&lt;StreamEdge&gt; createChain(</span><br><span class="line">    Integer startNodeId,</span><br><span class="line">    Integer currentNodeId,</span><br><span class="line">    Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 过渡用的出边集合, 用来生成最终的 JobEdge, 注意不包括 chain 内部的边</span></span><br><span class="line">    List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">    List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">    List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将当前节点的出边分成 chainable 和 nonChainable 两类</span></span><br><span class="line">    <span class="built_in">for</span> (StreamEdge outEdge : streamGraph.getStreamNode(currentNodeId).getOutEdges()) &#123;</span><br><span class="line">      <span class="built_in">if</span> (isChainable(outEdge)) &#123;</span><br><span class="line">        chainableOutputs.add(outEdge);</span><br><span class="line">      &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">        nonChainableOutputs.add(outEdge);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//==&gt; 递归调用</span></span><br><span class="line">    <span class="built_in">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">      transitiveOutEdges.addAll(createChain(startNodeId, chainable.getTargetId(), hashes));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">      transitiveOutEdges.add(nonChainable);</span><br><span class="line">      createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成当前节点的显示名，如："Keyed Aggregation -&gt; Sink: Unnamed"</span></span><br><span class="line">    chainedNames.<span class="built_in">put</span>(currentNodeId, createChainedName(currentNodeId, chainableOutputs));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前节点是起始节点, 则直接创建 JobVertex 并返回 StreamConfig, 否则先创建一个空的 StreamConfig</span></span><br><span class="line">    <span class="comment">// createJobVertex 函数就是根据 StreamNode 创建对应的 JobVertex, 并返回了空的 StreamConfig</span></span><br><span class="line">    StreamConfig <span class="built_in">config</span> = currentNodeId.equals(startNodeId)</span><br><span class="line">        ? createJobVertex(startNodeId, hashes)</span><br><span class="line">        : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置 JobVertex 的 StreamConfig, 基本上是序列化 StreamNode 中的配置到 StreamConfig 中.</span></span><br><span class="line">    <span class="comment">// 其中包括 序列化器, StreamOperator, Checkpoint 等相关配置</span></span><br><span class="line">    setVertexConfig(currentNodeId, <span class="built_in">config</span>, chainableOutputs, nonChainableOutputs);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">      <span class="comment">// 如果是chain的起始节点。（不是chain中的节点，也会被标记成 chain start）</span></span><br><span class="line">      <span class="built_in">config</span>.setChainStart();</span><br><span class="line">      <span class="comment">// 我们也会把物理出边写入配置, 部署时会用到</span></span><br><span class="line">      <span class="built_in">config</span>.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">      <span class="built_in">config</span>.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将当前节点(headOfChain)与所有出边相连</span></span><br><span class="line">      <span class="built_in">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">        <span class="comment">// 通过StreamEdge构建出JobEdge，创建IntermediateDataSet，用来将JobVertex和JobEdge相连</span></span><br><span class="line">        <span class="built_in">connect</span>(startNodeId, edge);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将chain中所有子节点的StreamConfig写入到 headOfChain 节点的 CHAINED_TASK_CONFIG 配置中</span></span><br><span class="line">      <span class="built_in">config</span>.setTransitiveChainedTaskConfigs(chainedConfigs.<span class="built_in">get</span>(startNodeId));</span><br><span class="line"></span><br><span class="line">    &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">      <span class="comment">// 如果是 chain 中的子节点</span></span><br><span class="line">      </span><br><span class="line">      Map&lt;Integer, StreamConfig&gt; chainedConfs = chainedConfigs.<span class="built_in">get</span>(startNodeId);</span><br><span class="line"></span><br><span class="line">      <span class="built_in">if</span> (chainedConfs == null) &#123;</span><br><span class="line">        chainedConfigs.<span class="built_in">put</span>(startNodeId, <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 将当前节点的StreamConfig添加到该chain的config集合中</span></span><br><span class="line">      chainedConfigs.<span class="built_in">get</span>(startNodeId).<span class="built_in">put</span>(currentNodeId, <span class="built_in">config</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回连往chain外部的出边集合</span></span><br><span class="line">    <span class="built_in">return</span> transitiveOutEdges;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="built_in">else</span> &#123;</span><br><span class="line">    <span class="built_in">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 JobVertex 都会对应一个可序列化的 StreamConfig, 用来发送给 JobManager 和 TaskManager。最后在 TaskManager 中起 Task 时,需要从这里面反序列化出所需要的配置信息, 其中就包括了含有用户代码的StreamOperator。</p><p><code>setChaining</code>会对source调用<code>createChain</code>方法，该方法会递归调用下游节点，从而构建出node chains。<code>createChain</code>会分析当前节点的出边，根据<a href="http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/#Operator-Chains" target="_blank" rel="noopener">Operator Chains</a>中的chainable条件，将出边分成chainalbe和noChainable两类，并分别递归调用自身方法。之后会将StreamNode中的配置信息序列化到StreamConfig中。如果当前不是chain中的子节点，则会构建 JobVertex 和 JobEdge相连。如果是chain中的子节点，则会将StreamConfig添加到该chain的config集合中。一个node chains，除了 headOfChain node会生成对应的 JobVertex，其余的nodes都是以序列化的形式写入到StreamConfig中，并保存到headOfChain的 <code>CHAINED_TASK_CONFIG</code> 配置项中。直到部署时，才会取出并生成对应的ChainOperators，具体过程请见<a href="http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/#Operator-Chains" target="_blank" rel="noopener">理解 Operator Chains</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要对 Flink 中将 StreamGraph 转变成 JobGraph 的核心源码进行了分析。思想还是很简单的，StreamNode 转成 JobVertex，StreamEdge 转成 JobEdge，JobEdge 和 JobVertex 之间创建 IntermediateDataSet 来连接。关键点在于将多个 SteamNode chain 成一个 JobVertex的过程，这部分源码比较绕，有兴趣的同学可以结合源码单步调试分析。下一章将会介绍 JobGraph 提交到 JobManager 后是如何转换成分布式化的 ExecutionGraph 的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：计算资源</title>
      <link href="/2018/05/15/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/"/>
      <url>/2018/05/15/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<p>本文所讨论的计算资源是指用来执行 Task 的资源，是一个逻辑概念。本文会介绍 Flink 计算资源相关的一些核心概念，如：Slot、SlotSharingGroup、CoLocationGroup、Chain等。并会着重讨论 Flink 如何对计算资源进行管理和隔离，如何将计算资源利用率最大化等等。理解 Flink 中的计算资源对于理解 Job 如何在集群中运行的有很大的帮助，也有利于我们更透彻地理解 Flink 原理，更快速地定位问题。</p><h2 id="Operator-Chains"><a href="#Operator-Chains" class="headerlink" title="Operator Chains"></a>Operator Chains</h2><p>为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化/反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</p><p>我们仍以经典的 WordCount 为例，下面这幅图，展示了Source并行度为1，FlatMap、KeyAggregation、Sink并行度均为2，最终以5个并行的线程来执行的优化过程。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB18Gv5JFXXXXcDXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB18Gv5JFXXXXcDXXXXXXXXXXXX" alt="img"></a></p><p>上图中将KeyAggregation和Sink两个operator进行了合并，因为这两个合并后并不会改变整体的拓扑结构。但是，并不是任意两个 operator 就能 chain 一起的。其条件还是很苛刻的：</p><ol><li>上下游的并行度一致</li><li>下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</li><li>上下游节点都在同一个 slot group 中（下面会解释 slot group）</li><li>下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</li><li>上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</li><li>两个节点间数据分区方式是 forward</li><li>用户没有禁用 chain</li></ol><p>Operator chain的行为可以通过编程API中进行指定。可以通过在DataStream的operator后面（如<code>someStream.map(..)</code>)调用<code>startNewChain()</code>来指示从该operator开始一个新的chain（与前面截断，不会被chain到前面）。或者调用<code>disableChaining()</code>来指示该operator不参与chaining（不会与前后的operator chain一起）。在底层，这两个方法都是通过调整operator的 chain 策略（HEAD、NEVER）来实现的。另外，也可以通过调用<code>StreamExecutionEnvironment.disableOperatorChaining()</code>来全局禁用chaining。</p><h3 id="原理与实现"><a href="#原理与实现" class="headerlink" title="原理与实现"></a>原理与实现</h3><p>那么 Flink 是如何将多个 operators chain在一起的呢？chain在一起的operators是如何作为一个整体被执行的呢？它们之间的数据流又是如何避免了序列化/反序列化以及网络传输的呢？下图展示了operators chain的内部实现：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1cFbJJFXXXXaIXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1cFbJJFXXXXaIXVXXXXXXXXXX" alt="img"></a></p><p>如上图所示，Flink内部是通过<code>OperatorChain</code>这个类来将多个operator链在一起形成一个新的operator。<code>OperatorChain</code>形成的框框就像一个黑盒，Flink 无需知道黑盒中有多少个ChainOperator、数据在chain内部是怎么流动的，只需要将input数据交给 HeadOperator 就可以了，这就使得<code>OperatorChain</code>在行为上与普通的operator无差别，上面的OperaotrChain就可以看做是一个入度为1，出度为2的operator。所以在实现中，对外可见的只有HeadOperator，以及与外部连通的实线输出，这些输出对应了JobGraph中的JobEdge，在底层通过<code>RecordWriterOutput</code>来实现。另外，框中的虚线是operator chain内部的数据流，这个流内的数据不会经过序列化/反序列化、网络传输，而是直接将消息对象传递给下游的 ChainOperator 处理，这是性能提升的关键点，在底层是通过 <code>ChainingOutput</code> 实现的，源码如下方所示，</p><p><em>注：HeadOperator和ChainOperator并不是具体的数据结构，前者指代chain中的第一个operator，后者指代chain中其余的operator，它们实际上都是StreamOperator。</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ChainingOutput</span>&lt;T&gt; <span class="title">implements</span> <span class="title">Output</span>&lt;StreamRecord&lt;T&gt;&gt; &#123;</span></span><br><span class="line">  <span class="comment">// 注册的下游operator</span></span><br><span class="line">  <span class="keyword">protected</span> final OneInputStreamOperator&lt;T, ?&gt; <span class="keyword">operator</span>;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ChainingOutput</span><span class="params">(OneInputStreamOperator&lt;T, ?&gt; <span class="keyword">operator</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.<span class="keyword">operator</span> = <span class="keyword">operator</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  @Override</span><br><span class="line">  <span class="comment">// 发送消息方法的实现，直接将消息对象传递给operator处理，不经过序列化/反序列化、网络传输</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">collect</span><span class="params">(StreamRecord&lt;T&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">operator</span>.setKeyContextElement1(record);</span><br><span class="line">      <span class="comment">// 下游operator直接处理消息对象</span></span><br><span class="line">      <span class="keyword">operator</span>.processElement(record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> ExceptionInChainedOperatorException(e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Task-Slot"><a href="#Task-Slot" class="headerlink" title="Task Slot"></a>Task Slot</h2><p>在架构概览一文中我们介绍了 TaskManager 是一个 JVM 进程，并会以独立的线程来执行一个task或多个subtask。为了控制一个 TaskManager 能接受多少个 task，Flink 提出了 <em>Task Slot</em> 的概念。</p><p>Flink 中的计算资源通过 <em>Task Slot</em> 来定义。每个 task slot 代表了 TaskManager 的一个固定大小的资源子集。例如，一个拥有3个slot的 TaskManager，会将其管理的内存平均分成三份分给各个 slot。将资源 slot 化意味着来自不同job的task不会为了内存而竞争，而是每个task都拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的内存。</p><p>通过调整 task slot 的数量，用户可以定义task之间是如何相互隔离的。每个 TaskManager 有一个slot，也就意味着每个task运行在独立的 JVM 中。每个 TaskManager 有多个slot的话，也就是说多个task运行在同一个JVM中。而在同一个JVM进程中的task，可以共享TCP连接（基于多路复用）和心跳消息，可以减少数据的网络传输。也能共享一些数据结构，一定程度上减少了每个task的消耗。</p><p>每一个 TaskManager 会拥有一个或多个的 task slot，每个 slot 都能跑由多个连续 task 组成的一个 pipeline，比如 MapFunction 的第n个并行实例和 ReduceFunction 的第n个并行实例可以组成一个 pipeline。</p><p>如上文所述的 WordCount 例子，5个Task可能会在TaskManager的slots中如下图分布，2个TaskManager，每个有3个slot：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1Q4zUJFXXXXXnXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1Q4zUJFXXXXXnXVXXXXXXXXXX" alt="img"></a></p><h2 id="SlotSharingGroup-与-CoLocationGroup"><a href="#SlotSharingGroup-与-CoLocationGroup" class="headerlink" title="SlotSharingGroup 与 CoLocationGroup"></a>SlotSharingGroup 与 CoLocationGroup</h2><p>默认情况下，Flink 允许subtasks共享slot，条件是它们都来自同一个Job的不同task的subtask。结果可能一个slot持有该job的整个pipeline。允许slot共享有以下两点好处：</p><ol><li>Flink 集群所需的task slots数与job中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个task了。</li><li>更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将基线的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks。</li></ol><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1VTj4JFXXXXX8XFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1VTj4JFXXXXX8XFXXXXXXXXXX" alt="img"></a></p><p>我们将 WordCount 的并行度从之前的2个增加到6个（Source并行度仍为1），并开启slot共享（所有operator都在default共享组），将得到如上图所示的slot分布图。首先，我们不用去计算这个job会其多少个task，总之该任务最终会占用6个slots（最高并行度为6）。其次，我们可以看到密集型操作 keyAggregation/sink 被平均地分配到各个 TaskManager。</p><p><code>SlotSharingGroup</code>是Flink中用来实现slot共享的类，它尽可能地让subtasks共享一个slot。相应的，还有一个 <code>CoLocationGroup</code> 类用来强制将 subtasks 放到同一个 slot 中。<code>CoLocationGroup</code>主要用于<a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/index.html#iterations" target="_blank" rel="noopener">迭代流</a>中，用来保证迭代头与迭代尾的第i个subtask能被调度到同一个TaskManager上。这里我们不会详细讨论<code>CoLocationGroup</code>的实现细节。</p><p>怎么判断operator属于哪个 slot 共享组呢？默认情况下，所有的operator都属于默认的共享组<code>default</code>，也就是说默认情况下所有的operator都是可以共享一个slot的。而当所有input operators具有相同的slot共享组时，该operator会继承这个共享组。最后，为了防止不合理的共享，用户也能通过API来强制指定operator的共享组，比如：<code>someStream.filter(...).slotSharingGroup(&quot;group1&quot;);</code>就强制指定了filter的slot共享组为<code>group1</code>。</p><h3 id="原理与实现-1"><a href="#原理与实现-1" class="headerlink" title="原理与实现"></a>原理与实现</h3><p>那么多个tasks（或者说operators）是如何共享slot的呢？</p><p>我们先来看一下用来定义计算资源的slot的类图：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1NxjUJFXXXXXoaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1NxjUJFXXXXXoaXXXXXXXXXXX" alt="img"></a></p><p>抽象类<code>Slot</code>定义了该槽位属于哪个TaskManager（<code>instance</code>）的第几个槽位（<code>slotNumber</code>），属于哪个Job（<code>jobID</code>）等信息。最简单的情况下，一个slot只持有一个task，也就是<code>SimpleSlot</code>的实现。复杂点的情况，一个slot能共享给多个task使用，也就是<code>SharedSlot</code>的实现。SharedSlot能包含其他的SharedSlot，也能包含SimpleSlot。所以一个SharedSlot能定义出一棵slots树。</p><p>接下来我们来看看 Flink 为subtask分配slot的过程。关于Flink调度，有两个非常重要的原则我们必须知道：（1）同一个operator的各个subtask是不能呆在同一个SharedSlot中的，例如<code>FlatMap[1]</code>和<code>FlatMap[2]</code>是不能在同一个SharedSlot中的。（2）Flink是按照拓扑顺序从Source一个个调度到Sink的。例如WordCount（Source并行度为1，其他并行度为2），那么调度的顺序依次是：<code>Source</code> -&gt; <code>FlatMap[1]</code> -&gt; <code>FlatMap[2]</code> -&gt; <code>KeyAgg-&gt;Sink[1]</code> -&gt; <code>KeyAgg-&gt;Sink[2]</code>。假设现在有2个TaskManager，每个只有1个slot（为简化问题），那么分配slot的过程如图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1TM_3JFXXXXb8XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1TM_3JFXXXXb8XVXXXXXXXXXX" alt="img"></a></p><p><em>注：图中 SharedSlot 与 SimpleSlot 后带的括号中的数字代表槽位号（slotNumber）</em></p><ol><li>为<code>Source</code>分配slot。首先，我们从TaskManager1中分配出一个SharedSlot。并从SharedSlot中为<code>Source</code>分配出一个SimpleSlot。如上图中的①和②。</li><li>为<code>FlatMap[1]</code>分配slot。目前已经有一个SharedSlot，则从该SharedSlot中分配出一个SimpleSlot用来部署<code>FlatMap[1]</code>。如上图中的③。</li><li>为<code>FlatMap[2]</code>分配slot。由于TaskManager1的SharedSlot中已经有同operator的<code>FlatMap[1]</code>了，我们只能分配到其他SharedSlot中去。从TaskManager2中分配出一个SharedSlot，并从该SharedSlot中为<code>FlatMap[2]</code>分配出一个SimpleSlot。如上图的④和⑤。</li><li>为<code>Key-&gt;Sink[1]</code>分配slot。目前两个SharedSlot都符合条件，从TaskManager1的SharedSlot中分配出一个SimpleSlot用来部署<code>Key-&gt;Sink[1]</code>。如上图中的⑥。</li><li>为<code>Key-&gt;Sink[2]</code>分配slot。TaskManager1的SharedSlot中已经有同operator的<code>Key-&gt;Sink[1]</code>了，则只能选择另一个SharedSlot中分配出一个SimpleSlot用来部署<code>Key-&gt;Sink[2]</code>。如上图中的⑦。</li></ol><p>最后<code>Source</code>、<code>FlatMap[1]</code>、<code>Key-&gt;Sink[1]</code>这些subtask都会部署到TaskManager1的唯一一个slot中，并启动对应的线程。<code>FlatMap[2]</code>、<code>Key-&gt;Sink[2]</code>这些subtask都会被部署到TaskManager2的唯一一个slot中，并启动对应的线程。从而实现了slot共享。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了Flink中计算资源的相关概念以及原理实现。最核心的是 Task Slot，每个slot能运行一个或多个task。为了拓扑更高效地运行，Flink提出了Chaining，尽可能地将operators chain在一起作为一个task来处理。为了资源更充分的利用，Flink又提出了SlotSharingGroup，尽可能地让多个task共享一个slot。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：生成 StreamGraph</title>
      <link href="/2018/05/12/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20StreamGraph/"/>
      <url>/2018/05/12/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E7%94%9F%E6%88%90%20StreamGraph/</url>
      
        <content type="html"><![CDATA[<p>继上文Flink 底层原理：架构和拓扑中介绍了Flink的四层执行图模型，本文将主要介绍 Flink 是如何根据用户用Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。</p><p><em>注：本文比较偏源码分析，所有代码都是基于 flink-1.0.x 版本，建议在阅读本文前先对Stream API有个了解，详见官方文档。</em></p><p>StreamGraph 相关的代码主要在 <code>org.apache.flink.streaming.api.graph</code> 包中。构造StreamGraph的入口函数是 <code>StreamGraphGenerator.generate(env, transformations)</code>。该函数会由触发程序执行的方法<code>StreamExecutionEnvironment.execute()</code>调用到。也就是说 StreamGraph 是在 Client 端构造的，这也意味着我们可以在本地通过调试观察 StreamGraph 的构造过程。</p><h2 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h2><p><code>StreamGraphGenerator.generate</code> 的一个关键的参数是 <code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>。<code>StreamTransformation</code>代表了从一个或多个<code>DataStream</code>生成新<code>DataStream</code>的操作。<code>DataStream</code>的底层其实就是一个 <code>StreamTransformation</code>，描述了这个<code>DataStream</code>是怎么来的。</p><p>StreamTransformation的类图如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1yQmNJFXXXXXnXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1yQmNJFXXXXXnXpXXXXXXXXXX" alt="img"></a></p><p>DataStream 上常见的 transformation 有 map、flatmap、filter等（见<a href="https://ci.apache.org/projects/flink/flink-docs-master/apis/streaming/index.html#datastream-transformations" target="_blank" rel="noopener">DataStream Transformation</a>了解更多）。这些transformation会构造出一棵 StreamTransformation 树，通过这棵树转换成 StreamGraph。比如 <code>DataStream.map</code>源码如下，其中<code>SingleOutputStreamOperator</code>为DataStream的子类：</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">map</span>(<span class="params">MapFunction&lt;T, R&gt; mapper</span>)</span> &#123;</span><br><span class="line">  <span class="comment">// 通过java reflection抽出mapper的返回值类型</span></span><br><span class="line">  TypeInformation&lt;R&gt; outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),</span><br><span class="line">      Utils.getCallLocationName(), <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回一个新的DataStream，SteramMap 为 StreamOperator 的实现类</span></span><br><span class="line">  <span class="keyword">return</span> transform(<span class="string">"Map"</span>, outType, <span class="keyword">new</span> StreamMap&lt;&gt;(clean(mapper)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;R&gt; <span class="function">SingleOutputStreamOperator&lt;R&gt; <span class="title">transform</span>(<span class="params">String operatorName, TypeInformation&lt;R&gt; outTypeInfo, OneInputStreamOperator&lt;T, R&gt; <span class="keyword">operator</span></span>)</span> &#123;</span><br><span class="line">  <span class="comment">// read the output type of the input Transform to coax out errors about MissingTypeInfo</span></span><br><span class="line">  transformation.getOutputType();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 新的transformation会连接上当前DataStream中的transformation，从而构建成一棵树</span></span><br><span class="line">  OneInputTransformation&lt;T, R&gt; resultTransform = <span class="keyword">new</span> OneInputTransformation&lt;&gt;(</span><br><span class="line">      <span class="keyword">this</span>.transformation,</span><br><span class="line">      operatorName,</span><br><span class="line">      <span class="keyword">operator</span>,</span><br><span class="line">      outTypeInfo,</span><br><span class="line">      environment.getParallelism());</span><br><span class="line"></span><br><span class="line">  @SuppressWarnings(&#123; <span class="string">"unchecked"</span>, <span class="string">"rawtypes"</span> &#125;)</span><br><span class="line">  SingleOutputStreamOperator&lt;R&gt; returnStream = <span class="keyword">new</span> SingleOutputStreamOperator(environment, resultTransform);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 所有的transformation都会存到 env 中，调用execute时遍历该list生成StreamGraph</span></span><br><span class="line">  getExecutionEnvironment().addOperator(resultTransform);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> returnStream;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上方代码可以了解到，map转换将用户自定义的函数<code>MapFunction</code>包装到<code>StreamMap</code>这个Operator中，再将<code>StreamMap</code>包装到<code>OneInputTransformation</code>，最后该transformation存到env中，当调用<code>env.execute</code>时，遍历其中的transformation集合构造出StreamGraph。其分层实现如下图所示：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB12u5yJFXXXXXhaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB12u5yJFXXXXXhaXXXXXXXXXXX" alt="img"></a></p><p>另外，并不是每一个 StreamTransformation 都会转换成 runtime 层中物理操作。有一些只是逻辑概念，比如 union、split/select、partition等。如下图所示的转换树，在运行时会优化成下方的操作图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1XgmOJFXXXXaYXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1XgmOJFXXXXaYXpXXXXXXXXXX" alt="img"></a></p><p>union、split/select、partition中的信息会被写入到 Source –&gt; Map 的边中。通过源码也可以发现，<code>UnionTransformation</code>,<code>SplitTransformation</code>,<code>SelectTransformation</code>,<code>PartitionTransformation</code>由于不包含具体的操作所以都没有StreamOperator成员变量，而其他StreamTransformation的子类基本上都有。</p><h2 id="StreamOperator"><a href="#StreamOperator" class="headerlink" title="StreamOperator"></a>StreamOperator</h2><p>DataStream 上的每一个 Transformation 都对应了一个 StreamOperator，StreamOperator是运行时的具体实现，会决定UDF(User-Defined Funtion)的调用方式。下图所示为 StreamOperator 的类图（点击查看大图）：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1l9aYJFXXXXbAXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1l9aYJFXXXXbAXXXXXXXXXXXX" alt="img"></a></p><p>可以发现，所有实现类都继承了<code>AbstractStreamOperator</code>。另外除了 project 操作，其他所有可以执行UDF代码的实现类都继承自<code>AbstractUdfStreamOperator</code>，该类是封装了UDF的StreamOperator。UDF就是实现了<code>Function</code>接口的类，如<code>MapFunction</code>,<code>FilterFunction</code>。</p><h2 id="生成-StreamGraph-的源码分析"><a href="#生成-StreamGraph-的源码分析" class="headerlink" title="生成 StreamGraph 的源码分析"></a>生成 StreamGraph 的源码分析</h2><p>我们通过在DataStream上做了一系列的转换（map、filter等）得到了StreamTransformation集合，然后通过<code>StreamGraphGenerator.generate</code>获得StreamGraph，该方法的源码如下：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构造 StreamGraph 入口函数</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> StreamGraph generate(StreamExecutionEnvironment env, <span class="keyword">List</span>&lt;StreamTransformation<span class="meta">&lt;?</span>&gt;&gt; transformations) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> StreamGraphGenerator(env).generateInternal(transformations);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自底向上（sink-&gt;source）对转换树的每个transformation进行转换。</span></span><br><span class="line"><span class="keyword">private</span> StreamGraph generateInternal(<span class="keyword">List</span>&lt;StreamTransformation<span class="meta">&lt;?</span>&gt;&gt; transformations) &#123;</span><br><span class="line">  <span class="keyword">for</span> (StreamTransformation<span class="meta">&lt;?</span>&gt; transformation: transformations) &#123;</span><br><span class="line">    transform(transformation);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> streamGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对具体的一个transformation进行转换，转换成 StreamGraph 中的 StreamNode 和 StreamEdge</span></span><br><span class="line"><span class="comment">// 返回值为该transform的id集合，通常大小为1个（除FeedbackTransformation）</span></span><br><span class="line"><span class="keyword">private</span> Collection&lt;Integer&gt; transform(StreamTransformation<span class="meta">&lt;?</span>&gt; transform) &#123;  </span><br><span class="line">  <span class="comment">// 跳过已经转换过的transformation</span></span><br><span class="line">  <span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">    <span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为了触发 MissingTypeInfo 的异常</span></span><br><span class="line">  transform.getOutputType();</span><br><span class="line"></span><br><span class="line">  Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">  <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation<span class="meta">&lt;?</span>, <span class="meta">?&gt;</span>) &#123;</span><br><span class="line">    transformedIds = transformOnInputTransform((OneInputTransformation<span class="meta">&lt;?</span>, <span class="meta">?&gt;</span>) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation<span class="meta">&lt;?</span>, ?, <span class="meta">?&gt;</span>) &#123;</span><br><span class="line">    transformedIds = transformTwoInputTransform((TwoInputTransformation<span class="meta">&lt;?</span>, ?, <span class="meta">?&gt;</span>) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSource((SourceTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSink((SinkTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformUnion((UnionTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSplit((SplitTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformSelect((SelectTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformFeedback((FeedbackTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformCoFeedback((CoFeedbackTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation<span class="meta">&lt;?</span>&gt;) &#123;</span><br><span class="line">    transformedIds = transformPartition((PartitionTransformation<span class="meta">&lt;?</span>&gt;) transform);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">  <span class="comment">// transforming the feedback edges</span></span><br><span class="line">  <span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">    alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    streamGraph.setTransformationId(transform.getId(), transform.getUid());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> transformedIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最终都会调用 <code>transformXXX</code> 来对具体的StreamTransformation进行转换。我们可以看下<code>transformOnInputTransform(transform)</code>的实现：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">private &lt;IN, OUT&gt; Collection&lt;Integer&gt; transformOnInputTransform(OneInputTransformation&lt;IN, OUT&gt; <span class="built_in">transform</span>) &#123;</span><br><span class="line">  // 递归对该<span class="built_in">transform</span>的直接上游<span class="built_in">transform</span>进行转换，获得直接上游id集合</span><br><span class="line">  Collection&lt;Integer&gt; inputIds = <span class="built_in">transform</span>(<span class="built_in">transform</span>.getInput());</span><br><span class="line"></span><br><span class="line">  // 递归调用可能已经处理过该<span class="built_in">transform</span>了</span><br><span class="line">  <span class="keyword">if</span> (alreadyTransformed.containsKey(<span class="built_in">transform</span>)) &#123;</span><br><span class="line">    <span class="built_in">return</span> alreadyTransformed.<span class="built_in">get</span>(<span class="built_in">transform</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  String slotSharingGroup = determineSlotSharingGroup(<span class="built_in">transform</span>.getSlotSharingGroup(), inputIds);</span><br><span class="line"></span><br><span class="line">  // 添加 StreamNode</span><br><span class="line">  streamGraph.addOperator(<span class="built_in">transform</span>.getId(),</span><br><span class="line">      slotSharingGroup,</span><br><span class="line">      <span class="built_in">transform</span>.getOperator(),</span><br><span class="line">      <span class="built_in">transform</span>.getInputType(),</span><br><span class="line">      <span class="built_in">transform</span>.getOutputType(),</span><br><span class="line">      <span class="built_in">transform</span>.getName());</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">transform</span>.getStateKeySelector() != null) &#123;</span><br><span class="line">    TypeSerializer&lt;?&gt; keySerializer = <span class="built_in">transform</span>.getStateKeyType().createSerializer(env.getConfig());</span><br><span class="line">    streamGraph.setOneInputStateKey(<span class="built_in">transform</span>.getId(), <span class="built_in">transform</span>.getStateKeySelector(), keySerializer);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  streamGraph.setParallelism(<span class="built_in">transform</span>.getId(), <span class="built_in">transform</span>.getParallelism());</span><br><span class="line"></span><br><span class="line">  // 添加 StreamEdge</span><br><span class="line">  <span class="keyword">for</span> (Integer inputId: inputIds) &#123;</span><br><span class="line">    streamGraph.addEdge(inputId, <span class="built_in">transform</span>.getId(), <span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> Collections.singleton(<span class="built_in">transform</span>.getId());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数首先会对该transform的上游transform进行递归转换，确保上游的都已经完成了转化。然后通过transform构造出StreamNode，最后与上游的transform进行连接，构造出StreamNode。</p><p>最后再来看下对逻辑转换（partition、union等）的处理，如下是<code>transformPartition</code>函数的源码：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> &lt;T&gt; Collection&lt;<span class="built_in">Integer</span>&gt; transformPartition(PartitionTransformation&lt;T&gt; partition) &#123;</span><br><span class="line">  StreamTransformation&lt;T&gt; input = partition.getInput();</span><br><span class="line">  <span class="built_in">List</span>&lt;<span class="built_in">Integer</span>&gt; resultIds = <span class="literal">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 直接上游的id</span></span><br><span class="line">  Collection&lt;<span class="built_in">Integer</span>&gt; transformedIds = transform(input);</span><br><span class="line">  for (<span class="built_in">Integer</span> transformedId: transformedIds) &#123;</span><br><span class="line">    <span class="comment">// 生成一个新的虚拟id</span></span><br><span class="line">    int virtualId = StreamTransformation.getNewNodeId();</span><br><span class="line">    <span class="comment">// 添加一个虚拟分区节点，不会生成 StreamNode</span></span><br><span class="line">    streamGraph.addVirtualPartitionNode(transformedId, virtualId, partition.getPartitioner());</span><br><span class="line">    resultIds.add(virtualId);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> resultIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对partition的转换没有生成具体的StreamNode和StreamEdge，而是添加一个虚节点。当partition的下游transform（如map）添加edge时（调用<code>StreamGraph.addEdge</code>），会把partition信息写入到edge中。如<code>StreamGraph.addEdgeInternal</code>所示：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> addEdge(Integer upStreamVertexID, Integer downStreamVertexID, <span class="built_in">int</span> typeNumber) &#123;</span><br><span class="line">  addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, <span class="keyword">null</span>, <span class="keyword">new</span> ArrayList&lt;<span class="keyword">String</span>&gt;());</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> addEdgeInternal(Integer upStreamVertexID,</span><br><span class="line">    Integer downStreamVertexID,</span><br><span class="line">    <span class="built_in">int</span> typeNumber,</span><br><span class="line">    StreamPartitioner&lt;?&gt; partitioner,</span><br><span class="line">    List&lt;<span class="keyword">String</span>&gt; outputNames) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 当上游是select时，递归调用，并传入select信息</span></span><br><span class="line">  <span class="keyword">if</span> (virtualSelectNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">    <span class="built_in">int</span> virtualId = upStreamVertexID;</span><br><span class="line">    <span class="comment">// select上游的节点id</span></span><br><span class="line">    upStreamVertexID = virtualSelectNodes.<span class="built_in">get</span>(virtualId).f0;</span><br><span class="line">    <span class="keyword">if</span> (outputNames.isEmpty()) &#123;</span><br><span class="line">      <span class="comment">// selections that happen downstream override earlier selections</span></span><br><span class="line">      outputNames = virtualSelectNodes.<span class="built_in">get</span>(virtualId).f1;</span><br><span class="line">    &#125;</span><br><span class="line">    addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames);</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="comment">// 当上游是partition时，递归调用，并传入partitioner信息</span></span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (virtuaPartitionNodes.containsKey(upStreamVertexID)) &#123;</span><br><span class="line">    <span class="built_in">int</span> virtualId = upStreamVertexID;</span><br><span class="line">    <span class="comment">// partition上游的节点id</span></span><br><span class="line">    upStreamVertexID = virtuaPartitionNodes.<span class="built_in">get</span>(virtualId).f0;</span><br><span class="line">    <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">      partitioner = virtuaPartitionNodes.<span class="built_in">get</span>(virtualId).f1;</span><br><span class="line">    &#125;</span><br><span class="line">    addEdgeInternal(upStreamVertexID, downStreamVertexID, typeNumber, partitioner, outputNames);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 真正构建StreamEdge</span></span><br><span class="line">    StreamNode upstreamNode = getStreamNode(upStreamVertexID);</span><br><span class="line">    StreamNode downstreamNode = getStreamNode(downStreamVertexID);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 未指定partitioner的话，会为其选择 forward 或 rebalance 分区。</span></span><br><span class="line">    <span class="keyword">if</span> (partitioner == <span class="keyword">null</span> &amp;&amp; upstreamNode.getParallelism() == downstreamNode.getParallelism()) &#123;</span><br><span class="line">      partitioner = <span class="keyword">new</span> ForwardPartitioner&lt;<span class="keyword">Object</span>&gt;();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (partitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">      partitioner = <span class="keyword">new</span> RebalancePartitioner&lt;<span class="keyword">Object</span>&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 健康检查， forward 分区必须要上下游的并发度一致</span></span><br><span class="line">    <span class="keyword">if</span> (partitioner <span class="keyword">instanceof</span> ForwardPartitioner) &#123;</span><br><span class="line">      <span class="keyword">if</span> (upstreamNode.getParallelism() != downstreamNode.getParallelism()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Forward partitioning does not allow "</span> +</span><br><span class="line">            <span class="string">"change of parallelism. Upstream operation: "</span> + upstreamNode + <span class="string">" parallelism: "</span> + upstreamNode.getParallelism() +</span><br><span class="line">            <span class="string">", downstream operation: "</span> + downstreamNode + <span class="string">" parallelism: "</span> + downstreamNode.getParallelism() +</span><br><span class="line">            <span class="string">" You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global."</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建 StreamEdge</span></span><br><span class="line">    StreamEdge edge = <span class="keyword">new</span> StreamEdge(upstreamNode, downstreamNode, typeNumber, outputNames, partitioner);</span><br><span class="line">    <span class="comment">// 将该 StreamEdge 添加到上游的输出，下游的输入</span></span><br><span class="line">    getStreamNode(edge.getSourceId()).addOutEdge(edge);</span><br><span class="line">    getStreamNode(edge.getTargetId()).addInEdge(edge);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="实例讲解"><a href="#实例讲解" class="headerlink" title="实例讲解"></a>实例讲解</h2><p>如下程序，是一个从 Source 中按行切分成单词并过滤输出的简单流程序，其中包含了逻辑转换：随机分区shuffle。我们会分析该程序是如何生成StreamGraph的。</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;<span class="keyword">String</span>&gt; <span class="built_in">text</span> = env.socketTextStream(hostName, port);</span><br><span class="line"><span class="built_in">text</span>.flatMap(<span class="keyword">new</span> LineSplitter()).shuffle().<span class="built_in">filter</span>(<span class="keyword">new</span> HelloFilter()).<span class="built_in">print</span>();</span><br></pre></td></tr></table></figure><p>首先会在env中生成一棵transformation树，用<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>保存。其结构图如下：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1w3SQJFXXXXalXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1w3SQJFXXXXalXVXXXXXXXXXX" alt="img"></a></p><p>其中符号<code>*</code>为input指针，指向上游的transformation，从而形成了一棵transformation树。然后，通过调用<code>StreamGraphGenerator.generate(env, transformations)</code>来生成StreamGraph。自底向上递归调用每一个transformation，也就是说处理顺序是Source-&gt;FlatMap-&gt;Shuffle-&gt;Filter-&gt;Sink。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1s7SpJFXXXXXjaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1s7SpJFXXXXXjaXXXXXXXXXXX" alt="img"></a></p><p>如上图所示：</p><ol><li>首先处理的Source，生成了Source的StreamNode。</li><li>然后处理的FlatMap，生成了FlatMap的StreamNode，并生成StreamEdge连接上游Source和FlatMap。由于上下游的并发度不一样（1:4），所以此处是Rebalance分区。</li><li>然后处理的Shuffle，由于是逻辑转换，并不会生成实际的节点。将partitioner信息暂存在<code>virtuaPartitionNodes</code>中。</li><li>在处理Filter时，生成了Filter的StreamNode。发现上游是shuffle，找到shuffle的上游FlatMap，创建StreamEdge与Filter相连。并把ShufflePartitioner的信息写到StreamEdge中。</li><li>最后处理Sink，创建Sink的StreamNode，并生成StreamEdge与上游Filter相连。由于上下游并发度一样（4:4），所以此处选择 Forward 分区。</li></ol><p>最后可以通过 <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">UI可视化</a> 来观察得到的 StreamGraph。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1y_1FJFXXXXapaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1y_1FJFXXXXapaXXXXXXXXXXX" alt="img"></a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要介绍了 Stream API 中 Transformation 和 Operator 的概念，以及如何根据Stream API编写的程序，构造出一个代表拓扑结构的StreamGraph的。本文的源码分析涉及到较多代码，如果有兴趣建议结合完整源码进行学习。下一篇文章将介绍 StreamGraph 如何转换成 JobGraph 的，其中设计到了图优化的技巧。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：架构和拓扑</title>
      <link href="/2018/05/08/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E6%8B%93%E6%89%91/"/>
      <url>/2018/05/08/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%9E%B6%E6%9E%84%E5%92%8C%E6%8B%93%E6%89%91/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>要了解一个系统，一般都是从架构开始。我们关心的问题是：系统部署成功后各个节点都启动了哪些服务，各个服务之间又是怎么交互和协调的。下方是 Flink 集群启动后架构图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1ObBnJFXXXXXtXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1ObBnJFXXXXXtXVXXXXXXXXXX" alt="img"></a></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><ul><li><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li><li><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</li><li><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</li></ul><p>可以看到 Flink 的任务调度是多线程模型，并且不同Job/Task混合在一个 TaskManager 进程中。虽然这种方式可以有效提高 CPU 利用率，但是个人不太喜欢这种设计，因为不仅缺乏资源隔离机制，同时也不方便调试。类似 Storm 的进程模型，一个JVM 中只跑该 Job 的 Tasks 实际应用中更为合理。</p><h2 id="Job-例子"><a href="#Job-例子" class="headerlink" title="Job 例子"></a>Job 例子</h2><blockquote><p>本文所示例子为 flink-1.0.x 版本</p></blockquote><p>我们使用 Flink 自带的 examples 包中的 <code>SocketTextStreamWordCount</code>，这是一个从 socket 流中统计单词出现次数的例子。</p><ul><li><p>首先，使用 <strong>netcat</strong> 启动本地服务器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nc -l 9000</span></span><br></pre></td></tr></table></figure></li><li><p>然后提交 Flink 程序</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/flink run examples/streaming/SocketTextStreamWordCount.jar \</span><br><span class="line">  --hostname <span class="number">10.218</span><span class="number">.130</span><span class="number">.9</span> \</span><br><span class="line">  --port <span class="number">9000</span></span><br></pre></td></tr></table></figure></li></ul><p>在netcat端输入单词并监控 taskmanager 的输出可以看到单词统计的结果。</p><p><code>SocketTextStreamWordCount</code> 的具体代码如下：</p><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(<span class="keyword">String</span>[] args) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">  <span class="comment">// 检查输入</span></span><br><span class="line">  <span class="keyword">final</span> ParameterTool params = ParameterTool.fromArgs(args);</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set up the execution environment</span></span><br><span class="line">  <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// get input data</span></span><br><span class="line">  DataStream&lt;<span class="keyword">String</span>&gt; <span class="built_in">text</span> =</span><br><span class="line">      env.socketTextStream(params.<span class="built_in">get</span>(<span class="string">"hostname"</span>), params.getInt(<span class="string">"port"</span>), <span class="string">'\n'</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  DataStream&lt;Tuple2&lt;<span class="keyword">String</span>, Integer&gt;&gt; counts =</span><br><span class="line">      <span class="comment">// split up the lines in pairs (2-tuples) containing: (word,1)</span></span><br><span class="line">      <span class="built_in">text</span>.flatMap(<span class="keyword">new</span> Tokenizer())</span><br><span class="line">          <span class="comment">// group by the tuple field "0" and sum up tuple field "1"</span></span><br><span class="line">          .keyBy(<span class="number">0</span>)</span><br><span class="line">          .sum(<span class="number">1</span>);</span><br><span class="line">  counts.<span class="built_in">print</span>();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// execute program</span></span><br><span class="line">  env.execute(<span class="string">"WordCount from SocketTextStream Example"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们将最后一行代码 <code>env.execute</code> 替换成 <code>System.out.println(env.getExecutionPlan());</code> 并在本地运行该代码（并发度设为2），可以得到该拓扑的逻辑执行计划图的 JSON 串，将该 JSON 串粘贴到 <a href="http://flink.apache.org/visualizer/" target="_blank" rel="noopener">http://flink.apache.org/visualizer/</a> 中，能可视化该执行图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1vB1uJFXXXXbaXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1vB1uJFXXXXbaXpXXXXXXXXXX" alt="img"></a></p><p>但这并不是最终在 Flink 中运行的执行图，只是一个表示拓扑节点关系的计划图，在 Flink 中对应了 SteramGraph。另外，提交拓扑后（并发度设为2）还能在 UI 中看到另一张执行计划图，如下所示，该图对应了 Flink 中的 JobGraph。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1QKR2JFXXXXbyaXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1QKR2JFXXXXbyaXXXXXXXXXXX" alt="img"></a></p><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p>看起来有点乱，怎么有这么多不一样的图。实际上，还有更多的图。Flink 中的执行图可以分成四层：StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li><strong>StreamGraph：</strong>是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li><strong>JobGraph：</strong>StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li><strong>ExecutionGraph：</strong>JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li><strong>物理执行图：</strong>JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>例如上文中的2个并发度（Source为1个并发度）的 <code>SocketTextStreamWordCount</code> 四层执行图的演变过程如下图所示（点击查看大图）：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1tA_GJFXXXXapXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1tA_GJFXXXXapXFXXXXXXXXXX" alt="img"></a></p><p>这里对一些名词进行简单的解释。</p><ul><li><p>StreamGraph：</p><p>根据用户通过 Stream API 编写的代码生成的最初的图。</p><ul><li>StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</li><li>StreamEdge：表示连接两个StreamNode的边。</li></ul></li><li><p>JobGraph：</p><p>StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。</p><ul><li>JobVertex：经过优化后符合条件的多个StreamNode可能会chain在一起生成一个JobVertex，即一个JobVertex包含一个或多个operator，JobVertex的输入是JobEdge，输出是IntermediateDataSet。</li><li>IntermediateDataSet：表示JobVertex的输出，即经过operator处理产生的数据集。producer是JobVertex，consumer是JobEdge。</li><li>JobEdge：代表了job graph中的一条数据传输通道。source 是 IntermediateDataSet，target 是 JobVertex。即数据通过JobEdge由IntermediateDataSet传递给目标JobVertex。</li></ul></li><li><p>ExecutionGraph：</p><p>JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><ul><li>ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的 ExecutionVertex。</li><li>ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition。</li><li>IntermediateResult：和JobGraph中的IntermediateDataSet一一对应。一个IntermediateResult包含多个IntermediateResultPartition，其个数等于该operator的并发度。</li><li>IntermediateResultPartition：表示ExecutionVertex的一个输出分区，producer是ExecutionVertex，consumer是若干个ExecutionEdge。</li><li>ExecutionEdge：表示ExecutionVertex的输入，source是IntermediateResultPartition，target是ExecutionVertex。source和target都只能是一个。</li><li>Execution：是执行一个 ExecutionVertex 的一次尝试。当发生故障或者数据需要重算的情况下 ExecutionVertex 可能会有多个 ExecutionAttemptID。一个 Execution 通过 ExecutionAttemptID 来唯一标识。JM和TM之间关于 task 的部署和 task status 的更新都是通过 ExecutionAttemptID 来确定消息接受者。</li></ul></li><li><p>物理执行图：</p><p>JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p><ul><li>Task：Execution被调度后在分配的 TaskManager 中启动对应的 Task。Task 包裹了具有用户执行逻辑的 operator。</li><li>ResultPartition：代表由一个Task的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。</li><li>ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费 Task 数和 DistributionPattern 来决定。</li><li>InputGate：代表Task的输入封装，和JobGraph中JobEdge一一对应。每个InputGate消费了一个或多个的ResultPartition。</li><li>InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。</li></ul></li></ul><p>那么 Flink 为什么要设计这4张图呢，其目的是什么呢？Spark 中也有多张图，数据依赖图以及物理执行的DAG。其目的都是一样的，就是解耦，每张图各司其职，每张图对应了 Job 不同的阶段，更方便做该阶段的事情。我们给出更完整的 Flink Graph 的层次图。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1qmtpJVXXXXagXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1qmtpJVXXXXagXXXXXXXXXXXX" alt="img"></a></p><p>首先我们看到，JobGraph 之上除了 StreamGraph 还有 OptimizedPlan。OptimizedPlan 是由 Batch API 转换而来的。StreamGraph 是由 Stream API 转换而来的。为什么 API 不直接转换成 JobGraph？因为，Batch 和 Stream 的图结构和优化方法有很大的区别，比如 Batch 有很多执行前的预分析用来优化图的执行，而这种优化并不普适于 Stream，所以通过 OptimizedPlan 来做 Batch 的优化会更方便和清晰，也不会影响 Stream。JobGraph 的责任就是统一 Batch 和 Stream 的图，用来描述清楚一个拓扑图的结构，并且做了 chaining 的优化，chaining 是普适于 Batch 和 Stream 的，所以在这一层做掉。ExecutionGraph 的责任是方便调度和各个 tasks 状态的监控和跟踪，所以 ExecutionGraph 是并行化的 JobGraph。而“物理执行图”就是最终分布式在各个机器上运行着的tasks了。所以可以看到，这种解耦方式极大地方便了我们在各个层所做的工作，各个层之间是相互隔离的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：内存管理</title>
      <link href="/2018/05/06/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2018/05/06/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>如今，大数据领域的开源框架（Hadoop，Spark，Storm）都使用的 JVM，当然也包括 Flink。基于 JVM 的数据分析引擎都需要面对将大量数据存到内存中，这就不得不面对 JVM 存在的几个问题：</p><ol><li>Java 对象存储密度低。一个只包含 boolean 属性的对象占用了16个字节内存：对象头占了8个，boolean 属性占了1个，对齐填充占了7个。而实际上只需要一个bit（1/8字节）就够了。</li><li>Full GC 会极大地影响性能，尤其是为了处理更大数据而开了很大内存空间的JVM来说，GC 会达到秒级甚至分钟级。</li><li>OOM 问题影响稳定性。OutOfMemoryError是分布式计算框架经常会遇到的问题，当JVM中所有对象大小超过分配给JVM的内存大小时，就会发生OutOfMemoryError错误，导致JVM崩溃，分布式框架的健壮性和性能都会受到影响。</li></ol><p>所以目前，越来越多的大数据项目开始自己管理JVM内存了，像 Spark、Flink、HBase，为的就是获得像 C 一样的性能以及避免 OOM 的发生。本文将会讨论 Flink 是如何解决上面的问题的，主要内容包括内存管理、定制的序列化工具、缓存友好的数据结构和算法、堆外内存、JIT编译优化等。</p><h2 id="积极的内存管理"><a href="#积极的内存管理" class="headerlink" title="积极的内存管理"></a>积极的内存管理</h2><p>Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 <code>MemorySegment</code>，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。你可以把 MemorySegment 想象成是为 Flink 定制的 <code>java.nio.ByteBuffer</code>。它的底层可以是一个普通的 Java 字节数组（<code>byte[]</code>），也可以是一个申请在堆外的 <code>ByteBuffer</code>。每条记录都会以序列化的形式存储在一个或多个<code>MemorySegment</code>中。</p><p>Flink 中的 Worker 名叫 TaskManager，是用来运行用户代码的 JVM 进程。TaskManager 的堆内存主要被分成了三个部分：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB17qs5JpXXXXXhXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB17qs5JpXXXXXhXpXXXXXXXXXX" alt="img"></a></p><ul><li>Network Buffers: 一定数量的32KB大小的 buffer，主要用于数据的网络传输。在 TaskManager 启动的时候就会分配。默认数量是 2048 个，可以通过 taskmanager.network.numberOfBuffers 来配置。（可阅读上一篇文章了解更多Network Buffer的管理）</li><li><strong>Memory Manager Pool:</strong> 这是一个由 <code>MemoryManager</code> 管理的，由众多<code>MemorySegment</code>组成的超大集合。Flink 中的算法（如 sort/shuffle/join）会向这个内存池申请 MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的 70% 的大小。</li><li><strong>Remaining (Free) Heap:</strong> 这部分的内存是留给用户代码以及 TaskManager 的数据结构使用的。因为这些数据结构一般都很小，所以基本上这些内存都是给用户代码使用的。从GC的角度来看，可以把这里看成的新生代，也就是说这里主要都是由用户代码生成的短期对象。</li></ul><p><strong>注意：Memory Manager Pool 主要在Batch模式下使用。在Steaming模式下，该池子不会预分配内存，也不会向该池子请求内存块。也就是说该部分的内存都是可以给用户代码使用的。不过社区是打算在 Streaming 模式下也能将该池子利用起来。</strong></p><p>Flink 采用类似 DBMS 的 sort 和 join 算法，直接操作二进制数据，从而使序列化/反序列化带来的开销达到最小。所以 Flink 的内部实现更像 C/C++ 而非 Java。如果需要处理的数据超出了内存限制，则会将部分数据存储到硬盘上。如果要操作多块MemorySegment就像操作一块大的连续内存一样，Flink会使用逻辑视图（<code>AbstractPagedInputView</code>）来方便操作。下图描述了 Flink 如何存储序列化后的数据到内存块中，以及在需要的时候如何将数据存储到磁盘上。</p><p>从上面我们能够得出 Flink 积极的内存管理以及直接操作二进制数据有以下几点好处：</p><ol><li><strong>减少GC压力。</strong>显而易见，因为所有常驻型数据都以二进制的形式存在 Flink 的<code>MemoryManager</code>中，这些<code>MemorySegment</code>一直呆在老年代而不会被GC回收。其他的数据对象基本上是由用户代码生成的短生命周期对象，这部分对象可以被 Minor GC 快速回收。只要用户不去创建大量类似缓存的常驻型对象，那么老年代的大小是不会变的，Major GC也就永远不会发生。从而有效地降低了垃圾回收的压力。另外，这里的内存块还可以是堆外内存，这可以使得 JVM 内存更小，从而加速垃圾回收。</li><li><strong>避免了OOM。</strong>所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生OOM。在内存吃紧的情况下，算法（sort/join等）会高效地将一大批内存块写到磁盘，之后再读回来。因此，<code>OutOfMemoryErrors</code>可以有效地被避免。</li><li><strong>节省内存空间。</strong>Java 对象在存储上有很多额外的消耗（如上一节所谈）。如果只存储实际数据的二进制内容，就可以避免这部分消耗。</li><li><strong>高效的二进制操作 &amp; 缓存友好的计算。</strong>二进制数据以定义好的格式存储，可以高效地比较与操作。另外，该二进制形式可以把相关的值，以及hash值，键值和指针等相邻地放进内存中。这使得数据结构可以对高速缓存更友好，可以从 L1/L2/L3 缓存获得性能的提升（下文会详细解释）。</li></ol><h2 id="为-Flink-量身定制的序列化框架"><a href="#为-Flink-量身定制的序列化框架" class="headerlink" title="为 Flink 量身定制的序列化框架"></a>为 Flink 量身定制的序列化框架</h2><p>目前 Java 生态圈提供了众多的序列化框架：Java serialization, Kryo, Apache Avro 等等。但是 Flink 实现了自己的序列化框架。因为在 Flink 中处理的数据流通常是同一类型，由于数据集对象的类型固定，对于数据集可以只保存一份对象Schema信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。当我们需要访问某个对象成员变量的时候，通过定制的序列化工具，并不需要反序列化整个Java对象，而是可以直接通过偏移量，只是反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少Java对象的创建开销，以及内存数据的拷贝大小。</p><p>Flink支持任意的Java或是Scala类型。Flink 在数据类型上有很大的进步，不需要实现一个特定的接口（像Hadoop中的<code>org.apache.hadoop.io.Writable</code>），Flink 能够自动识别数据类型。Flink 通过 Java Reflection 框架分析基于 Java 的 Flink 程序 UDF (User Define Function)的返回类型的类型信息，通过 Scala Compiler 分析基于 Scala 的 Flink 程序 UDF 的返回类型的类型信息。类型信息由 <code>TypeInformation</code> 类表示，TypeInformation 支持以下几种类型：</p><ul><li><code>BasicTypeInfo</code>: 任意Java 基本类型（装箱的）或 String 类型。</li><li><code>BasicArrayTypeInfo</code>: 任意Java基本类型数组（装箱的）或 String 数组。</li><li><code>WritableTypeInfo</code>: 任意 Hadoop Writable 接口的实现类。</li><li><code>TupleTypeInfo</code>: 任意的 Flink Tuple 类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现。</li><li><code>CaseClassTypeInfo</code>: 任意的 Scala CaseClass(包括 Scala tuples)。</li><li><code>PojoTypeInfo</code>: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter/setter 方法。</li><li><code>GenericTypeInfo</code>: 任意无法匹配之前几种类型的类。</li></ul><p>前六种数据类型基本上可以满足绝大部分的Flink程序，针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。对于最后一种数据类型，Flink会使用Kryo进行序列化和反序列化。每个TypeInformation中，都包含了serializer，类型会自动通过serializer进行序列化，然后用Java Unsafe接口写入MemorySegments。对于可以用作key的数据类型，Flink还同时自动生成TypeComparator，用来辅助直接对序列化后的二进制数据进行compare、hash等操作。对于 Tuple、CaseClass、POJO 等组合类型，其TypeSerializer和TypeComparator也是组合的，序列化和比较时会委托给对应的serializers和comparators。如下图展示 一个内嵌型的Tuple3&lt;Integer,Double,Person&gt; 对象的序列化过程。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1lvdbJFXXXXa9XVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1lvdbJFXXXXa9XVXXXXXXXXXX" alt="img"></a></p><p>可以看出这种序列化方式存储密度是相当紧凑的。其中 int 占4字节，double 占8字节，POJO多个一个字节的header，PojoSerializer只负责将header序列化进去，并委托每个字段对应的serializer对字段进行序列化。</p><p>Flink 的类型系统可以很轻松地扩展出自定义的TypeInformation、Serializer以及Comparator，来提升数据类型在序列化和比较时的性能。</p><h2 id="Flink-如何直接操作二进制数据"><a href="#Flink-如何直接操作二进制数据" class="headerlink" title="Flink 如何直接操作二进制数据"></a>Flink 如何直接操作二进制数据</h2><p>Flink 提供了如 group、sort、join 等操作，这些操作都需要访问海量数据。这里，我们以sort为例，这是一个在 Flink 中使用非常频繁的操作。</p><p>首先，Flink 会从 MemoryManager 中申请一批 MemorySegment，我们把这批 MemorySegment 称作 sort buffer，用来存放排序的数据。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1_hhgJFXXXXc2XFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1_hhgJFXXXXc2XFXXXXXXXXXX" alt="img"></a></p><p>我们会把 sort buffer 分成两块区域。一个区域是用来存放所有对象完整的二进制数据。另一个区域用来存放指向完整二进制数据的指针以及定长的序列化后的key（key+pointer）。如果需要序列化的key是个变长类型，如String，则会取其前缀序列化。如上图所示，当一个对象要加到 sort buffer 中时，它的二进制数据会被加到第一个区域，指针（可能还有key）会被加到第二个区域。</p><p>将实际的数据和指针加定长key分开存放有两个目的。第一，交换定长块（key+pointer）更高效，不用交换真实的数据也不用移动其他key和pointer。第二，这样做是缓存友好的，因为key都是连续存储在内存中的，可以大大减少 cache miss（后面会详细解释）。</p><p>排序的关键是比大小和交换。Flink 中，会先用 key 比大小，这样就可以直接用二进制的key比较而不需要反序列化出整个对象。因为key是定长的，所以如果key相同（或者没有提供二进制key），那就必须将真实的二进制数据反序列化出来，然后再做比较。之后，只需要交换key+pointer就可以达到排序的效果，真实的数据不用移动。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1f6BnJFXXXXbnXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1f6BnJFXXXXbnXFXXXXXXXXXX" alt="img"></a></p><p>最后，访问排序后的数据，可以沿着排好序的key+pointer区域顺序访问，通过pointer找到对应的真实数据，并写到内存或外部（更多细节可以看这篇文章 <a href="http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html" target="_blank" rel="noopener">Joins in Flink</a>）。</p><h2 id="缓存友好的数据结构和算法"><a href="#缓存友好的数据结构和算法" class="headerlink" title="缓存友好的数据结构和算法"></a>缓存友好的数据结构和算法</h2><p>随着磁盘IO和网络IO越来越快，CPU逐渐成为了大数据领域的瓶颈。从 L1/L2/L3 缓存读取数据的速度比从主内存读取数据的速度快好几个量级。通过性能分析可以发现，CPU时间中的很大一部分都是浪费在等待数据从主内存过来上。如果这些数据可以从 L1/L2/L3 缓存过来，那么这些等待时间可以极大地降低，并且所有的算法会因此而受益。</p><p>在上面讨论中我们谈到的，Flink 通过定制的序列化框架将算法中需要操作的数据（如sort中的key）连续存储，而完整数据存储在其他地方。因为对于完整的数据来说，key+pointer更容易装进缓存，这大大提高了缓存命中率，从而提高了基础算法的效率。这对于上层应用是完全透明的，可以充分享受缓存友好带来的性能提升。</p><h2 id="走向堆外内存"><a href="#走向堆外内存" class="headerlink" title="走向堆外内存"></a>走向堆外内存</h2><p>Flink 基于堆内存的内存管理机制已经可以解决很多JVM现存问题了，为什么还要引入堆外内存？</p><ol><li>启动超大内存（上百GB）的JVM需要很长时间，GC停留时间也会很长（分钟级）。使用堆外内存的话，可以极大地减小堆内存（只需要分配Remaining Heap那一块），使得 TaskManager 扩展到上百GB内存不是问题。</li><li>高效的 IO 操作。堆外内存在写磁盘或网络传输时是 zero-copy，而堆内存的话，至少需要 copy 一次。</li><li>堆外内存是进程间共享的。也就是说，即使JVM进程崩溃也不会丢失数据。这可以用来做故障恢复（Flink暂时没有利用起这个，不过未来很可能会去做）。</li></ol><p>但是强大的东西总是会有其负面的一面，不然为何大家不都用堆外内存呢。</p><ol><li>堆内存的使用、监控、调试都要简单很多。堆外内存意味着更复杂更麻烦。</li><li>Flink 有时需要分配短生命周期的 <code>MemorySegment</code>，这个申请在堆上会更廉价。</li><li>有些操作在堆内存上会快一点点。</li></ol><p>Flink用通过<code>ByteBuffer.allocateDirect(numBytes)</code>来申请堆外内存，用 <code>sun.misc.Unsafe</code> 来操作堆外内存。</p><p>基于 Flink 优秀的设计，实现堆外内存是很方便的。Flink 将原来的 <code>MemorySegment</code> 变成了抽象类，并生成了两个子类。<code>HeapMemorySegment</code> 和 <code>HybridMemorySegment</code>。从字面意思上也很容易理解，前者是用来分配堆内存的，后者是用来分配堆外内存<strong>和堆内存</strong>的。是的，你没有看错，后者既可以分配堆外内存又可以分配堆内存。为什么要这样设计呢？</p><p>首先假设<code>HybridMemorySegment</code>只提供分配堆外内存。在上述堆外内存的不足中的第二点谈到，Flink 有时需要分配短生命周期的 buffer，这些buffer用<code>HeapMemorySegment</code>会更高效。那么当使用堆外内存时，为了也满足堆内存的需求，我们需要同时加载两个子类。这就涉及到了 JIT 编译优化的问题。因为以前 <code>MemorySegment</code> 是一个单独的 final 类，没有子类。JIT 编译时，所有要调用的方法都是确定的，所有的方法调用都可以被去虚化（de-virtualized）和内联（inlined），这可以极大地提高性能（MemroySegment的使用相当频繁）。然而如果同时加载两个子类，那么 JIT 编译器就只能在真正运行到的时候才知道是哪个子类，这样就无法提前做优化。实际测试的性能差距在 2.7 被左右。</p><p>Flink 使用了两种方案：</p><p><strong>方案1：只能有一种 MemorySegment 实现被加载</strong></p><p>代码中所有的短生命周期和长生命周期的MemorySegment都实例化其中一个子类，另一个子类根本没有实例化过（使用工厂模式来控制）。那么运行一段时间后，JIT 会意识到所有调用的方法都是确定的，然后会做优化。</p><p><strong>方案2：提供一种实现能同时处理堆内存和堆外内存</strong></p><p>这就是 <code>HybridMemorySegment</code> 了，能同时处理堆与堆外内存，这样就不需要子类了。这里 Flink 优雅地实现了一份代码能同时操作堆和堆外内存。这主要归功于 <code>sun.misc.Unsafe</code>提供的一系列方法，如getLong方法：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sun<span class="selector-class">.misc</span><span class="selector-class">.Unsafe</span><span class="selector-class">.getLong</span>(Object reference, long offset)</span><br></pre></td></tr></table></figure><ul><li>如果reference不为空，则会取该对象的地址，加上后面的offset，从相对地址处取出8字节并得到 long。这对应了堆内存的场景。</li><li>如果reference为空，则offset就是要操作的绝对地址，从该地址处取出数据。这对应了堆外内存的场景。</li></ul><p>这里我们看下 <code>MemorySegment</code> 及其子类的实现。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 堆内存引用</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">byte</span>[] heapMemory;</span><br><span class="line">  <span class="comment">// 堆外内存地址</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">long</span> address;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//堆内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = buffer;</span><br><span class="line">    <span class="keyword">this</span>.address = BYTE_ARRAY_BASE_OFFSET;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//堆外内存的初始化</span></span><br><span class="line">  MemorySegment(<span class="keyword">long</span> offHeapAddress, <span class="keyword">int</span> size, Object owner) &#123;</span><br><span class="line">    <span class="comment">//一些先验检查</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">this</span>.heapMemory = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">this</span>.address = offHeapAddress;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> getLong(<span class="keyword">int</span> <span class="keyword">index</span>) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> pos = address + <span class="keyword">index</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">index</span> &gt;= <span class="number">0</span> &amp;&amp; pos &lt;= addressLimit - <span class="number">8</span>) &#123;</span><br><span class="line">      <span class="comment">// 这是我们关注的地方，使用 Unsafe 来操作 on-heap &amp; off-heap</span></span><br><span class="line">      <span class="keyword">return</span> UNSAFE.getLong(heapMemory, pos);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (address &gt; addressLimit) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"segment has been freed"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// index is in fact invalid</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> IndexOutOfBoundsException();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 指向heapMemory的额外引用，用来如数组越界的检查</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] memory;</span><br><span class="line">  <span class="comment">// 只能初始化堆内存</span></span><br><span class="line">  HeapMemorySegment(<span class="keyword">byte</span>[] memory, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(Objects.requireNonNull(memory), owner);</span><br><span class="line">    <span class="keyword">this</span>.memory = memory;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">HybridMemorySegment</span> <span class="keyword">extends</span> <span class="title">MemorySegment</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ByteBuffer offHeapBuffer;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆外内存初始化</span></span><br><span class="line">  HybridMemorySegment(ByteBuffer buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(checkBufferAndGetAddress(buffer), buffer.capacity(), owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = buffer;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 堆内存初始化</span></span><br><span class="line">  HybridMemorySegment(<span class="keyword">byte</span>[] buffer, Object owner) &#123;</span><br><span class="line">    <span class="keyword">super</span>(buffer, owner);</span><br><span class="line">    <span class="keyword">this</span>.offHeapBuffer = <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，HybridMemorySegment 中的很多方法其实都下沉到了父类去实现。包括堆内堆外内存的初始化。<code>MemorySegment</code> 中的 <code>getXXX</code>/<code>putXXX</code> 方法都是调用了 unsafe 方法，可以说<code>MemorySegment</code>已经具有了些 Hybrid 的意思了。<code>HeapMemorySegment</code>只调用了父类的<code>MemorySegment(byte[] buffer, Object owner)</code>方法，也就只能申请堆内存。另外，阅读代码你会发现，许多方法（大量的 getXXX/putXXX）都被标记成了 final，两个子类也是 final 类型，为的也是优化 JIT 编译器，会提醒 JIT 这个方法是可以被去虚化和内联的。</p><p>对于堆外内存，使用 <code>HybridMemorySegment</code> 能同时用来代表堆和堆外内存。这样只需要一个类就能代表长生命周期的堆外内存和短生命周期的堆内存。既然<code>HybridMemorySegment</code>已经这么全能，为什么还要方案1呢？因为我们需要工厂模式来保证只有一个子类被加载（为了更高的性能），而且HeapMemorySegment比heap模式的HybridMemorySegment要快。</p><p>下方是一些性能测试数据，更详细的数据请参考<a href="http://flink.apache.org/news/2015/09/16/off-heap-memory.html#appendix-detailed-micro-benchmarks" target="_blank" rel="noopener">这篇文章</a>。</p><table><thead><tr><th>Segment</th><th>Time</th></tr></thead><tbody><tr><td>HeapMemorySegment, exclusive</td><td>1,441 msecs</td></tr><tr><td>HeapMemorySegment, mixed</td><td>3,841 msecs</td></tr><tr><td>HybridMemorySegment, heap, exclusive</td><td>1,626 msecs</td></tr><tr><td>HybridMemorySegment, off-heap, exclusive</td><td>1,628 msecs</td></tr><tr><td>HybridMemorySegment, heap, mixed</td><td>3,848 msecs</td></tr><tr><td>HybridMemorySegment, off-heap, mixed</td><td>3,847 msecs</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要总结了 Flink 面对 JVM 存在的问题，而在内存管理的道路上越走越深。从自己管理内存，到序列化框架，再到堆外内存。其实纵观大数据生态圈，其实会发现各个开源项目都有同样的趋势。比如最近炒的很火热的 Spark Tungsten 项目，与 Flink 在内存管理上的思想是及其相似的。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink 底层原理：如何处理反压问题</title>
      <link href="/2018/05/05/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%8F%8D%E5%8E%8B%E9%97%AE%E9%A2%98/"/>
      <url>/2018/05/05/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E5%8F%8D%E5%8E%8B%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>流处理系统需要能优雅地处理反压（backpressure）问题。反压通常产生于这样的场景：短时负载高峰导致系统接收数据的速率远高于它处理数据的速率。许多日常问题都会导致反压，例如，垃圾回收停顿可能会导致流入的数据快速堆积，或者遇到大促或秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。</p><p>目前主流的流处理系统 Storm/JStorm/Spark Streaming/Flink 都已经提供了反压机制，不过其实现各不相同。</p><p>Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。具体实现可以看这个 JIRA <a href="https://github.com/apache/storm/pull/700" target="_blank" rel="noopener">STORM-886</a>。</p><p>JStorm 认为直接停止 Spout 的发送太过暴力，存在大量问题。当下游出现阻塞时，上游停止发送，下游消除阻塞后，上游又开闸放水，过了一会儿，下游又阻塞，上游又限流，如此反复，整个数据流会一直处在一个颠簸状态。所以 JStorm 是通过逐级降速来进行反压的，效果会较 Storm 更为稳定，但算法也更复杂。另外 JStorm 没有引入 Zookeeper 而是通过 TopologyMaster 来协调拓扑进入反压状态，这降低了 Zookeeper 的负载。</p><h2 id="Flink-中的反压"><a href="#Flink-中的反压" class="headerlink" title="Flink 中的反压"></a>Flink 中的反压</h2><p>那么 Flink 是怎么处理反压的呢？答案非常简单：Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。下面我们会深入分析 Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速的。</p><p>Flink 在运行时主要由 <strong>operators</strong> 和 <strong>streams</strong> 两大组件构成。每个 operator 会消费中间态的流，并在流上进行转换，然后生成新的流。对于 Flink 的网络机制一种形象的类比是，Flink 使用了高效有界的分布式阻塞队列，就像 Java 通用的阻塞队列（BlockingQueue）一样。还记得经典的线程间通信案例：生产者消费者模型吗？使用 BlockingQueue 的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。Flink 解决反压的方案就是这种感觉。</p><p>在 Flink 中，这些分布式阻塞队列就是这些逻辑流，而队列容量是通过缓冲池（<code>LocalBufferPool</code>）来实现的。每个被生产和被消费的流都会被分配一个缓冲池。缓冲池管理着一组缓冲(<code>Buffer</code>)，缓冲在被消费后可以被回收循环利用。这很好理解：你从池子中拿走一个缓冲，填上数据，在数据消费完之后，又把缓冲还给池子，之后你可以再次使用它。</p><p>在解释 Flink 的反压原理之前，我们必须先对 Flink 中网络传输的内存管理有个了解。</p><h3 id="网络传输中的内存管理"><a href="#网络传输中的内存管理" class="headerlink" title="网络传输中的内存管理"></a>网络传输中的内存管理</h3><p>如下图所示展示了 Flink 在网络传输场景下的内存管理。网络上传输的数据会写到 Task 的 InputGate（IG） 中，经过 Task 的处理后，再由 Task 写到 ResultPartition（RS） 中。每个 Task 都包括了输入和输入，输入和输出的数据存在 <code>Buffer</code> 中（都是字节数据）。Buffer 是 MemorySegment 的包装类。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB14fLsHVXXXXXWXFXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB14fLsHVXXXXXWXFXXXXXXXXXX" alt="img"></a></p><ol><li>TaskManager（TM）在启动时，会先初始化NetworkEnvironment对象，TM 中所有与网络相关的东西都由该类来管理（如 Netty 连接），其中就包括NetworkBufferPool。根据配置，Flink 会在 NetworkBufferPool 中生成一定数量（默认2048）的内存块 MemorySegment（关于 Flink 的内存管理，后续文章会详细谈到），内存块的总数量就代表了网络传输中所有可用的内存。NetworkEnvironment 和 NetworkBufferPool 是 Task 之间共享的，每个 TM 只会实例化一个。</li><li>Task 线程启动时，会向 NetworkEnvironment 注册，NetworkEnvironment 会为 Task 的 InputGate（IG）和 ResultPartition（RP） 分别创建一个 LocalBufferPool（缓冲池）并设置可申请的 MemorySegment（内存块）数量。IG 对应的缓冲池初始的内存块数量与 IG 中 InputChannel 数量一致，RP 对应的缓冲池初始的内存块数量与 RP 中的 ResultSubpartition 数量一致。不过，每当创建或销毁缓冲池时，NetworkBufferPool 会计算剩余空闲的内存块数量，并平均分配给已创建的缓冲池。注意，这个过程只是指定了缓冲池所能使用的内存块数量，并没有真正分配内存块，只有当需要时才分配。为什么要动态地为缓冲池扩容呢？因为内存越多，意味着系统可以更轻松地应对瞬时压力（如GC），不会频繁地进入反压状态，所以我们要利用起那部分闲置的内存块。</li><li>在 Task 线程执行过程中，当 Netty 接收端收到数据时，为了将 Netty 中的数据拷贝到 Task 中，InputChannel（实际是 RemoteInputChannel）会向其对应的缓冲池申请内存块（上图中的①）。如果缓冲池中也没有可用的内存块且已申请的数量还没到池子上限，则会向 NetworkBufferPool 申请内存块（上图中的②）并交给 InputChannel 填上数据（上图中的③和④）。如果缓冲池已申请的数量达到上限了呢？或者 NetworkBufferPool 也没有可用内存块了呢？这时候，Task 的 Netty Channel 会暂停读取，上游的发送端会立即响应停止发送，拓扑会进入反压状态。当 Task 线程写数据到 ResultPartition 时，也会向缓冲池请求内存块，如果没有可用内存块时，会阻塞在请求内存块的地方，达到暂停写入的目的。</li><li>当一个内存块被消费完成之后（在输入端是指内存块中的字节被反序列化成对象了，在输出端是指内存块中的字节写入到 Netty Channel 了），会调用 <code>Buffer.recycle()</code> 方法，会将内存块还给 LocalBufferPool （上图中的⑤）。如果LocalBufferPool中当前申请的数量超过了池子容量（由于上文提到的动态容量，由于新注册的 Task 导致该池子容量变小），则LocalBufferPool会将该内存块回收给 NetworkBufferPool（上图中的⑥）。如果没超过池子容量，则会继续留在池子中，减少反复申请的开销。</li></ol><h3 id="反压的过程"><a href="#反压的过程" class="headerlink" title="反压的过程"></a>反压的过程</h3><p>下面这张图简单展示了两个 Task 之间的数据传输以及 Flink 如何感知到反压的：</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1rCIvJpXXXXcKXXXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1rCIvJpXXXXcKXXXXXXXXXXXX" alt="img"></a></p><ol><li>记录“A”进入了 Flink 并且被 Task 1 处理。（这里省略了 Netty 接收、反序列化等过程）</li><li>记录被序列化到 buffer 中。</li><li>该 buffer 被发送到 Task 2，然后 Task 2 从这个 buffer 中读出记录。</li></ol><p><strong>不要忘了：记录能被 Flink 处理的前提是，必须有空闲可用的 Buffer。</strong></p><p>结合上面两张图看：Task 1 在输出端有一个相关联的 LocalBufferPool（称缓冲池1），Task 2 在输入端也有一个相关联的 LocalBufferPool（称缓冲池2）。如果缓冲池1中有空闲可用的 buffer 来序列化记录 “A”，我们就序列化并发送该 buffer。</p><p>这里我们需要注意两个场景：</p><ul><li>本地传输：如果 Task 1 和 Task 2 运行在同一个 worker 节点（TaskManager），该 buffer 可以直接交给下一个 Task。一旦 Task 2 消费了该 buffer，则该 buffer 会被缓冲池1回收。如果 Task 2 的速度比 1 慢，那么 buffer 回收的速度就会赶不上 Task 1 取 buffer 的速度，导致缓冲池1无可用的 buffer，Task 1 等待在可用的 buffer 上。最终形成 Task 1 的降速。</li><li>远程传输：如果 Task 1 和 Task 2 运行在不同的 worker 节点上，那么 buffer 会在发送到网络（TCP Channel）后被回收。在接收端，会从 LocalBufferPool 中申请 buffer，然后拷贝网络中的数据到 buffer 中。如果没有可用的 buffer，会停止从 TCP 连接中读取数据。在输出端，通过 Netty 的水位值机制来保证不往网络中写入太多数据（后面会说）。如果网络中的数据（Netty输出缓冲中的字节数）超过了高水位值，我们会等到其降到低水位值以下才继续写入数据。这保证了网络中不会有太多的数据。如果接收端停止消费网络中的数据（由于接收端缓冲池没有可用 buffer），网络中的缓冲数据就会堆积，那么发送端也会暂停发送。另外，这会使得发送端的缓冲池得不到回收，writer 阻塞在向 LocalBufferPool 请求 buffer，阻塞了 writer 往 ResultSubPartition 写数据。</li></ul><p>这种固定大小缓冲池就像阻塞队列一样，保证了 Flink 有一套健壮的反压机制，使得 Task 生产数据的速度不会快于消费的速度。我们上面描述的这个方案可以从两个 Task 之间的数据传输自然地扩展到更复杂的 pipeline 中，保证反压机制可以扩散到整个 pipeline。</p><h3 id="Netty-水位值机制"><a href="#Netty-水位值机制" class="headerlink" title="Netty 水位值机制"></a>Netty 水位值机制</h3><p>下方的代码是初始化 NettyServer 时配置的水位值参数。</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认高水位值为2个buffer大小, 当接收端消费速度跟不上，发送端会立即感知到</span></span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, <span class="built_in">config</span>.getMemorySegmentSize() + <span class="number">1</span>);</span><br><span class="line">bootstrap.childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, <span class="number">2</span> * <span class="built_in">config</span>.getMemorySegmentSize());</span><br></pre></td></tr></table></figure><p>当输出缓冲中的字节数超过了高水位值, 则 Channel.isWritable() 会返回false。当输出缓存中的字节数又掉到了低水位值以下, 则 Channel.isWritable() 会重新返回true。Flink 中发送数据的核心代码在 <code>PartitionRequestQueue</code> 中，该类是 server channel pipeline 的最后一层。发送数据关键代码如下所示。</p><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">writeAndFlushNextMessageIfPossible</span><span class="params">(<span class="keyword">final</span> Channel channel)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (fatalError) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Buffer buffer = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// channel.isWritable() 配合 WRITE_BUFFER_LOW_WATER_MARK </span></span><br><span class="line">    <span class="comment">// 和 WRITE_BUFFER_HIGH_WATER_MARK 实现发送端的流量控制</span></span><br><span class="line">    <span class="keyword">if</span> (channel.isWritable()) &#123;</span><br><span class="line">      <span class="comment">// 注意: 一个while循环也就最多只发送一个BufferResponse, 连续发送BufferResponse是通过writeListener回调实现的</span></span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (currentPartitionQueue == <span class="keyword">null</span> &amp;&amp; (currentPartitionQueue = queue.poll()) == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        buffer = currentPartitionQueue.getNextBuffer();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (buffer == <span class="keyword">null</span>) &#123;</span><br><span class="line">          <span class="comment">// 跳过这部分代码</span></span><br><span class="line">          ...</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 构造一个response返回给客户端</span></span><br><span class="line">          BufferResponse resp = <span class="keyword">new</span> BufferResponse(buffer, currentPartitionQueue.getSequenceNumber(), currentPartitionQueue.getReceiverId());</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (!buffer.isBuffer() &amp;&amp;</span><br><span class="line">              EventSerializer.fromBuffer(buffer, getClass().getClassLoader()).getClass() == EndOfPartitionEvent.class) &#123;</span><br><span class="line">            <span class="comment">// 跳过这部分代码。batch 模式中 subpartition 的数据准备就绪，通知下游消费者。</span></span><br><span class="line">            ...</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 将该response发到netty channel, 当写成功后, </span></span><br><span class="line">          <span class="comment">// 通过注册的writeListener又会回调进来, 从而不断地消费 queue 中的请求</span></span><br><span class="line">          channel.writeAndFlush(resp).addListener(writeListener);</span><br><span class="line"></span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    <span class="keyword">if</span> (buffer != <span class="keyword">null</span>) &#123;</span><br><span class="line">      buffer.recycle();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(t.getMessage(), t);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当水位值降下来后（channel 再次可写），会重新触发发送函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">channelWritabilityChanged</span><span class="params">(ChannelHandlerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  writeAndFlushNextMessageIfPossible(ctx.channel());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心发送方法中如果channel不可写，则会跳过发送。当channel再次可写后，Netty 会调用该Handle的 <code>channelWritabilityChanged</code> 方法，从而重新触发发送函数。</p><h3 id="反压实验"><a href="#反压实验" class="headerlink" title="反压实验"></a>反压实验</h3><p>另外，<a href="http://data-artisans.com/how-flink-handles-backpressure/" target="_blank" rel="noopener">官方博客</a>中为了展示反压的效果，给出了一个简单的实验。下面这张图显示了：随着时间的改变，生产者（黄色线）和消费者（绿色线）每5秒的平均吞吐与最大吞吐（在单一JVM中每秒达到8百万条记录）的百分比。我们通过衡量task每5秒钟处理的记录数来衡量平均吞吐。该实验运行在单 JVM 中，不过使用了完整的 Flink 功能栈。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1geTaHVXXXXcXXVXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1geTaHVXXXXcXXVXXXXXXXXXX" alt="img"></a></p><p>首先，我们运行生产task到它最大生产速度的60%（我们通过Thread.sleep()来模拟降速）。消费者以同样的速度处理数据。然后，我们将消费task的速度降至其最高速度的30%。你就会看到背压问题产生了，正如我们所见，生产者的速度也自然降至其最高速度的30%。接着，停止消费task的人为降速，之后生产者和消费者task都达到了其最大的吞吐。接下来，我们再次将消费者的速度降至30%，pipeline给出了立即响应：生产者的速度也被自动降至30%。最后，我们再次停止限速，两个task也再次恢复100%的速度。总而言之，我们可以看到：生产者和消费者在 pipeline 中的处理都在跟随彼此的吞吐而进行适当的调整，这就是我们希望看到的反压的效果。</p><h2 id="反压监控"><a href="#反压监控" class="headerlink" title="反压监控"></a>反压监控</h2><p>在 Storm/JStorm 中，只要监控到队列满了，就可以记录下拓扑进入反压了。但是 Flink 的反压太过于天然了，导致我们无法简单地通过监控队列来监控反压状态。Flink 在这里使用了一个 trick 来实现对反压的监控。如果一个 Task 因为反压而降速了，那么它会卡在向 <code>LocalBufferPool</code> 申请内存块上。那么这时候，该 Task 的 stack trace 就会长下面这样：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java<span class="selector-class">.lang</span><span class="selector-class">.Object</span><span class="selector-class">.wait</span>(Native Method)</span><br><span class="line">o<span class="selector-class">.a</span><span class="selector-class">.f</span>.[...]<span class="selector-class">.LocalBufferPool</span><span class="selector-class">.requestBuffer</span>(LocalBufferPool<span class="selector-class">.java</span>:<span class="number">163</span>)</span><br><span class="line">o<span class="selector-class">.a</span><span class="selector-class">.f</span>.[...]<span class="selector-class">.LocalBufferPool</span><span class="selector-class">.requestBufferBlocking</span>(LocalBufferPool<span class="selector-class">.java</span>:<span class="number">133</span>) &lt;--- BLOCKING request</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><p>那么事情就简单了。通过不断地采样每个 task 的 stack trace 就可以实现反压监控。</p><p><a href="http://img3.tbcdn.cn/5476e8b07b923/TB1T3cJJpXXXXXLXpXXXXXXXXXX" target="_blank" rel="noopener"><img src="http://img3.tbcdn.cn/5476e8b07b923/TB1T3cJJpXXXXXLXpXXXXXXXXXX" alt="img"></a></p><p>Flink 的实现中，只有当 Web 页面切换到某个 Job 的 Backpressure 页面，才会对这个 Job 触发反压检测，因为反压检测还是挺昂贵的。JobManager 会通过 Akka 给每个 TaskManager 发送<code>TriggerStackTraceSample</code>消息。默认情况下，TaskManager 会触发100次 stack trace 采样，每次间隔 50ms（也就是说一次反压检测至少要等待5秒钟）。并将这 100 次采样的结果返回给 JobManager，由 JobManager 来计算反压比率（反压出现的次数/采样的次数），最终展现在 UI 上。UI 刷新的默认周期是一分钟，目的是不对 TaskManager 造成太大的负担。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Flink 不需要一种特殊的机制来处理反压，因为 Flink 中的数据传输相当于已经提供了应对反压的机制。因此，Flink 所能获得的最大吞吐量由其 pipeline 中最慢的组件决定。相对于 Storm/JStorm 的实现，Flink 的实现更为简洁优雅，源码中也看不见与反压相关的代码，无需 Zookeeper/TopologyMaster 的参与也降低了系统的负载，也利于对反压更迅速的响应。</p><h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>CN: 伍 翀（WuChong） | EN: Jark | 花名: 云邪</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink底层原理：流处理基本概念</title>
      <link href="/2018/05/03/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/05/03/Flink%20%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%EF%BC%9A%E6%B5%81%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="Dataflow-Programming"><a href="#Dataflow-Programming" class="headerlink" title="Dataflow Programming"></a>Dataflow Programming</h2><p>在讨论流处理的基本概念之前，我们首先介绍一下数据流编程（dataflow programming）的基本概念与术语。</p><h3 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h3><p>数据流程序一般由数据流图表示，数据流图描述了数据如何在操作之间流动。在数据流图中，节点被称为operator，代表计算；边代表数据依赖。</p><p>Operator是dataflow 应用中的基本单元，它们从输入消费数据，在之上执行计算，并生产数据提供给下一步处理。</p><p>没有输入的operators 称为数据源（data sources），没有输出的operator称为数据接收器（data sink）。一个dataflow graph 必须有至少一个data source以及一个data sink。例如：</p><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091510356-305611451.png" alt="img"></p><p>类似上图的dataflow graph 称为逻辑的（logical）数据流图，因为它们从高层的视角展示了计算逻辑。在执行时，逻辑图会被转换为物理图（physical dataflow graph），具体的执行逻辑会在物理数据流图中给出，如下图：</p><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091535522-555450626.png" alt="img"></p><p>例如，如果我们使用分布式处理引擎，每个operator可能有多个并行的任务跑在不同的物理机器上。逻辑图表示了执行的逻辑，而物理图表示了具体的任务。</p><h3 id="数据并行与任务并行"><a href="#数据并行与任务并行" class="headerlink" title="数据并行与任务并行"></a>数据并行与任务并行</h3><p>数据并行是指：将输入数据做partition，然后使用多个同样的task并行处理数据的子集。数据并行的意义在于将数据分散到多个计算节点上。</p><p>任务并行是指：有多个不同的task任务并行处理相同的或不同的数据。任务并行的意义在于更好的使用集群中的计算资源。</p><h3 id="数据交换策略"><a href="#数据交换策略" class="headerlink" title="数据交换策略"></a>数据交换策略</h3><p>数据交换策略定义了：在physical dataflow graph中，数据条目如何分发到task 中。下面是几种常见的数据交换策略：</p><ol><li>前向（forward）策略：从一个task发送数据到另一个接受task。如果两个task均在一个机器上，则可以避免网络传输</li><li>广播（broadcast）策略：数据发送到所有并行task中。此策略涉及到数据复制及网络传输，所以较为消耗资源</li><li>key-based 策略：根据key做partition，使具有相同key 的条目可以被同一个task处理</li><li>随机（random）策略：随机均匀分布数据到task中，均衡集群计算负载</li></ol><p><img src="https://img2018.cnblogs.com/blog/1287132/201905/1287132-20190510091552451-1460818339.png" alt="img"></p><h2 id="并行流处理"><a href="#并行流处理" class="headerlink" title="并行流处理"></a>并行流处理</h2><p>在了解以上概念后，我们接下来讨论并行流处理。首先，我们定义数据流（data stream）：数据流是一个（可能）无限的事件序列。</p><h3 id="延迟与吞吐"><a href="#延迟与吞吐" class="headerlink" title="延迟与吞吐"></a>延迟与吞吐</h3><p>对于批处理应用，我们一般关注的是一个job的整个执行时间，或是处理引擎需要多长时间读数据、计算、以及写入结果。而流处理应用是持续运行的，并且输入数据可能是无限的，所以对于整个应用的执行时间其实并没有太多关注。但是，流处理程序在处理高频率的事件输入的同时，还必须要在输入数据后尽可能快的提供结果。我们使用延迟（latency）与吞吐（throughput）来衡量这个需求。</p><h3 id="延迟"><a href="#延迟" class="headerlink" title="延迟"></a>延迟</h3><p>延迟表示的是处理一个event所需要的时间。本质上，它是从：接受到event -&gt; 到处理完此event -&gt; 并在结果中有体现，这段时间。举个例子，假设你去咖啡店买咖啡，前面有人排队，在到你点完单后，店里会做咖啡，做好后叫号，然后你来取，取完后开始喝。这里的latency指的就是从你进咖啡店开始，一直到你喝到第一口咖啡的间隔时间。</p><p>在data streaming 中，latency由时间衡量，例如毫秒。根据application的不同，你可能会关注平均延迟、最高延迟、或是百分位数延迟（percentile latency）。例如：平均延迟为10ms，表示events平均在10ms内被处理。而百分位 95 的延迟为10ms表示的是有95% 的events在10ms内被处理。平均延迟值隐藏了处理延迟的分布，可能会难以定位问题。例如：如果咖啡师在为你准备咖啡时用光了牛奶，则你不得不去等待咖啡师去拿牛奶，这里你的咖啡会有更大的延迟，但是其他大部分用户并不会受到影响。</p><p>对于大部分流应用来说（例如系统告警、欺诈检测、网络监控等），保证低延迟至关重要。低延迟在流处理中是一个重要的特性，它是实现“实时”应用的基础。当前主流的流处理器（如Flink），可以提供低至几毫秒的延迟。相对而言，传统的批处理系统的延迟可一般会达到几分钟到几小时不等。在批处理中，首先需要的是将events收集为batch，然后再处理它。所以它的延迟取决于batch中最后一个event到达的时间，以及batch 的大小。真正的流处理并不引入这种延迟，所以可以实现真正的低延迟。在真正的流模型中，events在到达流系统后可以被立即处理，此时的延迟反应的是：在此event上执行的操作时间。</p><h3 id="吞吐"><a href="#吞吐" class="headerlink" title="吞吐"></a>吞吐</h3><p>吞吐用于衡量系统的处理能力：处理率。也就是说，它可以告诉我们，系统在每个时间片内可以处理多少个events。以咖啡店为例，如果咖啡店从早上7点开到晚上7点，每天服务600个客户，则它的平均吞吐为 50个顾客/每小时。在流系统中，我们需要延迟尽可能的低，而吞吐尽可能的高。</p><p>吞吐由每个时间单位内处理的evnets衡量。这里需要注意的是：处理速率取决于events的到达速率。低吞吐并不能完全说明系统性能低。在流系统中，一般希望确保系统最高能处理events的速率。也就是说，我们主要关心的是确定吞吐的峰值（peak throughput）：在系统处于最高负载时的性能极限。为了更好地理解顶峰吞吐（peak throughput），我们考虑一个流处理应用，它一开始并不接收任何输入，所以此时并不消耗任何系统资源。当第一个event到来时，它会立即（尽量）以最小的latency 处理。例如你是咖啡馆开门的第一个顾客，店员会立即为你去做咖啡。在理想情况下，你会希望随着更多events的进入，latency 可以保持较小值不发生太大的变动。然而，一旦输入的events到达某个速率，使得系统资源被完全使用时，就不得不开始缓存（buffering）events。拿咖啡店举例，在中午的时候，人流量会特别大，达到了咖啡店的顶峰，则这时候就需要开始排队了。这时候系统即达到了它的peak throughput，而更大的event rate只会使得latency变得更糟。如果系统继续以更高的速率接收输入（超过了它可以处理的速率），缓冲区可能会爆掉，并导致数据丢失。常规的解决方案是背压（backpressure），并有不同的策略去处理。</p><h3 id="延迟-vs-吞吐"><a href="#延迟-vs-吞吐" class="headerlink" title="延迟 vs 吞吐"></a>延迟 vs 吞吐</h3><p>在这里需要明确的是，延迟与吞吐并不是两个互相独立的指标。如果事件到达数据处理管道的事件较长，便无法保证高吞吐。类似的，如果系统的性能较低，则events 会被缓存并等待，直到系统有能力处理。</p><p>再次以咖啡店为例，首先比较好理解的是，在负载低的时候，可以达到很好的一个latency。例如咖啡店里你是第一个也是唯一的一个顾客。但是在咖啡店较忙的时候，顾客就需要排队等待，此时的latency即会增加。另外一个影响延迟的因素（并继而影响到吞吐）是处理一个事件的时间。例如咖啡店为每个顾客做咖啡所消耗的时间。假设在一个圣诞节，咖啡师需要在每杯咖啡上画一个圣诞老人。也就是说，每杯咖啡制作的时间会增加，导致每个顾客在咖啡店消耗更多的时间，最终使得整体吞吐下降。</p><p>那是否可以同时达到低延迟与高吞吐？在咖啡店的例子中，你可以招聘更有经验的咖啡师，让做咖啡的效率更高。这里主要考量的地方是：减少延迟以增加吞吐。如果一个系统执行的操作更快，则它就可以在同一时间内处理更多的event。另外的方法是招聘更多的咖啡师，让同一时间有更多的客户被服务到。在流处理管道中，通过使用多个stream并行处理events，在获取更低的延时的同时，也可以在同一时间内处理更多的events。</p>]]></content>
      
      
      <categories>
          
          <category> 实时计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink </tag>
            
            <tag> 实时计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS 7 命令行静默安装部署oracle11g数据库</title>
      <link href="/2017/12/10/CentOS%207%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2oracle11g%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
      <url>/2017/12/10/CentOS%207%20%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%9D%99%E9%BB%98%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2oracle11g%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h2 id="准备Oracle-11g安装包"><a href="#准备Oracle-11g安装包" class="headerlink" title="准备Oracle 11g安装包"></a>准备Oracle 11g安装包</h2><p><a href="https://www.oracle.com/technetwork/database/enterprise-edition/downloads/index.html" target="_blank" rel="noopener">官方下载地址</a></p><h2 id="检查硬件需求"><a href="#检查硬件需求" class="headerlink" title="检查硬件需求"></a>检查硬件需求</h2><ol><li>查看系统物理内存,以下输出可以看出，有8G的内存，内存最低要求256M。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep MemTotal /proc/meminfo</span><br></pre></td></tr></table></figure><ol start="2"><li>查看交换空间大小,以下输出可以看出，有5G的交换空间，交换空间的最优设置与你物理内存大小相关，详细说明请参考安装文档</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep SwapTotal /proc/meminfo</span><br></pre></td></tr></table></figure><h2 id="本来的交换空间大小为0，所以重新设置，有以下步骤："><a href="#本来的交换空间大小为0，所以重新设置，有以下步骤：" class="headerlink" title="本来的交换空间大小为0，所以重新设置，有以下步骤："></a>本来的交换空间大小为0，所以重新设置，有以下步骤：</h2><p>关闭swap：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo swapoff -a</span><br></pre></td></tr></table></figure><p>设置swap的大小：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudd dd if=/dev/zero of=/swapfile bs=1M count=5120</span><br></pre></td></tr></table></figure><p>bs指的是Block Size，就是每一块的大小。这里的例子是1M，意思就是count的数字，是以1M为单位的。<br>count是告诉程序，新的swapfile要多少个block。这里是1024，就是说，新的swap文件是5G大小。<br>注意：有些公司的权限需要重新输入密码，而我们就是这样，输入后会看见卡在那里没动，请耐心等待，机器不一样，等待时间也不一样。</p><p>把增大后的文件变为swap文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkswap /swapfile</span><br></pre></td></tr></table></figure><p>重新打开swap：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo swapon  /swapfile</span><br></pre></td></tr></table></figure><p>让swap在启动的时候，自动生效。打开/etc/fstab文件，加上以下命令。然后保存。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc</span><br><span class="line">sudo vim fstab</span><br></pre></td></tr></table></figure><p>因为我的权限不是root权限，所以输入命令前必须加sudo才可以修改资料</p><p>附Linux编辑文件命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vi打开一个文件时，进入的是阅读模式，只有输入相关命令才会进入编辑模式：</span><br><span class="line">i :在当前位置插入</span><br><span class="line">a:在当前位置后追加</span><br><span class="line">o:在当前位置的后面插入一行</span><br><span class="line">I :在行头插入</span><br><span class="line">A:在行尾追加</span><br><span class="line">O:在当前位置的前面插入一行</span><br><span class="line">'ESC'键从编辑模式转换到阅读模式</span><br><span class="line">阅读模式（或叫命令模式）下：</span><br><span class="line">:w 保存文件</span><br><span class="line">:w filename 保存成filename文件</span><br><span class="line">:q 退出</span><br><span class="line">:q! 强行退出</span><br><span class="line">:w! 强行写</span><br><span class="line">:wq 保存退出</span><br><span class="line">:x 同wq</span><br></pre></td></tr></table></figure><p>在fstab文件加入这行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/swapfile swap swap default 0 0</span><br></pre></td></tr></table></figure><p>保存退出，再次查swap大小,就发现变成5g了</p><h2 id="查当前发行版本检查并安装依赖包："><a href="#查当前发行版本检查并安装依赖包：" class="headerlink" title="查当前发行版本检查并安装依赖包："></a>查当前发行版本检查并安装依赖包：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/redhat-release</span><br></pre></td></tr></table></figure><p>发现是7.2的，需要安装包如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> binutils-2.23.52.0.1-12.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> compat-libcap1-1.10-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gcc-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> gcc-c++-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-2.17-36.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-2.17-36.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-devel-2.17-36.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> glibc-devel-2.17-36.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ksh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-0.3.109-9.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-0.3.109-9.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-devel-0.3.109-9.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libaio-devel-0.3.109-9.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libgcc-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libgcc-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-devel-4.8.2-3.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libstdc++-devel-4.8.2-3.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXi-1.7.2-1.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXi-1.7.2-1.el7.x86_64 libXtst-1.2.2-1.el7.i686</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> libXtst-1.2.2-1.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> make-3.82-19.el7.x86_64</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sysstat-10.1.5-1.el7.x86_64</span></span><br></pre></td></tr></table></figure><p>我安装的版本是有安装包的，所以版本不一样</p><p>检查安装oracle11g所需要的安装包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q install binutils compat-libcap1  gcc gcc-c++ glibc glibc glibc-devel glibc-devel ksh libaio libaio libaio-devel libaio-devel libgcc libstdc++ libstdc++ libstdc++-devel libstdc++-devel libXi libXi libXtst libXtst sysstat</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libstdc++ libstdc++-devel libXi libXtst  make sysstat  unixODBC unixODBC-devel</span><br></pre></td></tr></table></figure><p>单独检查：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q 包名</span><br></pre></td></tr></table></figure><p>单独安装： </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y 包名</span><br></pre></td></tr></table></figure><p>注意：安装 elfutils-libelf-devel 时候，因为存在互相依存关系，需要2个同时安装（这个我也是参考别人的，我也没试过怎么 搞，所以我也直接用下面的几条安装命令，安装比较多的包，再安装剩下单独）。</p><p>多包安装：</p><p><strong>命令(强烈推荐使用yum安装)：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y binutils compat-libcap1 gcc gcc-c++ glibc glibc glibc-devel glibc-devel ksh libaio libaio libaio-devel libaio-devel libgcc libstdc++ libstdc++ libstdc++-devel libstdc++-devel libXi libXi libXtst libXtst sysstat</span><br></pre></td></tr></table></figure><p>或：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -q binutils compat-libcap1 compat-libstdc++-33 gcc gcc-c++ glibc glibc-devel ksh libaio libaio-devel libgcc libstdc++ libstdc++-devel libXi libXtst  make sysstat  unixODBC unixODBC-devel</span><br></pre></td></tr></table></figure><p>发现下图最后ksh没安装就可以使用单独安装命令，以上命令没有权限时加 sudo 或者登录root权限安装，因地而异。</p><p>发现找不到ksh这个包，报以下错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile No package ksh available.</span><br></pre></td></tr></table></figure><p>只好查百度了，找了很久有很多方法，我这边用的是下载的方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/7/os/x86_64/Packages/ksh-20120801-137.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>附上下载jar包地址的：</p><p><a href="https://altlinux.pkgs.org/sisyphus/classic-x86_64/pdksh-5.2.14-alt5.x86_64.rpm.html" target="_blank" rel="noopener">https://altlinux.pkgs.org/sisyphus/classic-x86_64/pdksh-5.2.14-alt5.x86_64.rpm.html</a><br>接着是安装这个包，进入这个有这个包的目录，wget命令默认下载的文件放在当前目录,附上Linux命令大全地址：</p><p><a href="http://man.linuxde.net/wget" target="_blank" rel="noopener">http://man.linuxde.net/wget</a><br>接着安装这个包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install ksh-20120801-137.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><h2 id="安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹："><a href="#安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹：" class="headerlink" title="安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹："></a>安装完检查后，有这些包后接着是创建安装oracle和放解压包的文件夹：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据库配置文件目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/oraInventory</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库软件包解压目录</span></span><br><span class="line">sudo mkdir -p /oracledata/data/database</span><br></pre></td></tr></table></figure><p>检查文件夹是否创建，进入data目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /oracledata/data</span><br></pre></td></tr></table></figure><h2 id="接着是创建oracle用户组和用户："><a href="#接着是创建oracle用户组和用户：" class="headerlink" title="接着是创建oracle用户组和用户："></a>接着是创建oracle用户组和用户：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysdba</span></span><br><span class="line">sudo groupadd oraclesysdba</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysoinstall </span></span><br><span class="line">sudo groupadd oraclesysoinstall</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建用户组 oraclesysoper</span></span><br><span class="line">sudo groupadd oraclesysoper</span><br><span class="line"><span class="meta">#</span><span class="bash"> 创建oracle用户，并加入到oraclesysoinstall用户组</span></span><br><span class="line">sudo useradd -g oraclesysoinstall -G oraclesysdba,oraclesysoper oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置oracle用户的登陆密码，需要确认一次，注意两次密码要一样(注意：此处的密码是linux端oracle用户登录密码)</span></span><br><span class="line">sudo passwd oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看创建的用户：</span></span><br><span class="line">id oracle</span><br></pre></td></tr></table></figure><p>为啥要创建三个用户组呢？参考：<a href="http://www.oracle.com/technetwork/cn/articles/hunter-rac11gr2-iscsi-2-092412-zhs.html#13" target="_blank" rel="noopener">http://www.oracle.com/technetwork/cn/articles/hunter-rac11gr2-iscsi-2-092412-zhs.html#13</a></p><p>a.oracle 清单组（一般为oinstall):<br> OINSTALL 组的成员被视为 Oracle 软件的“所有者”，拥有对 Oracle 中央清单 (oraInventory) 的写入权限。在一个 Linux 系统上首次安装 Oracle 软件时，OUI 会创建 /etc/oraInst.loc 文件。该文件指定 Oracle 清单组的名称（默认为 oinstall）以及 Oracle 中央清单目录的路径。<br>b.数据库管理员（OSDBA，一般为 dba）:<br> OSDBA 组的成员可通过操作系统身份验证使用 SQL 以 SYSDBA 身份连接到一个 Oracle 实例。该组的成员可执行关键的数据库管理任务，如创建数据库、启动和关闭实例。该组的默认名称为dba。SYSDBA 系统权限甚至在数据库未打开时也允许访问数据库实例。对此权限的控制完全超出了数据库本身的范围。不要混淆 SYSDBA 系统权限与数据库角色 DBA。DBA 角色不包括 SYSDBA 或 SYSOPER 系统权限。<br>c.数据库操作员组（OSOPER，一般为 oper）:<br> OSOPER 组的成员可通过操作系统身份验证使用 SQL 以 SYSOPER 身份连接到一个 Oracle 实例。这个可选组的成员拥有一组有限的数据库管理权限，如管理和运行备份。该组的默认名称为oper。SYSOPER 系统权限甚至在数据库未打开时也允许访问数据库实例。对此权限的控制完全超出了数据库本身的范围。</p><h2 id="设置目录所有者为oraclesysoinstall-用户组的oracle用户"><a href="#设置目录所有者为oraclesysoinstall-用户组的oracle用户" class="headerlink" title="设置目录所有者为oraclesysoinstall 用户组的oracle用户"></a>设置目录所有者为oraclesysoinstall 用户组的oracle用户</h2><p>进入到data的目录，敲以下命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/oracle　</span><br><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/oraInventory</span><br><span class="line">sudo chown -R oracle:oraclesysoinstall /oracledata/data/database</span><br></pre></td></tr></table></figure><h2 id="修改内核参数，以便支持oracle"><a href="#修改内核参数，以便支持oracle" class="headerlink" title="修改内核参数，以便支持oracle"></a>修改内核参数，以便支持oracle</h2><p>进入/etc/sysctl.conf：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /etc</span><br><span class="line">sudo vim sysctl.conf</span><br></pre></td></tr></table></figure><p>在最后增加上以下参数：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kernel<span class="selector-class">.shmall</span> = <span class="number">2097152</span></span><br><span class="line">kernel<span class="selector-class">.shmmax</span> = <span class="number">2147483648</span></span><br><span class="line">kernel<span class="selector-class">.shmmni</span> = <span class="number">4096</span></span><br><span class="line">kernel<span class="selector-class">.sem</span> = <span class="number">250</span> <span class="number">32000</span> <span class="number">100</span> <span class="number">128</span></span><br><span class="line">fs<span class="selector-class">.file-max</span> = <span class="number">65536</span></span><br><span class="line">net<span class="selector-class">.ipv4</span><span class="selector-class">.ip_local_port_range</span> = <span class="number">1024</span> <span class="number">65000</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.rmem_default</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.rmem_max</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.wmem_default</span>=<span class="number">262144</span></span><br><span class="line">net<span class="selector-class">.core</span><span class="selector-class">.wmem_max</span>=<span class="number">262144</span></span><br></pre></td></tr></table></figure><p>执行如下命令使更改的内核生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /sbin/sysctl -p</span><br></pre></td></tr></table></figure><h2 id="修改用户的限制："><a href="#修改用户的限制：" class="headerlink" title="修改用户的限制："></a>修改用户的限制：</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="keyword">vim</span> /etc/security/limits.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure><p>在limits.conf文件中， 使用vim进行编辑，在最后增加上以下参数：</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">oracle soft nproc <span class="number">2047</span>  </span><br><span class="line">oracle hard nproc <span class="number">16384</span>  </span><br><span class="line">oracle soft nofile <span class="number">1024</span>  </span><br><span class="line">oracle hard nofile <span class="number">65536</span>  </span><br><span class="line">oracle soft stack <span class="number">10240</span></span><br></pre></td></tr></table></figure><p>接着在文件/etc/pam.d/login中修改，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/pam.d/login</span><br></pre></td></tr></table></figure><p>在最后添加以下内容:</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session required /<span class="class"><span class="keyword">lib</span>/<span class="title">security</span>/<span class="title">pam_limits</span>.<span class="title">so</span></span></span><br><span class="line">session required pam_limits.so</span><br></pre></td></tr></table></figure><p>最后在etc/profile添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="variable">$USER</span> = “oracle” ];<span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$SHELL</span> = “/bin/ksh”];<span class="keyword">then</span></span><br><span class="line"><span class="built_in">ulimit</span> -p 16384</span><br><span class="line"><span class="built_in">ulimit</span> -n 65536</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">ulimit</span> -u 16384 -n 65536</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>生效命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="设置环境变量："><a href="#设置环境变量：" class="headerlink" title="设置环境变量："></a>设置环境变量：</h2><p>切记，一定要切换到oracle用户(切换用户一定要是 su-)，然后执行以下命令</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~<span class="string">/.bash_profile</span></span><br></pre></td></tr></table></figure><p>增加以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">export ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line">export ORACLE_SID=dbsrv2</span><br></pre></td></tr></table></figure><p>然后使之生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="接着是解压安装包"><a href="#接着是解压安装包" class="headerlink" title="接着是解压安装包"></a>接着是解压安装包</h2><p>安装zip和unzip组件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y unzip zip</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/7/os/x86_64/Packages/zip-3.0-11.el7.x86_64.rpm  //zip</span><br><span class="line">wget http://downloads.naulinux.ru/pub/SLCE/7x/x86_64/CyrEd/RPMS//unzip-6.0-15.1.el7.x86_64.rpm </span><br><span class="line">sudo yum install zip-3.0-11.el7.x86_64.rpm</span><br><span class="line">sudo yum install unzip-6.0-15.1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>unzip 压缩文件 -c 指定目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unzip p13390677_112040_Linux-x86-64_1of7.zip  -d  /oracledata/data/oraInventory</span><br><span class="line">unzip p13390677_112040_Linux-x86-64_2of7.zip  -d  /oracledata/data/oraInventory</span><br></pre></td></tr></table></figure><p>看到Complete就完成了，然后切换到oracle用户，cd进入解压目录，解压oracle安装包：</p><h2 id="接着是关闭防火墙和selinux"><a href="#接着是关闭防火墙和selinux" class="headerlink" title="接着是关闭防火墙和selinux"></a>接着是关闭防火墙和selinux</h2><p>关闭防火墙是为了其他客户端能够访问到oracle</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看防火墙状态</span></span><br><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure><p>接着是关闭selinux，查看selinux状态：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usr<span class="regexp">/sbin/</span>sestatus -v</span><br></pre></td></tr></table></figure><p>已经是关闭状态，如果是enforcing ，输入以下命令：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/<span class="built_in">config</span></span><br></pre></td></tr></table></figure><p>然后进行修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELINUX=disabled  //将SELINUX=enforcing 此处修改为SELINUX=disabled</span><br></pre></td></tr></table></figure><p>需重启系统生效</p><h2 id="修改响应文件模板"><a href="#修改响应文件模板" class="headerlink" title="修改响应文件模板"></a>修改响应文件模板</h2><p>1、复制响应文件模板</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /home/oracle/etc</span><br><span class="line">cp /oracledata/data/oraInventory/database/response/* /home/oracle/etc</span><br></pre></td></tr></table></figure><p>2、设置响应文件权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod 700 /home/oracle/etc/*.rsp</span><br></pre></td></tr></table></figure><h2 id="修改安装Oracle软件的响应文件-oracledata-data-database-etc-db-install-rsp"><a href="#修改安装Oracle软件的响应文件-oracledata-data-database-etc-db-install-rsp" class="headerlink" title="修改安装Oracle软件的响应文件/oracledata/data/database/etc/db_install.rsp"></a>修改安装Oracle软件的响应文件/oracledata/data/database/etc/db_install.rsp</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim <span class="regexp">/home/</span>oracle<span class="regexp">/etc/</span>db_install.rsp</span><br></pre></td></tr></table></figure><p>按照下面修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装类型</span></span><br><span class="line">oracle.install.option=INSTALL_DB_SWONLY</span><br><span class="line"><span class="meta">#</span><span class="bash"> 主机名称（hostname查询）</span></span><br><span class="line">ORACLE_HOSTNAME=cdh01</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装组</span></span><br><span class="line">UNIX_GROUP_NAME=oraclesysoinstall</span><br><span class="line"><span class="meta">#</span><span class="bash"> INVENTORY中央库存目录（不填就是默认值）</span></span><br><span class="line">INVENTORY_LOCATION=/home/oracle/oraInventory</span><br><span class="line"><span class="meta">#</span><span class="bash"> 选择语言</span></span><br><span class="line">SELECTED_LANGUAGES=en,zh_CN,zh_TW</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle_home</span></span><br><span class="line">ORACLE_HOME=/oracledata/data/oracle/product/11.2.0.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle_base</span></span><br><span class="line">ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle版本</span></span><br><span class="line">oracle.install.db.InstallEdition=EE</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自定义安装，否，使用默认组件</span></span><br><span class="line">oracle.install.db.isCustomInstall=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> dba用户组</span></span><br><span class="line">oracle.install.db.DBA_GROUP= oraclesysdba</span><br><span class="line"><span class="meta">#</span><span class="bash"> oper用户组</span></span><br><span class="line">oracle.install.db.OPER_GROUP=oraclesysoper</span><br><span class="line"><span class="meta">#</span><span class="bash"> 数据库类型</span></span><br><span class="line">oracle.install.db.config.starterdb.type=GENERAL_PURPOSE</span><br><span class="line"><span class="meta">#</span><span class="bash"> globalDBName</span></span><br><span class="line">oracle.install.db.config.starterdb.globalDBName=orcl</span><br><span class="line"><span class="meta">#</span><span class="bash"> SID</span></span><br><span class="line">oracle.install.db.config.starterdb.SID=dbsrv2</span><br><span class="line"><span class="meta">#</span><span class="bash"> 自动管理内存的内存(M)</span></span><br><span class="line">oracle.install.db.config.starterdb.memoryLimit=81920</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设定所有数据库用户使用同一个密码</span></span><br><span class="line">oracle.install.db.config.starterdb.password.ALL=123456</span><br><span class="line"><span class="meta">#</span><span class="bash">（手动写了<span class="literal">false</span>）</span></span><br><span class="line">SECURITY_UPDATES_VIA_MYORACLESUPPORT=false</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置安全更新（貌似是有bug，这个一定要选<span class="literal">true</span>，否则会无限提醒邮件地址有问题，终止安装。）</span></span><br><span class="line">DECLINE_SECURITY_UPDATES=true</span><br></pre></td></tr></table></figure><h2 id="开始安装："><a href="#开始安装：" class="headerlink" title="开始安装："></a>开始安装：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 重启系统，保证所有配置完成</span></span><br><span class="line">sudo reboot</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接着登录到oracle用户，进入解压后的data目录</span></span><br><span class="line">cd /oracledata/data/oraInventory/database</span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行安装</span></span><br><span class="line">./runInstaller -silent -force -ignorePrereq -responseFile /home/oracle/etc/db_install.rsp</span><br><span class="line"><span class="meta">#</span><span class="bash"> 接下来需要等一会儿，时间略长，出现提示信息后安装成功，告知要新开会话用root用户执行脚本</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 新开会话切换用户root或者其他用户可以用sudo的，我的就是用sudo的，根据提示输入两个脚本：</span></span><br><span class="line">sudo /home/oracle/oraInventory/orainstRoot.sh</span><br><span class="line">sudo /oracledata/data/oracle/product/11.2.0.1/root.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后切回原会话按Enter即可</span></span><br></pre></td></tr></table></figure><p>接下来是增加或者修改环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库安装目录</span></span><br><span class="line">export ORACLE_BASE=/oracledata/data/oracle</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle数据库路径</span></span><br><span class="line">export ORACLE_HOME=/oracledata/data/oracle/product/11.2.0.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> oracle启动数据库实例名</span></span><br><span class="line">export ORACLE_SID=dbsrv2</span><br><span class="line">export ROACLE_PID=ora11g</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加系统环境变量</span></span><br><span class="line">export PATH=$PATH:$ORACLE_HOME/bin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加系统环境变量</span></span><br><span class="line">export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib</span><br><span class="line"><span class="meta">#</span><span class="bash"> 防止安装过程出现乱码</span></span><br><span class="line">export LANG=C</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置Oracle客户端字符集，必须与Oracle安装时设置的字符集保持一致</span></span><br><span class="line">export NLS_LANG= "AL32UTF8"</span><br><span class="line">export NLS_DATE_FORMAT='yyyy-mm-dd hh24:mi:ss'</span><br></pre></td></tr></table></figure><p>保存退出,使其生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h2 id="配置监听程序"><a href="#配置监听程序" class="headerlink" title="配置监听程序"></a>配置监听程序</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netca /silent /responsefile /home/oracle/etc/netca.rsp</span><br></pre></td></tr></table></figure><h2 id="启动监控程序"><a href="#启动监控程序" class="headerlink" title="启动监控程序"></a>启动监控程序</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsnrctl start</span><br></pre></td></tr></table></figure><p>发现监听已经启动好了</p><h2 id="静默dbca建库"><a href="#静默dbca建库" class="headerlink" title="静默dbca建库"></a>静默dbca建库</h2><p>编辑应答文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /home/oracle/etc/dbca.rsp</span><br></pre></td></tr></table></figure><p>内容如下</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">RESPONSEFILE_VERSION</span> = <span class="string">"11.2.0"</span></span><br><span class="line"><span class="attr">OPERATION_TYPE</span> = <span class="string">"createDatabase"</span></span><br><span class="line"><span class="attr">GDBNAME</span> = <span class="string">"dbsrv2"</span></span><br><span class="line"><span class="attr">SID</span> = <span class="string">"dbsrv2"</span></span><br><span class="line"><span class="attr">TEMPLATENAME</span> = <span class="string">"General_Purpose.dbc"</span></span><br></pre></td></tr></table></figure><p>建库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> dbca -silent -responseFile /home/oracle/etc/dbca.rsp （此命令无效以下命令代替）</span></span><br><span class="line">dbca -silent -createDatabase -templateName General_Purpose.dbc -gdbName dbsrv2 -sysPassword 123456 -systemPassword 123456</span><br></pre></td></tr></table></figure><p>完成后查看输出日记：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /oracledata/data/oracle/cfgtoollogs/dbca/dbsrv2/dbsrv2.log</span><br></pre></td></tr></table></figure><p> 至此完成数据库实例的创建。</p><p>查看监听状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsnrctl status</span><br></pre></td></tr></table></figure><p>输入命令连接数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlplus / as sysdba</span><br></pre></td></tr></table></figure><p>在sqlplus终端输入sql命令启动数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">startup</span><br></pre></td></tr></table></figure><p>发现已经启动实例</p><p>如何退出sqlpuls</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quit 或 exit</span><br></pre></td></tr></table></figure><p>如何删除实例（清楚自己做什么的情况下再执行此命令）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dbca -silent -deleteDatabase -sourcedb dbsrv2</span><br></pre></td></tr></table></figure><h2 id="创建可以外部访问的数据库"><a href="#创建可以外部访问的数据库" class="headerlink" title="创建可以外部访问的数据库"></a>创建可以外部访问的数据库</h2><p>输入：sqlplus “/as sysdba” (此处是用dba身份登录数据库，系统的超级用户)</p><p>1、创建临时表空间：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir /oracledata/data/oracle/tablespace</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">temporary</span> <span class="keyword">tablespace</span> dp_test tempfile <span class="string">'/oracledata/data/oracle/tablespace/dp_test.dbf'</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">size</span> <span class="number">1024</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">autoextend</span> <span class="keyword">on</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">next</span> <span class="number">100</span>m <span class="keyword">maxsize</span> <span class="number">10240</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">extent</span> <span class="keyword">management</span> <span class="keyword">local</span>; 点击Enter</span><br></pre></td></tr></table></figure><p>说明：</p><p>1) dp_test是临时表空间的名字</p><p>2) /oracledata/data/oracle/tablespace/dp_test.dbf是在/oracledata/data/oracle/tablespace下建一个名为dp_test.dbf的表(注意：单引号为英文状态下的输入)，</p><p>3) 1024m是表空间初始大小，</p><p>4) 100m是表空间自动增长大小，</p><p>5) 10240m是表空间最大的大小。</p><p>2、创建数据表空间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">tablespace</span> dp <span class="keyword">logging</span> <span class="keyword">datafile</span> <span class="string">'/oracledata/data/oracle/tablespace/dp.dbf'</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">size</span> <span class="number">1024</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">autoextend</span> <span class="keyword">on</span> 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">next</span> <span class="number">100</span>m <span class="keyword">maxsize</span> <span class="number">10240</span>m 点击Enter</span><br><span class="line"></span><br><span class="line"><span class="keyword">extent</span> <span class="keyword">management</span> <span class="keyword">local</span>; 点击Enter</span><br></pre></td></tr></table></figure><p>3、创建用户并指定表空间</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create<span class="built_in"> user </span>dp identified by dp123<span class="built_in"> default </span>tablespace dp temporary tablespace dp_test;</span><br></pre></td></tr></table></figure><p>其中dp为用户名，dp123为用户密码，dp_test是临时表空间的名字。</p><p>4、给用户授予权限</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">grant</span> dba <span class="keyword">to</span> dp;</span><br></pre></td></tr></table></figure><p>至此，oracle在centos7下的安装和配置也就完成了，别人已经可以访问你的数据库了</p><h2 id="修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8"><a href="#修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8" class="headerlink" title="修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8"></a>修改oracle字符集将字符编码WE8MSWIN1252修改为AL32UTF8</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">采用的是操作系统默认字符集：WE8MSWIN1252，将字符集修改为：AL32UTF8。</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select userenv(<span class="string">'language'</span>) from dual;</span></span><br><span class="line"></span><br><span class="line">SIMPLIFIED CHINESE_CHINA.WE8MSWIN1252</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select * from nls_database_parameters <span class="built_in">where</span> parameter <span class="keyword">in</span> (<span class="string">'NLS_CHARCTERSET'</span>,<span class="string">'NLS_NCHAR_CHARACTERSET'</span>);</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> select* from v<span class="variable">$nls_parameters</span> <span class="built_in">where</span> parameter=<span class="string">'NLS_CHARACTERSET'</span>;</span></span><br><span class="line"></span><br><span class="line">操作过程如下：</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> shutdown immediate</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> startup</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter session <span class="built_in">set</span> sql_trace=<span class="literal">true</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">enable</span> restricted session;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> show parameter job_queue_processes;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">set</span> job_queue_processes=0;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter system <span class="built_in">set</span> aq_tm_processes=0;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database open;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> alter database character <span class="built_in">set</span> INTERNAL_USE AL32UTF8;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash"> update props$ <span class="built_in">set</span> VALUE$=<span class="string">'UTF8'</span> <span class="built_in">where</span> NAME=<span class="string">'NLS_NCHAR_CHARACTERSET'</span>;</span></span><br><span class="line"></span><br><span class="line">维护完以后需要</span><br><span class="line"></span><br><span class="line"><span class="meta">SQL&gt;</span><span class="bash">ALTER SYSTEM DISABLE RESTRICTED SESSION;</span></span><br><span class="line"></span><br><span class="line">改变字符集后，原来已有的数据不会改变，只是之后新增的数据会是新的字符集。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oracle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解JVM</title>
      <link href="/2017/11/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM/"/>
      <url>/2017/11/03/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM/</url>
      
        <content type="html"><![CDATA[<h2 id="Java运行时数据区："><a href="#Java运行时数据区：" class="headerlink" title="Java运行时数据区："></a>Java运行时数据区：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Java虚拟机在执行Java程序的过程中会将其管理的内存划分为若干个不同的数据区域，这些区域有各自的用途、创建和销毁的时间，有些区域随虚拟机进程的启动而存在，有些区域则是依赖用户线程的启动和结束来建立和销毁。Java虚拟机所管理的内存包括以下几个运行时数据区域，如图：</span><br></pre></td></tr></table></figure><img src="/2017/11/03/深入理解JVM/1.png"><p>1、程序计数器：指向当前线程正在执行的字节码指令。线程私有的。<br>2、虚拟机栈：虚拟机栈是Java执行方法的内存模型。每个方法被执行的时候，都会创建一个栈帧，把栈帧压人栈，当方法正常返回或者抛出未捕获的异常时，栈帧就会出栈。<br>（1）栈帧：栈帧存储方法的相关信息，包含局部变量数表、返回值、操作数栈、动态链接<br>a、局部变量表：包含了方法执行过程中的所有变量。局部变量数组所需要的空间在编译期间完成分配，在方法运行期间不会改变局部变量数组的大小。<br>b、返回值：如果有返回值的话，压入调用者栈帧中的操作数栈中，并且把PC的值指向 方法调用指令 后面的一条指令地址。<br>c、操作数栈：操作变量的内存模型。操作数栈的最大深度在编译的时候已经确定（写入方法区code属性的max_stacks项中）。操作数栈的的元素可以是任意Java类型，包括long和double，32位数据占用栈空间为1，64位数据占用2。方法刚开始执行的时候，栈是空的，当方法执行过程中，各种字节码指令往栈中存取数据。<br>d、动态链接：每个栈帧都持有在运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态链接。<br>（2）线程私有<br>3、本地方法栈：<br>（1）调用本地native的内存模型<br>（2）线程独享。<br>4、方法区：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据<br>（1）线程共享的<br>（2）运行时常量池：</p><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A、是方法区的一部分</span><br><span class="line">B、存放编译期生成的各种字面量和符号引用</span><br><span class="line">C、<span class="class"><span class="keyword">Class</span>文件中除了存有类的版本、字段、方法、接口等描述信息，还有一项是常量池，存有这个类的 编译期生成的各种字面量和符号引用，这部分内容将在类加载后，存放到方法区的运行时常量池中。</span></span><br></pre></td></tr></table></figure><p>5、堆（Heap）：Java对象存储的地方<br>（1）Java堆是虚拟机管理的内存中最大的一块<br>（2）Java堆是所有线程共享的区域<br>（3）在虚拟机启动时创建<br>（4）此内存区域的唯一目的就是存放对象实例，几乎所有对象实例都在这里分配内存。存放new生成的对象和数组<br>（5）Java堆是垃圾收集器管理的内存区域，因此很多时候称为“GC堆”</p><h2 id="JMM-Java内存模型："><a href="#JMM-Java内存模型：" class="headerlink" title="JMM Java内存模型："></a>JMM Java内存模型：</h2><p>1、 Java的并发采用“共享内存”模型，线程之间通过读写内存的公共状态进行通讯。多个线程之间是不能通过直接传递数据交互的，它们之间交互只能通过共享变量实现。<br>2、 主要目的是定义程序中各个变量的访问规则。<br>3、 Java内存模型规定所有变量都存储在主内存中，每个线程还有自己的工作内存。<br>（1） 线程的工作内存中保存了被该线程使用到的变量的拷贝（从主内存中拷贝过来），线程对变量的所有操作都必须在工作内存中执行，而不能直接访问主内存中的变量。<br>（2） 不同线程之间无法直接访问对方工作内存的变量，线程间变量值的传递都要通过主内存来完成。<br>（3） 主内存主要对应Java堆中实例数据部分。工作内存对应于虚拟机栈中部分区域。</p><img src="/2017/11/03/深入理解JVM/2.png"><p>4、Java线程之间的通信由内存模型JMM（Java Memory Model）控制。<br>（1）JMM决定一个线程对变量的写入何时对另一个线程可见。<br>（2）线程之间共享变量存储在主内存中<br>（3）每个线程有一个私有的本地内存，里面存储了读/写共享变量的副本。<br>（4）JMM通过控制每个线程的本地内存之间的交互，来为程序员提供内存可见性保证。<br>5、可见性、有序性：<br>（1）当一个共享变量在多个本地内存中有副本时，如果一个本地内存修改了该变量的副本，其他变量应该能够看到修改后的值，此为可见性。<br>（2）保证线程的有序执行，这个为有序性。（保证线程安全）<br>6、内存间交互操作：<br>（1）lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。<br>（2）unlock（解锁）：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。<br>（3）read（读取）：作用于主内存变量，把主内存的一个变量读取到工作内存中。<br>（4）load（载入）：作用于工作内存，把read操作读取到工作内存的变量载入到工作内存的变量副本中<br>（5）use（使用）：作用于工作内存的变量，把工作内存中的变量值传递给一个执行引擎。<br>（6）assign（赋值）：作用于工作内存的变量。把执行引擎接收到的值赋值给工作内存的变量。<br>（7）store（存储）：把工作内存的变量的值传递给主内存<br>（8）write（写入）：把store操作的值入到主内存的变量中<br>6.1、注意：<br>（1）不允许read、load、store、write操作之一单独出现<br>（2）不允许一个线程丢弃assgin操作<br>（3）不允许一个线程不经过assgin操作，就把工作内存中的值同步到主内存中<br>（4）一个新的变量只能在主内存中生成<br>（5）一个变量同一时刻只允许一条线程对其进行lock操作。但lock操作可以被同一条线程执行多次，只有执行相同次数的unlock操作，变量才会解锁<br>（6）如果对一个变量进行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或者assgin操作初始化变量的值。<br>（7）如果一个变量没有被锁定，不允许对其执行unlock操作，也不允许unlock一个被其他线程锁定的变量<br>（8）对一个变量执行unlock操作之前，需要将该变量同步回主内存中</p><h2 id="堆的内存划分："><a href="#堆的内存划分：" class="headerlink" title="堆的内存划分："></a>堆的内存划分：</h2><img src="/2017/11/03/深入理解JVM/3.png"><p>Java堆的内存划分如图所示，分别为年轻代、Old Memory（老年代）、Perm（永久代）。其中在Jdk1.8中，永久代被移除，使用MetaSpace代替。<br>1、新生代：<br>（1）使用复制清除算法（Copinng算法），原因是年轻代每次GC都要回收大部分对象。新生代里面分成一份较大的Eden空间和两份较小的Survivor空间。每次只使用Eden和其中一块Survivor空间，然后垃圾回收的时候，把存活对象放到未使用的Survivor（划分出from、to）空间中，清空Eden和刚才使用过的Survivor空间。<br>（2）分为Eden、Survivor From、Survivor To，比例默认为8：1：1<br>（3）内存不足时发生Minor GC<br>2、老年代：<br>（1）采用标记-整理算法（mark-compact），原因是老年代每次GC只会回收少部分对象。<br>3、Perm：用来存储类的元数据，也就是方法区。<br>（1）Perm的废除：在jdk1.8中，Perm被替换成MetaSpace，MetaSpace存放在本地内存中。原因是永久代进场内存不够用，或者发生内存泄漏。<br>（2）MetaSpace（元空间）：元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。<br>4、堆内存的划分在JVM里面的示意图：</p><img src="/2017/11/03/深入理解JVM/4.png"><h2 id="GC垃圾回收："><a href="#GC垃圾回收：" class="headerlink" title="GC垃圾回收："></a>GC垃圾回收：</h2><p>一、 判断对象是否要回收的方法：可达性分析法<br>1、 可达性分析法：通过一系列“GC Roots”对象作为起点进行搜索，如果在“GC Roots”和一个对象之间没有可达路径，则称该对象是不可达的。不可达对象不一定会成为可回收对象。进入DEAD状态的线程还可以恢复，GC不会回收它的内存。（把一些对象当做root对象，JVM认为root对象是不可回收的，并且root对象引用的对象也是不可回收的）<br>2、 以下对象会被认为是root对象：<br>（1） 虚拟机栈（栈帧中本地变量表）中引用的对象<br>（2） 方法区中静态属性引用的对象<br>（3） 方法区中常量引用的对象<br>（4） 本地方法栈中Native方法引用的对象<br>3、 对象被判定可被回收，需要经历两个阶段：<br>（1） 第一个阶段是可达性分析，分析该对象是否可达<br>（2） 第二个阶段是当对象没有重写finalize()方法或者finalize()方法已经被调用过，虚拟机认为该对象不可以被救活，因此回收该对象。（finalize()方法在垃圾回收中的作用是，给该对象一次救活的机会）<br>4、 方法区中的垃圾回收：<br>（1） 常量池中一些常量、符号引用没有被引用，则会被清理出常量池<br>（2） 无用的类：被判定为无用的类，会被清理出方法区。判定方法如下：<br>A、 该类的所有实例被回收<br>B、 加载该类的ClassLoader被回收<br>C、 该类的Class对象没有被引用<br>5、 finalize():<br>（1） GC垃圾回收要回收一个对象的时候，调用该对象的finalize()方法。然后在下一次垃圾回收的时候，才去回收这个对象的内存。<br>（2） 可以在该方法里面，指定一些对象在释放前必须执行的操作。</p><p>二、 发现虚拟机频繁full GC时应该怎么办：<br>（full GC指的是清理整个堆空间，包括年轻代和永久代）<br>（1） 首先用命令查看触发GC的原因是什么 jstat –gccause 进程id<br>（2） 如果是System.gc()，则看下代码哪里调用了这个方法<br>（3） 如果是heap inspection(内存检查)，可能是哪里执行jmap –histo[:live]命令<br>（4） 如果是GC locker，可能是程序依赖的JNI库的原因</p><p>三、常见的垃圾回收算法：<br>1、Mark-Sweep（标记-清除算法）：<br>（1）思想：标记清除算法分为两个阶段，标记阶段和清除阶段。标记阶段任务是标记出所有需要回收的对象，清除阶段就是清除被标记对象的空间。<br>（2）优缺点：实现简单，容易产生内存碎片<br>2、Copying（复制清除算法）：<br>（1）思想：将可用内存划分为大小相等的两块，每次只使用其中的一块。当进行垃圾回收的时候了，把其中存活对象全部复制到另外一块中，然后把已使用的内存空间一次清空掉。<br>（2）优缺点：不容易产生内存碎片；可用内存空间少；存活对象多的话，效率低下。<br>3、Mark-Compact（标记-整理算法）：<br>（1）思想：先标记存活对象，然后把存活对象向一边移动，然后清理掉端边界以外的内存。<br>（2）优缺点：不容易产生内存碎片；内存利用率高；存活对象多并且分散的时候，移动次数多，效率低下</p><p>4、分代收集算法：（目前大部分JVM的垃圾收集器所采用的算法）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">思想：把堆分成新生代和老年代。（永久代指的是方法区）</span><br></pre></td></tr></table></figure><p>（1） 因为新生代每次垃圾回收都要回收大部分对象，所以新生代采用Copying算法。新生代里面分成一份较大的Eden空间和两份较小的Survivor空间。每次只使用Eden和其中一块Survivor空间，然后垃圾回收的时候，把存活对象放到未使用的Survivor（划分出from、to）空间中，清空Eden和刚才使用过的Survivor空间。<br>（2） 由于老年代每次只回收少量的对象，因此采用mark-compact算法。<br>（3） 在堆区外有一个永久代。对永久代的回收主要是无效的类和常量<br>5、GC使用时对程序的影响？<br>垃圾回收会影响程序的性能，Java虚拟机必须要追踪运行程序中的有用对象，然后释放没用对象，这个过程消耗处理器时间<br>6、几种不同的垃圾回收类型：<br>（1）Minor GC：从年轻代（包括Eden、Survivor区）回收内存。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A、当<span class="keyword">JVM无法为一个新的对象分配内存的时候，越容易触发Minor </span>GC。所以分配率越高，内存越来越少，越频繁执行Minor GC</span><br><span class="line"><span class="keyword">B、执行Minor </span>GC操作的时候，不会影响到永久代（Tenured）。从永久代到年轻代的引用，被当成GC Roots，从年轻代到老年代的引用在标记阶段直接被忽略掉。</span><br></pre></td></tr></table></figure><p>（2）Major GC：清理整个老年代，当eden区内存不足时触发。<br>（3）Full GC：清理整个堆空间，包括年轻代和老年代。当老年代内存不足时触发</p><h2 id="HotSpot-虚拟机详解："><a href="#HotSpot-虚拟机详解：" class="headerlink" title="HotSpot 虚拟机详解："></a>HotSpot 虚拟机详解：</h2><p>1、 Java对象创建过程：<br>（1）虚拟机遇到一条new指令时，首先检查这个指令的参数能否在常量池中定位到一个类的符号引用，并检查这个符号引用代表的类是否已经加载、连接和初始化。如果没有，就执行该类的加载过程。<br>（2）为该对象分配内存。<br>A、假设Java堆是规整的，所有用过的内存放在一边，空闲的内存放在另外一边，中间放着一个指针作为分界点的指示器。那分配内存只是把指针向空闲空间那边挪动与对象大小相等的距离，这种分配称为“指针碰撞”<br>B、假设Java堆不是规整的，用过的内存和空闲的内存相互交错，那就没办法进行“指针碰撞”。虚拟机通过维护一个列表，记录哪些内存块是可用的，在分配的时候找出一块足够大的空间分配给对象实例，并更新表上的记录。这种分配方式称为“空闲列表“。<br>C、使用哪种分配方式由Java堆是否规整决定。Java堆是否规整由所采用的垃圾收集器是否带有压缩整理功能决定。<br>D、分配对象保证线程安全的做法：虚拟机使用CAS失败重试的方式保证更新操作的原子性。（实际上还有另外一种方案：每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲，TLAB。哪个线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完并分配新的TLAB时，才进行同步锁定。虚拟机是否使用TLAB，由-XX:+/-UseTLAB参数决定）<br>（3）虚拟机为分配的内存空间初始化为零值（默认值）<br>（4）虚拟机对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到对象的元数据信息、对象的Hash码、对象的GC分代年龄等信息。这些信息存放在对象的对象头中。<br>（5） 执行<init>方法，把对象按照程序员的意愿进行初始化。<br>2、 对象的定位访问的方式（通过引用如何去定位到堆上的具体对象的位置）：<br>（1）句柄：使用句柄的方式，Java堆中将会划分出一块内存作为作为句柄池，引用中存储的就是对象的句柄的地址。而句柄中包含了对象实例数据和对象类型数据的地址。</init></p><img src="/2017/11/03/深入理解JVM/5.png"><p>（2）直接指针：使用直接指针的方式，引用中存储的就是对象的地址。Java堆对象的布局必须必须考虑如何去访问对象类型数据。</p><img src="/2017/11/03/深入理解JVM/6.png"><p>（3）两种方式各有优点：<br>A、使用句柄访问的好处是引用中存放的是稳定的句柄地址，当对象被移动（比如说垃圾回收时移动对象），只会改变句柄中实例数据指针，而引用本身不会被修改。<br>B、使用直接指针，节省了一次指针定位的时间开销。<br>3、HotSpot的GC算法实现：<br>（1）HotSpot怎么快速找到GC Root？<br>HotSpot使用一组称为OopMap的数据结构。在类加载完成的时候，HotSpot就把对象内什么偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在栈和寄存器中哪些位置是引用。这样子，在GC扫描的时候，就可以直接知道哪些是可达对象了。<br>（2）安全点：<br>A、HotSpot只在特定的位置生成OopMap，这些位置称为安全点。<br>B、程序执行过程中并非所有地方都可以停下来开始GC，只有在到达安全点是才可以暂停。<br>C、安全点的选定基本上以“是否具有让程序长时间执行“的特征选定的。比如说方法调用、循环跳转、异常跳转等。具有这些功能的指令才会产生Safepoint。<br>（3）中断方式：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">A</span>、抢占式中断：在GC发生时，首先把所有线程中断，如果发现有线程不在安全点上，就恢复线程，让它跑到安全点上。</span><br><span class="line">B、主动式中断：GC需要中断线程时，不直接对线程操作，仅仅设置一个标志，各个线程执行时主动去轮询这个标志，当发现中断标记为真就自己中断挂起。轮询标记的地方和安全点是重合的。</span><br></pre></td></tr></table></figure><p>（5）安全区域：一段代码片段中，对象的引用关系不会发生变化，在这个区域中任何地方开始GC都是安全的。在线程进入安全区域时，它首先标志自己已经进入安全区域，在这段时间里，当JVM发起GC时，就不用管进入安全区域的线程了。在线程将要离开安全区域时，它检查系统是否完成了GC过程，如果完成了，它就继续前行。否则，它就必须等待直到收到可以离开安全区域的信号。<br>4、 GC时为什么要停顿所有Java线程？<br>因为GC先进行可达性分析。可达性分析是判断GC Root对象到其他对象是否可达，假如分析过程中对象的引用关系在不断变化，分析结果的准确性就无法得到保证。<br>5、 CMS收集器：<br>（1）一种以获取最短回收停顿时间为目标的收集器。<br>（2）一般用于互联网站或者B/S系统的服务端<br>（3）基于标记-清除算法的实现，不过更为复杂，整个过程为4个步骤：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、初始标记：标记GC <span class="keyword">Root</span>能直接引用的对象</span><br><span class="line">B、并发标记：利用多线程对每个GC <span class="keyword">Root</span>对象进行tracing搜索，在堆中查找其下所有能关联到的对象。</span><br><span class="line"><span class="keyword">C</span>、重新标记：为了修正并发标记期间，用户程序继续运作而导致标志产生变动的那一部分对象的标记记录。</span><br><span class="line"><span class="keyword">D</span>、并发清除：利用多个线程对标记的对象进行清除</span><br></pre></td></tr></table></figure><p>（4）由于耗时最长的并发标记和并发清除操作都是用户线程一起工作，所以总体来说，CMS的内存回收工作是和用户线程一起并发执行的。<br>（5）缺点：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、对CPU资源占用比较多。可能因为占用一部分CPU资源导致应用程序响应变慢。</span><br><span class="line">B、CMS无法处理浮动垃圾。在并发清除阶段，用户程序继续运行，可能产生新的内存垃圾，这一部分垃圾出现在标记过程之后，因此，CMS无法清除。这部分垃圾称为“浮动垃圾“</span><br><span class="line">C、需要预留一部分内存，在垃圾回收时，给用户程序使用。</span><br><span class="line">D、基于标记-清除算法，容易产生大量内存碎片，导致<span class="literal">full</span> GC（<span class="literal">full</span> GC进行内存碎片的整理）</span><br></pre></td></tr></table></figure><p>6、 对象头部分的内存布局：HotSpot的对象头分为两部分，第一部分用于存储对象自身的运行时数据，比如哈希码、GC分代年龄等。另外一部分用于指向方法区对象类型数据的指针。<br>7、 偏向锁：偏向锁偏向于第一个获取它的线程，如果在接下来的执行过程，没有其他线程获取该锁，则持有偏向锁的线程永远不需要同步。（当一个线程获取偏向锁，它每次进入这个锁相关的同步块，虚拟机不在进行任何同步操作。当有另外一个线程尝试获取这个锁时，偏向模式宣告结束）</p><h2 id="JVM优化："><a href="#JVM优化：" class="headerlink" title="JVM优化："></a>JVM优化：</h2><p>1、一般来说，当survivor区不够大或者占用量达到50%，就会把一些对象放到老年区。通过设置合理的eden区，survivor区及使用率，可以将年轻对象保存在年轻代，从而避免full GC，使用<code>-Xmn</code>设置年轻代的大小</p><p>2、对于占用内存比较多的大对象，一般会选择在老年代分配内存。如果在年轻代给大对象分配内存，年轻代内存不够了，就要在eden区移动大量对象到老年代，然后这些移动的对象可能很快消亡，因此导致full GC。通过设置参数：<code>-XX:PetenureSizeThreshold=1000000</code>，单位为B，标明对象大小超过1M时，在老年代(tenured)分配内存空间。</p><p>3、一般情况下，年轻对象放在eden区，当第一次GC后，如果对象还存活，放到survivor区，此后，每GC一次，年龄增加1，当对象的年龄达到阈值，就被放到tenured老年区。这个阈值可以同构<code>-XX:MaxTenuringThreshold</code>设置。如果想让对象留在年轻代，可以设置比较大的阈值。</p><p>4、设置最小堆和最大堆：<code>-Xmx</code>和<code>-Xms</code>稳定的堆大小堆垃圾回收是有利的，获得一个稳定的堆大小的方法是设置-Xms和-Xmx的值一样，即最大堆和最小堆一样，如果这样子设置，系统在运行时堆大小理论上是恒定的，稳定的堆空间可以减少GC次数，因此，很多服务端都会将这两个参数设置为一样的数值。稳定的堆大小虽然减少GC次数，但是增加每次GC的时间，因为每次GC要把堆的大小维持在一个区间内。</p><p>5、一个不稳定的堆并非毫无用处。在系统不需要使用大内存的时候，压缩堆空间，使得GC每次应对一个较小的堆空间，加快单次GC次数。基于这种考虑，JVM提供两个参数，用于压缩和扩展堆空间。<br>（1）<code>-XX:MinHeapFreeRatio</code> 参数用于设置堆空间的最小空闲比率。默认值是40，当堆空间的空闲内存比率小于40，JVM便会扩展堆空间<br>（2）<code>-XX:MaxHeapFreeRatio</code> 参数用于设置堆空间的最大空闲比率。默认值是70， 当堆空间的空闲内存比率大于70，JVM便会压缩堆空间。<br>（3）当-Xmx和-Xmx相等时，上面两个参数无效</p><p>6、通过增大吞吐量提高系统性能，可以通过设置并行垃圾回收收集器。<br>（1）<code>-XX:+UseParallelGC</code>:年轻代使用并行垃圾回收收集器。这是一个关注吞吐量的收集器，可以尽可能的减少垃圾回收时间。<br>（2）<code>-XX:+UseParallelOldGC</code>:设置老年代使用并行垃圾回收收集器。</p><p>7、尝试使用大的内存分页：使用大的内存分页增加CPU的内存寻址能力，从而系统的性能。<code>-XX:+LargePageSizeInBytes</code>设置内存页的大小</p><p>8、使用非占用的垃圾收集器。<code>-XX:+UseConcMarkSweepGC</code>老年代使用CMS收集器降低停顿。</p><p>9、<code>-XXSurvivorRatio=3</code>，表示年轻代中的分配比率：survivor:eden = 2:3</p><p>10、JVM性能调优的工具：<br>（1）jps（Java Process Status）：输出JVM中运行的进程状态信息(现在一般使用jconsole)<br>（2）jstack：查看java进程内线程的堆栈信息。<br>（3）jmap：用于生成堆转存快照<br>（4）jhat：用于分析jmap生成的堆转存快照（一般不推荐使用，而是使用Ecplise Memory Analyzer）<br>（3）jstat是JVM统计监测工具。可以用来显示垃圾回收信息、类加载信息、新生代统计信息等。<br>（4）VisualVM：故障处理工具</p><h2 id="类加载机制："><a href="#类加载机制：" class="headerlink" title="类加载机制："></a>类加载机制：</h2><p>一、 概念：类加载器把class文件中的二进制数据读入到内存中，存放在方法区，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类加载的步骤如下：<br>1、加载：查找并加载类的二进制数据（把class文件里面的信息加载到内存里面）<br>2、连接：把内存中类的二进制数据合并到虚拟机的运行时环境中<br>（1）验证：确保被加载的类的正确性。包括：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A、类文件的结构检查：检查是否满足<span class="keyword">Java类文件的固定格式</span></span><br><span class="line"><span class="keyword">B、语义检查：确保类本身符合Java的语法规范</span></span><br><span class="line"><span class="keyword">C、字节码验证：确保字节码流可以被Java虚拟机安全的执行。字节码流是操作码组成的序列。每一个操作码后面都会跟着一个或者多个操作数。字节码检查这个步骤会检查每一个操作码是否合法。</span></span><br><span class="line"><span class="keyword">D、二进制兼容性验证：确保相互引用的类之间是协调一致的。</span></span><br></pre></td></tr></table></figure><p>（2）准备：为类的静态变量分配内存，并将其初始化为默认值<br>（3）解析：把类中的符号引用转化为直接引用（比如说方法的符号引用，是有方法名和相关描述符组成，在解析阶段，JVM把符号引用替换成一个指针，这个指针就是直接引用，它指向该类的该方法在方法区中的内存位置）<br>3、初始化：为类的静态变量赋予正确的初始值。当静态变量的等号右边的值是一个常量表达式时，不会调用static代码块进行初始化。只有等号右边的值是一个运行时运算出来的值，才会调用static初始化。</p><p>二、双亲委派模型：<br>1、当一个类加载器收到类加载请求的时候，它首先不会自己去加载这个类的信息，而是把该<br>请求转发给父类加载器，依次向上。所以所有的类加载请求都会被传递到父类加载器中，只有当父类加载器中无法加载到所需的类，子类加载器才会自己尝试去加载该类。当当前类加载器和所有父类加载器都无法加载该类时，抛出ClassNotFindException异常。<br>2、意义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">提高系统的安全性。用户自定义的类加载器不可能加载应该由父加载器加载的可靠类。（比如用户定义了一个恶意代码，自定义的类加载器首先让系统加载器去加载，系统加载器检查该代码不符合规范，于是就不继续加载了）</span><br></pre></td></tr></table></figure><p>3、定义类加载器：如果某个类加载器能够加载一个类，那么这个类加载器就叫做定义类加载器<br>4、初始类加载器：定义类加载器及其所有子加载器都称作初始类加载器。<br>5、运行时包：<br>（1）由同一个类加载器加载并且拥有相同包名的类组成运行时包<br>（2）只有属于同一个运行时包的类，才能访问包可见（default）的类和类成员。作用是 限制用户自定义的类冒充核心类库的类去访问核心类库的包可见成员。<br>6、加载两份相同的class对象的情况：A和B不属于父子类加载器关系，并且各自都加载了同一个类。</p><p>三、特点：<br>1、全盘负责：当一个类加载器加载一个类时，该类所依赖的其他类也会被这个类加载器加载到内存中。<br>2、缓存机制：所有的Class对象都会被缓存，当程序需要使用某个Class时，类加载器先从缓存中查找，找不到，才从class文件中读取数据，转化成Class对象，存入缓存中。</p><p>三、 类加载器：<br>两种类型的类加载器：<br>1、 JVM自带的类加载器（3种）：<br>（1）根类加载器（Bootstrap）：<br>a、C++编写的，程序员无法在程序中获取该类<br>b、负责加载虚拟机的核心库，比如java.lang.Object<br>c、没有继承ClassLoader类<br>（2）扩展类加载器（Extension）：<br>a、Java编写的，从指定目录中加载类库<br>b、父加载器是根类加载器<br>c、是ClassLoader的子类<br>d、如果用户把创建的jar文件放到指定目录中，也会被扩展加载器加载。<br>（3）系统加载器（System）或者应用加载器(App)：<br>a、Java编写的<br>b、父加载器是扩展类加载器<br>c、从环境变量或者class.path中加载类<br>d、是用户自定义类加载的默认父加载器<br>e、是ClassLoader的子类</p><p>2、用户自定义的类加载器：<br>（1）Java.lang.ClassLoader类的子类<br>（2）用户可以定制类的加载方式<br>（3）父类加载器是系统加载器<br>（4）编写步骤：<br>A、继承ClassLoader<br>B、重写findClass方法。从特定位置加载class文件，得到字节数组，然后利用defineClass把字节数组转化为Class对象<br>（5）为什么要自定义类加载器？<br>A、可以从指定位置加载class文件，比如说从数据库、云端加载class文件<br>B、加密：Java代码可以被轻易的反编译，因此，如果需要对代码进行加密，那么加密以后的代码，就不能使用Java自带的ClassLoader来加载这个类了，需要自定义ClassLoader，对这个类进行解密，然后加载。</p><p>问题：Java程序对类的执行有几种方式：<br>1、 主动使用（6种情况）：<br>JVM必须在每个类“首次 主动使用”的时候，才会初始化这些类。<br>（1） 创建类的实例<br>（2） 读写某个类或者接口的静态变量<br>（3） 调用类的静态方法<br>（4） 同过反射的API（Class.forName()）获取类<br>（5） 初始化一个类的子类<br>（6） JVM启动的时候，被标明启动类的类（包含Main方法的类）<br>只有当程序使用的静态变量或者静态方法确实在该类中定义时，该可以认为是对该类或者接口的主动使用。<br>2、 被动使用：除了主动使用的6种情况，其他情况都是被动使用，都不会导致类的初始化。<br>3、 JVM规范允许类加载器在预料某个类将要被使用的时候，就预先加载它。如果该class文件缺失或者存在错误，则在程序“首次 主动使用”的时候，才报告这个错误。（Linkage Error错误）。如果这个类一直没有被程序“主动使用”，就不会报错。</p><p>类加载机制与接口：<br>1、 当Java虚拟机初始化一个类时，不会初始化该类实现的接口。<br>2、 在初始化一个接口时，不会初始化这个接口父接口。<br>3、 只有当程序首次使用该接口的静态变量时，才导致该接口的初始化。</p><p>ClassLoader：<br>1、 调用Classloader的loadClass方法去加载一个类，不是主动使用，因此不会进行类的初始化。</p><p>类的卸载：<br>1、 有JVM自带的三种类加载器（根、扩展、系统）加载的类始终不会卸载。因为JVM始终引用这些类加载器，这些类加载器使用引用他们所加载的类，因此这些Class类对象始终是可到达的。<br>2、 由用户自定义类加载器加载的类，是可以被卸载的。</p><p>补充：</p><ul><li>JDK和JRK</li></ul><p>（1）JDK ： Java Development Kit，开发的时候用到的类包。<br>（2）JRE ： Java Runtime Environment，Java运行的基础，包含运行时需要的所有类库。</p><ul><li>图解java文件转化成机器码</li></ul><img src="/2017/11/03/深入理解JVM/7.png"><p>JVM虚拟机先将java文件编译成class文件（字节码文件），然后再将class文件转换成所有操作系统都能运行的机器指令。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 内存回收 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM垃圾回收底层原理</title>
      <link href="/2017/11/03/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/"/>
      <url>/2017/11/03/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h2><h3 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h3><p>最基础的收集算法是“标记-清除”(Mark-Sweep)算法，分两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。</p><p>不足：一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能导致以后在程序运行过程需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一个的垃圾收集动作。</p><h3 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h3><p>为了解决效率问题，一种称为复制(Copying)的收集算法出现了，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完了，就将还存活着的对象复制到另外一块上，然后再把已经使用过的内存空间一次清理掉。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。代价是内存缩小为原来的一半。</p><p>商业虚拟机用这个回收算法来回收新生代。IBM研究表明98%的对象是“朝生夕死“，不需要按照1-1的比例来划分内存空间，而是将内存分为一块较大的”Eden“空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活的对象一次性复制到另外一个Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。Hotspot虚拟机默认Eden和Survivor的比例是8-1.即每次可用整个新生代的90%, 只有一个survivor，即1/10被”浪费“。当然，98%的对象回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够时，需要依赖其他内存(老年代)进行分配担保(Handle Promotion).</p><p>如果另外一块survivor空间没有足够空间存放上一次新生代收集下来的存活对象时，这些对象将直接通过分配担保机制进入老年代。</p><h3 id="eden-survivor复制过程概述"><a href="#eden-survivor复制过程概述" class="headerlink" title="eden survivor复制过程概述"></a>eden survivor复制过程概述</h3><p>Eden Space字面意思是伊甸园，对象被创建的时候首先放到这个区域，进行垃圾回收后，不能被回收的对象被放入到空的survivor区域。</p><p>Survivor Space幸存者区，用于保存在eden space内存区域中经过垃圾回收后没有被回收的对象。Survivor有两个，分别为To Survivor、 From Survivor，这个两个区域的空间大小是一样的。执行垃圾回收的时候Eden区域不能被回收的对象被放入到空的survivor（也就是To Survivor，同时Eden区域的内存会在垃圾回收的过程中全部释放），另一个survivor（即From Survivor）里不能被回收的对象也会被放入这个survivor（即To Survivor），然后To Survivor 和 From Survivor的标记会互换，始终保证一个survivor是空的。</p><p>为啥需要两个survivor？因为需要一个完整的空间来复制过来。当满的时候晋升。每次都往标记为to的里面放，然后互换，这时from已经被清空，可以当作to了。</p><h3 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h3><p>复制收集算法在对象成活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以，老年代一般不能直接选用这种算法。</p><p>根据老年代的特点，有人提出一种”标记-整理“Mark-Compact算法，标记过程仍然和标记-清除一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理端边界以外的内存.</p><h3 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h3><p>当前商业虚拟机的垃圾收集都采用”分代收集“(Generational Collection)算法，这种算法根据对象存活周期的不同将内存划分为几块。一般把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代，每次垃圾收集时都发现大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率较高，没有额外的空间对它进行分配担保，就必须使用”标记-清理“和”标记-整理“算法来进行回收。</p><h2 id="HotSpot算法实现"><a href="#HotSpot算法实现" class="headerlink" title="HotSpot算法实现"></a>HotSpot算法实现</h2><p>在Java语言中，可作为GC Roots的对象包括下面几种：</p><ul><li>虚拟机栈(栈帧中的本地变量表)中引用的对象</li><li>方法去中类静态属性引用的对象</li><li>方法区中常量引用的对象</li><li>本地方法栈中JNI(即一般说的Native方法)引用的对象</li></ul><p>从可达性分析中从GC Roots节点找引用链这个操作为例，可作为GC Roots的节点主要在全局性的引用(例如常量或类静态属性)与执行上下文(例如栈帧中的本地变量表)中，现在很多应用仅仅方法区就有数百兆，如果要逐个检查里面的引用，必然消耗很多时间。</p><p>可达性分析对执行时间的敏感还体现在GC停顿上，因为这项分析工作必须在一个能确保一致性的快照中进行–这里”一致性“的意思是指整个分析期间整个执行系统看起来就像被冻结在某个时间点，不可以出现分析过程中对象引用关系还在不断变化的情况，该点不满足的话分析结果准确性就无法得到保证。这点是导致GC进行时必须停顿所有Java执行线程(Sun公司将这件事情称为”Stop The World“)的一个重要原因，即使是在号称(几乎)不会发生停顿的CMS收集器中，枚举根节点时也必须停顿的。</p><p>安全点，Safepoint</p><h2 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h2><h3 id="Serial-收集器"><a href="#Serial-收集器" class="headerlink" title="Serial 收集器"></a>Serial 收集器</h3><p>标记-复制。</p><p>单线程，一个CPU或一条收集线程去完成垃圾收集工作，收集时必须暂停其他所有的工作线程，直到它结束。</p><p>虽然如此，它依然是虚拟机运行在Client模式下的默认<strong>新生代</strong>收集器。简单而高效。</p><h3 id="ParNew-收集器"><a href="#ParNew-收集器" class="headerlink" title="ParNew 收集器"></a>ParNew 收集器</h3><p>ParNew是Serial收集器的多线程版本。Server模式下默认<strong>新生代</strong>收集器，除了Serial收集器之外，只有它能与CMS收集器配合工作。</p><h3 id="并行-Parallel"><a href="#并行-Parallel" class="headerlink" title="并行 Parallel"></a>并行 Parallel</h3><p>指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。</p><h3 id="并发-Concurrent"><a href="#并发-Concurrent" class="headerlink" title="并发 Concurrent"></a>并发 Concurrent</h3><p>指用户线程与垃圾收集线程同时执行(但不一定是并行的，可能会交替执行)，用户程序再继续运行，而垃圾收集程序运行于另一个CPU上。</p><h3 id="Parallel-Scavenge-收集器"><a href="#Parallel-Scavenge-收集器" class="headerlink" title="Parallel Scavenge 收集器"></a>Parallel Scavenge 收集器</h3><p>Parallel Scavenge 收集器是一个<strong>新生代</strong>收集器，它也是使用复制算法的收集器。看上去来ParNew一样，有什么特别？</p><p>Parallel Scavenge 收集器的特点是它的关注点与其他收集器不同，CMS等收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间。而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。所谓吞吐量就是CPU用于运行用户代码的时间和CPU总小号时间的比值，即吞吐量 = 运行用户代码时间 / (运行用户代码时间+垃圾收集时间)，虚拟机总共运行了100min，其中垃圾收集花费了1min，那吞吐量就是99%.</p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可以高效地利用CPU时间，主要适合在后台运算而不需要太多交互的任务。</p><p>Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 <code>-XX:MaxGCPauseMillis</code>以及直接设置吞吐量大小的<code>-XX:GCTimeRatio</code>。</p><h3 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h3><p>Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器。给Client模式下的虚拟机使用。</p><p>新生代采用复制算法，暂停所有用户线程；</p><p>老年代采用标记-整理算法，暂停所有用户线程；</p><h3 id="Parallel-Old-收集器"><a href="#Parallel-Old-收集器" class="headerlink" title="Parallel Old 收集器"></a>Parallel Old 收集器</h3><p>这里注意，Parallel Scavage 收集器架构中本身有PS MarkSweep收集器来收集老年代，并非直接使用了Serial Old,但二者接近。本人win10 64位系统，jdk1.8.0_102，测试默认垃圾收集器为：<strong>PS MarkSweep *<em>和 *</em>PS Scavenge</strong>。 也就是说Java8的默认并不是G1。</p><p>这是”吞吐量优先“，注重吞吐量以及CPU资源敏感的场合都可以优先考虑Parallel Scavenge和Parallel Old(PS Mark Sweep)。Java8 默认就是这个。</p><h3 id="CMS-收集器"><a href="#CMS-收集器" class="headerlink" title="CMS 收集器"></a>CMS 收集器</h3><p>CMS(Concurrent Mark Sweep) 收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类尤其重视服务的响应速度，希望系统停顿时间最短。CMS收集器就非常符合这类应用的需求。</p><p>CMS基于 <code>标记-清除</code>算法实现。整个过程分为4个步骤：</p><ol><li>初始标记(CMS initial mark) -stop the world</li><li>并发标记(CMS concurrent mark)</li><li>重新标记(CMS remark) -stop the world</li><li>并发清除(CMS concurrent sweep)</li></ol><p>初始标记，重新标记这两个步骤仍然需要Stop The World, 初始标记仅仅标记以下GC Roots能直接关联的对象，速度很快。</p><p>并发标记就是进行GC Roots Tracing的过程；</p><p>而重新标记阶段则是为了修正并发标记期间因为用户程序继续运作而导致标记产生变动的那一部分对象的标记记录。这个阶段停顿比初始标记稍微长，但远比并发标记的时间短。</p><p>整个过程耗时最长的并发标记和并发清除过程，收集器都可以与用户线程一起工作。总体上来说，CMS收集器的内存回收过程与用户线程一起并发执行的。</p><p>CMS特点：并发收集，低停顿。</p><p><strong>缺点</strong></p><p>1.CMS收集器对CPU资源非常敏感。默认启动的回收线程数是(CPU+3)/4. 当CPU 4个以上时，并发回收垃圾收集线程不少于25%的CPU资源。</p><p>2.CMS收集器无法处理浮动垃圾(Floating Garbage), 可能出现”Concurrent Mode Failure“失败而导致另一次Full GC的产生。由于CMS并发清理时，用户线程还在运行，伴随产生新垃圾，而这一部分出现在标记之后，只能下次GC时再清理。这一部分垃圾就称为”浮动垃圾“。</p><p>由于CMS运行时还需要给用户空间继续运行，则不能等老年代几乎被填满再进行收集，需要预留一部分空间提供并发收集时，用户程序运行。JDK1.6中，CMS启动阈值为92%. 若预留内存不够用户使用，则出现一次<code>Concurent Mode Failure</code>失败。这时虚拟机启动后备预案，临时启用Serial Old收集老年代，这样停顿时间很长。</p><p>3.CMS基于”标记-清除“算法实现的，则会产生大量空间碎片，空间碎片过多时，没有连续空间分配给大对象，不得不提前触发一次FUll GC。当然可以开启-XX:+UseCMSCompactAtFullCollection(默认开)，在CMS顶不住要FullGC时开启内存碎片合并整理过程。内存整理过程是无法并发的，空间碎片问题没了，但停顿时间变长。</p><p><strong>面试题：CMS一共会有几次STW</strong></p><p>首先，回答两次，初始标记和重新标记需要。</p><p>然后，CMS并发的代价是预留空间给用户，预留不足的时候触发FUllGC，这时Serail Old会STW.</p><p>然后，CMS是标记-清除算法，导致空间碎片，则没有连续空间分配大对象时，FUllGC, 而FUllGC会开始碎片整理， STW.</p><p>即2次或多次。</p><h2 id="CMS什么时候FUll-GC"><a href="#CMS什么时候FUll-GC" class="headerlink" title="CMS什么时候FUll GC"></a>CMS什么时候FUll GC</h2><p>除直接调用System.gc外，触发Full GC执行的情况有如下四种。</p><h3 id="1-旧生代空间不足"><a href="#1-旧生代空间不足" class="headerlink" title="1. 旧生代空间不足"></a>1. 旧生代空间不足</h3><p>旧生代空间只有在新生代对象转入及创建为大对象、大数组时才会出现不足的现象，当执行Full GC后空间仍然不足，则抛出如下错误： java.lang.OutOfMemoryError: Java heap space 为避免以上两种状况引起的FullGC，调优时应尽量做到让对象在Minor GC阶段被回收、让对象在新生代多存活一段时间及不要创建过大的对象及数组。</p><h3 id="2-Permanet-Generation空间满"><a href="#2-Permanet-Generation空间满" class="headerlink" title="2. Permanet Generation空间满"></a>2. Permanet Generation空间满</h3><p>PermanetGeneration中存放的为一些class的信息等，当系统中要加载的类、反射的类和调用的方法较多时，Permanet Generation可能会被占满，在未配置为采用CMS GC的情况下会执行Full GC。如果经过Full GC仍然回收不了，那么JVM会抛出如下错误信息： java.lang.OutOfMemoryError: PermGen space 为避免Perm Gen占满造成Full GC现象，可采用的方法为增大Perm Gen空间或转为使用CMS GC。</p><h3 id="3-CMS-GC时出现promotion-failed和concurrent-mode-failure"><a href="#3-CMS-GC时出现promotion-failed和concurrent-mode-failure" class="headerlink" title="3. CMS GC时出现promotion failed和concurrent mode failure"></a>3. CMS GC时出现promotion failed和concurrent mode failure</h3><p>对于采用CMS进行旧生代GC的程序而言，尤其要注意GC日志中是否有promotion failed和concurrent mode failure两种状况，当这两种状况出现时可能会触发Full GC。 promotionfailed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。 应对措施为：增大survivorspace、旧生代空间或调低触发并发GC的比率，但在JDK 5.0+、6.0+的版本中有可能会由于JDK的bug29导致CMS在remark完毕后很久才触发sweeping动作。对于这种状况，可通过设置-XX:CMSMaxAbortablePrecleanTime=5（单位为ms）来避免。</p><h3 id="4-统计得到的Minor-GC晋升到旧生代的平均大小大于旧生代的剩余空间"><a href="#4-统计得到的Minor-GC晋升到旧生代的平均大小大于旧生代的剩余空间" class="headerlink" title="4. 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间"></a>4. 统计得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间</h3><p>这是一个较为复杂的触发情况，Hotspot为了避免由于新生代对象晋升到旧生代导致旧生代空间不足的现象，在进行Minor GC时，做了一个判断，如果之前统计所得到的Minor GC晋升到旧生代的平均大小大于旧生代的剩余空间，那么就直接触发Full GC。 例如程序第一次触发MinorGC后，有6MB的对象晋升到旧生代，那么当下一次Minor GC发生时，首先检查旧生代的剩余空间是否大于6MB，如果小于6MB，则执行Full GC。 当新生代采用PSGC时，方式稍有不同，PS GC是在Minor GC后也会检查，例如上面的例子中第一次Minor GC后，PS GC会检查此时旧生代的剩余空间是否大于6MB，如小于，则触发对旧生代的回收。 除了以上4种状况外，对于使用RMI来进行RPC或管理的Sun JDK应用而言，默认情况下会一小时执行一次Full GC。可通过在启动时通过- java-Dsun.rmi.dgc.client.gcInterval=3600000来设置Full GC执行的间隔时间或通过-XX:+ DisableExplicitGC来禁止RMI调用System.gc。</p><h2 id="G1"><a href="#G1" class="headerlink" title="G1"></a>G1</h2><h3 id="什么是垃圾回收"><a href="#什么是垃圾回收" class="headerlink" title="什么是垃圾回收"></a>什么是垃圾回收</h3><p>首先，在了解G1之前，我们需要清楚的知道，垃圾回收是什么？简单的说垃圾回收就是回收内存中不再使用的对象。</p><p>垃圾回收的基本步骤</p><p>回收的步骤有2步：</p><p>1.查找内存中不再使用的对象</p><p>2.释放这些对象占用的内存</p><h4 id="1-查找内存中不再使用的对象"><a href="#1-查找内存中不再使用的对象" class="headerlink" title="1,查找内存中不再使用的对象"></a>1,查找内存中不再使用的对象</h4><p>那么问题来了，如何判断哪些对象不再被使用呢？我们也有2个方法：</p><p><strong>1.引用计数法</strong> 引用计数法就是如果一个对象没有被任何引用指向，则可视之为垃圾。这种方法的缺点就是不能检测到环的存在。</p><p><strong>2.根搜索算法</strong></p><p>根搜索算法的基本思路就是通过一系列名为”GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链(Reference Chain)，当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。</p><p>现在我们已经知道如何找出垃圾对象了，如何把这些对象清理掉呢？</p><h4 id="2-释放这些对象占用的内存"><a href="#2-释放这些对象占用的内存" class="headerlink" title="2. 释放这些对象占用的内存"></a>2. 释放这些对象占用的内存</h4><p>常见的方式有复制或者直接清理，但是直接清理会存在内存碎片，于是就会产生了清理再压缩的方式。</p><p>总得来说就产生了三种类型的回收算法。</p><p>1.标记-复制</p><p>2.标记-清理</p><p>3.标记-整理</p><p>基于分代的假设</p><p>由于对象的存活时间有长有短，所以对于存活时间长的对象，减少被gc的次数可以避免不必要的开销。这样我们就把内存分成新生代和老年代，新生代存放刚创建的和存活时间比较短的对象，老年代存放存活时间比较长的对象。这样每次仅仅清理年轻代，老年代仅在必要时时再做清理可以极大的提高GC效率，节省GC时间。</p><h3 id="Java垃圾收集器的历史"><a href="#Java垃圾收集器的历史" class="headerlink" title="Java垃圾收集器的历史"></a>Java垃圾收集器的历史</h3><p>第一阶段，Serial（串行）收集器</p><p>在jdk1.3.1之前，java虚拟机仅仅能使用Serial收集器。 Serial收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。</p><p>PS：开启Serial收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseSerialGC</span></span><br></pre></td></tr></table></figure><p>第二阶段，Parallel（并行）收集器</p><p>Parallel收集器也称吞吐量收集器，相比Serial收集器，Parallel最主要的优势在于使用多线程去完成垃圾清理工作，这样可以充分利用多核的特性，大幅降低gc时间。</p><p>PS:开启Parallel收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParallelGC</span> <span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParallelOldGC</span></span><br></pre></td></tr></table></figure><p>第三阶段，CMS（并发）收集器</p><p>CMS收集器在Minor GC时会暂停所有的应用线程，并以多线程的方式进行垃圾回收。在Full GC时不再暂停应用线程，而是使用若干个后台线程定期的对老年代空间进行扫描，及时回收其中不再使用的对象。</p><p>PS:开启CMS收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseParNewGC</span> <span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseConcMarkSweepGC</span></span><br></pre></td></tr></table></figure><p>第四阶段，G1（并发）收集器</p><p>G1收集器（或者垃圾优先收集器）的设计初衷是为了尽量缩短处理超大堆（大于4GB）时产生的停顿。相对于CMS的优势而言是内存碎片的产生率大大降低。</p><p>PS:开启G1收集器的方式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">-XX</span><span class="selector-pseudo">:+UseG1GC</span></span><br></pre></td></tr></table></figure><h3 id="了解G1"><a href="#了解G1" class="headerlink" title="了解G1"></a>了解G1</h3><p>G1的第一篇paper（附录1）发表于2004年，在2012年才在jdk1.7u4中可用。oracle官方计划在jdk9中将G1变成默认的垃圾收集器，以替代CMS。为何oracle要极力推荐G1呢，G1有哪些优点</p><blockquote><p> <strong>首先，G1的设计原则就是简单可行的性能调优</strong> </p></blockquote><p>开发人员仅仅需要声明以下参数即可：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-<span class="string">XX:</span>+UseG1GC -Xmx32g -<span class="string">XX:</span>MaxGCPauseMillis=<span class="number">200</span></span><br></pre></td></tr></table></figure><p>其中-XX:+UseG1GC为开启G1垃圾收集器，-Xmx32g 设计堆内存的最大内存为32G，-XX:MaxGCPauseMillis=200设置GC的最大暂停时间为200ms。如果我们需要调优，在内存大小一定的情况下，我们只需要修改最大暂停时间即可。</p><blockquote><p> <strong>其次，G1将新生代，老年代的物理空间划分取消了。</strong> </p></blockquote><p>这样我们再也不用单独的空间对每个代进行设置了，不用担心每个代内存是否足够。</p><img src="/2017/11/03/JVM垃圾回收底层原理/1.png"><p>取而代之的是，G1算法将堆划分为若干个区域（Region），它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有cms内存碎片问题的存在了。</p><img src="/2017/11/03/JVM垃圾回收底层原理/2.png"><p>在G1中，还有一种特殊的区域，叫Humongous区域。 如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。这些巨型对象，默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。</p><blockquote><p> PS：在java 8中，持久代也移动到了普通的堆内存空间中，改为元空间。 </p></blockquote><h3 id="对象分配策略"><a href="#对象分配策略" class="headerlink" title="对象分配策略"></a>对象分配策略</h3><p>说起大对象的分配，我们不得不谈谈对象的分配策略。它分为3个阶段：</p><p>1.TLAB(Thread Local Allocation Buffer)线程本地分配缓冲区 2.Eden区中分配 3.Humongous区分配</p><p>TLAB为线程本地分配缓冲区，它的目的为了使对象尽可能快的分配出来。如果对象在一个共享的空间中分配，我们需要采用一些同步机制来管理这些空间内的空闲空间指针。在Eden空间中，每一个线程都有一个固定的分区用于分配对象，即一个TLAB。分配对象时，线程之间不再需要进行任何的同步。</p><p>对TLAB空间中无法分配的对象，JVM会尝试在Eden空间中进行分配。如果Eden空间无法容纳该对象，就只能在老年代中进行分配空间。</p><p>最后，G1提供了两种GC模式，Young GC和Mixed GC，两种都是Stop The World(STW)的。下面我们将分别介绍一下这2种模式。</p><h3 id="G1-Young-GC"><a href="#G1-Young-GC" class="headerlink" title="G1 Young GC"></a>G1 Young GC</h3><p>Young GC主要是对Eden区进行GC，它在Eden空间耗尽时会被触发。在这种情况下，Eden空间的数据移动到Survivor空间中，如果Survivor空间不够，Eden空间的部分数据会直接晋升到年老代空间。Survivor区的数据移动到新的Survivor区中，也有部分数据晋升到老年代空间中。最终Eden空间的数据为空，GC停止工作，应用线程继续执行。</p><img src="/2017/11/03/JVM垃圾回收底层原理/3.png"><img src="/2017/11/03/JVM垃圾回收底层原理/4.png"><p>这时，我们需要考虑一个问题，如果仅仅GC 新生代对象，我们如何找到所有的根对象呢？ 老年代的所有对象都是根么？那这样扫描下来会耗费大量的时间。于是，G1引进了RSet的概念。它的全称是Remembered Set，作用是跟踪指向某个heap区内的对象引用。</p><img src="/2017/11/03/JVM垃圾回收底层原理/5.png"><p>在CMS中，也有RSet的概念，在老年代中有一块区域用来记录指向新生代的引用。这是一种point-out，在进行Young GC时，扫描根时，仅仅需要扫描这一块区域，而不需要扫描整个老年代。</p><p>但在G1中，并没有使用point-out，这是由于一个分区太小，分区数量太多，如果是用point-out的话，会造成大量的扫描浪费，有些根本不需要GC的分区引用也扫描了。于是G1中使用point-in来解决。point-in的意思是哪些分区引用了当前分区中的对象。这样，仅仅将这些对象当做根来扫描就避免了无效的扫描。由于新生代有多个，那么我们需要在新生代之间记录引用吗？这是不必要的，原因在于每次GC时，所有新生代都会被扫描，所以只需要记录老年代到新生代之间的引用即可。</p><p>需要注意的是，如果引用的对象很多，赋值器需要对每个引用做处理，赋值器开销会很大，为了解决赋值器开销这个问题，在G1 中又引入了另外一个概念，卡表（Card Table）。一个Card Table将一个分区在逻辑上划分为固定大小的连续区域，每个区域称之为卡。卡通常较小，介于128到512字节之间。Card Table通常为字节数组，由Card的索引（即数组下标）来标识每个分区的空间地址。默认情况下，每个卡都未被引用。当一个地址空间被引用时，这个地址空间对应的数组索引的值被标记为”0″，即标记为脏被引用，此外RSet也将这个数组下标记录下来。一般情况下，这个RSet其实是一个Hash Table，Key是别的Region的起始地址，Value是一个集合，里面的元素是Card Table的Index。</p><p><strong>Young GC 阶段</strong>：</p><p><strong>阶段1：根扫描</strong></p><p>静态和本地对象被扫描</p><p><strong>阶段2：更新RS</strong></p><p>处理dirty card队列更新RS</p><p><strong>阶段3：处理RS</strong></p><p>检测从年轻代指向年老代的对象</p><p><strong>阶段4：对象拷贝</strong></p><p>拷贝存活的对象到survivor/old区域</p><p><strong>阶段5：处理引用队列</strong></p><p>软引用，弱引用，虚引用处理</p><h3 id="G1-Mix-GC"><a href="#G1-Mix-GC" class="headerlink" title="G1 Mix GC"></a>G1 Mix GC</h3><p>Mix GC不仅进行正常的新生代垃圾收集，同时也回收部分后台扫描线程标记的老年代分区。</p><p>它的GC步骤分2步：</p><p>1.全局并发标记（global concurrent marking） 2.拷贝存活对象（evacuation）</p><p>在进行Mix GC之前，会先进行global concurrent marking（全局并发标记）。 global concurrent marking的执行过程是怎样的呢？</p><p>在G1 GC中，它主要是为Mixed GC提供标记服务的，并不是一次GC过程的一个必须环节。global concurrent marking的执行过程分为五个步骤：</p><p><strong>初始标记（initial mark，STW）</strong></p><p>在此阶段，G1 GC 对根进行标记。该阶段与常规的 (STW) 年轻代垃圾回收密切相关。</p><p><strong>根区域扫描（root region scan</strong></p><p>G1 GC 在初始标记的存活区扫描对老年代的引用，并标记被引用的对象。该阶段与应用程序（非 STW）同时运行，并且只有完成该阶段后，才能开始下一次 STW 年轻代垃圾回收。</p><p><strong>并发标记（Concurrent Marking）</strong></p><p>G1 GC 在整个堆中查找可访问的（存活的）对象。该阶段与应用程序同时运行，可以被 STW 年轻代垃圾回收中断</p><p><strong>最终标记（Remark，STW）</strong></p><p>该阶段是 STW 回收，帮助完成标记周期。G1 GC 清空 SATB 缓冲区，跟踪未被访问的存活对象，并执行引用处理。</p><p><strong>清除垃圾（Cleanup，STW）</strong></p><p>在这个最后阶段，G1 GC 执行统计和 RSet 净化的 STW 操作。在统计期间，G1 GC 会识别完全空闲的区域和可供进行混合垃圾回收的区域。清理阶段在将空白区域重置并返回到空闲列表时为部分并发。</p><h3 id="三色标记算法"><a href="#三色标记算法" class="headerlink" title="三色标记算法"></a>三色标记算法</h3><p>提到并发标记，我们不得不了解并发标记的三色标记算法。它是描述追踪式回收器的一种有用的方法，利用它可以推演回收器的正确性。 首先，我们将对象分成三种类型的。</p><p><strong>黑色</strong>:根对象，或者该对象与它的子对象都被扫描</p><p><strong>灰色</strong>:对象本身被扫描,但还没扫描完该对象中的子对象</p><p><strong>白色</strong>:未被扫描对象，扫描完成所有对象之后，最终为白色的为不可达对象，即垃圾对象</p><p>当GC开始扫描对象时，按照如下图步骤进行对象的扫描：</p><p>根对象被置为黑色，子对象被置为灰色。</p><img src="/2017/11/03/JVM垃圾回收底层原理/6.png"><p>继续由灰色遍历,将已扫描了子对象的对象置为黑色。</p><img src="/2017/11/03/JVM垃圾回收底层原理/7.png"><p>遍历了所有可达的对象后，所有可达的对象都变成了黑色。不可达的对象即为白色，需要被清理。</p><img src="/2017/11/03/JVM垃圾回收底层原理/8.png"><p>这看起来很美好，但是如果在标记过程中，应用程序也在运行，那么对象的指针就有可能改变。这样的话，我们就会遇到一个问题：对象丢失问题</p><p>我们看下面一种情况，当垃圾收集器扫描到下面情况时:</p><img src="/2017/11/03/JVM垃圾回收底层原理/9.png"><p>这时候应用程序执行了以下操作：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">A.c</span>=C</span><br><span class="line"><span class="attr">B.c</span>=null</span><br></pre></td></tr></table></figure><p>这样，对象的状态图变成如下情形：</p><img src="/2017/11/03/JVM垃圾回收底层原理/10.png"><p>这时候垃圾收集器再标记扫描的时候就会下图成这样：</p><img src="/2017/11/03/JVM垃圾回收底层原理/11.png"><p>很显然，此时C是白色，被认为是垃圾需要清理掉，显然这是不合理的。那么我们如何保证应用程序在运行的时候，GC标记的对象不丢失呢？有如下2中可行的方式：</p><p>1.在插入的时候记录对象 2.在删除的时候记录对象</p><p>刚好这对应CMS和G1的2种不同实现方式：</p><p>在CMS采用的是增量更新（Incremental update），只要在写屏障（write barrier）里发现要有一个白对象的引用被赋值到一个黑对象 的字段里，那就把这个白对象变成灰色的。即插入的时候记录下来。</p><p>在G1中，使用的是STAB（snapshot-at-the-beginning）的方式，删除的时候记录所有的对象，它有3个步骤：</p><p>1.在开始标记的时候生成一个快照图标记存活对象</p><p>2.在并发标记的时候所有被改变的对象入队（在write barrier里把所有旧的引用所指向的对象都变成非白的）</p><p>3.可能存在游离的垃圾，将在下次被收集</p><p>这样，G1到现在可以知道哪些老的分区可回收垃圾最多。 当全局并发标记完成后，在某个时刻，就开始了Mix GC。这些垃圾回收被称作“混合式”是因为他们不仅仅进行正常的新生代垃圾收集，同时也回收部分后台扫描线程标记的分区。混合式垃圾收集如下图：</p><img src="/2017/11/03/JVM垃圾回收底层原理/12.png"><p>混合式GC也是采用的复制的清理策略，当GC完成后，会重新释放空间。 </p><img src="/2017/11/03/JVM垃圾回收底层原理/13.png"><h3 id="调优实践"><a href="#调优实践" class="headerlink" title="调优实践"></a>调优实践</h3><p><strong>MaxGCPauseMillis</strong>调优</p><p>前面介绍过使用GC的最基本的参数：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-<span class="string">XX:</span>+UseG1GC -Xmx32g -<span class="string">XX:</span>MaxGCPauseMillis=<span class="number">200</span></span><br></pre></td></tr></table></figure><p>前面2个参数都好理解，后面这个MaxGCPauseMillis参数该怎么配置呢？这个参数从字面的意思上看，就是允许的GC最大的暂停时间。G1尽量确保每次GC暂停的时间都在设置的MaxGCPauseMillis范围内。 那G1是如何做到最大暂停时间的呢？这涉及到另一个概念，CSet(collection set)。它的意思是在一次垃圾收集器中被收集的区域集合。</p><p>Young GC：选定所有新生代里的region。通过控制新生代的region个数来控制young GC的开销。</p><p>Mixed GC：选定所有新生代里的region，外加根据global concurrent marking统计得出收集收益高的若干老年代region。在用户指定的开销目标范围内尽可能选择收益高的老年代region。</p><p>在理解了这些后，我们再设置最大暂停时间就好办了。 首先，我们能容忍的最大暂停时间是有一个限度的，我们需要在这个限度范围内设置。但是应该设置的值是多少呢？我们需要在吞吐量跟MaxGCPauseMillis之间做一个平衡。如果MaxGCPauseMillis设置的过小，那么GC就会频繁，吞吐量就会下降。如果MaxGCPauseMillis设置的过大，应用程序暂停时间就会变长。G1的默认暂停时间是200毫秒，我们可以从这里入手，调整合适的时间。</p><p><strong>其他调优参数</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">G1HeapRegionSize</span>=n</span><br></pre></td></tr></table></figure><p>设置的 G1 区域的大小。值是 2 的幂，范围是 1 MB 到 32 MB 之间。目标是根据最小的 Java 堆大小划分出约 2048 个区域。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">ParallelGCThreads</span>=n</span><br></pre></td></tr></table></figure><p>设置 STW 工作线程数的值。将 n 的值设置为逻辑处理器的数量。n 的值与逻辑处理器的数量相同，最多为 8。</p><p>如果逻辑处理器不止八个，则将 n 的值设置为逻辑处理器数的 5/8 左右。这适用于大多数情况，除非是较大的 SPARC 系统，其中 n 的值可以是逻辑处理器数的 5/16 左右。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">ConcGCThreads</span>=n</span><br></pre></td></tr></table></figure><p>设置并行标记的线程数。将 n 设置为并行垃圾回收线程数 (ParallelGCThreads) 的 1/4 左右。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-XX:<span class="attribute">InitiatingHeapOccupancyPercent</span>=45</span><br></pre></td></tr></table></figure><p>设置触发标记周期的 Java 堆占用率阈值。默认占用率是整个 Java 堆的 45%。</p><p>避免使用以下参数：</p><p>避免使用 -Xmn 选项或 -XX:NewRatio 等其他相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标。</p><h3 id="触发Full-GC"><a href="#触发Full-GC" class="headerlink" title="触发Full GC"></a>触发Full GC</h3><p>在某些情况下，G1触发了Full GC，这时G1会退化使用Serial收集器来完成垃圾的清理工作，它仅仅使用单线程来完成GC工作，GC暂停时间将达到秒级别的。整个应用处于假死状态，不能处理任何请求，我们的程序当然不希望看到这些。那么发生Full GC的情况有哪些呢？</p><h4 id="并发模式失败"><a href="#并发模式失败" class="headerlink" title="并发模式失败"></a>并发模式失败</h4><p>G1启动标记周期，但在Mix GC之前，老年代就被填满，这时候G1会放弃标记周期。这种情形下，需要增加堆大小，或者调整周期（例如增加线程数-XX:ConcGCThreads等）。</p><h4 id="晋升失败或者疏散失败"><a href="#晋升失败或者疏散失败" class="headerlink" title="晋升失败或者疏散失败"></a>晋升失败或者疏散失败</h4><p>G1在进行GC的时候没有足够的内存供存活对象或晋升对象使用，由此触发了Full GC。可以在日志中看到(to-space exhausted)或者（to-space overflow）。解决这种问题的方式是：</p><p>a. 增加 <code>-XX:G1ReservePercent</code> 选项的值（并相应增加总的堆大小），为“目标空间”增加预留内存量。</p><p>b. 通过减少<code>-XX:InitiatingHeapOccupancyPercent</code> 提前启动标记周期。</p><p>c. 也可以通过增加 <code>-XX:ConcGCThreads</code> 选项的值来增加并行标记线程的数目。</p><h4 id="巨型对象分配失败"><a href="#巨型对象分配失败" class="headerlink" title="巨型对象分配失败"></a>巨型对象分配失败</h4><p>当巨型对象找不到合适的空间进行分配时，就会启动Full GC，来释放空间。这种情况下，应该避免分配大量的巨型对象，增加内存或者增大-XX:G1HeapRegionSize，使巨型对象不再是巨型对象。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
            <tag> 内存回收 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
